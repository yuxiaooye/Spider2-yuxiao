[
    {
        "instance_id": "bq011",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "How many pseudo users were active in the last 7 days but inactive in the last 2 days as of January 7, 2021?",
        "SQL": "SELECT\n  COUNT(DISTINCT MDaysUsers.user_pseudo_id) AS n_day_inactive_users_count\nFROM\n  (\n    SELECT\n      user_pseudo_id\n    FROM\n      `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*` AS T\n    CROSS JOIN\n      UNNEST(T.event_params) AS event_params\n    WHERE\n      event_params.key = 'engagement_time_msec' AND event_params.value.int_value > 0\n      /* Has engaged in last M = 7 days */\n      AND event_timestamp > UNIX_MICROS(TIMESTAMP_SUB(TIMESTAMP('2021-01-07 23:59:59'), INTERVAL 7 DAY))\n      /* Include only relevant tables based on the fixed timestamp */\n      AND _TABLE_SUFFIX BETWEEN '20210101' AND '20210107'\n  ) AS MDaysUsers\nLEFT JOIN\n  (\n    SELECT\n      user_pseudo_id\n    FROM\n      `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*` AS T\n    CROSS JOIN\n      UNNEST(T.event_params) AS event_params\n    WHERE\n      event_params.key = 'engagement_time_msec' AND event_params.value.int_value > 0\n      /* Has engaged in last N = 2 days */\n      AND event_timestamp > UNIX_MICROS(TIMESTAMP_SUB(TIMESTAMP('2021-01-07 23:59:59'), INTERVAL 2 DAY))\n      /* Include only relevant tables based on the fixed timestamp */\n      AND _TABLE_SUFFIX BETWEEN '20210105' AND '20210107'\n  ) AS NDaysUsers\nON MDaysUsers.user_pseudo_id = NDaysUsers.user_pseudo_id\nWHERE\n  NDaysUsers.user_pseudo_id IS NULL;",
        "external_knowledge": "ga4_obfuscated_sample_ecommerce.events.md",
        "plan": "1. Identify pseudo users (`user_pseudo_id`) active in the last 7 days: query the `events_*` tables to find users who were active in the last 7 days based on engagement time and filter them by the fixed timestamp and relevant table suffixes (from `20210101` to `20210107`).\n2. Identify pseudo users (`user_pseudo_id`) active in the last 2 days: query the `events_*` tables to find users who were active in the last 2 days based on engagement time and filter them by the fixed timestamp and relevant table suffixes (from `20210105` to `20210107`).\n3. Combine results and filter:\n- Use a `LEFT JOIN` to combine the two sets of users and filter out users who were active in the last 2 days.\n- Count the distinct user IDs who meet the criteria of being active in the last 7 days but not in the last 2 days.",
        "special_function": [
            "timestamp-functions/TIMESTAMP",
            "timestamp-functions/TIMESTAMP_SUB",
            "timestamp-functions/UNIX_MICROS",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq010",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "Find the top-selling product among customers who bought 'Youtube Men\u2019s Vintage Henley' in July 2017, excluding itself.",
        "SQL": "WITH GET_CUS_ID AS (\n    SELECT \n        DISTINCT fullVisitorId as Henley_CUSTOMER_ID\n    FROM \n        `bigquery-public-data.google_analytics_sample.ga_sessions_201707*`,\n        UNNEST(hits) AS hits,\n        UNNEST(hits.product) as product\n    WHERE\n        product.v2ProductName = \"YouTube Men's Vintage Henley\"\n        AND product.productRevenue IS NOT NULL\n    )\n\nSELECT\n    product.v2ProductName AS other_purchased_products\nFROM\n    `bigquery-public-data.google_analytics_sample.ga_sessions_201707*` TAB_A \n    RIGHT JOIN GET_CUS_ID\n    ON GET_CUS_ID.Henley_CUSTOMER_ID=TAB_A.fullVisitorId,\n    UNNEST(hits) AS hits,\n    UNNEST(hits.product) as product\nWHERE\n    TAB_A.fullVisitorId IN (\n        SELECT * FROM GET_CUS_ID\n    )\n    AND product.v2ProductName <> \"YouTube Men's Vintage Henley\"\n    AND product.productRevenue IS NOT NULL\nGROUP BY\n    product.v2ProductName\nORDER BY\n    SUM(product.productQuantity) DESC\nLIMIT 1;",
        "external_knowledge": "google_analytics_sample.ga_sessions.md",
        "plan": "1. Extract a distinct list of customers (`fullVisitorId`) who purchased the \"YouTube Men's Vintage Henley\" in July 2017.\n2. Find other products purchased by these customers in July 2017.\n3. Filter out the \"YouTube Men's Vintage Henley\" product itself and aggregate other products purchased by the same customers.\n4. Sort to find the most purchased product.",
        "special_function": [
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq009",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "Which traffic source receives the top revenue in 2017 and what is the difference (millions, rounded to two decimal places) between its highest and lowest revenue months?",
        "SQL": "WITH MONTHLY_REVENUE AS (\n    SELECT \n        FORMAT_DATE(\"%Y%m\", PARSE_DATE(\"%Y%m%d\", date)) AS month,\n        trafficSource.source AS source,\n        ROUND(SUM(totals.totalTransactionRevenue) / 1000000, 2) AS revenue\n    FROM `bigquery-public-data.google_analytics_sample.ga_sessions_2017*`\n    GROUP BY 1, 2\n),\n\nYEARLY_REVENUE AS (\n    SELECT\n        source,\n        SUM(revenue) AS total_revenue\n    FROM MONTHLY_REVENUE\n    GROUP BY source\n),\n\nTOP_SOURCE AS (\n    SELECT \n        source\n    FROM YEARLY_REVENUE\n    ORDER BY total_revenue DESC\n    LIMIT 1\n),\n\nSOURCE_MONTHLY_REVENUE AS (\n    SELECT\n        month,\n        source,\n        revenue\n    FROM MONTHLY_REVENUE\n    WHERE source IN (SELECT source FROM TOP_SOURCE)\n),\n\nREVENUE_DIFF AS (\n    SELECT \n        source,\n        ROUND(MAX(revenue), 2) AS max_revenue,\n        ROUND(MIN(revenue), 2) AS min_revenue,\n        ROUND(MAX(revenue) - MIN(revenue), 2) AS diff_revenue\n    FROM SOURCE_MONTHLY_REVENUE\n    GROUP BY source\n)\n\nSELECT \n    source,\n    diff_revenue\nFROM REVENUE_DIFF;",
        "external_knowledge": "google_analytics_sample.ga_sessions.md",
        "plan": "1. Calculate monthly revenue for each traffic source.\n2. Aggregate the monthly revenues to compute the total yearly revenue for each traffic source.\n3. Determine which traffic source has the highest total revenue for the year 2017.\n4. Retrieve the monthly revenue data for the top traffic source identified in the previous step.\n5. Calculate the difference between the highest and lowest monthly revenues for the top traffic source.\n6. Retrieve the traffic source and the revenue difference.",
        "special_function": [
            "date-functions/DATE",
            "date-functions/FORMAT_DATE",
            "date-functions/PARSE_DATE",
            "mathematical-functions/ROUND"
        ]
    },
    {
        "instance_id": "bq001",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "I wonder how many days between the first transaction and the first visit both in Feburary 2017 for each transacting visitor, along with the device used in the transaction.",
        "SQL": "DECLARE start_date STRING DEFAULT '20170201';\nDECLARE end_date STRING DEFAULT '20170228';\n\nWITH visit AS (\n    SELECT\n        fullvisitorid,\n        MIN(date) AS date_first_visit\n    FROM\n        `bigquery-public-data.google_analytics_sample.ga_sessions_*`\n    WHERE\n       _TABLE_SUFFIX BETWEEN start_date AND end_date\n    GROUP BY fullvisitorid\n),\n\ntransactions AS (\n    SELECT\n        fullvisitorid,\n        MIN(date) AS date_transactions\n    FROM\n        `bigquery-public-data.google_analytics_sample.ga_sessions_*` AS ga,\n        UNNEST(ga.hits) AS hits\n    WHERE\n        hits.transaction.transactionId IS NOT NULL\n        AND\n        _TABLE_SUFFIX BETWEEN start_date AND end_date\n    GROUP BY fullvisitorid\n),\n\ndevice_transactions AS (\n    SELECT DISTINCT\n        fullvisitorid,\n        date,\n        device.deviceCategory\n    FROM\n        `bigquery-public-data.google_analytics_sample.ga_sessions_*` AS ga,\n        UNNEST(ga.hits) AS hits\n    WHERE\n        hits.transaction.transactionId IS NOT NULL\n        AND\n        _TABLE_SUFFIX BETWEEN start_date AND end_date\n),\n\nvisits_transactions AS (\n    SELECT\n        visit.fullvisitorid,\n        date_first_visit,\n        date_transactions,\n        device_transactions.deviceCategory AS device_transaction\n    FROM\n        visit\n        JOIN transactions\n        ON visit.fullvisitorid = transactions.fullvisitorid\n        JOIN device_transactions\n        ON visit.fullvisitorid = device_transactions.fullvisitorid \n        AND transactions.date_transactions = device_transactions.date\n)\n\nSELECT\n       fullvisitorid,\n       DATE_DIFF(PARSE_DATE('%Y%m%d', date_transactions), PARSE_DATE('%Y%m%d', date_first_visit), DAY) AS time,\n       device_transaction\nFROM visits_transactions\nORDER BY fullvisitorid;",
        "external_knowledge": "google_analytics_sample.ga_sessions.md",
        "plan": "1. Firstly, extract the first visit date for each visitor in the specified range `201702`.\n2. Next, extract the first transaction date for each visitor in Feb 2017.\n3. Then, extract the device categories used for transactions.\n4. Combine the visit, transaction and device data.\n5. Calculate the number of days between the first transaction and the first visit date for each visitor using `DATE_DIFF`.",
        "special_function": [
            "date-functions/DATE",
            "date-functions/DATE_DIFF",
            "date-functions/PARSE_DATE",
            "json-functions/STRING",
            "time-functions/TIME",
            "timestamp-functions/STRING",
            "other-functions/DECLARE",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq002",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "What's the maximum monthly, weekly, and daily product revenues (in millions) generated by the top-performing traffic source in the first half of 2017?",
        "SQL": "DECLARE start_date STRING DEFAULT '20170101';\nDECLARE end_date STRING DEFAULT '20170630';\n\nWITH daily_revenue AS (\n    SELECT\n        trafficSource.source AS source,\n        date,\n        SUM(productRevenue) / 1000000 AS revenue\n    FROM\n        `bigquery-public-data.google_analytics_sample.ga_sessions_*`,\n        UNNEST (hits) AS hits,\n        UNNEST (hits.product) AS product\n    WHERE\n        _table_suffix BETWEEN start_date AND end_date\n    GROUP BY\n        source, date\n),\nweekly_revenue AS (\n    SELECT\n        source,\n        CONCAT(EXTRACT(YEAR FROM (PARSE_DATE('%Y%m%d', date))), 'W', EXTRACT(WEEK FROM (PARSE_DATE('%Y%m%d', date)))) AS week,\n        SUM(revenue) AS revenue\n    FROM daily_revenue\n    GROUP BY source, week\n),\nmonthly_revenue AS (\n    SELECT\n        source,\n        CONCAT(EXTRACT(YEAR FROM (PARSE_DATE('%Y%m%d', date))),'0', EXTRACT(MONTH FROM (PARSE_DATE('%Y%m%d', date)))) AS month,\n        SUM(revenue) AS revenue\n    FROM daily_revenue\n    GROUP BY source, month\n),\n\ntop_source AS (\n    SELECT source, SUM(revenue) AS total_revenue\n    FROM daily_revenue\n    GROUP BY source\n    ORDER BY total_revenue DESC\n    LIMIT 1\n),\n\nmax_revenues AS (\n    (\n      SELECT\n        'Daily' AS time_type,\n        date AS time,\n        source,\n        MAX(revenue) AS max_revenue\n      FROM daily_revenue\n      WHERE source = (SELECT source FROM top_source)\n      GROUP BY source, date\n      ORDER BY max_revenue DESC\n      LIMIT 1\n    )\n\n    UNION ALL\n\n    (\n      SELECT\n        'Weekly' AS time_type,\n        week AS time,\n        source,\n        MAX(revenue) AS max_revenue\n      FROM weekly_revenue\n      WHERE source = (SELECT source FROM top_source)\n      GROUP BY source, week\n      ORDER BY max_revenue DESC\n      LIMIT 1\n    )\n\n    UNION ALL\n\n    (\n      SELECT\n          'Monthly' AS time_type,\n          month AS time,\n          source,\n          MAX(revenue) AS max_revenue\n      FROM monthly_revenue\n      WHERE source = (SELECT source FROM top_source)\n      GROUP BY source, month\n      ORDER BY max_revenue DESC\n      LIMIT 1\n    )\n)\n\nSELECT\n    max_revenue\nFROM max_revenues\nORDER BY max_revenue DESC;",
        "external_knowledge": "google_analytics_sample.ga_sessions.md",
        "plan": "1. Firstly, we define the date range to be the first half of year 2017: 20170101 to 20170630.\n2. Next, calculate daily revenues for each traffic source.\n3. Similarly, calculate weekly and monthly revenues for each traffic source.\n4. Determine the top-performing traffic source through aggregation and sorting.\n5. Calculate the maximum revenues for this traffic source on daily/weekly/monthly basis respectively.\n6. Return the final results.",
        "special_function": [
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "date-functions/PARSE_DATE",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "json-functions/STRING",
            "string-functions/CONCAT",
            "time-functions/EXTRACT",
            "time-functions/TIME",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/STRING",
            "other-functions/DECLARE",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq003",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "Compare the average pageviews per visitor between purchase and non-purchase sessions for each month from April to July in 2017.",
        "SQL": "WITH cte1 AS (\n    SELECT\n        CONCAT(EXTRACT(YEAR FROM (PARSE_DATE('%Y%m%d', date))), '0',\n            EXTRACT(MONTH FROM (PARSE_DATE('%Y%m%d', date)))) AS month,\n        SUM(totals.pageviews) / COUNT(DISTINCT fullVisitorId) AS avg_pageviews_non_purchase\n    FROM\n        `bigquery-public-data.google_analytics_sample.ga_sessions_2017*`,\n        UNNEST (hits) AS hits,\n        UNNEST (hits.product) AS product\n    WHERE\n        _table_suffix BETWEEN '0401' AND '0731'\n        AND totals.transactions IS NULL\n        AND product.productRevenue IS NULL\n    GROUP BY month\n),\ncte2 AS (\n    SELECT\n        CONCAT(EXTRACT(YEAR FROM (PARSE_DATE('%Y%m%d', date))), '0',\n            EXTRACT(MONTH FROM (PARSE_DATE('%Y%m%d', date)))) AS month,\n        SUM(totals.pageviews) / COUNT(DISTINCT fullVisitorId) AS avg_pageviews_purchase\n    FROM\n        `bigquery-public-data.google_analytics_sample.ga_sessions_2017*`,\n        UNNEST (hits) AS hits,\n        UNNEST (hits.product) AS product\n    WHERE\n        _table_suffix BETWEEN '0401' AND '0731'\n        AND totals.transactions >= 1\n        AND product.productRevenue IS NOT NULL\n    GROUP BY month\n)\nSELECT\n    month, avg_pageviews_purchase, avg_pageviews_non_purchase\nFROM cte1 INNER JOIN cte2\nUSING(month)\nORDER BY month;",
        "external_knowledge": "google_analytics_sample.ga_sessions.md",
        "plan": "1. Calculate average pageviews for non-purchase sessions:\n- Extracts the year and month from the `date` field.\n- Filters sessions with no transactions and no product revenue.\n- Aggregates data by month and calculates the average pageviews per visitor.\n2. Similarly, calculate average pageviews for purchase sessions. The difference is that we only include sessions with at least one transaction and product revenue.\n3. Combine and compare the results, that is select and order results by month.",
        "special_function": [
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "date-functions/PARSE_DATE",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "string-functions/CONCAT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq004",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "What's the most popular other purchased product in July 2017 with consumers who bought products relevant to YouTube?",
        "SQL": "with product_and_quatity AS (\n    SELECT \n        DISTINCT v2ProductName AS other_purchased_products,\n        SUM(productQuantity) AS quatity\n    FROM\n        `bigquery-public-data.google_analytics_sample.ga_sessions_2017*`,\n        UNNEST(hits) AS hits,\n        UNNEST(hits.product) AS product\n    WHERE\n        _table_suffix BETWEEN '0701' AND '0731'\n        AND NOT REGEXP_CONTAINS(LOWER(v2ProductName), 'youtube')\n        AND fullVisitorID IN (\n            SELECT \n            DISTINCT fullVisitorId\n            FROM\n                `bigquery-public-data.google_analytics_sample.ga_sessions_2017*`,\n                UNNEST(hits) AS hits,\n                UNNEST(hits.product) AS product\n            WHERE\n                _table_suffix BETWEEN '0701' AND '0731'\n                AND REGEXP_CONTAINS(LOWER(v2ProductName), 'youtube')\n        )\n    GROUP BY v2ProductName\n)\nSELECT other_purchased_products\nFROM product_and_quatity\nORDER BY quatity DESC\nLIMIT 1;",
        "external_knowledge": "google_analytics_sample.ga_sessions.md",
        "plan": "1. Identify visitors who purchased any YouTube product in July 2017.\n2. Calculate the total quantity of each product (excluding YouTube products) purchased by visitors who bought any YouTube product in July 2017.\n3. Retrieve the product name with the highest total quantity.",
        "special_function": [
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "date-functions/PARSE_DATE",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "string-functions/CONCAT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq008",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "What's the most common next page for visitors who were part of \"Data Share\" campaign and after they accessed the page starting with '/home' in January 2017. And what's the maximum duration time (in seconds) when they visit the corresponding home page?",
        "SQL": "with page_visit_sequence AS (\n    SELECT\n        fullVisitorID,\n        visitID,\n        pagePath,\n        LEAD(timestamp, 1) OVER (PARTITION BY fullVisitorId, visitID order by timestamp) - timestamp AS page_duration,\n        LEAD(pagePath, 1) OVER (PARTITION BY fullVisitorId, visitID order by timestamp) AS next_page,\n        RANK() OVER (PARTITION BY fullVisitorId, visitID order by timestamp) AS step_number\n    FROM (\n        SELECT\n            pages.fullVisitorID,\n            pages.visitID,\n            pages.pagePath,\n            visitors.campaign,\n            MIN(pages.timestamp) timestamp\n        FROM (\n            SELECT\n                fullVisitorId,\n                visitId,\n                trafficSource.campaign campaign\n            FROM\n                `bigquery-public-data.google_analytics_sample.ga_sessions_*`,\n                UNNEST(hits) as hits\n            WHERE\n                _TABLE_SUFFIX BETWEEN '20170101' AND '20170131'\n                AND hits.type='PAGE'\n                AND REGEXP_CONTAINS(hits.page.pagePath, r'^/home')\n                AND REGEXP_CONTAINS(trafficSource.campaign, r'Data Share')\n        ) AS visitors\n        JOIN(\n            SELECT\n                fullVisitorId,\n                visitId,\n                visitStartTime + hits.time / 1000 AS timestamp,\n                hits.page.pagePath AS pagePath\n            FROM\n                `bigquery-public-data.google_analytics_sample.ga_sessions_*`,\n                UNNEST(hits) as hits\n            WHERE\n                _TABLE_SUFFIX BETWEEN '20170101' AND '20170131'\n        ) as pages\n        ON\n            visitors.fullVisitorID = pages.fullVisitorID\n            AND visitors.visitID = pages.visitID\n        GROUP BY \n            pages.fullVisitorID, visitors.campaign, pages.visitID, pages.pagePath\n        ORDER BY \n            pages.fullVisitorID, pages.visitID, timestamp\n    )\n    ORDER BY fullVisitorId, visitID, step_number\n),\nmost_common_next_page AS (\n    SELECT\n        next_page,\n        COUNT(next_page) as page_count\n    FROM page_visit_sequence\n    WHERE\n        next_page IS NOT NULL\n        AND REGEXP_CONTAINS(pagePath, r'^/home')\n    GROUP BY next_page\n    ORDER BY page_count DESC\n    LIMIT 1\n),\nmax_page_duration AS (\n    SELECT MAX(page_duration) as max_duration\n    FROM page_visit_sequence\n    WHERE\n        page_duration IS NOT NULL\n        AND REGEXP_CONTAINS(pagePath, r'^/home')\n)\nSELECT\n    next_page,\n    max_duration\nFROM\n    most_common_next_page,\n    max_page_duration;",
        "external_knowledge": "google_analytics_sample.ga_sessions.md",
        "plan": "1. Identify relevant visits and pages: extract sessions from the `ga_sessions_*` table within January 2017 that contain page hits with paths starting with '/home' and were part of the \"Data Share\" campaign.\n2. Generate visitor page sequence: combine the filtered sessions with their corresponding page hits to get the full visitor ID, visit ID, timestamp, and page path for each page visit. And order the pages by their visit timestamps.\n3. Calculate the next page and duration:\n   - Use the `LEAD` window function to determine the next page and calculate the duration spent on the current page (by subtracting the current timestamp from the next timestamp).\n   - Rank the pages within each session to maintain the sequence of page visits.\n4. Create the page visit sequence CTE: combine the results into a Common Table Expression (CTE) called that includes the visitor ID, visit ID, page path, duration on the page, next page path, and visit step number.\n5. Determine the most common next page after visiting '/home' page. From the table in Step 4, filter for entries where the current page path starts with '/home', group by the next page, and count occurrences to find the most common next page.\n6. Calculate the maximum duration on home pages. Filter for entries where the current page path starts with '/home' and return the maximum duration spent on it.\n7. Combine and return the results in Step 5 and Step 6, which include the most common next page and the maximum duration.",
        "special_function": [
            "navigation-functions/LEAD",
            "numbering-functions/RANK",
            "string-functions/REGEXP_CONTAINS",
            "timestamp-functions/TIMESTAMP",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq269",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "Compute the average pageviews per visitor for non-purchase events and purchase events each month between June 1st and July 31st in 2017.",
        "SQL": "WITH visitor_pageviews AS (\n  SELECT\n    FORMAT_DATE('%Y%m', PARSE_DATE('%Y%m%d', date)) AS month,\n    CASE WHEN totals.transactions > 0 THEN 'purchase' ELSE 'non_purchase' END AS purchase_status,\n    fullVisitorId,\n    SUM(totals.pageviews) AS total_pageviews\n  FROM\n    `bigquery-public-data.google_analytics_sample.ga_sessions_*`\n  WHERE\n    _TABLE_SUFFIX BETWEEN '20170601' AND '20170731'\n    AND totals.pageviews IS NOT NULL\n  GROUP BY\n    month, purchase_status, fullVisitorId\n),\navg_pageviews AS (\n  SELECT\n    month,\n    purchase_status,\n    AVG(total_pageviews) AS avg_pageviews_per_visitor\n  FROM\n    visitor_pageviews\n  GROUP BY\n    month, purchase_status\n)\nSELECT\n  month,\n  MAX(CASE WHEN purchase_status = 'purchase' THEN avg_pageviews_per_visitor END) AS avg_pageviews_purchase,\n  MAX(CASE WHEN purchase_status = 'non_purchase' THEN avg_pageviews_per_visitor END) AS avg_pageviews_non_purchase\nFROM\n  avg_pageviews\nGROUP BY\n  month\nORDER BY\n  month",
        "external_knowledge": null,
        "plan": "1. Calculate the average pageviews per visitor for sessions without transactions and non-earning product revenue through dividing pageviews by the number of visitors.\n2. Compute the average pageviews per visitor for sessions with at least one transaction and product revenue between June 1 and July 31, 2017.\n3, Join the results and order the final output by month.\n",
        "special_function": [
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "date-functions/PARSE_DATE",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "string-functions/CONCAT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq268",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "Identify the longest number of days between the first visit and the last recorded event (either the last visit or the first transaction) for a user where the last recorded event was associated with a mobile device.",
        "SQL": "WITH \n\nvisit AS (\nSELECT fullvisitorid, MIN(date) AS date_first_visit, MAX(date) AS date_last_visit \nFROM `bigquery-public-data.google_analytics_sample.ga_sessions_*` GROUP BY fullvisitorid),\n\ndevice_visit AS (\nSELECT DISTINCT fullvisitorid, date, device.deviceCategory\nFROM `bigquery-public-data.google_analytics_sample.ga_sessions_*`),\n\ntransactions AS (\nSELECT fullvisitorid, MIN(date) AS date_transactions, 1 AS transaction\nFROM `bigquery-public-data.google_analytics_sample.ga_sessions_*` AS ga, UNNEST(ga.hits) AS hits\nWHERE  hits.transaction.transactionId IS NOT NULL GROUP BY fullvisitorid),\n\ndevice_transactions AS (\nSELECT DISTINCT fullvisitorid, date, device.deviceCategory\nFROM `bigquery-public-data.google_analytics_sample.ga_sessions_*` AS ga, UNNEST(ga.hits) AS hits\nWHERE hits.transaction.transactionId IS NOT NULL),\n\nvisits_transactions AS (\nSELECT visit.fullvisitorid, date_first_visit, date_transactions, date_last_visit , \n       device_visit.deviceCategory AS device_last_visit, device_transactions.deviceCategory AS device_transaction, \n       IFNULL(transactions.transaction,0) AS transaction\nFROM visit LEFT JOIN transactions ON visit.fullvisitorid = transactions.fullvisitorid\nLEFT JOIN device_visit ON visit.fullvisitorid = device_visit.fullvisitorid \nAND visit.date_last_visit = device_visit.date\n\nLEFT JOIN device_transactions ON visit.fullvisitorid = device_transactions.fullvisitorid \nAND transactions.date_transactions = device_transactions.date ),\n\nmortality_table AS (\nSELECT fullvisitorid, date_first_visit, \n       CASE WHEN date_transactions IS NULL THEN date_last_visit ELSE date_transactions  END AS date_event, \n       CASE WHEN device_transaction IS NULL THEN device_last_visit ELSE device_transaction END AS device, transaction\nFROM visits_transactions )\n\nSELECT DATE_DIFF(PARSE_DATE('%Y%m%d',date_event), PARSE_DATE('%Y%m%d', date_first_visit),DAY) AS time \nFROM mortality_table\nWHERE device = 'mobile'\nORDER BY DATE_DIFF(PARSE_DATE('%Y%m%d',date_event), PARSE_DATE('%Y%m%d', date_first_visit),DAY) DESC\nLIMIT 1",
        "external_knowledge": null,
        "plan": "1. Find the first and last visit dates for each unique visitor.\n2. Determine the first transaction date for visitors with transaction information.\n3. Combine visit, transaction, and device information.\n4. Calculate the time duration in days between the event date (either transactions or last visit) and the first visit date for visitors on mobile devices.\n5. Sorted by duration and return the longest time duration.\n",
        "special_function": [
            "date-functions/DATE",
            "date-functions/DATE_DIFF",
            "date-functions/PARSE_DATE",
            "time-functions/TIME",
            "conditional-functions/CASE",
            "conditional-functions/IFNULL",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq270",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "What were the monthly add-to-cart and purchase conversion rates, calculated as a percentage of pageviews on product details, from January to March 2017?",
        "SQL": "WITH\n  cte1 AS\n    (SELECT\n      CONCAT(EXTRACT(YEAR FROM (PARSE_DATE('%Y%m%d', date))),'0',\n                EXTRACT(MONTH FROM (PARSE_DATE('%Y%m%d', date)))) AS month,\n      COUNT(hits.eCommerceAction.action_type) AS num_product_view\n    FROM `bigquery-public-data.google_analytics_sample.ga_sessions_2017*`,\n      UNNEST(hits) AS hits\n    WHERE _table_suffix BETWEEN '0101' AND '0331'\n      AND hits.eCommerceAction.action_type = '2'\n    GROUP BY month),\n  cte2 AS\n    (SELECT\n      CONCAT(EXTRACT(YEAR FROM (PARSE_DATE('%Y%m%d', date))),'0',\n                EXTRACT(MONTH FROM (PARSE_DATE('%Y%m%d', date)))) AS month,\n      COUNT(hits.eCommerceAction.action_type) AS num_addtocart\n    FROM `bigquery-public-data.google_analytics_sample.ga_sessions_2017*`,\n      UNNEST(hits) AS hits\n    WHERE _table_suffix BETWEEN '0101' AND '0331'\n      AND hits.eCommerceAction.action_type = '3'\n    GROUP BY month),\n  cte3 AS\n    (SELECT\n      CONCAT(EXTRACT(YEAR FROM (PARSE_DATE('%Y%m%d', date))),'0',\n                EXTRACT(MONTH FROM (PARSE_DATE('%Y%m%d', date)))) AS month,\n      COUNT(hits.eCommerceAction.action_type) AS num_purchase\n    FROM `bigquery-public-data.google_analytics_sample.ga_sessions_2017*`,\n      UNNEST(hits) AS hits,\n      UNNEST(hits.product) AS product\n    WHERE _table_suffix BETWEEN '0101' AND '0331'\n      AND hits.eCommerceAction.action_type = '6'\n      AND product.productRevenue IS NOT NULL\n    GROUP BY month)\nSELECT \n  ROUND((num_addtocart/num_product_view * 100),2) AS add_to_cart_rate,\n  ROUND((num_purchase/num_product_view * 100),2) AS purchase_rate\nFROM cte1\n  LEFT JOIN cte2\n  USING(month) \n  LEFT JOIN cte3\n  USING(month)\nORDER BY month;",
        "external_knowledge": "ga360_hits.eCommerceAction.action_type.md",
        "plan": "1. Filter and count the number of product views with eCommerce action type '2' between January 1, 2017, and March 31, 2017.\n2. Filter and count the number of add-to-cart actions in the specified date range.\n3. Filter and count the number of successful purchase actions and non-null product revenue in the designated period.\n4. Calculate the add-to-cart rate through dividing the number of add-to-cart actions by the number of product views. \n5. Analogously, compute the purchase rate.\n6. Sort the final output by month and return.\n",
        "special_function": [
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "date-functions/PARSE_DATE",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "mathematical-functions/ROUND",
            "string-functions/CONCAT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq275",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "Can you provide a list of visitor IDs for those who made their first transaction on a mobile device on a different day than their first visit?",
        "SQL": "WITH \n  visit AS (\n    SELECT fullvisitorid, MIN(date) AS date_first_visit\n    FROM `bigquery-public-data.google_analytics_sample.ga_sessions_*` \n    GROUP BY fullvisitorid\n  ),\n  \n  transactions AS (\n    SELECT fullvisitorid, MIN(date) AS date_transactions, 1 AS transaction\n    FROM `bigquery-public-data.google_analytics_sample.ga_sessions_*` AS ga, \n    UNNEST(ga.hits) AS hits \n    WHERE hits.transaction.transactionId IS NOT NULL \n    GROUP BY fullvisitorid\n  ),\n\n  device_transactions AS (\n    SELECT DISTINCT fullvisitorid, date, device.deviceCategory AS device_transaction\n    FROM `bigquery-public-data.google_analytics_sample.ga_sessions_*` AS ga, \n    UNNEST(ga.hits) AS hits \n    WHERE hits.transaction.transactionId IS NOT NULL\n  ),\n\n  visits_transactions AS (\n    SELECT visit.fullvisitorid, date_first_visit, date_transactions, device_transaction\n    FROM visit \n    LEFT JOIN transactions ON visit.fullvisitorid = transactions.fullvisitorid\n    LEFT JOIN device_transactions ON visit.fullvisitorid = device_transactions.fullvisitorid \n    AND transactions.date_transactions = device_transactions.date\n  )\n\nSELECT fullvisitorid \nFROM visits_transactions\nWHERE DATE_DIFF(PARSE_DATE('%Y%m%d', date_transactions), PARSE_DATE('%Y%m%d', date_first_visit), DAY) > 0\nAND device_transaction = \"mobile\";",
        "external_knowledge": null,
        "plan": "Calculate the first and last visit dates for each visitor by grouping on visitor ID and taking the minimum and maximum of visit dates respectively.\n1. Identify distinct combinations of visitor ID, visit date, and device category used during each visit.\n2. For each visitor, identify the earliest date on which a transaction occurred and flag these records as having a transaction.\n3. For each transaction, capture the distinct visitor ID, date of transaction, and device category used during the transaction.\n4. Merge the datasets obtained above.\n5. From the combined data, prepare a table where for each visitor, the event date is determined (use the transaction date if it exists; otherwise, use the last visit date) and the device used is specified (use the transaction device if it exists; otherwise, use the device from the last visit).\n6. Select visitors with at least one transaction whose event happened after the first visit and the device used was mobile.\n",
        "special_function": [
            "date-functions/DATE",
            "date-functions/DATE_DIFF",
            "date-functions/PARSE_DATE",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq374",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "Calculates the percentage of new users who, between August 1, 2016, and April 30, 2017, both stayed on the site for more than 5 minutes during their initial visit and made a purchase on a subsequent visit at any later time, relative to the total number of new users in the same period.",
        "SQL": "WITH initial_visits AS (\n    SELECT\n        fullVisitorId,\n        MIN(visitStartTime) AS initialVisitStartTime\n    FROM\n        `bigquery-public-data.google_analytics_sample.*`\n    WHERE\n        totals.newVisits = 1\n        AND date BETWEEN '20160801' AND '20170430'\n    GROUP BY\n        fullVisitorId\n),\nqualified_initial_visits AS (\n  SELECT\n    s.fullVisitorId,\n    s.visitStartTime AS initialVisitStartTime,\n    s.totals.timeOnSite AS time_on_site\n  FROM\n    `bigquery-public-data.google_analytics_sample.*` s\n  JOIN initial_visits i\n    ON s.fullVisitorId = i.fullVisitorId\n    AND s.visitStartTime = i.initialVisitStartTime\n  WHERE\n    s.totals.timeOnSite > 300\n),\nfiltered_data AS (\n  SELECT\n    q.fullVisitorId,\n    q.time_on_site,\n    IF(COUNTIF(s.visitStartTime > q.initialVisitStartTime AND s.totals.transactions > 0) > 0, 1, 0) AS will_buy_on_return_visit\n  FROM\n    qualified_initial_visits q\n  LEFT JOIN `bigquery-public-data.google_analytics_sample.*` s\n    ON q.fullVisitorId = s.fullVisitorId\n  GROUP BY\n    q.fullVisitorId, q.time_on_site\n),\nmatching_users AS (\n  SELECT\n    fullVisitorId\n  FROM\n    filtered_data\n  WHERE\n    time_on_site > 300 AND will_buy_on_return_visit = 1\n),\ntotal_new_users AS (\n  SELECT\n    COUNT(DISTINCT fullVisitorId) AS total_new_users\n  FROM\n    `bigquery-public-data.google_analytics_sample.*`\n  WHERE\n    totals.newVisits = 1\n    AND date BETWEEN '20160801' AND '20170430'\n),\nfinal_counts AS (\n  SELECT\n    COUNT(DISTINCT fullVisitorId) AS users_matching_criteria\n  FROM\n    matching_users\n)\nSELECT\n  (final_counts.users_matching_criteria / total_new_users.total_new_users) * 100 AS percentage_matching_criteria\nFROM\n  final_counts,\n  total_new_users;",
        "external_knowledge": null,
        "plan": "1. **Data Preparation**:\n   - Extract user data for new users within the specified date range.\n   - Ensure relevant metrics (bounces, time on site) are available, defaulting to zero if not present.\n\n2. **Initial Filtering**:\n   - Identify new users by checking a specific flag indicating their first visit.\n\n3. **Subsequent Visit Check**:\n   - Create a secondary dataset to determine if users made a purchase on a subsequent visit (where they are no longer considered new users).\n   - Use a conditional count to mark users who have made such purchases.\n\n4. **Data Merging**:\n   - Combine the datasets from the initial filtering and subsequent visit check based on unique user identifiers.\n\n5. **Order Data**:\n   - Sort the combined data based on the time spent on the site during the initial visit.\n\n6. **Calculate Metrics**:\n   - Use a conditional count to determine users who stayed on the site for more than 5 minutes and made a purchase on a subsequent visit.\n   - Calculate the percentage of these users relative to the total number of new users.\n\n7. **Result Output**:\n   - Return the calculated percentage as the final result, representing the proportion of new users who met the criteria.",
        "special_function": [
            "mathematical-functions/SAFE_DIVIDE",
            "aggregate-functions/COUNTIF",
            "aggregate-functions/COUNT",
            "conditional-functions/IF",
            "conditional-functions/IFNULL"
        ]
    },
    {
        "instance_id": "bq029",
        "db": "spider2-public-data.patents",
        "question": "Get the number of patent publications and the average number of inventors per patent in CA every five years from 1960 to 2020, based on when the patents were filed. Focus only on patents with inventor details.",
        "SQL": "SELECT \n  AVG(num_inventors) AS avg_inventors,\n  COUNT(*) AS cnt,\n  filing_year\nFROM (\n  SELECT \n    ANY_VALUE(ARRAY_LENGTH(inventor)) AS num_inventors,\n    ANY_VALUE(country_code) AS country_code,\n    ANY_VALUE(CAST(FLOOR(publication_date / (5 * 10000)) AS INT64)) * 5 AS filing_year\n  FROM \n    `spider2-public-data.patents.publications` AS pubs\n  WHERE \n    publication_date > 19600000 \n    AND publication_date < 20200000\n    AND ARRAY_LENGTH(inventor) > 0\n  GROUP BY \n    publication_number\n)\nWHERE country_code in ('CA')\nGROUP BY \n  filing_year, \n  country_code\nORDER BY \n  filing_year;",
        "external_knowledge": null,
        "plan": "1.Extract information on patent applications in the United States since 1945.\n2.Divide the data into five-year intervals.\n3.Within each interval, count the number of applicants for each patent, ensuring each patent has more than zero applicants.\n4.Calculate both the total number of patents and the average number of applicants per patent for each interval.",
        "special_function": [
            "aggregate-functions/ANY_VALUE",
            "array-functions/ARRAY_LENGTH",
            "conversion-functions/CAST",
            "mathematical-functions/FLOOR"
        ]
    },
    {
        "instance_id": "bq026",
        "db": "spider2-public-data.patents",
        "question": "For the assignee who has been the most active in the patent category 'A61', I'd like to know the five patent jurisdictions code where they filed the most patents during their busiest year, separated by commas.",
        "SQL": "WITH PatentApplications AS (\n   SELECT \n        assignee_harmonized,\n        filing_date,\n        country_code,\n        application_number\n    FROM \n        `spider2-public-data.patents.publications` AS pubs,\n        UNNEST(pubs.cpc) AS cpc_entry\n    WHERE cpc_entry.code LIKE 'A61%'\n)\n\n,AssigneeApplications AS (\n    SELECT \n        COUNT(*) AS year_country_cnt,\n        a.name AS assignee_name,\n        CAST(FLOOR(filing_date / 10000) AS INT64) AS filing_year,\n        apps.country_code\n    FROM \n        PatentApplications AS apps\n    CROSS JOIN\n    UNNEST(assignee_harmonized) AS a\n    GROUP BY \n        a.name, filing_year, country_code\n)\n\n,AggregatedData AS (\n    SELECT \n        SUM(year_country_cnt) AS year_cnt,\n        assignee_name, \n        filing_year, \n        STRING_AGG(country_code ORDER BY year_country_cnt DESC LIMIT 5) AS countries\n    FROM \n        AssigneeApplications\n    GROUP BY \n        assignee_name, filing_year\n)\n\n,FinalAggregation AS (\n    SELECT \n        SUM(year_cnt) AS total_count,\n        assignee_name,\n        ARRAY_AGG(\n            STRUCT<cnt INT64, filing_year INT64, countries STRING>\n            (year_cnt, filing_year, countries) \n            ORDER BY year_cnt DESC LIMIT 1\n        )[SAFE_ORDINAL(1)] AS largest_year\n    FROM \n        AggregatedData\n    GROUP BY \n        assignee_name\n),\nFinal_data AS (\nSELECT \n    total_count,\n    assignee_name,\n    largest_year\nFROM \n    FinalAggregation\nORDER BY \n    total_count DESC\n)\n\nSELECT largest_year.countries\nFROM Final_data\nLIMIT 1",
        "external_knowledge": null,
        "plan": "1. First, access the patents database to retrieve all patent application data where the CPC code matches \"A61K39\".\n2. For each assignee, categorize the data by year and country of application.\n3. Identify the top five countries with the most applications for each assignee per year.\n4. Sort to find out which assignee has the highest total number of applications.\n5. Select the year with the most applications for this assignee.",
        "special_function": [
            "aggregate-functions/ANY_VALUE",
            "aggregate-functions/ARRAY_AGG",
            "aggregate-functions/GROUPING",
            "aggregate-functions/STRING_AGG",
            "conversion-functions/CAST",
            "mathematical-functions/FLOOR",
            "string-functions/REGEXP_CONTAINS",
            "conditional-functions/IF",
            "other-functions/UNNEST",
            "other-functions/ARRAY_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "bq091",
        "db": "spider2-public-data.patents",
        "question": "In which year did the assignee with the most applications in the patent category 'A61' file the most?",
        "SQL": "WITH PatentApplications AS (\n    SELECT \n        ANY_VALUE(assignee_harmonized) AS assignee_harmonized,\n        ANY_VALUE(filing_date) AS filing_date,\n        application_number\n    FROM \n        `spider2-public-data.patents.publications` AS pubs\n    WHERE EXISTS (\n        SELECT 1 FROM UNNEST(pubs.cpc) AS c WHERE c.code LIKE \"A61%\"\n    )\n    GROUP BY \n        application_number\n)\n\n, AssigneeApplications AS (\n    SELECT \n        COUNT(*) AS total_applications,\n        a.name AS assignee_name,\n        CAST(FLOOR(filing_date / 10000) AS INT64) AS filing_year\n    FROM \n        PatentApplications\n    CROSS JOIN\n        UNNEST(assignee_harmonized) AS a\n    GROUP BY \n        a.name, filing_year\n)\n\n, TotalApplicationsPerAssignee AS (\n    SELECT\n        assignee_name,\n        SUM(total_applications) AS total_applications\n    FROM \n        AssigneeApplications\n    GROUP BY \n        assignee_name\n    ORDER BY \n        total_applications DESC\n    LIMIT 1\n)\n\n, MaxYearForTopAssignee AS (\n    SELECT\n        aa.assignee_name,\n        aa.filing_year,\n        aa.total_applications\n    FROM \n        AssigneeApplications aa\n    INNER JOIN\n        TotalApplicationsPerAssignee tapa ON aa.assignee_name = tapa.assignee_name\n    ORDER BY \n        aa.total_applications DESC\n    LIMIT 1\n)\n\nSELECT filing_year\nFROM \n    MaxYearForTopAssignee",
        "external_knowledge": null,
        "plan": "1. First, access the patents database to retrieve all patent application data where the CPC code matches \"A61K39\".\n2. For each assignee, categorize the data by year and country of application.\n3. Identify the assignee with the most total applications for the \u201cA61K39\u201d patent.\n4. Select the year with the most applications for this assignee",
        "special_function": [
            "aggregate-functions/ANY_VALUE",
            "aggregate-functions/GROUPING",
            "array-functions/ARRAY",
            "conversion-functions/CAST",
            "date-functions/DATE",
            "mathematical-functions/FLOOR",
            "string-functions/REGEXP_CONTAINS",
            "conditional-functions/IF",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq099",
        "db": "spider2-public-data.patents",
        "question": "For patent class A01B3, I want to analyze the information of the top 3 assignees based on the total number of applications. Please provide the following five pieces of information: the name of this assignee,  total number of applications, the year with the most applications, the number of applications in that year, and the country code with the most applications during that year.",
        "SQL": "WITH PatentApplications AS (\n   SELECT \n        assignee_harmonized,\n        filing_date,\n        country_code,\n        application_number\n    FROM \n        `spider2-public-data.patents.publications` AS pubs,\n        UNNEST(pubs.cpc) AS cpc_entry\n    WHERE cpc_entry.code LIKE 'A01B3%'\n\n)\n\n,AssigneeApplications AS (\n    SELECT \n        COUNT(*) AS year_country_cnt,\n        a.name AS assignee_name,\n        CAST(FLOOR(filing_date / 10000) AS INT64) AS filing_year,\n        apps.country_code\n    FROM \n        PatentApplications AS apps\n    CROSS JOIN\n    UNNEST(assignee_harmonized) AS a\n    GROUP BY \n        a.name, filing_year, country_code\n)\n\n,AggregatedData AS (\n    SELECT \n        SUM(year_country_cnt) AS year_cnt,\n        assignee_name, \n        filing_year, \n        STRING_AGG(country_code ORDER BY year_country_cnt DESC LIMIT 1) AS countries\n    FROM \n        AssigneeApplications\n    GROUP BY \n        assignee_name, filing_year\n)\n\n,FinalAggregation AS (\n    SELECT \n        SUM(year_cnt) AS total_count,\n        assignee_name,\n        ARRAY_AGG(\n            STRUCT<cnt INT64, filing_year INT64, countries STRING>\n            (year_cnt, filing_year, countries) \n            ORDER BY year_cnt DESC LIMIT 1\n        )[SAFE_ORDINAL(1)] AS largest_year\n    FROM \n        AggregatedData\n    GROUP BY \n        assignee_name\n)\n\nSELECT \n    total_count,\n    assignee_name,\n    largest_year.cnt,\n    largest_year.filing_year,\n    largest_year.countries\nFROM \n    FinalAggregation\nORDER BY \n    total_count DESC\nLIMIT 3;",
        "external_knowledge": null,
        "plan": "1. **Filter Patent Data**: \n   - Identify and collect patent applications related to a specific patent class.\n   - Ensure each application is uniquely identified and includes relevant assignee, filing date, and country information.\n\n2. **Count Applications by Assignee, Year, and Country**:\n   - For each application, determine the number of filings per assignee, broken down by year and country.\n   - Group the data by assignee name, year, and country to prepare for aggregation.\n\n3. **Aggregate Yearly Application Data**:\n   - Summarize the total number of applications for each assignee by year.\n   - Identify the country with the highest number of applications for each assignee and year.\n\n4. **Determine Peak Application Year for Each Assignee**:\n   - Calculate the overall number of applications for each assignee.\n   - Identify the year with the highest application count and the corresponding country with the most applications for each assignee.\n\n5. **Select and Rank Top Assignees**:\n   - Order the assignees by their total number of applications in descending order.\n   - Limit the results to the top 20 assignees, including their total application count, the year with the highest applications, the number of applications in that year, and the most frequent country code during that year.",
        "special_function": [
            "aggregate-functions/ANY_VALUE",
            "aggregate-functions/ARRAY_AGG",
            "aggregate-functions/GROUPING",
            "aggregate-functions/STRING_AGG",
            "conversion-functions/CAST",
            "mathematical-functions/FLOOR",
            "string-functions/REGEXP_CONTAINS",
            "other-functions/UNNEST",
            "other-functions/ARRAY_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "bq033",
        "db": "spider2-public-data.patents",
        "question": "How many U.S. publications related to IoT (where the abstract includes the phrase 'internet of things') were filed each month from 2008 to 2022, including months with no filings?",
        "SQL": "WITH\n  Patent_Matches AS (\n    SELECT\n      PARSE_DATE('%Y%m%d', SAFE_CAST(ANY_VALUE(patentsdb.filing_date) AS STRING)) AS Patent_Filing_Date,\n      patentsdb.application_number AS Patent_Application_Number,\n      ANY_VALUE(abstract_info.text) AS Patent_Title,\n      ANY_VALUE(abstract_info.language) AS Patent_Title_Language\n    FROM\n      `spider2-public-data.patents.publications` AS patentsdb,\n      UNNEST(abstract_localized) AS abstract_info\n    WHERE\n      LOWER(abstract_info.text) LIKE '%internet of things%'\n      AND patentsdb.country_code = 'US'\n    GROUP BY\n      Patent_Application_Number\n  ),\n\n  Date_Series_Table AS (\n    SELECT\n      day,\n      0 AS Number_of_Patents\n    FROM\n      UNNEST(GENERATE_DATE_ARRAY(\n        DATE '2008-01-01', \n        DATE '2022-12-31'\n      )) AS day\n  )\n\nSELECT\n  SAFE_CAST(FORMAT_DATE('%Y-%m', Date_Series_Table.day) AS STRING) AS Patent_Date_YearMonth,\n  COUNT(Patent_Matches.Patent_Application_Number) AS Number_of_Patent_Applications\nFROM\n  Date_Series_Table\n  LEFT JOIN Patent_Matches\n    ON Date_Series_Table.day = Patent_Matches.Patent_Filing_Date\nGROUP BY\n  Patent_Date_YearMonth\nORDER BY\n  Patent_Date_YearMonth;",
        "external_knowledge": null,
        "plan": "1. Extract patents with abstracts containing \"Internet of Things\" that were applied for in the United States.\n2. Generate a record set starting from January 2008 to December 2022.\n3. Count and record the number of \"Internet of Things\" patents applied for each month.\n4. Sort and return the monthly totals with the highest number of patent applications.",
        "special_function": [
            "aggregate-functions/ANY_VALUE",
            "array-functions/GENERATE_DATE_ARRAY",
            "conversion-functions/SAFE_CAST",
            "date-functions/DATE",
            "date-functions/FORMAT_DATE",
            "date-functions/PARSE_DATE",
            "json-functions/STRING",
            "string-functions/LOWER",
            "timestamp-functions/STRING",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq209",
        "db": "spider2-public-data.patents",
        "question": "Can you find how many utility patents granted in 2010 have exactly one forward citation within the ten years following their application date?",
        "SQL": "WITH patents_sample AS (               -- name of our table\nSELECT \n  t1.publication_number, \n  t1.application_number \nFROM \n  `spider2-public-data.patents.publications` t1 \nWHERE \n  grant_date between 20100101 AND 20101231                               -- grant dates between 2002 and 2006                                                      -- only consider granted patents                                        -- only consider patents with kind code B2\n),\nForward_citation AS (\n     SELECT\n     DISTINCT patents_sample.publication_number,\n     COUNT(DISTINCT t3.citing_application_number) AS forward_citations\n     FROM\n     patents_sample\n     LEFT JOIN (\n     SELECT\n     x2.publication_number,\n     PARSE_DATE('%Y%m%d', CAST(x2.filing_date AS STRING)) AS filing_date\n     FROM\n     `patents-public-data.patents.publications` x2\n     WHERE\n     x2.filing_date != 0) t2\n     ON\n     t2.publication_number = patents_sample.publication_number\n     LEFT JOIN (\n     SELECT\n     x3.publication_number AS citing_publication_number,\n     x3.application_number AS citing_application_number,\n     PARSE_DATE('%Y%m%d', CAST(x3.filing_date AS STRING)) AS joined_filing_date,\n     citation_u.publication_number AS cited_publication_number\n     FROM\n     `spider2-public-data.patents.publications` x3,\n     UNNEST(citation) AS citation_u\n     WHERE\n     x3.filing_date!=0) t3\n     ON\n     patents_sample.publication_number = t3.cited_publication_number\n     AND t3.joined_filing_date BETWEEN t2.filing_date\n     AND DATE_ADD(t2.filing_date, INTERVAL 10 YEAR)\n     GROUP BY\n     patents_sample.publication_number\n)\n\nSELECT \nCOUNT(*)\nFROM\n     Forward_citation\nWHERE forward_citations=1",
        "external_knowledge": null,
        "plan": "1. Retrieve all patents granted in January 2010, considering only those with the kind code B2, which are utility patents issued with a published application.\n2. Query other patents to extract forward citation information from patent publications, ensuring that the citation date of the citing patent falls within ten years of the filing date of the cited patent.\n3. Calculate the number of distinct citation applications for each patent to identify the patent with the most citations.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE_ADD",
            "date-functions/PARSE_DATE",
            "json-functions/STRING",
            "timestamp-functions/STRING",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq027",
        "db": "spider2-public-data.patents",
        "question": "For patents granted between 2010 and 2018, provide the publication number of each patent and the number of backward citations it has received in the SEA category.",
        "SQL": "WITH patents_sample AS (\nSELECT \n  t1.publication_number, \n  t1.application_number \nFROM \n  `spider2-public-data.patents.publications` t1 \nWHERE\n  grant_date between 20100101 AND 20181231\n)\n\nSELECT\n  t1.publication_number,\n  COUNT(DISTINCT t3.application_number) AS backward_citations\nFROM\n  patents_sample t1\nLEFT OUTER JOIN (\n  SELECT\n    x2.publication_number AS citing_publication_number,\n    citation_u.publication_number AS cited_publication_number,\n    citation_u.category AS cited_publication_category\n  FROM\n    `spider2-public-data.patents.publications` x2,\n    UNNEST(citation) AS citation_u ) t2\nON\n  t2.citing_publication_number = t1.publication_number\n  AND CONTAINS_SUBSTR(t2.cited_publication_category, 'SEA')\nLEFT OUTER JOIN\n  `spider2-public-data.patents.publications` t3\nON\n  t2.cited_publication_number = t3.publication_number\nGROUP BY\n  t1.publication_number\nORDER BY\n  backward_citations DESC",
        "external_knowledge": null,
        "plan": "1. **Define the Initial Dataset**: Create a temporary dataset that includes the publication number and application number of patents.\n    - Filter for patents from a specific country.\n    - Ensure the patents have a grant date within the first week of January 2018.\n    - Exclude any entries without a grant date.\n    - Include only patents with a specific kind code.\n\n2. **Create the Main Query**: Use the temporary dataset to find the required information.\n    - Select the publication number from the temporary dataset.\n\n3. **Join with Citation Data**: Perform a left join with another table that contains citation details.\n    - Extract the citing publication number, cited publication number, and category from the citation details.\n    - Ensure that the citation category contains a specific keyword.\n\n4. **Join with Publication Data Again**: Perform another left join to link the cited publication number with the application number from the publication data.\n\n5. **Aggregate the Results**: Group the results by the publication number from the initial dataset.\n    - Count the distinct application numbers that are cited by the patents in the specified category.\n\n6. **Order the Results**: Sort the final results by the publication number.",
        "special_function": [
            "string-functions/CONTAINS_SUBSTR",
            "string-functions/LEFT",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq210",
        "db": "spider2-public-data.patents",
        "question": "How many US B2 patents granted between 2008 and 2018 contain claims that do not include the word 'claim'?",
        "SQL": "WITH patents_sample AS (\n  SELECT \n    t1.publication_number,\n    claim.text AS claims_text\n  FROM \n    spider2-public-data.patents.publications t1,\n    UNNEST(t1.claims_localized) AS claim\n  WHERE \n    t1.country_code = 'US'\n    AND t1.grant_date BETWEEN 20080101 AND 20181231\n    AND t1.grant_date != 0\n    AND t1.publication_number LIKE '%B2%'\n),\nPublication_data AS (\n  SELECT\n    publication_number,\n    COUNTIF(claims_text NOT LIKE '%claim%') AS nb_indep_claims\n  FROM\n    patents_sample\n  GROUP BY\n    publication_number\n)\n\nSELECT COUNT(nb_indep_claims)\nFROM Publication_data\nWHERE nb_indep_claims != 0",
        "external_knowledge": null,
        "plan": "1. **Extract Relevant Patent Data**: Create a temporary dataset by selecting publication numbers and their associated claims from the main data source. Ensure that only patents granted in the US between the years 2015 and 2018 are included. Also, filter to include only those patents whose publication numbers end with 'B2'.\n\n2. **Unnest Claims**: Expand the nested claims data so that each claim associated with a patent is treated as a separate row in the temporary dataset.\n\n3. **Filter Claims Text**: Check each claim text to see if it does not contain the word 'claim'. Count the number of such claims for each publication number.\n\n4. **Group by Publication Number**: Aggregate the results by publication number, counting how many claims per patent do not contain the word 'claim'.\n\n5. **Filter Non-zero Counts**: From the aggregated data, select only those patents that have at least one claim not containing the word 'claim'.\n\n6. **Final Count**: Count the number of patents that meet the criteria established in the previous steps. This final result represents the number of US B2 patents granted between 2015 and 2018 that have at least one claim not containing the word 'claim'.",
        "special_function": [
            "aggregate-functions/COUNTIF",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq211",
        "db": "spider2-public-data.patents",
        "question": "Among patents granted between 2010 and 2023 in CN, how many of them belong to families that have a total of over one distinct applications?",
        "SQL": "WITH patents_sample AS (               -- name of our table\nSELECT \n  t1.publication_number, \n  t1.application_number \nFROM \n  spider2-public-data.patents.publications t1 \nWHERE         \n country_code = 'CN'       \n AND\n  grant_date between 20100101 AND 20231231  \n),\n\nfamily_number AS (\nSELECT\n  t1.publication_number,\n  COUNT(DISTINCT t3.application_number) AS family_size,\nFROM\n  patents_sample t1\nLEFT JOIN\n  spider2-public-data.patents.publications t2\nON\n  t1.publication_number = t2.publication_number\nLEFT JOIN\n  spider2-public-data.patents.publications t3\nON\n  t2.family_id = t3.family_id\nGROUP BY\n  t1.publication_number\n)\n\nSELECT COUNT(*) FROM family_number WHERE family_size>1",
        "external_knowledge": null,
        "plan": "1.Extract information on patents that were applied for and granted in the United States in January 2008.\n2.Obtain the family IDs associated with these patents.\n3.Retrieve the total number of patents within these families.\n4.Keep the application IDs of patents where the family size exceeds 300 and count their total number.",
        "special_function": null
    },
    {
        "instance_id": "bq213",
        "db": "spider2-public-data.patents",
        "question": "What is the most common 4-digit IPC code among US B2 utility patents granted from June to August in 2022?",
        "SQL": "WITH interim_table as(\nSELECT \n    t1.publication_number, \n    SUBSTR(ipc_u.code, 0, 4) as ipc4\nFROM \n    `spider2-public-data.patents.publications` t1, \n    UNNEST(ipc) AS ipc_u \nWHERE\ncountry_code = 'US'  \nAND grant_date between 20220601 AND 20220831\n  AND grant_date != 0\n  AND publication_number LIKE '%B2%'  \nGROUP BY \n    t1.publication_number, \n    ipc4\n) \nSELECT \nipc4\nFROM \ninterim_table \nGROUP BY ipc4\nORDER BY COUNT(publication_number) DESC\nLIMIT 1",
        "external_knowledge": "patents_info.md",
        "plan": "1.Data Retrieval: Access patents-public-data.patents.publications, unnest IPC codes, and filter for US patents granted in April 2015 with a 'B2' kind code.\n2.IPC Analysis: Extract the first four characters of each IPC code, count their occurrences within each patent, and group by publication number and IPC subcategory.\n3.Determine Dominance: For each patent, identify the most frequent IPC subcategory using a combination of concatenation and maximum count comparison within the grouped data.\n4.Result Extraction: Order the results by the count of IPC occurrences and limit the output to the top entry to find the single most prevalent IPC subcategory.",
        "special_function": [
            "string-functions/SUBSTR",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq212",
        "db": "spider2-public-data.patents",
        "question": "For United States utility patents under the B2 classification granted between June and September of 2022, identify the most frequent 4-digit IPC code for each patent. Then, list the publication numbers and IPC4 codes of patents where this code appears 10 or more times.",
        "SQL": "WITH interim_table as(\nSELECT \n    t1.publication_number, \n    SUBSTR(ipc_u.code, 0, 4) as ipc4, \n    COUNT(\n    SUBSTR(ipc_u.code, 0, 4)\n    ) as ipc4_count \nFROM \n    `spider2-public-data.patents.publications` t1, \n    UNNEST(ipc) AS ipc_u \nWHERE\ncountry_code = 'US'  \nAND grant_date between 20220601 AND 20220930\n  AND grant_date != 0\n  AND publication_number LIKE '%B2%'  \nGROUP BY \n    t1.publication_number, \n    ipc4\n) \nSELECT \npublication_number, ipc4\nFROM \ninterim_table \nwhere \nconcat(\n    interim_table.publication_number, \n    interim_table.ipc4_count\n) IN (\n    SELECT \n    concat(\n        publication_number, \n        MAX(ipc4_count)\n    ) \n    FROM \n    interim_table \n    group by \n    publication_number\n)\nAND ipc4_count >= 10",
        "external_knowledge": "patents_info.md",
        "plan": "1. **Create an Interim Table:**\n   - Select the publication number and extract the first 4 characters of the IPC code.\n   - Count the occurrences of each 4-character IPC code for each publication.\n   - Filter records to include only those from the specified country and date range, ensuring the grant date is valid and the publication format matches the specified pattern.\n   - Group the results by publication number and the 4-character IPC code.\n\n2. **Identify Most Frequent IPC Code:**\n   - In the interim table, determine the most frequent 4-character IPC code for each publication.\n   - Concatenate the publication number with the count of the most frequent IPC code.\n   - Use this concatenated value to filter the interim table, ensuring you select rows where the IPC code count matches the maximum count for each publication.\n\n3. **Filter by Frequency:**\n   - Further filter the results to include only those rows where the most frequent IPC code appears 20 or more times.\n\n4. **Final Selection:**\n   - From the filtered results, select and list the publication numbers and their corresponding 4-character IPC codes.",
        "special_function": [
            "string-functions/CONCAT",
            "string-functions/SUBSTR",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq214",
        "db": "spider2-public-data.patents_google",
        "question": "For United States utility patents under the B2 classification granted between 2010 and 2014, find the one with the most forward citations within a month of its filing date, and identify the most similar patent from the same filing year, regardless of its type.",
        "SQL": "WITH patents_sample AS (\n    SELECT \n        t1.publication_number, \n        t1.application_number \n    FROM \n        `spider2-public-data.patents_google.publications` t1 \n    WHERE \n        country_code = 'US' AND\n        grant_date BETWEEN 20100101 AND 20141231 AND\n        publication_number LIKE '%B2%'\n),\n\nForward_citation AS (\n    SELECT\n        DISTINCT patents_sample.publication_number,\n        COUNT(DISTINCT t3.citing_application_number) AS forward_citations\n    FROM\n        patents_sample\n    LEFT JOIN (\n        SELECT\n            x2.publication_number,\n            PARSE_DATE('%Y%m%d', CAST(x2.filing_date AS STRING)) AS filing_date\n        FROM\n            `spider2-public-data.patents_google.publications` x2\n        WHERE\n            x2.filing_date != 0\n    ) t2 ON t2.publication_number = patents_sample.publication_number\n    LEFT JOIN (\n        SELECT\n            x3.publication_number AS citing_publication_number,\n            x3.application_number AS citing_application_number,\n            PARSE_DATE('%Y%m%d', CAST(x3.filing_date AS STRING)) AS joined_filing_date,\n            citation_u.publication_number AS cited_publication_number\n        FROM\n            `spider2-public-data.patents_google.publications` x3,\n            UNNEST(citation) AS citation_u\n        WHERE\n            x3.filing_date != 0\n    ) t3 ON patents_sample.publication_number = t3.cited_publication_number AND\n             t3.joined_filing_date BETWEEN t2.filing_date AND DATE_ADD(t2.filing_date, INTERVAL 1 MONTH)\n    GROUP BY\n        patents_sample.publication_number\n),\n\nselect_sample AS (\n    SELECT \n        publication_number\n    FROM\n        Forward_citation\n    ORDER BY\n        forward_citations DESC\n    LIMIT 1\n),\n\nt AS (\n    SELECT\n        t1.publication_number,\n        t4.publication_number AS similar_publication_number,\n        (SELECT SUM(element1 * element2)\n         FROM t5.embedding_v1 element1 WITH OFFSET pos\n         JOIN t6.embedding_v1 element2 WITH OFFSET pos USING (pos)) AS similarity\n    FROM \n        (SELECT * FROM select_sample LIMIT 1) t1\n    LEFT JOIN (\n        SELECT \n            x3.publication_number,\n            EXTRACT(YEAR FROM PARSE_DATE('%Y%m%d', CAST(x3.filing_date AS STRING))) AS focal_filing_year\n        FROM \n            `spider2-public-data.patents_google.publications` x3\n        WHERE \n            x3.filing_date != 0\n    ) t3 ON t3.publication_number = t1.publication_number\n    LEFT JOIN (\n        SELECT \n            x4.publication_number,\n            EXTRACT(YEAR FROM PARSE_DATE('%Y%m%d', CAST(x4.filing_date AS STRING))) AS filing_year\n        FROM \n            `spider2-public-data.patents_google.publications` x4\n        WHERE \n            x4.filing_date != 0\n    ) t4 ON  t4.publication_number != t1.publication_number AND\n             t3.focal_filing_year = t4.filing_year\n    LEFT JOIN `spider2-public-data.patents_google.abs_and_emb` t5 ON t5.publication_number = t1.publication_number\n    LEFT JOIN `spider2-public-data.patents_google.abs_and_emb` t6 ON t6.publication_number = t4.publication_number\n    ORDER BY \n        t1.publication_number, similarity DESC\n)\n\nSELECT\n    t.similar_publication_number\nFROM (\n    SELECT\n        t.*,\n        ROW_NUMBER() OVER (PARTITION BY publication_number ORDER BY similarity DESC) AS seqnum\n    FROM\n        t\n) t\nWHERE\n    seqnum <= 1;",
        "external_knowledge": "patents_info.md",
        "plan": "1.Extract Relevant Patent Data: Access the patents database to retrieve information for patents granted in the US on June 2nd, 2015, with the kind code B2, indicating they are granted utility patents.\n2.Determine the IPC Codes: For each patent, extract and count occurrences of four-digit IPC codes from the abstracts, identifying how frequently each IPC code appears within a given patent.\n3.Identify Backward Citations: For each selected patent, retrieve backward citations (i.e., earlier patents cited by the patent in question) and join these with the IPC data to analyze the diversity of IPC codes from these citations.\n4.Calculate Originality Score: Compute an originality score for each patent based on the diversity of IPC codes cited. This involves calculating a formula where the diversity is inversely related to the sum of the squares of IPC code occurrences.\n5.Select the Patent with the Highest Originality: From the calculated originality scores, identify the patent with the highest score, which indicates it has the broadest range of influences from prior art, suggesting a high level of innovation.\n6.Output the Result: Return the publication number of the patent with the highest originality score, highlighting it as the most original patent granted on that specific day.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE_ADD",
            "date-functions/EXTRACT",
            "date-functions/PARSE_DATE",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "json-functions/STRING",
            "numbering-functions/ROW_NUMBER",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/STRING",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq216",
        "db": "spider2-public-data.patents_google",
        "question": "Identify the top five patents filed in the same year as `US-9741766-B2` that are most similar to it based on technological similarities. Please provide the publication numbers.",
        "SQL": "WITH patents_sample AS (\n    SELECT publication_number, application_number\n    FROM\n        `spider2-public-data.patents_google.publications`\n    WHERE\n    publication_number = 'US-9741766-B2'\n),\nt AS (\n    SELECT\n        t1.publication_number,\n        t4.publication_number AS similar_publication_number,\n        (SELECT SUM(element1 * element2)\n         FROM t5.embedding_v1 element1 WITH OFFSET pos\n         JOIN t6.embedding_v1 element2 WITH OFFSET pos USING (pos)) AS similarity\n    FROM \n        (SELECT * FROM patents_sample LIMIT 1) t1\n    LEFT JOIN (\n        SELECT \n            x3.publication_number,\n            EXTRACT(YEAR FROM PARSE_DATE('%Y%m%d', CAST(x3.filing_date AS STRING))) AS focal_filing_year\n        FROM \n            `spider2-public-data.patents_google.publications` x3\n        WHERE \n            x3.filing_date != 0\n    ) t3 ON t3.publication_number = t1.publication_number\n    LEFT JOIN (\n        SELECT \n            x4.publication_number,\n            EXTRACT(YEAR FROM PARSE_DATE('%Y%m%d', CAST(x4.filing_date AS STRING))) AS filing_year\n        FROM \n            `spider2-public-data.patents_google.publications` x4\n        WHERE \n            x4.filing_date != 0\n    ) t4 ON\n    t4.publication_number != t1.publication_number\n    AND t3.focal_filing_year = t4.filing_year\n    LEFT JOIN `spider2-public-data.patents_google.abs_and_emb` t5 ON t5.publication_number = t1.publication_number\n    LEFT JOIN `spider2-public-data.patents_google.abs_and_emb` t6 ON t6.publication_number = t4.publication_number\n    ORDER BY \n        t1.publication_number, similarity DESC\n)\nSELECT\n    t.similar_publication_number,\n    t.similarity\nFROM (\n    SELECT\n        t.*,\n        ROW_NUMBER() OVER (PARTITION BY publication_number ORDER BY similarity DESC) AS seqnum\n    FROM\n        t\n) t\nWHERE\n    seqnum <= 5;",
        "external_knowledge": "patents_info.md",
        "plan": "1. **Extract the Target Patent**: Create a subset containing only the target patent's unique identifiers.\n\n2. **Calculate Filing Year for Target Patent**: Determine the filing year of the target patent by extracting the year from its filing date.\n\n3. **Identify Same-Year Patents**: Retrieve all patents filed in the same year as the target patent, excluding the target patent itself.\n\n4. **Calculate Technological Similarity**: For each patent from the same year, compute the technological similarity with the target patent using a predefined similarity metric based on their respective embeddings.\n\n5. **Rank Similar Patents**: Order the patents by their similarity scores in descending order.\n\n6. **Select Top Similar Patents**: From the ordered list, select the top five patents with the highest similarity scores for each target patent.\n\n7. **Output Similar Patents**: Return the publication numbers and similarity scores of the top five most similar patents.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/EXTRACT",
            "date-functions/PARSE_DATE",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "json-functions/STRING",
            "numbering-functions/ROW_NUMBER",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/STRING"
        ]
    },
    {
        "instance_id": "bq247",
        "db": "spider2-public-data.patents_google",
        "question": "For the top 6 valid family has the most publications? provide their family id and non-empty publication abstracts.",
        "SQL": "WITH\n  family_list AS (\n    SELECT\n      family_id,\n      COUNT(publication_number) AS publication_number_count\n    FROM\n      `spider2-public-data.patents_google.publications`\n    GROUP BY\n      family_id\n  ),\n  most_published_family AS (\n    SELECT\n      family_id\n    FROM\n      family_list\n    WHERE family_id != '-1'\n    ORDER BY\n      publication_number_count DESC\n    LIMIT 6\n  ),\n  publications_with_abstracts AS (\n    SELECT\n      p.family_id,\n      gpr.abstract\n    FROM\n      `spider2-public-data.patents_google.abs_and_emb` gpr\n    JOIN\n      `spider2-public-data.patents_google.publications` p\n    ON\n      p.publication_number = gpr.publication_number\n    WHERE\n      gpr.abstract IS NOT NULL AND gpr.abstract != ''\n  ),\n  abstracts_from_top_family AS (\n    SELECT\n      fwa.family_id,\n      fwa.abstract\n    FROM\n      publications_with_abstracts fwa\n    JOIN\n      most_published_family mpf\n    ON\n      fwa.family_id = mpf.family_id\n  )\nSELECT\n  family_id,\n  abstract\nFROM\n  abstracts_from_top_family;",
        "external_knowledge": null,
        "plan": "1. Aggregate the data to count the number of publications per unique family identifier.\n2. Select the family identifier that has the highest publication count, ensuring that only valid identifiers are considered.\n3. Filter the dataset to include only those records that have a non-empty abstract. \n4. Join the filtered list of publications with the most published family identifier obtained earlier to get a dataset containing only the abstracts from publications that belong to the family with the highest publication count.\n5. Retrieve and display the abstracts from publications associated with the family having the most publications. ",
        "special_function": null
    },
    {
        "instance_id": "bq127",
        "db": "spider2-public-data.patents_google",
        "question": "For each publication family whose earliest publication was first published in January 2015, please provide the earliest publication date, the distinct publication numbers, their country codes, the distinct CPC and IPC codes, distinct families (namely, the ids) that cite and are cited by this publication family. Please present all lists as comma-separated values, sorted by the first letter of the code for clarity.",
        "SQL": "WITH fam AS (\n  SELECT\n    DISTINCT family_id\n  FROM\n    `spider2-public-data.patents_google.publications`\n),\n\ncrossover AS (\n  SELECT\n    publication_number,\n    family_id\n  FROM\n    `spider2-public-data.patents_google.publications`\n),\n\npub AS (\n  SELECT\n    family_id,\n    MIN(publication_date) AS publication_date,\n    STRING_AGG(DISTINCT(p.publication_number) ORDER BY p.publication_number) AS publication_number,\n    STRING_AGG(DISTINCT(country_code) ORDER BY country_code) AS country_code\n  FROM\n    `spider2-public-data.patents_google.publications` AS p\n  GROUP BY\n    family_id\n),\n  \ntech_class AS (\n  SELECT\n    family_id,\n    STRING_AGG(DISTINCT(cpc.code) ORDER BY cpc.code) AS cpc,\n    STRING_AGG(DISTINCT(ipc.code) ORDER BY ipc.code) AS ipc\n  FROM\n    `spider2-public-data.patents_google.publications` AS p,\n    UNNEST(cpc) AS cpc,\n    UNNEST(ipc) AS ipc\n  GROUP BY\n    family_id\n),\n\ncit AS (\n  SELECT\n      p.family_id,\n      STRING_AGG(DISTINCT(crossover.family_id)) AS citation\n    FROM\n      `spider2-public-data.patents_google.publications` AS p,\n      UNNEST(citation) AS citation\n      LEFT JOIN \n        crossover \n        ON citation.publication_number = crossover.publication_number\n    GROUP BY\n      p.family_id\n),\n\ntmp_gpr AS (\n      SELECT\n        family_id,\n        SPLIT(STRING_AGG(DISTINCT(cited_by.publication_number))) AS cited_by_publication_number\n      FROM\n        `spider2-public-data.patents_google.abs_and_emb` AS p,\n        UNNEST(cited_by) AS cited_by\n      LEFT JOIN\n        crossover\n      ON\n        p.publication_number = crossover.publication_number\n      GROUP BY\n        family_id\n),\n\ngpr AS (\n      SELECT\n        tmp_gpr.family_id,\n        STRING_AGG(DISTINCT(crossover.family_id)) AS cited_by\n    FROM\n      tmp_gpr,\n      UNNEST(cited_by_publication_number) AS cited_by_publication_number\n    LEFT JOIN\n      crossover\n    ON\n      cited_by_publication_number = crossover.publication_number\n    GROUP BY\n      tmp_gpr.family_id\n)\n\nSELECT\n  fam.family_id,\n  pub.* EXCEPT(family_id),\n  tech_class.* EXCEPT(family_id),\n  cit.* EXCEPT(family_id),\n  gpr.* EXCEPT(family_id)\nFROM\n  fam\nLEFT JOIN\n  pub\nON\n  fam.family_id = pub.family_id\nLEFT JOIN\n  tech_class\nON\n  fam.family_id = tech_class.family_id\nLEFT JOIN\n  cit\nON\n  fam.family_id = cit.family_id\nLEFT JOIN\n  gpr\nON\n  fam.family_id = gpr.family_id\nWHERE\n  publication_date BETWEEN 20150101 AND 20150131",
        "external_knowledge": null,
        "plan": "1. **Identify Distinct Families**: Create a list of all distinct publication families from the dataset to ensure each family is considered uniquely.\n\n2. **Determine Earliest Publication**: For each family, find the earliest publication date and compile lists of distinct publication numbers and country codes associated with the family.\n\n3. **Extract Technical Classifications**: For each family, gather distinct CPC and IPC codes by unnesting these classifications from the dataset, ensuring all unique codes are captured.\n\n4. **Identify Citing Families**: Determine which other families cite the current family by linking citation numbers to family identifiers through a join operation.\n\n5. **Identify Cited Families**: Similarly, determine which families are cited by the current family using a join operation to link cited publication numbers back to family identifiers.\n\n6. **Filter and Compile Results**: Combine all gathered information for families whose earliest publication date falls within January 2015, presenting the results in a structured format with all relevant details for each family.",
        "special_function": null
    },
    {
        "instance_id": "bq215",
        "db": "spider2-public-data.patents",
        "question": "What is the publication number of US patent under the B2 classification granted during 2015 to 2018, with the highest originality score based on the diversity of 4-digits IPC codes from its backward citations?",
        "SQL": "WITH patents_sample AS (          \nSELECT \n  t1.publication_number, \n  t1.application_number \nFROM \n  `spider2-public-data.patents.publications` t1 \nWHERE \n  country_code = 'US'                      \n  AND grant_date between 20150101 AND 20181231                \n  AND grant_date != 0                               \n  AND publication_number LIKE '%B2%'                        \n),\ninterim_table AS (\n    SELECT\n        t1.publication_number,\n        SUBSTR(ipc_u.code, 0, 4) AS ipc4,\n        COUNT(SUBSTR(ipc_u.code, 0, 4)) AS ipc4_count\n    FROM\n        spider2-public-data.patents.publications t1,\n        UNNEST(ipc) AS ipc_u\n    GROUP BY\n        t1.publication_number,\n        ipc4\n),\nchosen_ipc4_view AS (\n    SELECT\n        *\n    FROM\n        interim_table\n    WHERE\n        CONCAT(interim_table.publication_number, interim_table.ipc4_count) IN (\n            SELECT\n                CONCAT(publication_number, MAX(ipc4_count))\n            FROM\n                interim_table\n            GROUP BY\n                publication_number\n        )\n    ORDER BY\n        ipc4_count DESC\n),\nipc_counts AS (\n  SELECT\n    t1.publication_number,\n    t3.ipc4,\n    COUNT(t3.ipc4) AS ipc_occurrences\n  FROM\n    patents_sample t1\n    -- joins backward citations\n  LEFT JOIN (\n    SELECT\n      x2.publication_number AS citing_publication_number,\n      citation_u.publication_number AS backward_citation\n    FROM\n      spider2-public-data.patents.publications x2,\n      UNNEST(citation) AS citation_u) t2\n  ON\n    t2.citing_publication_number = t1.publication_number\n    -- joins 4-digit ipc codes of backward citations\n  LEFT JOIN\n    chosen_ipc4_view t3\n  ON\n    t3.publication_number = t2.backward_citation\n  GROUP BY\n    t1.publication_number,\n    t3.ipc4\n),\nmax_originality AS (\n  SELECT\n    publication_number,\n    1 - SUM(POWER(ipc_occurrences, 2)) / POWER(SUM(ipc_occurrences), 2) AS originality\n  FROM\n    ipc_counts\n  GROUP BY\n    publication_number\n  HAVING\n    SUM(ipc_occurrences) > 0\n  ORDER BY\n    originality DESC\n  LIMIT 1\n)\nSELECT \n  publication_number\nFROM \n  max_originality",
        "external_knowledge": "patents_info.md",
        "plan": "1. **Filter US Patents:**\n   - Select publication numbers and application numbers from the dataset.\n   - Only include records where the country code is 'US'.\n   - Ensure the grant date is within January 2018.\n   - Exclude records with a grant date of 0.\n   - Only consider patents with a specific kind code pattern.\n\n2. **Extract IPC Codes:**\n   - For each selected patent, extract and count the unique 4-digit IPC codes from the associated IPC codes.\n\n3. **Identify Maximum IPC Code Count:**\n   - Create a subset of records that have the maximum count of a specific 4-digit IPC code for each patent.\n\n4. **Calculate IPC Occurrences in Backward Citations:**\n   - Join the filtered patents with their backward citations.\n   - For each backward citation, join with the subset of records to get the 4-digit IPC codes.\n   - Count the occurrences of each 4-digit IPC code in the backward citations for each patent.\n\n5. **Compute Originality Score:**\n   - For each patent, calculate an originality score based on the diversity of the 4-digit IPC codes from the backward citations.\n   - Use a formula that considers the sum of squared occurrences of each IPC code, normalized by the total number of occurrences.\n\n6. **Select Highest Originality Score:**\n   - From the computed originality scores, select the patent with the highest score.\n\n7. **Return Result:**\n   - Output the publication number of the patent with the highest originality score.",
        "special_function": [
            "mathematical-functions/POWER",
            "string-functions/CONCAT",
            "string-functions/SUBSTR",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq222",
        "db": "spider2-public-data.patents",
        "question": "Find the CPC technology areas in Germany with the highest exponential moving average of patent filings each year (smoothing factor 0.1) for patents granted in December 2016. Show me the full title, CPC group and the best year for each CPC group at level 4.",
        "SQL": "CREATE TEMPORARY FUNCTION highest_moving_avg(yearcnt ARRAY<STRUCT<filing_year INT64, cnt INT64>>)\nRETURNS STRUCT<filing_year INT64, avg INT64>\nLANGUAGE js AS \"\"\"\nlet a = 0.1;\nlet avg = yearcnt.length > 0 ? yearcnt[0].cnt : 0;\nlet highest = {filing_year: -1, avg: -1};\nfor (let x of yearcnt) {\n    avg = a * x.cnt + (1 - a) * avg;\n    if (avg > highest.avg) {\n        highest = {filing_year: x.filing_year, avg: avg};\n    }\n}\nreturn highest;\n\"\"\";\n    \nWITH patent_cpcs AS (\n    SELECT cd.parents,\n    CAST(FLOOR(filing_date/10000) AS INT64) AS filing_year\n    FROM (\n        SELECT ANY_VALUE(cpc) AS cpc, ANY_VALUE(filing_date) AS filing_date\n        FROM `spider2-public-data.patents.publications`\n        WHERE application_number != \"\"\n        AND country_code = 'DE'\n        AND grant_date >= 20161201\n        AND grant_date <= 20161231\n        GROUP BY application_number), UNNEST(cpc) AS cpcs\n    JOIN `spider2-public-data.patents.cpc_definition` cd ON cd.symbol = cpcs.code\n    WHERE cpcs.first = TRUE AND filing_date > 0)\n\nSELECT c.titleFull, cpc_group, best_year.filing_year\nFROM (\n    SELECT cpc_group, highest_moving_avg(ARRAY_AGG(STRUCT<filing_year INT64, cnt INT64>(filing_year, cnt) ORDER BY filing_year ASC)) AS best_year\n    FROM (\n        SELECT cpc_group, filing_year, COUNT(*) AS cnt\n        FROM (\n            SELECT cpc_parent AS cpc_group, filing_year\n            FROM patent_cpcs, UNNEST(parents) AS cpc_parent)\n        GROUP BY cpc_group, filing_year\n        ORDER BY filing_year DESC, cnt DESC)\n    GROUP BY cpc_group)\nJOIN `spider2-public-data.patents.cpc_definition` c ON cpc_group = c.symbol\nWHERE c.level = 4\nORDER BY titleFull, cpc_group ASC;",
        "external_knowledge": null,
        "plan": "1. Firstly, we create a temporary function to calculate the highest moving average of patent counts.\n2. Use a common table expression to extract CPC codes and filing years for patents granted in Germany between 2016-12-01 to 2016-12-31, including only the primary CPC code.\n3. Calculate the highest moving average of patent counts for each CPC group, record the filing year information.\n4. Combine the results with CPC definitions to get the full title and also return the results with CPC group and filing year.",
        "special_function": [
            "aggregate-functions/ANY_VALUE",
            "aggregate-functions/ARRAY_AGG",
            "conversion-functions/CAST",
            "mathematical-functions/FLOOR",
            "conditional-functions/IF",
            "other-functions/CREATE_FUNCTION",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq221",
        "db": "spider2-public-data.patents",
        "question": "Identify the CPC technology areas with the highest exponential moving average of patent filings each year (smoothing factor 0.2), and provide the full title and the best year for each CPC group at level 5.",
        "SQL": "CREATE TEMPORARY FUNCTION highest_moving_avg(yearcnt ARRAY<STRUCT<filing_year INT64, cnt INT64>>)\nRETURNS STRUCT<filing_year INT64, avg INT64>\nLANGUAGE js AS \"\"\"\n    let a = 0.2;\n    let avg = yearcnt.length > 0 ? yearcnt[0].cnt : 0;\n    let highest = {filing_year: -1, avg: -1};\n    for (let x of yearcnt) {\n        avg = a * x.cnt + (1 - a) * avg;\n        if (avg > highest.avg) {\n                highest = {filing_year: x.filing_year, avg: avg};\n        }\n    }\n    return highest;\n\"\"\";\n    \nWITH patent_cpcs AS (\n    SELECT\n        cd.parents,\n        CAST(FLOOR(filing_date/10000) AS INT64) AS filing_year\n    FROM (\n        SELECT\n            ANY_VALUE(cpc) AS cpc,\n            ANY_VALUE(filing_date) AS filing_date\n        FROM\n            `spider2-public-data.patents.publications`\n        WHERE \n            application_number != \"\"\n        GROUP BY\n            application_number\n        ), UNNEST(cpc) AS cpcs\n    JOIN\n        `spider2-public-data.patents.cpc_definition` cd\n    ON cd.symbol = cpcs.code\n    WHERE\n        cpcs.first = TRUE\n        AND filing_date > 0\n)\n\nSELECT c.titleFull, cpc_group, best_year.filing_year\nFROM (\n    SELECT\n        cpc_group,\n        highest_moving_avg(ARRAY_AGG(STRUCT<filing_year INT64, cnt INT64>(filing_year, cnt) ORDER BY filing_year ASC)) AS best_year\n    FROM (\n        SELECT\n            cpc_group,\n            filing_year,\n            COUNT(*) AS cnt\n        FROM (\n            SELECT\n                cpc_parent AS cpc_group,\n                filing_year\n            FROM\n                patent_cpcs,\n                UNNEST(parents) AS cpc_parent\n        )\n        GROUP BY cpc_group, filing_year\n        ORDER BY filing_year DESC, cnt DESC\n    )\n    GROUP BY cpc_group\n)\nJOIN `spider2-public-data.patents.cpc_definition` c\nON cpc_group = c.symbol\nWHERE c.level = 5\nORDER BY c.titleFull, cpc_group ASC;",
        "external_knowledge": null,
        "plan": "1. First, we define a temporary JavaScript function, which is used to calculate the year with the highest moving average of patent counts.\n2. Use common table expression to extract the CPC codes and filing years for patents, ensuring each patent has an application number and valid filing date. Also filter to include only the primary CPC code (`first = TRUE`).\n3. Calculate the most common patenting technology areas by year and identify the year with the highest moving average of patents for each CPC group. Concretely,\n- Aggregate patent counts by CPC group and filing year.\n- Use the defined function in Step 1. to find the year with the highest moving average of patents for each CPC group.\n- Join the results with the CPC definition table to get the full title for each CPC group. Remember, only include level 5 CPC groups.",
        "special_function": [
            "aggregate-functions/ANY_VALUE",
            "aggregate-functions/ARRAY_AGG",
            "conversion-functions/CAST",
            "mathematical-functions/FLOOR",
            "conditional-functions/IF",
            "other-functions/CREATE_FUNCTION",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq223",
        "db": "spider2-public-data.patents",
        "question": "Which assignees, excluding DENSO CORP itself, have cited patents assigned to DENSO CORP, and what are the titles of the primary CPC subclasses associated with these citations? Provide the name of each citing assignee, the full title of the CPC subclass, and the count of citations grouped by the assignee and the CPC subclass title. Please focus specifically on the main categories of the CPC codes,",
        "SQL": "SELECT\n    citing_assignee,\n    cpcdef.titleFull as cpc_title,\n    COUNT(*) as number\nFROM (\n    SELECT\n        pubs.publication_number AS citing_publication_number,\n        cite.publication_number AS cited_publication_number,\n        citing_assignee_s.name AS citing_assignee,\n        SUBSTR(cpcs.code, 0, 4) AS citing_cpc_subclass\n    FROM \n        `spider2-public-data.patents.publications` AS pubs,\n        UNNEST(citation) AS cite,\n        UNNEST(assignee_harmonized) AS citing_assignee_s,\n        UNNEST(cpc) AS cpcs\n    WHERE\n        cpcs.first = TRUE\n    ) AS pubs\n    JOIN (\n        SELECT\n            publication_number AS cited_publication_number,\n            cited_assignee_s.name AS cited_assignee\n        FROM\n            `spider2-public-data.patents.publications`,\n            UNNEST(assignee_harmonized) AS cited_assignee_s\n    ) AS refs\n    ON\n        pubs.cited_publication_number = refs.cited_publication_number\n    JOIN\n        `spider2-public-data.patents.cpc_definition` AS cpcdef\n    ON cpcdef.symbol = citing_cpc_subclass\nWHERE\n    cited_assignee = \"DENSO CORP\"\n    AND citing_assignee != \"DENSO CORP\"\nGROUP BY\n    citing_assignee, cpcdef.titleFull",
        "external_knowledge": "patents_info.md",
        "plan": "1. Firstly, extract citing publication and cited publication details, including only the primary CPC code.\n2. Combine the citing and cited data according the publication number.\n3. Join with CPC definition information to extract CPC subclass title.\n4. Add filter to include only relevant citations.\n5. Group the results by citing assignee and CPC full title, count the citations and order the result by count.\nWe only return the most frequent assignee and CPC subclass title.",
        "special_function": [
            "string-functions/SUBSTR",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq420",
        "db": "spider2-public-data.patents\nbigquery-public-data.uspto_oce_assignment\nbigquery-public-data.uspto_oce_cancer\nbigquery-public-data.uspto_oce_claims\nbigquery-public-data.uspto_oce_litigation\nbigquery-public-data.uspto_ptab",
        "question": "Can you identify the top 5 patents that were initially rejected under section 101 with no allowed claims, based on the length of their granted claims? The patents should have been granted in the US between 2010 and 2023. Additionally, ensure to select the first office action date for each application. Please include their first publication numbers, along with their first publication dates, length of the filed claims and grant dates.",
        "SQL": "WITH FIRST_OA AS (\n    SELECT\n        T1.app_id,\n        T1.mail_dt,\n        T1.ifw_number\n    FROM \n        `patents-public-data.uspto_oce_office_actions.office_actions` T1\n    INNER JOIN (\n        SELECT \n            app_id, \n            MIN(mail_dt) AS date\n        FROM \n            `patents-public-data.uspto_oce_office_actions.office_actions`\n        WHERE \n            rejection_101 = '1' AND \n            allowed_claims = '0'\n        GROUP BY \n            app_id\n    ) T2 ON T1.app_id = T2.app_id AND T1.mail_dt = T2.date\n    ORDER BY \n        app_id\n),\n\nGRNT AS (\n    SELECT\n        application_number,\n        grant_date\n    FROM \n        `spider2-public-data.patents.publications`\n    WHERE\n        grant_date > 20100000\n        AND grant_date < 20240000\n        AND country_code = 'US'\n),\n\nAS_FILED AS (\n    SELECT\n        PUB.application_number,\n        PUB.publication_number,\n        MIN(publication_date) AS first_pub_date,\n        claims.text AS filed_claims\n    FROM \n        `spider2-public-data.patents.publications` AS PUB,\n        UNNEST(claims_localized) AS claims\n    JOIN GRNT ON GRNT.application_number = PUB.application_number\n\n    GROUP BY \n        PUB.application_number, \n        PUB.publication_number, \n        claims.text\n)\n\nSELECT\n    AS_FILED.publication_number AS first_pub_no,\n    AS_FILED.first_pub_date,\n    LENGTH(AS_FILED.filed_claims) AS length_of_filed_claims,\n    GRNT.grant_date\nFROM \n    `patents-public-data.uspto_oce_office_actions.office_actions` AS OA\nINNER JOIN FIRST_OA ON OA.ifw_number = FIRST_OA.ifw_number\nJOIN `patents-public-data.uspto_oce_office_actions.match_app` AS APP ON OA.app_id = APP.app_id\nJOIN AS_FILED ON APP.application_number = AS_FILED.application_number\nJOIN GRNT ON APP.application_number = GRNT.application_number\nORDER BY \n    length_of_filed_claims DESC\nLIMIT 5;",
        "external_knowledge": null,
        "plan": "1. **Identify First Office Action Dates:**\n   - Extract records of office actions for each application.\n   - Filter to find the earliest office action date for applications that were initially rejected under a specific section and had no allowed claims.\n\n2. **Filter Granted Patents:**\n   - Select patents that were granted within a specified date range and in a specific country.\n\n3. **Get Initial Filed Claims:**\n   - From the granted patents, extract the first publication date and associated claims text for each application.\n\n4. **Get Granted Claims:**\n   - From the granted patents, extract the latest publication date and associated claims text for each application.\n\n5. **Combine Data:**\n   - Merge data on first office action dates, application numbers, initial filed claims, and granted claims.\n\n6. **Select and Sort Results:**\n   - From the combined data, select the first publication number, publication date of granted claims, length of granted claims, and grant date.\n   - Sort the results by the length of the granted claims in descending order.\n   - Limit the results to the top 5 entries.",
        "special_function": null
    },
    {
        "instance_id": "bq207",
        "db": "spider2-public-data.patents\nbigquery-public-data.uspto_oce_assignment\nbigquery-public-data.uspto_oce_cancer\nbigquery-public-data.uspto_oce_claims\nbigquery-public-data.uspto_oce_litigation\nbigquery-public-data.uspto_ptab",
        "question": "Can you provide the initial publication numbers for our top 100 independent patent claims with the highest word count?",
        "SQL": "WITH table_a AS(\n    SELECT \n        pat_no, claim_no, word_ct\n    FROM `patents-public-data.uspto_oce_claims.patent_claims_stats` \n    WHERE ind_flg='1'\n), matched_publn AS(\n    SELECT\n        publication_number,\n        claim_no,\n        CAST(word_ct AS INT64) AS word_ct  -- Cast word_ct to INT64 if it's stored as a string\n    FROM table_a\n    INNER JOIN `patents-public-data.uspto_oce_claims.match` USING(pat_no)\n), matched_appln AS(\n    SELECT\n        application_number appln_nr,\n        publication_number publn_nr,\n        claim_no,\n        word_ct\n    FROM matched_publn\n    INNER JOIN(\n        SELECT \n            publication_number, application_number, country_code,\n            ROW_NUMBER() OVER(PARTITION BY application_number ORDER BY publication_date ASC) row_num,\n            kind_code, publication_date\n        FROM `spider2-public-data.patents.publications`\n    ) USING(publication_number)\n    WHERE row_num=1\n)\n\nSELECT *\nFROM matched_appln\nORDER BY word_ct DESC\nLIMIT 100",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1.Select Independent Claims: Extract patent number, claim number, and word count for independent claims.\n2.Match Publication Numbers: Join with the match table to get the corresponding publication numbers.\n3.Match Application Numbers and Publication Information: Join with the publications table to get application details and select the earliest publication for each application.\n4.Select and Order Results: Retrieve all fields, order by word count in descending order, and limit to the top 100 records.",
        "special_function": [
            "conversion-functions/CAST",
            "json-functions/STRING",
            "numbering-functions/ROW_NUMBER",
            "timestamp-functions/STRING",
            "conditional-functions/IF"
        ]
    },
    {
        "instance_id": "bq128",
        "db": "spider2-public-data.patentsview",
        "question": "Tell me the patent title and abstract, as well as the publication date, the backward citation and forward citation count within 5 years for those published in January 2014. The detailed requirements are provided in `forward_backward_citation.md`.",
        "SQL": "SELECT\n    patent.title,\n    patent.abstract,\n    app.date as publication_date,\n    filterData.bkwdCitations,\n    filterData.fwrdCitations_5\nFROM\n    `spider2-public-data.patentsview.patent` as patent,\n    `spider2-public-data.patentsview.application` as app,\n    (\n        SELECT\n            DISTINCT cpc.patent_id,\n            IFNULL(citation_5.bkwdCitations, 0) as bkwdCitations,\n            IFNULL(citation_5.fwrdCitations_5, 0) as fwrdCitations_5\n        FROM\n            `spider2-public-data.patentsview.cpc_current` AS cpc\n        LEFT JOIN\n        (\n            SELECT  b.patent_id, b.bkwdCitations, f.fwrdCitations_5\n            FROM (\n                SELECT \n                    cited.citation_id as patent_id,\n                    IFNULL(COUNT(*), 0) as fwrdCitations_5\n                FROM \n                    `spider2-public-data.patentsview.uspatentcitation` AS cited,\n                    `spider2-public-data.patentsview.application` AS apps\n                WHERE\n                    apps.country = 'US'\n                    AND cited.citation_id = apps.patent_id \n                    AND cited.date >= apps.date\n                    AND SAFE_CAST(cited.date AS DATE) <= DATE_ADD(SAFE_CAST(apps.date AS DATE), INTERVAL 5 YEAR) -- cite in 5 year interval \n                GROUP BY cited.citation_id\n            ) AS f,\n            (\n                SELECT \n                    cited.patent_id,\n                    IFNULL(COUNT(*), 0) as bkwdCitations\n                FROM \n                    `spider2-public-data.patentsview.uspatentcitation` AS cited,\n                    `spider2-public-data.patentsview.application` AS apps\n                WHERE\n                    apps.country = 'US'\n                    AND cited.patent_id = apps.patent_id \n                    AND cited.date < apps.date -- get all backward citation count\n                GROUP BY cited.patent_id\n            ) AS b\n            WHERE\n                b.patent_id = f.patent_id\n                AND b.bkwdCitations IS NOT NULL\n                AND f.fwrdCitations_5 IS NOT NULL\n        ) AS citation_5 \n        ON cpc.patent_id = citation_5.patent_id\n        WHERE (\n            cpc.subsection_id IN ('C05', 'C06', 'C07', 'C08', 'C09', 'C10', 'C11', 'C12', 'C13')\n            OR cpc.group_id in ('A01G', 'A01H', 'A61K', 'A61P', 'A61Q', 'B01F', 'B01J', 'B81B', 'B82B', 'B82Y','G01N', 'G16H')\n        )\n    )\n    as filterData\nWHERE\n    app.patent_id = filterData.patent_id\n    AND app.patent_id = patent.id \n    AND SAFE_CAST(app.date AS DATE) < '2014-02-01' \n    AND SAFE_CAST(app.date AS DATE) >= '2014-01-01'",
        "external_knowledge": "forward_backward_citation.md",
        "plan": "1. **Select Required Fields**: Begin by selecting the necessary fields, including the patent title, abstract, publication date, backward citations, and forward citations within a 5-year window.\n\n2. **Join Patent and Application Data**: Join the patent data with application data to ensure each patent's title and abstract are associated with its publication date.\n\n3. **Filter by Publication Date**: Filter the results to include only those patents published in January 2014 by checking the publication date.\n\n4. **Subquery for Citation Counts**:\n   - **Backward Citations**: Count the number of times each patent was cited by others before its publication date.\n   - **Forward Citations**: Count the number of times each patent was cited by others within a 5-year period after its publication date.\n\n5. **Join Citation Data**: Use a subquery to retrieve the citation counts and join this data to associate backward and forward citation counts with each patent.\n\n6. **Filter by Classification Codes**: Apply additional filters to select patents based on specific classification codes or groups, focusing on relevant technological categories.\n\n7. **Finalize and Return Results**: Combine all filters and joins to finalize the query, ensuring that only the desired patents and their citation information are returned based on the specified criteria.",
        "special_function": null
    },
    {
        "instance_id": "bq246",
        "db": "spider2-public-data.patentsview",
        "question": "Can you figure out the number of forward citations within 1 years from the application date for the patent that has the most backward citations within 1 years from application among all U.S. patents?",
        "SQL": "SELECT filterData.fwrdCitations_3\nFROM\n  `spider2-public-data.patentsview.application` as app,\n  (SELECT DISTINCT cpc.patent_id, IFNULL(citation_3.bkwdCitations_3, 0) as bkwdCitations_3, IFNULL(citation_3.fwrdCitations_3, 0) as fwrdCitations_3\n  FROM\n    `spider2-public-data.patentsview.cpc_current` AS cpc\n    LEFT JOIN\n    (SELECT  b.patent_id, b.bkwdCitations_3, f.fwrdCitations_3\n      FROM \n        (SELECT \n          cited.patent_id,\n          COUNT(*) as fwrdCitations_3\n          FROM \n          `spider2-public-data.patentsview.uspatentcitation` AS cited,\n          `spider2-public-data.patentsview.application` AS apps\n        WHERE\n          apps.country = 'US'\n          AND cited.patent_id = apps.patent_id \n          AND cited.date >= apps.date AND SAFE_CAST(cited.date AS DATE) <= DATE_ADD(SAFE_CAST(apps.date AS DATE), INTERVAL 1 YEAR)\n         GROUP BY \n         cited.patent_id) AS f,\n\n       (SELECT \n          cited.patent_id,\n          COUNT(*) as bkwdCitations_3\n          FROM \n          `spider2-public-data.patentsview.uspatentcitation` AS cited,\n          `spider2-public-data.patentsview.application` AS apps\n        WHERE\n          apps.country = 'US'\n          AND cited.patent_id = apps.patent_id \n          AND cited.date < apps.date AND SAFE_CAST(cited.date AS DATE) >= DATE_SUB(SAFE_CAST(apps.date AS DATE), INTERVAL 1 YEAR)\n         GROUP BY \n         cited.patent_id) AS b\n      WHERE\n      b.patent_id = f.patent_id AND b.bkwdCitations_3 IS NOT NULL AND f.fwrdCitations_3 IS NOT NULL) AS citation_3\n      ON cpc.patent_id = citation_3.patent_id\n      )\n  as filterData\n  WHERE\n  app.patent_id = filterData.patent_id\n  ORDER BY filterData.bkwdCitations_3 DESC\n  LIMIT 1",
        "external_knowledge": null,
        "plan": "1. Correlate patent applications with patent citations based on patent identifiers. Ensure that only patents registered in the US are considered.\n2. Compute citations where the citation date is between the application date and three years after.\n3. Compute citations where the citation date is between three years prior to the application date and the application date itself.\n4. Join the information about current patent classifications with the aggregated citation data.\n5. Merge the detailed citation data with the patent applications data.\n6. Sort the resulting data set by the count of backward citations in descending order and restrict the output to the top record to find the patent with the highest count of backward citations within the specified period.\n7. From the final sorted and limited dataset, extract the count of forward citations for the patent with the highest number of backward citations. \n",
        "special_function": [
            "conversion-functions/SAFE_CAST",
            "date-functions/DATE",
            "date-functions/DATE_ADD",
            "date-functions/DATE_SUB",
            "conditional-functions/IFNULL"
        ]
    },
    {
        "instance_id": "bq052",
        "db": "spider2-public-data.patentsview",
        "question": "I wonder which patents within CPC subsection 'C05' or group 'A01G' in the USA have at least one forward or backward citations within one month of their application dates. Give me the ids, titles, application date, forward/backward citation counts and summary texts.",
        "SQL": "SELECT\n    app.patent_id as patent_id,\n    patent.title,\n    app.date as application_date,\n    filterData.bkwdCitations_1,\n    filterData.fwrdCitations_1,\n    summary.text as summary_text\nFROM\n    `spider2-public-data.patentsview.brf_sum_text` as summary,\n    `spider2-public-data.patentsview.patent` as patent,\n    `spider2-public-data.patentsview.application` as app,\n    (\n        SELECT DISTINCT\n            cpc.patent_id,\n            IFNULL(citation_1.bkwdCitations_1, 0) as bkwdCitations_1,\n            IFNULL(citation_1.fwrdCitations_1, 0) as fwrdCitations_1\n        FROM\n            `spider2-public-data.patentsview.cpc_current` AS cpc\n        JOIN\n        (\n            SELECT  b.patent_id, b.bkwdCitations_1, f.fwrdCitations_1\n            FROM (\n                SELECT \n                    cited.patent_id,\n                    COUNT(*) as fwrdCitations_1\n                FROM \n                    `spider2-public-data.patentsview.uspatentcitation` AS cited,\n                    `spider2-public-data.patentsview.application` AS apps\n                WHERE\n                    apps.country = 'US'\n                    AND cited.patent_id = apps.patent_id \n                    AND cited.date >= apps.date\n                    AND SAFE_CAST(cited.date AS DATE) <= DATE_ADD(SAFE_CAST(apps.date AS DATE), INTERVAL 1 MONTH)\n                GROUP BY \n                    cited.patent_id\n            ) AS f,\n            (\n                SELECT \n                    cited.patent_id,\n                    COUNT(*) as bkwdCitations_1\n                FROM \n                    `spider2-public-data.patentsview.uspatentcitation` AS cited,\n                    `spider2-public-data.patentsview.application` AS apps\n                WHERE\n                    apps.country = 'US'\n                    AND cited.patent_id = apps.patent_id \n                    AND cited.date < apps.date AND SAFE_CAST(cited.date AS DATE) >= DATE_SUB(SAFE_CAST(apps.date AS DATE), INTERVAL 1 MONTH) -- get in one year interval \n                GROUP BY \n                    cited.patent_id\n            ) AS b\n            WHERE\n                b.patent_id = f.patent_id\n                AND b.bkwdCitations_1 IS NOT NULL\n                AND f.fwrdCitations_1 IS NOT NULL\n                AND (b.bkwdCitations_1 > 0 OR f.fwrdCitations_1 > 0)\n        ) AS citation_1 \n        ON cpc.patent_id=citation_1.patent_id\n        WHERE (\n            cpc.subsection_id = 'C05'\n            OR cpc.group_id = 'A01G'\n        )\n    ) as filterData\nWHERE\n    app.patent_id = filterData.patent_id\n    AND summary.patent_id = app.patent_id\n    AND app.patent_id = patent.id\nORDER BY application_date;",
        "external_knowledge": null,
        "plan": "1. Identify Relevant Patents:\n   - Select patents in the USA that belong to specific CPC categories (either a particular subsection or group).\n\n2. Citation Filtering:\n   - Determine patents with at least one forward or backward citation within one month of their application dates. Count these citations separately for forward and backward directions.\n\n3. Data Aggregation:\n   - For each relevant patent, gather the patent ID, title, application date, forward and backward citation counts, and summary text.\n\n4. Combine and Order Results:\n   - Join the relevant patent data with citation counts and summary texts, and then order the results by the application date.",
        "special_function": [
            "conversion-functions/SAFE_CAST",
            "date-functions/DATE",
            "date-functions/DATE_ADD",
            "date-functions/DATE_SUB",
            "conditional-functions/IFNULL"
        ]
    },
    {
        "instance_id": "bq036",
        "db": "spider2-public-data.github_repos",
        "question": "What was the average number of GitHub commits made per month in 2016 for repositories containing Python code?",
        "SQL": "WITH \nCTERepoCommits AS (\n  SELECT\n    repo_name,\n    committer.date,\n    t1.COMMIT,\n    CONCAT(CAST(EXTRACT(YEAR FROM TIMESTAMP_SECONDS(committer.time_sec)) AS STRING), '-', \n           LPAD(CAST(EXTRACT(MONTH FROM TIMESTAMP_SECONDS(committer.time_sec)) AS STRING), 2, '0')) AS YearMonth\n  FROM `spider2-public-data.github_repos.sample_commits` t1\n  WHERE EXTRACT(YEAR FROM TIMESTAMP_SECONDS(committer.time_sec)) = 2016\n),\nCTERepoLang AS (\n  SELECT\n    t2.repo_name,\n    l.name AS LangName\n  FROM `spider2-public-data.github_repos.languages` t2\n  LEFT JOIN UNNEST(t2.LANGUAGE) AS l\n)\n, MonthlyCommits AS (\n  SELECT\n    CTERepoCommits.YearMonth,\n    COUNT(*) AS CommitCnts\n  FROM CTERepoCommits\n  INNER JOIN CTERepoLang ON CTERepoCommits.repo_name = CTERepoLang.repo_name\n  WHERE CTERepoLang.LangName = 'Python'\n  GROUP BY CTERepoCommits.YearMonth\n  ORDER BY CTERepoCommits.YearMonth\n)\nSELECT\n  AVG(CommitCnts) AS AvgMonthlyCommits\nFROM MonthlyCommits;",
        "external_knowledge": null,
        "plan": "1. Find out the table containing all commit info: `bigquery-public-data.github_repos.commits`\n2. Filter out the commit info in 2020, and for each commit-repo pair save its YearMonth timestamp.\n3. Find `bigquery-public-data.github_repos.languages` table, and range the data in commit - language entries.\n4. Combine them to find the commits which contain Python language and were made in  2020. Group the data according to YearMonth and calculate the number of commits for each month.\n5. Calculate the average monthly number of commits.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "json-functions/STRING",
            "string-functions/CONCAT",
            "string-functions/LPAD",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/STRING",
            "timestamp-functions/TIMESTAMP_SECONDS",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq100",
        "db": "spider2-public-data.github_repos",
        "question": "Find out the most frequently used package in all Go source files.",
        "SQL": "WITH imports AS (\n  SELECT\n    id,\n    SPLIT(REGEXP_EXTRACT(content, r'import\\s*\\(([^)]*)\\)'), '\\n') AS lines\n  FROM\n    `spider2-public-data.github_repos.sample_contents`\n  WHERE\n    REGEXP_CONTAINS(content, r'import\\s*\\([^)]*\\)')\n),\ngo_files AS (\n  SELECT\n    id\n  FROM\n    `spider2-public-data.github_repos.sample_files`\n  WHERE\n    path LIKE '%.go'\n  GROUP BY\n    id\n),\nfiltered_imports AS (\n  SELECT\n    id,\n    line\n  FROM\n    imports, UNNEST(lines) AS line\n),\njoined_data AS (\n  SELECT\n    fi.line\n  FROM\n    filtered_imports fi\n  JOIN\n    go_files gf\n  ON\n    fi.id = gf.id\n)\nSELECT\n  REGEXP_EXTRACT(line, r'\"([^\"]+)\"') AS package\nFROM\n  joined_data\nGROUP BY\n  package\nHAVING\n  package IS NOT NULL\nORDER BY\n  COUNT(*) DESC\nLIMIT\n  1;",
        "external_knowledge": null,
        "plan": "1. Extract Imports from File Contents (imports CTE):\n- Selects id and lines containing imports from files where the content matches the regex pattern r'import\\s*\\([^)]*\\)'.\n- SPLIT(REGEXP_EXTRACT(content, r'import\\s*\\(([^)]*)\\)'), '\\n') extracts the content inside the import(...) block and splits it into individual lines.\n2. Filter Go Files (go_files CTE):\n- Selects the id of files that have paths ending with .go (Go source files).\n- Uses GROUP BY id to ensure unique file IDs.\n3. Unnest Lines of Imports (filtered_imports CTE): Unnests the lines from the imports CTE to get each line as a separate row, resulting in rows with id and line.\n4. Join Imports with Go Files (joined_data CTE): Joins the filtered_imports with go_files on the id to filter only those import lines that belong to Go files.\n5. Extract the Package (SELECT statement):\n- Extracts the imported package using REGEXP_EXTRACT(line, r'\"([^\"]+)\"').\n- Filters out NULL string.\n- Groups the results by package, counts the occurrences, orders by the count in descending order, and limits the result to the most frequently imported package.",
        "special_function": [
            "string-functions/REGEXP_CONTAINS",
            "string-functions/REGEXP_EXTRACT",
            "string-functions/SPLIT",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq101",
        "db": "spider2-public-data.github_repos",
        "question": "Identify the top 10 most frequently imported packages and their counts in Java source files.",
        "SQL": "SELECT\n  package,\n  COUNT(*) count\nFROM (\n  SELECT\n    REGEXP_EXTRACT(line, r' ([a-z0-9\\._]*)\\.') AS package,\n    id\n  FROM (\n    SELECT\n      SPLIT(content, '\\n') AS lines,\n      id\n    FROM\n      `spider2-public-data.github_repos.sample_contents`\n    WHERE\n      REGEXP_CONTAINS(content, r'import')\n      AND sample_path LIKE '%.java'\n  ), UNNEST(lines) AS line\n  WHERE\n    LEFT(line, 6) = 'import'\n  GROUP BY\n    package,\n    id\n)\nGROUP BY\n  package\nORDER BY\n  count DESC\nLIMIT\n  10;",
        "external_knowledge": null,
        "plan": "1. Filter java source files: select Java files containing the \"import\" keyword.\n- condition: sample_path LIKE '%.java' to filter Java files.\n- condition: REGEXP_CONTAINS(content, r'import') to ensure the file contains \"import\" statements.\n2. Split the file content into lines and extract the import lines.\n3. Extract the package names with regex expression `r'([a-z0-9\\._]*)\\.'` to capture the package names.\n4. Count the number of imports for each package using GROUP BY clause.\n5. Order the results by count in descending order and limits to the top 10 packages.",
        "special_function": [
            "string-functions/LEFT",
            "string-functions/REGEXP_CONTAINS",
            "string-functions/REGEXP_EXTRACT",
            "string-functions/SPLIT",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq182",
        "db": "spider2-public-data.github_repos\ngithubarchive.day\ngithubarchive.month\ngithubarchive.year",
        "question": "Which primary programming languages, determined by the highest number of bytes in each repository, have the sum of over 100 pull requests on January 18, 2023 in all its repositories?",
        "SQL": "SELECT language AS name, count\nFROM (\n  SELECT *\n  FROM (\n    SELECT\n      lang AS language,\n      y AS year,\n      q AS quarter,\n      type,\n      COUNT(*) AS count\n    FROM (\n      SELECT\n        a.type AS type,\n        b.lang AS lang,\n        a.y AS y,\n        a.q AS q\n      FROM (\n        SELECT\n          type,\n          EXTRACT(YEAR FROM created_at) AS y,\n          EXTRACT(QUARTER FROM created_at) AS q,\n          REGEXP_REPLACE(\n            repo.url,\n            r'https:\\/\\/github\\.com\\/|https:\\/\\/api\\.github\\.com\\/repos\\/',\n            ''\n          ) AS name\n        FROM `githubarchive.day.20230118`\n      ) a\n      JOIN (\n        SELECT\n          repo_name AS name,\n          lang\n        FROM (\n          SELECT\n            repo_name,\n            FIRST_VALUE(language) OVER (\n              PARTITION BY repo_name ORDER BY bytes DESC\n            ) AS lang\n          FROM (\n            SELECT repo_name, language.name AS language, language.bytes\n            FROM `spider2-public-data.github_repos.languages`,\n            UNNEST(language) AS language\n          )\n        )\n        WHERE lang IS NOT NULL\n        GROUP BY repo_name, lang\n      ) b\n      ON a.name = b.name\n    )\n    GROUP BY type, language, year, quarter\n    ORDER BY year, quarter, count DESC\n  )\n  WHERE count >= 100\n)\nWHERE type = 'PullRequestEvent';",
        "external_knowledge": null,
        "plan": "1. Extract event data from `githubarchive.day.20230118` db and get repo name from url.\n2. combine with the db `bigquery-public-data.github_repos.languages` to group the event data by the language of its repo.\n3. Filter to keep the data that has a type of 'PullRequestEvent' and a count of more than 100.",
        "special_function": [
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "navigation-functions/FIRST_VALUE",
            "string-functions/REGEXP_REPLACE",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "conditional-functions/IF",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq217",
        "db": "spider2-public-data.github_repos\ngithubarchive.day\ngithubarchive.month\ngithubarchive.year",
        "question": "How many pull requests in total were created in repositories that include JavaScript as one of their languages, considering data from January 18, 2023?",
        "SQL": "SELECT COUNT(*) AS total_pull_requests\nFROM (\n  SELECT\n    a.type AS type,\n    b.language AS lang,\n    a.y AS y,\n    a.q AS q\n  FROM (\n    SELECT\n      type,\n      EXTRACT(YEAR FROM created_at) AS y,\n      EXTRACT(QUARTER FROM created_at) AS q,\n      REGEXP_REPLACE(\n        repo.url,\n        r'https:\\/\\/github\\.com\\/|https:\\/\\/api\\.github\\.com\\/repos\\/',\n        ''\n      ) AS name\n    FROM `githubarchive.day.20230118`\n  ) a\n  JOIN (\n    SELECT\n      repo_name AS name,\n      language\n    FROM (\n      SELECT\n        repo_name,\n        language\n      FROM (\n        SELECT repo_name, language.name AS language, language.bytes\n        FROM `spider2-public-data.github_repos.languages`,\n        UNNEST(language) AS language\n      )\n    )\n    WHERE language = 'JavaScript'\n    GROUP BY repo_name, language\n  ) b\n  ON a.name = b.name\n)\nWHERE type = 'PullRequestEvent';",
        "external_knowledge": null,
        "plan": "1. **Subquery `a` - Extract and Transform Data from `githubarchive` Table:**\r\n   - **Select Fields:** Extract the `type`, the year (`y`), and the quarter (`q`) from the `created_at` column.\r\n   - **Regular Expression Replace:** Clean the `repo.url` to extract the repository name, removing specific URL patterns.\r\n   - **Data Source:** Pull data from the `githubarchive.day.20230118` table.\r\n\r\n2. **Subquery `b` - Filter and Aggregate Data from `github_repos.languages`:**\r\n   - **Nested Subquery:** \r\n     - **Unnest Languages:** Expand the `languages` array to individual rows.\r\n     - **Select Fields:** Extract `repo_name` and `language` details.\r\n   - **Filter Language:** Retain only rows where `language` is 'JavaScript'.\r\n   - **Group By:** Aggregate by `repo_name` and `language` to ensure unique combinations.\r\n\r\n3. **Join Subquery `a` and Subquery `b`:**\r\n   - **Join Condition:** Match `name` from subquery `a` with `name` from subquery `b`.\r\n\r\n4. **Filter by Event Type:**\r\n   - **Condition:** Retain only rows where `type` is 'PullRequestEvent'.\r\n\r\n5. **Count the Results:**\r\n   - **Aggregate Function:** Count the total number of resulting rows and alias the result as `total_pull_requests`.\r\n\r\n6. **Final Output:**\r\n   - **Select Statement:** Return the total count of pull request events for repositories where the language is 'JavaScript'.",
        "special_function": [
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "string-functions/REGEXP_REPLACE",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "conditional-functions/IF",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq191",
        "db": "spider2-public-data.github_repos\ngithubarchive.day\ngithubarchive.month\ngithubarchive.year",
        "question": "Find the top 2 repositories from 2017, which have more than 30 unique users watching them, that also contain the text 'Copyright (c)'.",
        "SQL": "WITH repos as (\n  SELECT b.repo_with_watches as repo_name,\n         b.watches as watches\n  FROM (\n    SELECT DISTINCT repo_name AS repo_in_mirror\n    FROM `spider2-public-data.github_repos.sample_files` \n  ) a RIGHT JOIN (\n    SELECT repo.name AS repo_with_watches, APPROX_COUNT_DISTINCT(actor.id) watches \n    FROM `githubarchive.year.2017` \n    WHERE type='WatchEvent'\n    GROUP BY 1 \n    HAVING watches > 300\n  ) b\n  ON a.repo_in_mirror = b.repo_with_watches\n  WHERE\n    a.repo_in_mirror IS NOT NULL\n),\ncontents as (\n  SELECT *\n  FROM (\n    SELECT DISTINCT *\n    FROM `spider2-public-data.github_repos.sample_files` \n    WHERE repo_name IN (SELECT repo_name FROM repos)\n  ) a RIGHT JOIN (\n    SELECT id as idcontent,\n           content as content\n    FROM `spider2-public-data.github_repos.sample_contents` \n  ) b\n  ON a.id = b.idcontent \n)\nSELECT repos.repo_name,\n       repos.watches\nFROM repos\nJOIN\n  contents\nON\n  repos.repo_name = contents.repo_name \nORDER BY\n  repos.watches DESC\nLIMIT 2",
        "external_knowledge": null,
        "plan": "1. **Define CTE `repos`:**\r\n   - **Subquery `a`:**\r\n     - Select distinct repository names from the `sample_files` table, assigning the result to `repo_in_mirror`.\r\n   - **Subquery `b`:**\r\n     - Select repository names and approximate count of distinct actors who performed 'WatchEvent' in 2017 from `githubarchive.year.2017`, filtering for repositories with more than 300 stars.\r\n   - **Join `a` and `b`:**\r\n     - Perform a RIGHT JOIN on `a` and `b` using `repo_in_mirror` and `repo_with_stars` to keep all repositories with stars from subquery `b`.\r\n     - Filter out rows where `repo_in_mirror` is NULL.\r\n   - **Result:**\r\n     - CTE `repos` contains repository names and their star counts for repositories with more than 300 stars and which exist in `sample_files`.\r\n\r\n2. **Define CTE `contents`:**\r\n   - **Subquery `a`:**\r\n     - Select distinct entries from `sample_files` where the repository name exists in the `repos` CTE.\r\n   - **Subquery `b`:**\r\n     - Select content ID and content from `sample_contents`.\r\n   - **Join `a` and `b`:**\r\n     - Perform a RIGHT JOIN on `a` and `b` using the content ID.\r\n   - **Result:**\r\n     - CTE `contents` contains all content entries linked to repositories listed in the `repos` CTE.\r\n\r\n3. **Final Query:**\r\n   - **Join `repos` and `contents`:**\r\n     - Join `repos` with `contents` on `repo_name`.\r\n   - **Filter Results:**\r\n     - Filter for rows where the content contains the string '%junit</artifactId>%'.\r\n     - Filter for rows where the path is 'pom.xml'.\r\n   - **Order and Limit Results:**\r\n     - Order the results by the number of stars in descending order.\r\n     - Limit the output to the top 5 rows.\r\n\r\n4. **Output:**\r\n   - Select repository names and star counts from the filtered and joined results.",
        "special_function": [
            "approximate-aggregate-functions/APPROX_COUNT_DISTINCT"
        ]
    },
    {
        "instance_id": "bq224",
        "db": "spider2-public-data.github_repos\ngithubarchive.day\ngithubarchive.month\ngithubarchive.year",
        "question": "Which repository with an approved license in `licenses.md` had the highest combined total of forks, issues, and watches in April 2022?",
        "SQL": "WITH allowed_repos as (\n    select\n        repo_name,\n        license\n    from `spider2-public-data.github_repos.licenses`\n    where license in unnest(\n      [\"gpl-3.0\", \"artistic-2.0\", \"isc\", \"cc0-1.0\", \"epl-1.0\", \"gpl-2.0\",\n       \"mpl-2.0\", \"lgpl-2.1\", \"bsd-2-clause\", \"apache-2.0\", \"mit\", \"lgpl-3.0\"])\n),\nwatch_counts as (\n    SELECT \n        repo.name as repo,\n        COUNT(DISTINCT actor.login) watches,\n    FROM `githubarchive.month.202204`\n    WHERE type = \"WatchEvent\"\n    GROUP BY repo\n),\nissue_counts as (\n    SELECT \n        repo.name as repo,\n        COUNT(*) issue_events,\n    FROM `githubarchive.month.202204`\n    WHERE type = 'IssuesEvent'\n    GROUP BY repo\n),\nfork_counts as (\n    SELECT \n        repo.name as repo,\n        COUNT(*) forks,\n    FROM `githubarchive.month.202204`\n    WHERE type = 'ForkEvent'\n    GROUP BY repo\n)\nSELECT repo_name\nFROM allowed_repos\nINNER JOIN fork_counts ON repo_name = fork_counts.repo\nINNER JOIN issue_counts on repo_name = issue_counts.repo\nINNER JOIN watch_counts ON repo_name = watch_counts.repo\nORDER BY forks + issue_events + watches DESC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "1. Filter only those repositories whose licenses are listed in the provided array of approved licenses.\n2. Select repository names and count distinct users who watched the repositories, that is to include only `WatchEvent` types.\n3. Similarly, select repository names and count the number of issues (`IssuesEvent`) and forks (`ForkEvent`).\n4. Combine the data and calculate the total counts of forks, issue events, and watches for each repository.\n5. Sort the results in descending order and only return the top repository name.",
        "special_function": [
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq192",
        "db": "spider2-public-data.github_repos",
        "question": "Which repository that has a license of either \"artistic-2.0\", \"isc\", \"mit\", or \"apache-2.0\", contains Python files in the master branch, and has the highest combined count of forks, issues, and watch events in April 2022?",
        "SQL": "WITH allowed_repos AS (\n    SELECT \n        repo_name, \n        license \n    FROM \n        `spider2-public-data.github_repos.licenses`\n    WHERE \n        license IN UNNEST([\"artistic-2.0\", \"isc\", \"mit\", \"apache-2.0\"])\n),\nwatch_counts AS (\n    SELECT \n        repo.name AS repo,\n        COUNT(DISTINCT actor.login) AS watches\n    FROM \n        `githubarchive.month.202204`\n    WHERE \n        type = \"WatchEvent\"\n    GROUP BY \n        repo\n),\nissue_counts AS (\n    SELECT \n        repo.name AS repo,\n        COUNT(*) AS issue_events\n    FROM \n        `githubarchive.month.202204`\n    WHERE \n        type = 'IssuesEvent'\n    GROUP BY \n        repo\n),\nfork_counts AS (\n    SELECT \n        repo.name AS repo,\n        COUNT(*) AS forks\n    FROM \n        `githubarchive.month.202204`\n    WHERE \n        type = 'ForkEvent'\n    GROUP BY \n        repo\n),\nmetadata AS (\n    SELECT \n        repo_name, \n        license, \n        forks, \n        issue_events, \n        watches\n    FROM \n        allowed_repos\n    INNER JOIN \n        fork_counts \n    ON \n        repo_name = fork_counts.repo\n    INNER JOIN \n        issue_counts \n    ON \n        repo_name = issue_counts.repo\n    INNER JOIN \n        watch_counts \n    ON \n        repo_name = watch_counts.repo\n),\ngithub_files_at_head AS (\n    SELECT \n        repo_name\n    FROM \n        `spider2-public-data.github_repos.sample_files`\n    WHERE \n        ref = \"refs/heads/master\" \n        AND ENDS_WITH(path, \".py\")\n        AND symlink_target IS NULL\n    GROUP BY \n        repo_name\n)\nSELECT \n    metadata.repo_name AS repository\nFROM \n    metadata\nINNER JOIN \n    github_files_at_head\nON \n    metadata.repo_name = github_files_at_head.repo_name\nORDER BY \n    (metadata.forks + metadata.issue_events + metadata.watches) DESC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "1. **Define Allowed Repos:**\r\n   - Create a Common Table Expression (CTE) named `allowed_repos` to filter repositories that have specific licenses (`artistic-2.0`, `isc`, `mit`, `apache-2.0`).\r\n   - Select `repo_name` and `license` from the `bigquery-public-data.github_repos.licenses` table where the license matches one of the specified values.\r\n\r\n2. **Calculate Watch Counts:**\r\n   - Create a CTE named `watch_counts` to calculate the number of unique watch events per repository.\r\n   - Select the repository name and count the distinct `actor.login` from the `githubarchive.month.202204` table where the event type is `WatchEvent`.\r\n   - Group by repository name.\r\n\r\n3. **Calculate Issue Counts:**\r\n   - Create a CTE named `issue_counts` to calculate the number of issue events per repository.\r\n   - Select the repository name and count the total number of events from the `githubarchive.month.202204` table where the event type is `IssuesEvent`.\r\n   - Group by repository name.\r\n\r\n4. **Calculate Fork Counts:**\r\n   - Create a CTE named `fork_counts` to calculate the number of fork events per repository.\r\n   - Select the repository name and count the total number of events from the `githubarchive.month.202204` table where the event type is `ForkEvent`.\r\n   - Group by repository name.\r\n\r\n5. **Combine Metadata:**\r\n   - Create a CTE named `metadata` to combine the information from `allowed_repos`, `fork_counts`, `issue_counts`, and `watch_counts`.\r\n   - Perform INNER JOIN operations between `allowed_repos` and the other CTEs (`fork_counts`, `issue_counts`, and `watch_counts`) on `repo_name`.\r\n   - Select `repo_name`, `license`, `forks`, `issue_events`, and `watches` from the combined data.\r\n\r\n6. **Identify Repos with Python Files:**\r\n   - Create a CTE named `github_files_at_head` to identify repositories that have Python files at the head of the master branch.\r\n   - Select `repo_name` from the `bigquery-public-data.github_repos.sample_files` table where `ref` is `\"refs/heads/master\"`, the file path ends with `.py`, and `symlink_target` is `NULL`.\r\n   - Group by `repo_name`.\r\n\r\n7. **Select Top Repository:**\r\n   - Perform an INNER JOIN between `metadata` and `github_files_at_head` on `repo_name`.\r\n   - Select the `repo_name` from `metadata` as `repository`.\r\n   - Order the results by the sum of `forks`, `issue_events`, and `watches` in descending order.\r\n   - Limit the result to the top repository.\r\n\r\n8. **Return Result:**\r\n   - The final query returns the repository name of the top repository with the highest combined count of forks, issue events, and watches among those that have Python files at the head of the master branch and have an allowed license.",
        "special_function": [
            "string-functions/ENDS_WITH",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq225",
        "db": "spider2-public-data.github_repos",
        "question": "What's the top 10 widely used languages according to file counts?",
        "SQL": "WITH languages AS (\n  SELECT\n    files.id,\n    CASE REGEXP_EXTRACT(files.path, r'(\\.?[^\\/\\.]*)$')\n      WHEN '.js'          THEN 'JavaScript'\n      WHEN '.cjs'         THEN 'JavaScript'\n      WHEN '.ts'          THEN 'TypeScript'\n      WHEN '.java'        THEN 'Java'\n      WHEN '.py'          THEN 'Python'\n      WHEN '.kt'          THEN 'Kotlin'\n      WHEN '.ktm'         THEN 'Kotlin'\n      WHEN '.kts'         THEN 'Kotlin'\n      WHEN '.c'           THEN 'C'\n      WHEN '.h'           THEN 'C'\n      WHEN '.c++'         THEN 'C++'\n      WHEN '.cpp'         THEN 'C++'\n      WHEN '.h++'         THEN 'C++'\n      WHEN '.hpp'         THEN 'C++'\n      WHEN '.cs'          THEN 'C#'\n      WHEN '.erl'         THEN 'Erlang'\n      WHEN '.ex'          THEN 'Elixir'\n      WHEN '.exs'         THEN 'Elixir'\n      WHEN '.hs'          THEN 'Haskell'\n      WHEN '.go'          THEN 'Go'\n      WHEN '.php'         THEN 'PHP'\n      WHEN '.rb'          THEN 'Ruby'\n      WHEN '.rs'          THEN 'Rust'\n      WHEN '.scala'       THEN 'Scala'\n      WHEN '.swift'       THEN 'Swift'\n      WHEN '.lisp'        THEN 'Common Lisp'\n      WHEN '.clj'         THEN 'Clojure'\n      WHEN '.r'           THEN 'R'\n      WHEN '.matlab'      THEN 'MATLAB'\n      WHEN '.m'           THEN 'MATLAB'\n      WHEN '.asm'         THEN 'Assembly'\n      WHEN '.nasm'        THEN 'Assembly'\n      WHEN '.d'           THEN 'D'\n      WHEN '.dart'        THEN 'Dart'\n      WHEN '.jl'          THEN 'Julia'\n      WHEN '.groovy'      THEN 'Groovy'\n      WHEN '.hx'          THEN 'Haxe'\n      WHEN '.lua'         THEN 'Lua'\n      WHEN '.sh'          THEN 'Shell'\n      WHEN '.bash'        THEN 'Shell'\n      WHEN '.ps1'         THEN 'PowerShell'\n      WHEN '.psd1'        THEN 'PowerShell'\n      WHEN '.psm1'        THEN 'PowerShell'\n      WHEN '.sql'         THEN 'SQL'\n      WHEN 'Dockerfile'   THEN 'Dockerfile'\n      WHEN '.dockerfile'  THEN 'Dockerfile'\n      WHEN '.md'          THEN 'Markdown'\n      WHEN '.markdown'    THEN 'Markdown'\n      WHEN '.mdown'       THEN 'Markdown'\n      WHEN '.html'        THEN 'HTML'\n      WHEN '.htm'         THEN 'HTML'\n      WHEN '.css'         THEN 'CSS'\n      WHEN '.sass'        THEN 'Sass'\n      WHEN '.scss'        THEN 'SCSS'\n      WHEN '.vue'         THEN 'Vue'\n      WHEN '.json'        THEN 'JSON'\n      WHEN '.yml'         THEN 'YAML'\n      WHEN '.yaml'        THEN 'YAML'\n      WHEN '.xml'         THEN 'XML'\n    END AS language\n  FROM\n    `spider2-public-data.github_repos.sample_files` AS files\n)\n\nSELECT\n  languages.language\nFROM\n  languages\nINNER JOIN\n  `spider2-public-data.github_repos.sample_contents` AS contents\nON\n  contents.id = languages.id\nWHERE\n  languages.language IS NOT NULL\n  AND contents.content IS NOT NULL\nGROUP BY languages.language\nORDER BY COUNT(*) DESC\nLIMIT 10;",
        "external_knowledge": "lang_and_ext.md",
        "plan": "1. Extract languages for each file according to file extensions.\n    - Determine the programming language based on the file extension extracted from the `path` field of the `files` table.\n    - Use the `REGEXP_EXTRACT` function to extract the file extension.\n    - Use a `CASE` statement to map file extensions to their respective languages based on the provided document `lang_and_ext.md`.\n2. Perform an inner join between the `languages` CTE and the `sample_contents` table on the `id` column. This join ensures that only files which have a corresponding content entry are considered.\n3. Filter out rows where `language` or `content` is `NULL`. This ensures that only valid and meaningful entries are included in the final result.\n4. Group and count by languages, and limit the final results to the top 3 entries.",
        "special_function": [
            "string-functions/REGEXP_EXTRACT"
        ]
    },
    {
        "instance_id": "bq180",
        "db": "spider2-public-data.github_repos",
        "question": "Please help me retrieve the top 5 most frequently used module names from Python and R scripts.",
        "SQL": "SELECT module, COUNT(*) as occurrence_count\nFROM (\n  SELECT \n    file_id, \n    repo_name, \n    path, \n    line, \n    ARRAY_CONCAT(\n      IF(\n        ENDS_WITH(path, '.py'),\n        ARRAY_CONCAT(\n          REGEXP_EXTRACT_ALL(line, r'\\bimport\\s+(\\w+)'), \n          REGEXP_EXTRACT_ALL(line, r'\\bfrom\\s+(\\w+)')\n        ),\n        []\n      ),\n      IF(\n        ENDS_WITH(path, '.r'),\n        REGEXP_EXTRACT_ALL(line, r'library\\s*\\(\\s*([^\\s)]+)\\s*\\)'),\n        []\n      )\n    ) AS modules\n  FROM (\n    SELECT\n        ct.id AS file_id, \n        fl.repo_name, \n        path, \n        SPLIT(REPLACE(ct.content, \"\\n\", \" \\n\"), \"\\n\") AS lines\n    FROM `spider2-public-data.github_repos.sample_files` AS fl\n    JOIN `spider2-public-data.github_repos.sample_contents` AS ct ON fl.id = ct.id\n  ), UNNEST(lines) as line\n  WHERE\n    (ENDS_WITH(path, '.py') AND (REGEXP_CONTAINS(line, r'^import ') OR REGEXP_CONTAINS(line, r'^from '))) OR \n    (ENDS_WITH(path, '.r') AND REGEXP_CONTAINS(line, r'library\\s*\\('))\n),\nUNNEST(modules) as module\nGROUP BY module\nORDER BY occurrence_count DESC\nLIMIT 5",
        "external_knowledge": null,
        "plan": "1. Get all the sample github file data and unnest the lines. Record the file path for each line.\n2. Extract the module names from the \"import\" and \"from\" statements of Python files, and the \"library(...)\" lines of R files.\n3. Count the number of occurences for each module and limit to the top 10 most used ones.",
        "special_function": [
            "array-functions/ARRAY_CONCAT",
            "string-functions/ENDS_WITH",
            "string-functions/REGEXP_CONTAINS",
            "string-functions/REGEXP_EXTRACT_ALL",
            "string-functions/REPLACE",
            "string-functions/SPLIT",
            "conditional-functions/IF",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq233",
        "db": "spider2-public-data.github_repos",
        "question": "Can you find the imported Python modules and R libraries from the GitHub sample files and list them along with their occurrence counts? Please sort the results by language and then by the number of occurrences in descending order.",
        "SQL": "WITH extracted_modules AS (\n  SELECT \n    file_id, \n    repo_name, \n    path, \n    line, \n    IF(\n      ENDS_WITH(path, '.py'),\n      'python',\n      IF(ENDS_WITH(path, '.r'), 'r', NULL)\n    ) AS language,\n    IF(\n      ENDS_WITH(path, '.py'),\n      ARRAY_CONCAT(\n        REGEXP_EXTRACT_ALL(line, r'\\bimport\\s+(\\w+)'), \n        REGEXP_EXTRACT_ALL(line, r'\\bfrom\\s+(\\w+)')\n      ),\n      IF(\n        ENDS_WITH(path, '.r'),\n        REGEXP_EXTRACT_ALL(line, r'library\\s*\\(\\s*([^\\s)]+)\\s*\\)'),\n        []\n      )\n    ) AS modules\n  FROM (\n    SELECT\n      ct.id AS file_id, \n      fl.repo_name, \n      path, \n      SPLIT(REPLACE(ct.content, \"\\n\", \" \\n\"), \"\\n\") AS lines\n    FROM `spider2-public-data.github_repos.sample_files` AS fl\n    JOIN `spider2-public-data.github_repos.sample_contents` AS ct ON fl.id = ct.id\n  ), UNNEST(lines) as line\n  WHERE\n    (ENDS_WITH(path, '.py') AND (REGEXP_CONTAINS(line, r'^import ') OR REGEXP_CONTAINS(line, r'^from '))) OR \n    (ENDS_WITH(path, '.r') AND REGEXP_CONTAINS(line, r'library\\s*\\('))\n),\nmodule_counts AS (\n  SELECT \n    language,\n    module,\n    COUNT(*) AS occurrence_count\n  FROM (\n    SELECT \n      language,\n      modules\n    FROM extracted_modules\n    WHERE modules IS NOT NULL\n  ),UNNEST(modules) AS module\n  GROUP BY language, module\n),\ntop5_python AS (\n  SELECT \n    'python' AS language,\n    module,\n    occurrence_count\n  FROM module_counts\n  WHERE language = 'python'\n  ORDER BY occurrence_count DESC\n),\ntop5_r AS (\n  SELECT \n    'r' AS language,\n    module,\n    occurrence_count\n  FROM module_counts\n  WHERE language = 'r'\n  ORDER BY occurrence_count DESC\n)\nSELECT * FROM top5_python\nUNION ALL\nSELECT * FROM top5_r\nORDER BY language, occurrence_count DESC;",
        "external_knowledge": null,
        "plan": "1. **Extract File Information:**\r\n   - Join `sample_files` and `sample_contents` tables on `id` to get `file_id`, `repo_name`, `path`, and `content`.\r\n   - Split `content` into individual lines, treating each line as a separate record.\r\n\r\n2. **Identify and Extract Module Imports:**\r\n   - Create a CTE (`extracted_modules`) to filter and process lines containing import statements for Python (`import` or `from`) and R (`library`).\r\n   - For Python files (`.py`):\r\n     - Extract modules imported using `import` and `from` statements.\r\n     - Use `REGEXP_EXTRACT_ALL` to extract module names and concatenate results.\r\n   - For R files (`.r`):\r\n     - Extract modules imported using `library()` statements.\r\n     - Use `REGEXP_EXTRACT_ALL` to extract module names.\r\n   - Store extracted modules along with file information and detected language (Python or R).\r\n\r\n3. **Count Module Occurrences:**\r\n   - Create another CTE (`module_counts`) to count occurrences of each module per language.\r\n   - Unnest the modules array to have one module per row.\r\n   - Group by `language` and `module` and count occurrences.\r\n\r\n4. **Select Top 5 Modules for Python:**\r\n   - Create a CTE (`top5_python`) to select the top 5 most frequently used Python modules.\r\n   - Filter `module_counts` for `python` language.\r\n   - Order by `occurrence_count` in descending order.\r\n   - Limit results to 5.\r\n\r\n5. **Select Top 5 Modules for R:**\r\n   - Create a CTE (`top5_r`) to select the top 5 most frequently used R modules.\r\n   - Filter `module_counts` for `r` language.\r\n   - Order by `occurrence_count` in descending order.\r\n   - Limit results to 5.\r\n\r\n6. **Combine Results and Order:**\r\n   - Use `UNION ALL` to combine results from `top5_python` and `top5_r`.\r\n   - Order the final results by `language` and `occurrence_count` in descending order to display the top modules for each language.",
        "special_function": [
            "array-functions/ARRAY_CONCAT",
            "string-functions/ENDS_WITH",
            "string-functions/REGEXP_CONTAINS",
            "string-functions/REGEXP_EXTRACT_ALL",
            "string-functions/REPLACE",
            "string-functions/SPLIT",
            "conditional-functions/IF",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq248",
        "db": "spider2-public-data.github_repos",
        "question": "What is the proportion of files whose paths include 'readme.md' that contain the phrase 'Copyright (c)', among all repositories that do not use any programming language with 'python' in its name",
        "SQL": "WITH requests AS (\n    SELECT \n        D.id,\n        D.content,\n        E.repo_name,\n        E.path\n    FROM \n        (\n            SELECT \n                id,\n                content\n            FROM \n                `spider2-public-data.github_repos.sample_contents`\n            GROUP BY \n                id,\n                content\n        ) AS D\n        INNER JOIN \n        (\n            SELECT \n                C.id,\n                C.repo_name,\n                C.path\n            FROM \n                (\n                    SELECT \n                        id,\n                        repo_name,\n                        path\n                    FROM \n                        `spider2-public-data.github_repos.sample_files`\n                    WHERE \n                        LOWER(path) LIKE '%readme.md'\n                    GROUP BY \n                        path,\n                        id,\n                        repo_name\n                ) AS C\n            INNER JOIN \n                (\n                    SELECT \n                        repo_name,\n                        language_struct.name AS language_name\n                    FROM \n                        `spider2-public-data.github_repos.languages`,\n                        UNNEST(language) AS language_struct\n                    WHERE \n                        LOWER(language_struct.name) NOT LIKE '%python%'\n                    GROUP BY \n                        language_name,\n                        repo_name\n                ) AS F\n            ON \n                C.repo_name = F.repo_name\n        ) AS E\n    ON \n        D.id = E.id\n)\nSELECT (SELECT COUNT(*) FROM requests WHERE content LIKE '%Copyright (c)%') / COUNT(*) AS proportion\nFROM requests",
        "external_knowledge": null,
        "plan": "1. Get all ids and contents from table sample_contents.\n2. Extract ids, names and paths of github repositories which have requirements.txt.\n3. Extract names of github repositories whose language_name contains python.\n4. Identify the repositories whose requirements.txt contains \u2018requests\u2019.\n5. Count the number of python repositories whose requirements.txt contains \u2018requests\u2019.\n6. Count the number of python repositories which contain requirements.txt.\n7. Divide the above statistics to get the corresponding proportion.\n",
        "special_function": [
            "string-functions/LOWER",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq193",
        "db": "spider2-public-data.github_repos",
        "question": "Help me retrieve the top 5 most frequently occurring non-empty, non-commented lines of text in `readme.md` files from GitHub repositories that primarily use Python for development.",
        "SQL": "WITH content_extracted AS (\n    SELECT \n        D.id AS id,\n        repo_name,\n        path,\n        SPLIT(content, '\\n') AS lines\n    FROM \n        (\n            SELECT \n                id,\n                content\n            FROM \n                `spider2-public-data.github_repos.sample_contents`\n        ) AS D\n    INNER JOIN \n        (\n            SELECT \n                id,\n                C.repo_name AS repo_name,\n                path\n            FROM \n                (\n                    SELECT \n                        id,\n                        repo_name,\n                        path\n                    FROM \n                        `spider2-public-data.github_repos.sample_files`\n                    WHERE \n                        LOWER(path) LIKE '%readme.md'\n                ) AS C\n            INNER JOIN \n                (\n                    SELECT \n                        repo_name,\n                        language_struct.name AS language_name\n                    FROM \n                        (\n                            SELECT \n                                repo_name, \n                                language\n                            FROM \n                                `spider2-public-data.github_repos.languages`\n                        )\n                    CROSS JOIN \n                        UNNEST(language) AS language_struct\n                    WHERE \n                        LOWER(language_struct.name) LIKE '%python%'\n                ) AS F\n            ON \n                C.repo_name = F.repo_name\n        ) AS E\n    ON \n        E.id = D.id\n),\nnon_empty_lines AS (\n    SELECT \n        line\n    FROM \n        content_extracted,\n        UNNEST(lines) AS line\n    WHERE \n        TRIM(line) != ''\n        AND NOT STARTS_WITH(TRIM(line), '#')\n        AND NOT STARTS_WITH(TRIM(line), '//')\n),\nline_frequencies AS (\n    SELECT \n        line,\n        COUNT(*) AS frequency\n    FROM \n        non_empty_lines\n    GROUP BY \n        line\n    ORDER BY \n        frequency DESC\n)\nSELECT \n    line\nFROM \n    line_frequencies\nLIMIT 5;",
        "external_knowledge": null,
        "plan": "1. **Extract Content and Metadata**:\n   - Start by retrieving the repository content and metadata from the repository data source.\n   - Filter to include only files whose paths indicate they are `requirements.txt`.\n\n2. **Filter for Python Repositories**:\n   - Retrieve the list of repositories that primarily use Python for development.\n   - Join this list with the filtered files to ensure only files from Python repositories are considered.\n\n3. **Split File Content into Lines**:\n   - For each `requirements.txt` file, split its content into individual lines.\n\n4. **Filter Out Empty Lines**:\n   - Remove any lines that are empty or consist only of whitespace.\n\n5. **Calculate Line Frequencies**:\n   - Count the occurrences of each non-empty line across all the files.\n   - Sort these lines by their frequency of occurrence in descending order.\n\n6. **Retrieve the Third Most Frequent Line**:\n   - Select the line that is the third most frequently occurring from the sorted list.",
        "special_function": [
            "string-functions/LOWER",
            "string-functions/SPLIT",
            "string-functions/STARTS_WITH",
            "string-functions/TRIM",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq295",
        "db": "spider2-public-data.github_repos",
        "question": "Among the repositories from the GitHub Archive which include a Python file with less than 15,000 bytes in size and a keyword 'def' in the content, find the top 3 that have the highest number of watch events in 2017?",
        "SQL": "WITH watched_repos AS (\n    SELECT\n        repo.name AS repo\n    FROM \n        `githubarchive.month.2017*`\n    WHERE\n        type = \"WatchEvent\"\n),\nrepo_watch_counts AS (\n    SELECT\n        repo,\n        COUNT(*) AS watch_count\n    FROM\n        watched_repos\n    GROUP BY\n        repo\n)\n\nSELECT\n    r.repo,\n    r.watch_count\nFROM\n    `spider2-public-data.github_repos.sample_files` AS f\nJOIN\n    `spider2-public-data.github_repos.sample_contents` AS c\nON\n    f.id = c.id\nJOIN \n    repo_watch_counts AS r\nON\n    f.repo_name = r.repo\nWHERE\n    f.path LIKE '%.py' \n    AND c.size < 15000 \n    AND REGEXP_CONTAINS(c.content, r'def ')\nGROUP BY\n    r.repo, r.watch_count\nORDER BY\n    r.watch_count DESC\nLIMIT \n    3;",
        "external_knowledge": null,
        "plan": "1. **Common Table Expression (CTE) - `watched_repos`**:\n    - **Objective**: Extract repository names that have been watched.\n    - **Action**: \n        - Select `repo.name` as `repo` from the dataset `githubarchive.month.2017*` where the `type` is `\"WatchEvent\"`.\n\n2. **Common Table Expression (CTE) - `repo_watch_counts`**:\n    - **Objective**: Calculate the watch count for each repository.\n    - **Action**:\n        - Select `repo` and `COUNT(*)` as `watch_count` from the `watched_repos` CTE.\n        - Group the results by `repo` to aggregate watch counts.\n\n3. **Main Query - Data Source**:\n    - **Objective**: Fetch Python files with specific characteristics from GitHub repositories.\n    - **Action**:\n        - Select from `bigquery-public-data.github_repos.sample_files` as `f` and `bigquery-public-data.github_repos.sample_contents` as `c`.\n        - Join `f` and `c` on `f.id = c.id`.\n\n4. **Main Query - Additional Join**:\n    - **Objective**: Incorporate the watch counts into the main query.\n    - **Action**:\n        - Join the results from the previous step with `repo_watch_counts` as `r` on `f.repo_name = r.repo`.\n\n5. **Main Query - Filtering**:\n    - **Objective**: Filter the joined data to match specific criteria for Python files.\n    - **Action**:\n        - Apply a filter to select files where `f.path` ends with `.py`.\n        - Ensure the content size `c.size` is less than 15000 bytes.\n        - Check if the content `c.content` contains the pattern 'def ' using `REGEXP_CONTAINS`.\n\n6. **Main Query - Grouping and Ordering**:\n    - **Objective**: Prepare the final result set with grouping and sorting.\n    - **Action**:\n        - Group the results by `r.repo` and `r.watch_count`.\n        - Order the results by `r.watch_count` in descending order.\n\n7. **Main Query - Limiting Results**:\n    - **Objective**: Restrict the output to the top 3 repositories.\n    - **Action**:\n        - Use `LIMIT 3` to return only the top 3 repositories based on watch count.\n\n8. **Final Output**:\n    - **Objective**: Present the result.\n    - **Action**:\n        - Select and display the repository names and their corresponding watch counts.",
        "special_function": [
            "string-functions/REGEXP_CONTAINS"
        ]
    },
    {
        "instance_id": "bq249",
        "db": "spider2-public-data.github_repos",
        "question": "Please provide a report on the number files from the GitHub repository, categorized by the presence of specific line types. Categorize a file as 'trailing' if any line ends with a blank character, as 'Space' if any line starts with a space, and as 'Other' if it meets neither condition.",
        "SQL": "WITH\n  lines AS (\n    SELECT\n      SPLIT(content, '\\\\n') AS line,\n      id\n    FROM\n      `spider2-public-data.github_repos.sample_contents`\n)\nSELECT\n  Indentation,\n  COUNT(Indentation) AS number_of_occurence\nFROM (\n  SELECT\n    CASE\n        WHEN MIN(CHAR_LENGTH(REGEXP_EXTRACT(flatten_line, r\"\\s+$\")))>=1 THEN 'trailing'\n        WHEN MIN(CHAR_LENGTH(REGEXP_EXTRACT(flatten_line, r\"^ +\")))>=1 THEN 'Space'\n        ELSE 'Other'\n    END AS Indentation\n  FROM\n    lines\n  CROSS JOIN\n    UNNEST(lines.line) AS flatten_line\n  GROUP BY\n    id)\nGROUP BY\n  Indentation\nORDER BY\n  number_of_occurence DESC",
        "external_knowledge": null,
        "plan": "1. Filter records from a dataset where the file path ends with `.sql`.\n2. Use the `SPLIT` function to divide the content of each file into individual lines. This transformation creates an array of lines for each file.\n3. Each element (line) of the array is processed individually using the `CROSS JOIN UNNEST` technique. This flattens the array into a table format where each line is treated as a separate record.\n4. For each line, determine its type according to the definition of three types.\n5. Count how many lines fall into each indentation category ('trailing', 'Space', 'Other') across all files.\n6. Order the results by the count of occurrences in descending order.\n",
        "special_function": [
            "string-functions/CHAR_LENGTH",
            "string-functions/REGEXP_EXTRACT",
            "string-functions/SPLIT",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq375",
        "db": "spider2-public-data.github_repos",
        "question": "Determine which file type among Python (.py), C (.c), Jupyter Notebook (.ipynb), Java (.java), and JavaScript (.js) in the GitHub codebase has the most files with a directory depth greater than 10, and provide the file count.",
        "SQL": "WITH files_with_levels AS (\n  SELECT\n    files.path AS path,\n    LENGTH(files.path) - LENGTH(REPLACE(files.path, '/', '')) AS dir_level,\n    CASE\n      WHEN REGEXP_CONTAINS(files.path, r'\\.py$') THEN 'Python'\n      WHEN REGEXP_CONTAINS(files.path, r'\\.c$') THEN 'C'\n      WHEN REGEXP_CONTAINS(files.path, r'\\.ipynb$') THEN 'Jupyter Notebook'\n      WHEN REGEXP_CONTAINS(files.path, r'\\.java$') THEN 'Java'\n      WHEN REGEXP_CONTAINS(files.path, r'\\.js$') THEN 'JavaScript'\n      ELSE 'Other'\n    END AS file_type\n  FROM\n    `spider2-public-data.github_repos.sample_files` AS files\n  WHERE\n    REGEXP_CONTAINS(files.path, r'\\.py$') OR\n    REGEXP_CONTAINS(files.path, r'\\.c$') OR\n    REGEXP_CONTAINS(files.path, r'\\.ipynb$') OR\n    REGEXP_CONTAINS(files.path, r'\\.java$') OR\n    REGEXP_CONTAINS(files.path, r'\\.js$')\n)\nSELECT\n  file_type,\n  COUNT(*) AS file_count\nFROM\n  files_with_levels\nWHERE\n  dir_level > 10\nGROUP BY\n  file_type\nORDER BY\n  file_count DESC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "1. **Identify Files and Calculate Directory Depth**:\n   - Select all relevant files that have extensions of interest (Python, C, Jupyter Notebook, Java, JavaScript).\n   - Calculate the directory depth for each file by counting the number of slashes in the file path.\n\n2. **Classify File Types**:\n   - For each file, determine its type based on the file extension and assign a corresponding label (e.g., Python, C, Jupyter Notebook, Java, JavaScript).\n\n3. **Filter by Directory Depth**:\n   - Filter the files to include only those with a directory depth greater than 10.\n\n4. **Aggregate and Count Files by Type**:\n   - Group the filtered files by their type and count the number of files in each group.\n\n5. **Sort and Limit Results**:\n   - Sort the file counts in descending order.\n   - Select the file type with the highest count.\n\n6. **Output**:\n   - Return the file type with the highest number of files and the corresponding file count.",
        "special_function": [
            "string-functions/LENGTH",
            "string-functions/REGEXP_CONTAINS",
            "string-functions/REPLACE",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq255",
        "db": "spider2-public-data.github_repos",
        "question": "How many commit messages are there in repositories that use the 'Shell' programming language and 'apache-2.0' license, where the length of the commit message is more than 5 characters but less than 10,000 characters, and the messages do not start with the word 'merge', 'update' or 'test'?",
        "SQL": "SELECT\n  COUNT(commits_table.message) AS num_messages\nFROM (\n  SELECT\n    repo_name,\n    lang.name AS language_name\n  FROM\n    `spider2-public-data.github_repos.languages` AS lang_table,\n    UNNEST(LANGUAGE) AS lang) lang_table\nJOIN\n  `spider2-public-data.github_repos.licenses` AS license_table\nON\n  license_table.repo_name = lang_table.repo_name\nJOIN (\n  SELECT\n    *\n  FROM\n    `spider2-public-data.github_repos.sample_commits`) commits_table\nON\n  commits_table.repo_name = lang_table.repo_name\nWHERE\n    (license_table.license LIKE 'apache-2.0')\n  AND (lang_table.language_name LIKE 'Shell')\nAND\n  LENGTH(commits_table.message) > 5\nAND \n  LENGTH(commits_table.message) < 10000\nAND LOWER(commits_table.message) NOT LIKE 'update%'\nAND LOWER(commits_table.message) NOT LIKE 'test%'\nAND LOWER(commits_table.message) NOT LIKE 'merge%';",
        "external_knowledge": null,
        "plan": "1. Retrieve repositories which have an 'apache-2.0' license.\n2. Retrieve repositories which have 'Shell' as one of its languages.\n3. Only keep messages whose length is greater than 5 and less than 10000.\n4. Remove messages containing \u2018update\u2019, \u2018test\u2019 or \u2018merge%\u2019.\n5. Count the selected messages and return.\n",
        "special_function": [
            "string-functions/LENGTH",
            "string-functions/LOWER",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq194",
        "db": "spider2-public-data.github_repos",
        "question": "What is the second most frequently used module (imported library) across Python, R, and IPython script (.ipynb) files in the GitHub sample dataset?",
        "SQL": "WITH extracted_modules AS (\n  SELECT\n    file_id, repo_name, path, line,\n    IF(\n      ENDS_WITH(path, '.py'),\n      'Python',\n      IF(\n        (\n          ENDS_WITH(path, '.r') OR\n          ENDS_WITH(path, '.R') OR\n          ENDS_WITH(path, '.Rmd') OR\n          ENDS_WITH(path, '.rmd')\n        ),\n        'R',\n        IF(\n          ENDS_WITH(path, '.ipynb'),\n          'IPython',\n          'Others'\n        )\n      )\n    ) AS script_type,\n    IF(\n      ENDS_WITH(path, '.py'),\n      IF(\n        REGEXP_CONTAINS(line, r'^\\s*import\\s+'),\n        REGEXP_EXTRACT_ALL(line, r'(?:^\\s*import\\s|,)\\s*([a-zA-Z0-9\\_\\.]+)'),\n        REGEXP_EXTRACT_ALL(line, r'^\\s*from\\s+([a-zA-Z0-9\\_\\.]+)')\n      ),\n      IF(\n        (\n          ENDS_WITH(path, '.r') OR\n          ENDS_WITH(path, '.R') OR\n          ENDS_WITH(path, '.Rmd') OR\n          ENDS_WITH(path, '.rmd')\n        ),\n        REGEXP_EXTRACT_ALL(line, r'library\\s*\\((?:package=|)[\\\"\\']*([a-zA-Z0-9\\_\\.]+)[\\\"\\']*.*?\\)'), -- we're still ignoring commented out imports\n        IF(\n          ENDS_WITH(path, '.ipynb'),\n          IF(\n            REGEXP_CONTAINS(line, r'\"\\s*import\\s+'),\n            REGEXP_EXTRACT_ALL(line, r'(?:\"\\s*import\\s|,)\\s*([a-zA-Z0-9\\_\\.]+)'),\n            REGEXP_EXTRACT_ALL(line, r'\"\\s*from\\s+([a-zA-Z0-9\\_\\.]+)')\n          ),\n          ['']\n        )\n      )\n    ) AS modules\n  FROM (\n    SELECT\n      ct.id AS file_id, repo_name, path,\n      # Add a space after each line.\n      # It is required to ensure correct line numbering.\n      SPLIT(REPLACE(content, \"\\n\", \" \\n\"), \"\\n\") AS lines\n    FROM `spider2-public-data.github_repos.sample_files` AS fl\n    JOIN `spider2-public-data.github_repos.sample_contents` AS ct ON fl.id = ct.id\n    WHERE\n      ENDS_WITH(path, '.py') OR\n      (\n        ENDS_WITH(path, '.r') OR\n        ENDS_WITH(path, '.R') OR\n        ENDS_WITH(path, '.Rmd') OR\n        ENDS_WITH(path, '.rmd')\n      ) OR\n      ENDS_WITH(path, '.ipynb')\n  ), UNNEST(lines) AS line\n  WHERE\n    (ENDS_WITH(path, '.py') AND (REGEXP_CONTAINS(line, r'^\\s*import\\s+') OR REGEXP_CONTAINS(line, r'^\\s*from .* import '))) OR\n    (\n      (\n        ENDS_WITH(path, '.r') OR\n        ENDS_WITH(path, '.R') OR\n        ENDS_WITH(path, '.Rmd') OR\n        ENDS_WITH(path, '.rmd')\n      ) AND REGEXP_CONTAINS(line, r'library\\s*\\(')\n    ) OR\n    (\n      ENDS_WITH(path, '.ipynb') AND\n      (\n        REGEXP_CONTAINS(line, r'\"\\s*import\\s+') OR\n        REGEXP_CONTAINS(line, r'\"\\s*from .* import ')\n      )\n    )\n), unnested_modules AS (\n  SELECT\n    file_id, repo_name, path, script_type, module\n  FROM extracted_modules,\n  UNNEST(modules) AS module\n), module_frequencies AS (\n  SELECT\n    module,\n    script_type,\n    COUNT(*) AS frequency\n  FROM unnested_modules\n  GROUP BY module, script_type\n  ORDER BY frequency DESC\n)\n\nSELECT\n  module\nFROM module_frequencies\nORDER BY frequency DESC\nLIMIT 1\nOFFSET 1;",
        "external_knowledge": null,
        "plan": "1. **Define CTE `extracted_modules`:**\r\n   - **Select Columns:**\r\n     - `file_id`, `repo_name`, `path`, `line`\r\n     - Determine `script_type` based on file extension:\r\n       - `.py` -> 'Python'\r\n       - `.r`, `.R`, `.Rmd`, `.rmd` -> 'R'\r\n       - `.ipynb` -> 'IPython'\r\n       - Others -> 'Others'\r\n     - Extract `modules` using regex based on `script_type`:\r\n       - For Python files, extract modules from `import` and `from` statements.\r\n       - For R files, extract modules from `library()` statements.\r\n       - For IPython notebooks, handle `import` and `from` statements within double quotes.\r\n   - **From Subquery:**\r\n     - Join `bigquery-public-data.github_repos.sample_files` and `bigquery-public-data.github_repos.sample_contents` on `id`.\r\n     - Select `file_id`, `repo_name`, `path`, and split `content` into `lines`.\r\n   - **Filter Lines:**\r\n     - For Python files, include lines containing `import` or `from ... import`.\r\n     - For R files, include lines containing `library()`.\r\n     - For IPython notebooks, include lines containing `import` or `from ... import` within double quotes.\r\n\r\n2. **Define CTE `unnested_modules`:**\r\n   - **Select Columns:**\r\n     - `file_id`, `repo_name`, `path`, `script_type`, `module`\r\n   - **From `extracted_modules`:**\r\n     - Unnest `modules` to get individual `module` names.\r\n\r\n3. **Define CTE `module_frequencies`:**\r\n   - **Select Columns:**\r\n     - `module`\r\n     - `script_type`\r\n     - Count occurrences of each `module` and `script_type` combination as `frequency`\r\n   - **From `unnested_modules`:**\r\n     - Group by `module`, `script_type`\r\n     - Order by `frequency` in descending order.\r\n\r\n4. **Final Select:**\r\n   - **Select Column:**\r\n     - `module`\r\n   - **From `module_frequencies`:**\r\n     - Order by `frequency` in descending order.\r\n     - Limit result to the second most frequent module (using `LIMIT 1 OFFSET 1`).",
        "special_function": [
            "string-functions/ENDS_WITH",
            "string-functions/REGEXP_CONTAINS",
            "string-functions/REGEXP_EXTRACT_ALL",
            "string-functions/REPLACE",
            "string-functions/SPLIT",
            "conditional-functions/IF",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq377",
        "db": "spider2-public-data.github_repos",
        "question": "Extract and count the frequency of all package names listed in the require section of JSON-formatted content",
        "SQL": "WITH json_files AS (\n  SELECT\n    c.id,\n    JSON_EXTRACT(c.content, '$.require') AS dependencies\n  FROM\n    `spider2-public-data.github_repos.sample_contents` c\n),\npackage_names AS (\n  SELECT\n    TRIM(REGEXP_REPLACE(entry, r'^\"|\"$', '')) AS package_name\n  FROM\n    json_files,\n    UNNEST(REGEXP_EXTRACT_ALL(CAST(dependencies AS STRING), r'\"([^\"]+)\":')) AS entry\n)\nSELECT\n  package_name,\n  COUNT(*) AS count\nFROM\n  package_names\nWHERE\n  package_name IS NOT NULL\nGROUP BY\n  package_name\nORDER BY\n  count DESC;",
        "external_knowledge": null,
        "plan": "1. **Extract JSON Content**: For each identified file, extract the JSON content that includes the dependency information. This step involves parsing the JSON structure to isolate the relevant section containing dependencies.\n\n2. **Isolate Dependency Entries**: Within the extracted JSON content, isolate the individual entries that represent the dependencies. This involves using regular expressions to extract key-value pairs from the JSON structure.\n\n3. **Clean Package Names**: Once the dependency entries are isolated, further process the extracted strings to clean and standardize the package names. This involves removing extraneous characters such as quotes.\n\n4. **Filter Valid Entries**: Ensure that only valid package names are considered by filtering out any null or invalid entries.\n\n5. **Count Occurrences**: Group the cleaned package names and count the occurrences of each unique package name across all files.\n\n6. **Sort Results**: Finally, sort the counted results in descending order to identify the most frequently occurring package names.\n\nThis step-by-step plan ensures that the original instruction of extracting, cleaning, counting, and sorting package names from specific JSON files is comprehensively addressed.",
        "special_function": null
    },
    {
        "instance_id": "bq359",
        "db": "spider2-public-data.github_repos",
        "question": "List the repository names and commit counts for the top two GitHub repositories with JavaScript as the primary language and the highest number of commits.",
        "SQL": "WITH python_repo AS (\n    WITH\n        repositories AS (\n        SELECT\n        t2.repo_name,\n        t2.LANGUAGE\n        FROM (\n        SELECT\n            repo_name,\n            LANGUAGE,\n            RANK() OVER (PARTITION BY t1.repo_name ORDER BY t1.language_bytes DESC) AS rank\n        FROM (\n            SELECT\n            repo_name,\n            arr.name AS LANGUAGE,\n            arr.bytes AS language_bytes\n            FROM\n            `spider2-public-data.github_repos.languages`,\n            UNNEST(LANGUAGE) arr ) AS t1 ) AS t2\n        WHERE\n        rank = 1)\n    SELECT\n        repo_name,\n        LANGUAGE\n    FROM\n        repositories\n    WHERE\n        LANGUAGE = 'JavaScript'\n        )\nSELECT sc.repo_name, COUNT(commit) AS num_commits FROM `spider2-public-data.github_repos.sample_commits` sc\nINNER JOIN python_repo ON python_repo.repo_name = sc.repo_name\nGROUP BY sc.repo_name\nORDER BY num_commits \nDESC LIMIT 2",
        "external_knowledge": null,
        "plan": "1. **Identify Primary Language of Repositories:**\n   - Extract repository names and their associated languages.\n   - Rank the languages for each repository based on the size of code written in that language.\n   - Filter to retain only the primary language (the one with the highest code size) for each repository.\n\n2. **Filter for Python Repositories:**\n   - From the list of repositories with their primary languages, select only those repositories where the primary language is Python.\n\n3. **Join with Commit Data:**\n   - Join the filtered list of Python repositories with the commit data to associate each repository with its commits.\n\n4. **Count Commits per Repository:**\n   - For each repository, count the total number of commits.\n\n5. **Order and Limit Results:**\n   - Order the repositories by the count of commits in descending order.\n   - Limit the result to the top five repositories with the highest number of commits.\n\n6. **Select Required Fields:**\n   - From the final ordered list, select and display the repository names and their corresponding commit counts.",
        "special_function": [
            "numbering-functions/RANK",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq252",
        "db": "spider2-public-data.github_repos",
        "question": "Could you please find the name of the repository that contains the most copied non-binary Swift file in the dataset, ensuring each file is uniquely identified by its ID?",
        "SQL": "WITH selected_repos as (\n  SELECT\n    f.id,\n    f.repo_name as repo_name,\n    f.path as path,\n  FROM\n    `spider2-public-data.github_repos.sample_files` as f\n),\n\ndeduped_files as (\n  SELECT\n    f.id,\n    MIN(f.repo_name) as repo_name,\n    MIN(f.path) as path,\n  FROM\n    selected_repos as f\n  GROUP BY\n    f.id\n)\n\nSELECT\n  f.repo_name,\nFROM\n  deduped_files as f\n  JOIN `spider2-public-data.github_repos.sample_contents` as c on f.id = c.id\nWHERE\n  NOT c.binary\n  AND f.path like '%.swift'\nORDER BY c.copies DESC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "1. Get file details and remove potential duplicates by file id.\n2. Filter the results to include only non-binary files with a `.swift` extension.\n3. Sort and return the name of the file which has the highest number of copies.",
        "special_function": null
    },
    {
        "instance_id": "bq251",
        "db": "spider2-public-data.pypi",
        "question": "Could you find the GitHub URL of the Python package that has the highest number of downloads on PyPi and was updated most recently? Please ensure that only the main repository URL is provided, excluding specific subsections like issues, blobs, pull requests, or tree views.",
        "SQL": "WITH PyPiData AS (\n    SELECT\n        name AS pypi_name,\n        version AS pypi_version,\n        home_page,\n        download_url,\n        project_urls,\n        requires,\n        upload_time\n    FROM\n        `spider2-public-data.pypi.distribution_metadata`\n),\n\nGitHubURLs AS (\n    SELECT\n        pypi_name,\n        pypi_version,\n        REGEXP_REPLACE(REGEXP_EXTRACT(url, r'(https?://github\\.com/[^/]+/[^/?#]+)'), r'(/issues.*)|(blob/.*)|(/pull/.*)|(tree/.*)', '') AS github_url,\n        upload_time\n    FROM\n        PyPiData,\n        UNNEST(project_urls) AS url\n    WHERE\n        url LIKE '%github.com%'\n        AND (url LIKE '%https://github.com/%' OR url LIKE '%http://github.com/%')\n),\n\nMostRecentVersions AS (\n    SELECT\n        pypi_name,\n        pypi_version,\n        github_url\n    FROM (\n        SELECT *,\n               ROW_NUMBER() OVER (PARTITION BY pypi_name ORDER BY upload_time DESC) AS rn\n        FROM GitHubURLs\n    )\n    WHERE rn = 1\n),\n\nDownloadMetrics AS (\n    SELECT\n        project,\n        COUNT(*) AS pypi_downloads\n    FROM\n        `spider2-public-data.pypi.file_downloads`\n    GROUP BY\n        project\n)\n\nSELECT\n    mv.github_url\nFROM\n    MostRecentVersions mv\nLEFT JOIN\n    DownloadMetrics dm ON mv.pypi_name = dm.project\nWHERE\n    mv.github_url IS NOT NULL \n    AND dm.pypi_downloads IS NOT NULL\nORDER BY dm.pypi_downloads DESC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "1. Extract metadata including pypi_name, project_urls for PyPi packages.\n2. Filter the project_urls from PyPiData to extract clean GitHub repository URLs.\n3. Get the most recent version of each PyPi package based on the upload_time.\n4. Compute total downloads, yearly downloads for the past three years, and capture the earliest and latest download timestamps.\n5. Retrieve the number of watchers for each GitHub repository.\n6. Calculate the total bytes of code written in Python and the total bytes of code across all languages to get language data.\n7. Sort all data collected according to pypi_downloads and return the most downloaded package URL.\n",
        "special_function": [
            "numbering-functions/ROW_NUMBER",
            "string-functions/REGEXP_EXTRACT",
            "string-functions/REGEXP_REPLACE",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq019",
        "db": "bigquery-public-data.cms_medicare\nbigquery-public-data.cms_codes\nbigquery-public-data.cms_synthetic_patient_data_omop",
        "question": "For the most common inpatient diagnosis in the US in 2014, what was the citywise average payment respectively in the three cities that had the most cases from the cms_medicare database?",
        "SQL": "WITH most_common_drg AS (\n  SELECT\n    drg_definition,\n    SUM(total_discharges) AS national_num_cases\n  FROM\n    `bigquery-public-data.cms_medicare.inpatient_charges_2014`\n  GROUP BY\n    drg_definition\n  ORDER BY\n    national_num_cases DESC\n  LIMIT\n    1\n),\ncity_data AS (\n  SELECT\n    drg_definition,\n    provider_city,\n    provider_state,\n    SUM(total_discharges) AS citywise_num_cases,\n    SUM(average_total_payments * total_discharges) / SUM(total_discharges) AS citywise_avg_total_payments,\n    SUM(average_total_payments * total_discharges) AS citywise_sum_total_payments\n  FROM\n    `bigquery-public-data.cms_medicare.inpatient_charges_2014`\n  GROUP BY\n    drg_definition,\n    provider_city,\n    provider_state\n),\nranked_city_data AS (\n  SELECT\n    cd.drg_definition,\n    cd.provider_city,\n    cd.provider_state,\n    cd.citywise_avg_total_payments,\n    RANK() OVER (PARTITION BY cd.drg_definition ORDER BY cd.citywise_num_cases DESC) AS cityrank\n  FROM\n    city_data cd\n  WHERE\n    cd.drg_definition = (SELECT drg_definition FROM most_common_drg)\n)\nSELECT\n  drg_definition AS Diagnosis,\n  provider_city AS City,\n  provider_state AS State,\n  cityrank AS City_Rank,\n  CAST(ROUND(citywise_avg_total_payments) AS INT64) AS Citywise_Avg_Payments,\nFROM\n  (SELECT\n    drg_definition,\n    provider_city,\n    provider_state,\n    cityrank,\n    citywise_avg_total_payments\n  FROM\n    ranked_city_data\n  WHERE\n    cityrank <= 3)  # Limit to top 3 cities for the most common diagnosis\nORDER BY\n  cityrank;",
        "external_knowledge": null,
        "plan": "1. Decide which table to work on:  `bigquery-public-data.cms_medicare.inpatient_charges_2014`\n2. Rank all the diagnostic conditions by national number of cases to find the most common diagnostic condition.\n3. Group the data by diagnostic condition name, city and state, and calculate the citywise number of cases, citywise average payment for each entry.\n4. Accordingly, rank the cities by citywise number of cases for the most common diagnostic condition, and calculate the national avg. payments.\n5. Limit to the top 3 cities.",
        "special_function": [
            "conversion-functions/CAST",
            "mathematical-functions/ROUND",
            "numbering-functions/RANK"
        ]
    },
    {
        "instance_id": "bq234",
        "db": "bigquery-public-data.cms_medicare\nbigquery-public-data.cms_codes\nbigquery-public-data.cms_synthetic_patient_data_omop",
        "question": "What is the most prescribed medication in each state in 2014?",
        "SQL": "SELECT\n  A.state,\n  drug_name,\n  -- total_claim_count,\n  -- day_supply,\n  -- ROUND(total_cost_millions) AS total_cost_millions\nFROM (\n  SELECT\n    generic_name AS drug_name,\n    nppes_provider_state AS state,\n    ROUND(SUM(total_claim_count)) AS total_claim_count,\n    ROUND(SUM(total_day_supply)) AS day_supply,\n    ROUND(SUM(total_drug_cost)) / 1e6 AS total_cost_millions\n  FROM\n    `bigquery-public-data.cms_medicare.part_d_prescriber_2014`\n  GROUP BY\n    state,\n    drug_name) A\nINNER JOIN (\n  SELECT\n    state,\n    MAX(total_claim_count) AS max_total_claim_count\n  FROM (\n    SELECT\n      nppes_provider_state AS state,\n      ROUND(SUM(total_claim_count)) AS total_claim_count\n    FROM\n      `bigquery-public-data.cms_medicare.part_d_prescriber_2014`\n    GROUP BY\n      state,\n      generic_name)\n  GROUP BY\n    state) B\nON\n  A.state = B.state\n  AND A.total_claim_count = B.max_total_claim_count;\n-- ORDER BY\n--   A.total_claim_count DESC;",
        "external_knowledge": null,
        "plan": "1. Decide which table to work on:  `bigquery-public-data.cms_medicare.part_d_prescriber_2014`\n2. Group the data by drug name and state, and calculate the total claim count for each drug in each state.\n3. For each state, find out the max claim count number among different drugs. (most prescribed)\n4. List out corresponding most prescribed drug name for each state. (don\u2018t need to be in order",
        "special_function": [
            "mathematical-functions/ROUND"
        ]
    },
    {
        "instance_id": "bq235",
        "db": "bigquery-public-data.cms_medicare\nbigquery-public-data.cms_codes\nbigquery-public-data.cms_synthetic_patient_data_omop",
        "question": "Can you tell me which healthcare provider incurs the highest combined average costs for both outpatient and inpatient services in 2014?",
        "SQL": "SELECT\n  Provider_Name\nFROM\n(\nSELECT\n  OP.provider_state AS State,\n  OP.provider_city AS City,\n  OP.provider_id AS Provider_ID,\n  OP.provider_name AS Provider_Name,\n  ROUND(OP.average_OP_cost) AS Average_OP_Cost,\n  ROUND(IP.average_IP_cost) AS Average_IP_Cost,\n  ROUND(OP.average_OP_cost + IP.average_IP_cost) AS Combined_Average_Cost\nFROM (\n  SELECT\n    provider_state,\n    provider_city,\n    provider_id,\n    provider_name,\n    SUM(average_total_payments*outpatient_services)/SUM(outpatient_services) AS average_OP_cost\n  FROM\n    `bigquery-public-data.cms_medicare.outpatient_charges_2014`\n  GROUP BY\n    provider_state,\n    provider_city,\n    provider_id,\n    provider_name ) AS OP\nINNER JOIN (\n  SELECT\n    provider_state,\n    provider_city,\n    provider_id,\n    provider_name,\n    SUM(average_medicare_payments*total_discharges)/SUM(total_discharges) AS average_IP_cost\n  FROM\n    `bigquery-public-data.cms_medicare.inpatient_charges_2014`\n  GROUP BY\n    provider_state,\n    provider_city,\n    provider_id,\n    provider_name ) AS IP\nON\n  OP.provider_id = IP.provider_id\n  AND OP.provider_state = IP.provider_state\n  AND OP.provider_city = IP.provider_city\n  AND OP.provider_name = IP.provider_name\nORDER BY\n  combined_average_cost DESC\nLIMIT\n  1\n);",
        "external_knowledge": null,
        "plan": "1. find out the corresponding table as described in the instruction `bigquery-public-data.cms_medicare.outpatient_charges_2014` and `bigquery-public-data.cms_medicare.inpatient_charges_2014`\r\n2. Group the outpatient charges data by different medical provider, and calculate the average outpatient cost per service.\r\n3. Do the same to inpatient charges data.\r\n4. Combine them to calculate the sum of two average costs for each medical provider.\r\n5. Rank them to find the peak. (the highest one)",
        "special_function": [
            "mathematical-functions/ROUND"
        ]
    },
    {
        "instance_id": "bq172",
        "db": "bigquery-public-data.cms_medicare\nbigquery-public-data.cms_codes\nbigquery-public-data.cms_synthetic_patient_data_omop",
        "question": "For the drug with the highest total number of prescriptions in New York State during 2014, could you list the top five states with the highest total claim counts for this drug? Please also include their total claim counts and total drug costs. ",
        "SQL": "WITH ny_top_drug AS (\n  SELECT\n    drug_name AS drug_name,\n    ROUND(SUM(total_claim_count)) AS total_claim_count\n  FROM\n    `bigquery-public-data.cms_medicare.part_d_prescriber_2014`\n  WHERE\n    nppes_provider_state = 'NY'\n  GROUP BY\n    drug_name\n  ORDER BY\n    total_claim_count DESC\n  LIMIT 1\n),\ntop_5_states AS (\n  SELECT\n    nppes_provider_state AS state,\n    SUM(total_claim_count) AS total_claim_count,\n    SUM(total_drug_cost) AS total_drug_cost\n  FROM\n    `bigquery-public-data.cms_medicare.part_d_prescriber_2014`\n  WHERE\n    drug_name = (SELECT drug_name FROM ny_top_drug)\n  GROUP BY\n    state\n  ORDER BY\n    total_claim_count DESC\n  LIMIT 5\n)\nSELECT\n  state,\n  total_claim_count,\n  total_drug_cost\nFROM\n  top_5_states;",
        "external_knowledge": null,
        "plan": "1. **Identify the Top Drug in NY:**\r\n   - Create a Common Table Expression (CTE) named `ny_top_drug`.\r\n   - Select the `generic_name` of drugs as `drug_name` and calculate the total number of claims (`total_claim_count`) for each drug.\r\n   - Filter the data to include only records where the provider state (`nppes_provider_state`) is 'NY'.\r\n   - Group the results by `drug_name`.\r\n   - Order the results by `total_claim_count` in descending order.\r\n   - Limit the results to the top drug (highest `total_claim_count`).\r\n\r\n2. **Identify the Top 5 States for the Top Drug:**\r\n   - Create a second CTE named `top_5_states`.\r\n   - Select the provider state (`nppes_provider_state`) as `state`, and calculate the total number of claims (`total_claim_count`) and total drug cost (`total_drug_cost`) for each state.\r\n   - Filter the data to include only records where the `generic_name` matches the top drug identified in `ny_top_drug`.\r\n   - Group the results by `state`.\r\n   - Order the results by `total_claim_count` in descending order.\r\n   - Limit the results to the top 5 states with the highest `total_claim_count`.\r\n\r\n3. **Return the Final Results:**\r\n   - Select the `state`, `total_claim_count`, and `total_drug_cost` from the `top_5_states` CTE to produce the final output.\r\n\r\nThis sequence ensures that we first determine the most prescribed drug in New York, then find the top 5 states where this drug is most frequently prescribed, and finally, present the relevant statistics for these states.",
        "special_function": [
            "mathematical-functions/ROUND"
        ]
    },
    {
        "instance_id": "bq177",
        "db": "bigquery-public-data.cms_medicare\nbigquery-public-data.cms_codes\nbigquery-public-data.cms_synthetic_patient_data_omop",
        "question": "For the provider with the highest total inpatient service cost from 2011-2015, tell me its annual inpatient and outpatient revenues averaged by case for each year during that period.",
        "SQL": "WITH total_ip_cost AS (\n  SELECT\n    provider_id,\n    SUM(average_medicare_payments * total_discharges) AS total_ip_cost\n  FROM (\n    SELECT * FROM `bigquery-public-data.cms_medicare.inpatient_charges_2011`\n    UNION ALL\n    SELECT * FROM `bigquery-public-data.cms_medicare.inpatient_charges_2012`\n    UNION ALL\n    SELECT * FROM `bigquery-public-data.cms_medicare.inpatient_charges_2013`\n    UNION ALL\n    SELECT * FROM `bigquery-public-data.cms_medicare.inpatient_charges_2014`\n    UNION ALL\n    SELECT * FROM `bigquery-public-data.cms_medicare.inpatient_charges_2015`\n  )\n  GROUP BY\n    provider_id\n  ORDER BY\n    total_ip_cost DESC\n  LIMIT 1\n),\nprovider_id_with_highest_ip_cost AS (\n  SELECT provider_id FROM total_ip_cost\n)\n\n-- Step 2: Retrieve the annual inpatient and outpatient costs for the identified provider from 2011-2015\nSELECT\n  ip.provider_name AS Provider_Name,\n  IP.year,\n  ROUND(ip.average_ip_cost) AS Average_IP_Cost,\n  ROUND(op.average_op_cost) AS Average_OP_Cost\nFROM (\n  SELECT\n    provider_id,\n    provider_state,\n    provider_city,\n    provider_name,\n    _TABLE_SUFFIX AS year,\n    AVG(average_medicare_payments * total_discharges) AS average_ip_cost\n  FROM\n    `bigquery-public-data.cms_medicare.inpatient_charges_*`\n  WHERE\n    provider_id IN (SELECT provider_id FROM provider_id_with_highest_ip_cost)\n  GROUP BY\n    provider_id,\n    provider_state,\n    provider_city,\n    provider_name,\n    year\n) AS ip\nLEFT JOIN (\n  SELECT\n    provider_id,\n    provider_state,\n    provider_city,\n    provider_name,\n    _TABLE_SUFFIX AS year,\n    AVG(average_total_payments * outpatient_services) AS average_op_cost\n  FROM\n    `bigquery-public-data.cms_medicare.outpatient_charges_*`\n  WHERE \n    provider_id IN (SELECT provider_id FROM provider_id_with_highest_ip_cost)\n  GROUP BY\n    provider_id,\n    provider_state,\n    provider_city,\n    provider_name,\n    year\n) AS op\nON\n  ip.provider_id = op.provider_id\n  AND ip.provider_state = op.provider_state\n  AND ip.provider_city = op.provider_city\n  AND ip.provider_name = op.provider_name\n  AND ip.year = op.year\nORDER BY\n  year;",
        "external_knowledge": null,
        "plan": "1. **Identify Provider with Highest Inpatient Service Cost (2011-2015)**\r\n   - **Combine Inpatient Data:** Use `UNION ALL` to merge inpatient charge data from 2011 through 2015.\r\n   - **Calculate Total Inpatient Cost:** For each provider, compute the total inpatient cost as the sum of `average_medicare_payments` multiplied by `total_discharges`.\r\n   - **Find Top Provider:** Group the results by `provider_id`, order them by `total_ip_cost` in descending order, and limit the output to the top provider.\r\n\r\n2. **Extract Provider ID with Highest Inpatient Cost**\r\n   - **Select Provider ID:** From the previous step, extract the `provider_id` of the provider with the highest total inpatient service cost.\r\n\r\n3. **Retrieve Annual Inpatient Costs for Identified Provider (2011-2015)**\r\n   - **Query Inpatient Data:** For the identified provider, fetch the inpatient data from `cms_medicare.inpatient_charges_*`.\r\n   - **Calculate Annual Average Inpatient Cost:** Compute the average inpatient cost per year by averaging `average_medicare_payments` multiplied by `total_discharges`. Group the data by `provider_id`, `provider_state`, `provider_city`, `provider_name`, and year.\r\n\r\n4. **Retrieve Annual Outpatient Costs for Identified Provider (2011-2015)**\r\n   - **Query Outpatient Data:** For the identified provider, fetch the outpatient data from `cms_medicare.outpatient_charges_*`.\r\n   - **Calculate Annual Average Outpatient Cost:** Compute the average outpatient cost per year by averaging `average_total_payments` multiplied by `outpatient_services`. Group the data by `provider_id`, `provider_state`, `provider_city`, `provider_name`, and year.\r\n\r\n5. **Join Inpatient and Outpatient Data**\r\n   - **Merge Inpatient and Outpatient Data:** Perform a LEFT JOIN on the inpatient and outpatient datasets based on `provider_id`, `provider_state`, `provider_city`, `provider_name`, and year.\r\n\r\n6. **Format and Order Final Results**\r\n   - **Select and Format Fields:** Select relevant fields including `State`, `City`, `Provider_ID`, `Provider_Name`, year, and round the average inpatient and outpatient costs.\r\n   - **Order by Year:** Sort the final results by year for chronological presentation.\r\n\r\nThis plan ensures the correct identification of the top provider based on inpatient costs and retrieves comprehensive annual cost data for both inpatient and outpatient services for detailed analysis.",
        "special_function": [
            "mathematical-functions/ROUND"
        ]
    },
    {
        "instance_id": "bq354",
        "db": "bigquery-public-data.cms_medicare\nbigquery-public-data.cms_codes\nbigquery-public-data.cms_synthetic_patient_data_omop",
        "question": "Could you provide the percentage of participants for standard acne, atopic dermatitis, psoriasis, and vitiligo defined by the International Classification of Diseases 10-CM(ICD-10-CM), including their subcategories? The ICD-10 codes are: Acne (L70), Atopic dermatitis (L20), Psoriasis (L40), and Vitiligo (L80). ",
        "SQL": "WITH skin_condition_ICD_concept_ids AS (\n    SELECT\n        concept_id,\n        CASE concept_code\n            WHEN 'L70' THEN 'Acne'\n            WHEN 'L20' THEN 'Atopic dermatitis'\n            WHEN 'L40' THEN 'Psoriasis'\n            ELSE 'Vitiligo'\n        END AS skin_condition\n    FROM\n        `bigquery-public-data.cms_synthetic_patient_data_omop.concept`\n    WHERE\n        concept_code IN ('L70', 'L20', 'L40', 'L80')\n        AND vocabulary_id = 'ICD10CM'\n),\nstandard_concept_ids AS (\n    SELECT\n        concept_id\n    FROM\n        `bigquery-public-data.cms_synthetic_patient_data_omop.concept`\n    WHERE\n        standard_concept = 'S'\n),\nskin_condition_standard_concept_ids AS (\n    SELECT\n        s.skin_condition,\n        r.concept_id_2 AS concept_id\n    FROM\n        skin_condition_ICD_concept_ids s\n    JOIN\n        `bigquery-public-data.cms_synthetic_patient_data_omop.concept_relationship` r\n    ON\n        s.concept_id = r.concept_id_1\n    JOIN\n        standard_concept_ids sc\n    ON\n        sc.concept_id = r.concept_id_2\n    WHERE\n        r.relationship_id = 'Maps to'\n),\nall_skin_concept_ids AS (\n    SELECT DISTINCT\n        skin_condition,\n        concept_id\n    FROM\n        skin_condition_standard_concept_ids\n),\ndescendant_concept_ids AS (\n    SELECT\n        a.skin_condition,\n        ca.descendant_concept_id AS concept_id\n    FROM\n        all_skin_concept_ids a\n    JOIN\n        `bigquery-public-data.cms_synthetic_patient_data_omop.concept_ancestor` ca\n    ON\n        a.concept_id = ca.ancestor_concept_id\n),\nparticipants_with_condition AS (\n    SELECT\n        d.skin_condition,\n        COUNT(DISTINCT co.person_id) AS nb_of_participants_with_skin_condition\n    FROM\n        `bigquery-public-data.cms_synthetic_patient_data_omop.condition_occurrence` co\n    JOIN\n        descendant_concept_ids d\n    ON\n        co.condition_concept_id = d.concept_id\n    GROUP BY\n        d.skin_condition\n),\ntotal_participants AS (\n    SELECT\n        COUNT(DISTINCT person_id) AS nb_of_participants\n    FROM\n        `bigquery-public-data.cms_synthetic_patient_data_omop.person`\n)\nSELECT\n    p.skin_condition,\n    100 * p.nb_of_participants_with_skin_condition / t.nb_of_participants AS percentage_of_participants\nFROM\n    participants_with_condition p,\n    total_participants t",
        "external_knowledge": null,
        "plan": "1. **Identify Relevant ICD-10 Codes**:\n   - Extract concept IDs and their associated skin condition names for specified ICD-10 codes related to acne, atopic dermatitis, psoriasis, and vitiligo.\n\n2. **Filter Standard Concepts**:\n   - Select concept IDs that are marked as standard concepts.\n\n3. **Map ICD-10 Codes to Standard Concepts**:\n   - For each identified skin condition, map its concept ID to the corresponding standard concept ID using relationships between concepts.\n\n4. **Aggregate Concept IDs**:\n   - Create a list of all unique standard concept IDs associated with each skin condition.\n\n5. **Identify Descendant Concepts**:\n   - Retrieve all descendant concept IDs for the aggregated standard concept IDs, ensuring that all subcategories are included.\n\n6. **Count Participants with Skin Conditions**:\n   - Count the number of distinct participants diagnosed with each skin condition by matching condition occurrence data with the descendant concept IDs.\n\n7. **Total Participant Count**:\n   - Determine the total number of participants in the dataset.\n\n8. **Calculate Percentages**:\n   - For each skin condition, calculate the percentage of participants by dividing the count of participants with the condition by the total number of participants and multiplying by 100.\n\n9. **Output Results**:\n   - Present the skin condition names alongside their respective percentages of participants.",
        "special_function": null
    },
    {
        "instance_id": "bq355",
        "db": "bigquery-public-data.cms_medicare\nbigquery-public-data.cms_codes\nbigquery-public-data.cms_synthetic_patient_data_omop",
        "question": "Please tell me the percentage of participants not using quinapril and related medications(Quinapril RxCUI: 35208).",
        "SQL": "WITH quinapril_concept AS (\n    SELECT concept_id\n    FROM `bigquery-public-data.cms_synthetic_patient_data_omop.concept`\n    WHERE concept_code = \"35208\" AND vocabulary_id = \"RxNorm\"\n),\nquinapril_related_medications AS (\n    SELECT DISTINCT descendant_concept_id AS concept_id\n    FROM `bigquery-public-data.cms_synthetic_patient_data_omop.concept_ancestor`\n    WHERE ancestor_concept_id IN (SELECT concept_id FROM quinapril_concept)\n),\nparticipants_with_quinapril AS (\n    SELECT COUNT(DISTINCT person_id) AS count\n    FROM `bigquery-public-data.cms_synthetic_patient_data_omop.drug_exposure`\n    WHERE drug_concept_id IN (SELECT concept_id FROM quinapril_related_medications)\n),\ntotal_participants AS (\n    SELECT COUNT(DISTINCT person_id) AS count\n    FROM `bigquery-public-data.cms_synthetic_patient_data_omop.person`\n)\nSELECT\n    100 - (100 * participants_with_quinapril.count / total_participants.count) AS without_quinapril\nFROM\n    participants_with_quinapril, total_participants",
        "external_knowledge": null,
        "plan": "1. **Identify Target Medication:**\n   - Query a table to find the unique identifier for the specified medication based on its code and vocabulary system.\n\n2. **Determine Related Medications:**\n   - Use the identified medication's unique identifier to find all related medications by querying an ancestry table, which tracks medication hierarchies and relationships.\n\n3. **Count Participants Using Target Medications:**\n   - Query a drug exposure table to count the number of unique participants who have been prescribed any of the related medications found in the previous step.\n\n4. **Count Total Participants:**\n   - Query a participant table to count the total number of unique participants in the dataset.\n\n5. **Calculate Proportion Not Using Target Medications:**\n   - Use the counts from steps 3 and 4 to calculate the proportion of participants who are not using the target or related medications. This is done by subtracting the percentage of users from 100%.\n\n6. **Return Result:**\n   - Output the calculated proportion as the final result.",
        "special_function": null
    },
    {
        "instance_id": "bq032",
        "db": "bigquery-public-data.noaa_hurricanes\nbigquery-public-data.noaa_tsunami\nbigquery-public-data.noaa_significant_earthquakes\nbigquery-public-data.noaa_preliminary_severe_storms\nbigquery-public-data.noaa_pifsc_metadata\nbigquery-public-data.noaa_passive_bioacoustic\nbigquery-public-data.noaa_passive_acoustic_index\nbigquery-public-data.noaa_icoads\nbigquery-public-data.noaa_historic_severe_storms\nbigquery-public-data.noaa_gsod",
        "question": "Can you provide the latitude of the final coordinates for the hurricane that traveled the second longest distance in the North Atlantic during 2020?",
        "SQL": "WITH hurricane_geometry AS (\n  SELECT\n    * EXCEPT (longitude, latitude),\n    ST_GEOGPOINT(longitude, latitude) AS geom,\n    MAX(usa_wind) OVER (PARTITION BY sid) AS max_wnd_speed\n  FROM\n    `bigquery-public-data.noaa_hurricanes.hurricanes`\n  WHERE\n    season = '2020'\n    AND basin = 'NA'\n    AND name != 'NOT NAMED'\n),\ndist_between_points AS (\n  SELECT\n    sid,\n    name,\n    season,\n    iso_time,\n    max_wnd_speed,\n    geom,\n    ST_DISTANCE(geom, LAG(geom, 1) OVER (PARTITION BY sid ORDER BY iso_time ASC)) / 1000 AS dist\n  FROM\n    hurricane_geometry\n),\ntotal_distances AS (\n  SELECT\n    sid,\n    name,\n    season,\n    iso_time,\n    max_wnd_speed,\n    geom,\n    SUM(dist) OVER (PARTITION BY sid ORDER BY iso_time ASC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_distance,\n    SUM(dist) OVER (PARTITION BY sid) AS total_dist\n  FROM\n    dist_between_points\n),\nranked_hurricanes AS (\n  SELECT\n    *,\n    DENSE_RANK() OVER (ORDER BY total_dist DESC) AS dense_rank\n  FROM\n    total_distances\n)\n\nSELECT\n  ST_Y(geom)\nFROM\n  ranked_hurricanes\nWHERE\n  dense_rank = 2\nORDER BY\ncumulative_distance\nDESC\nLIMIT 1\n;",
        "external_knowledge": null,
        "plan": "1. Select hurricane data for the 2020 season located in the North Atlantic (NA).\n2. Obtain position change information for each hurricane based on the time of movement.\n3. Calculate the total distance traveled by each hurricane.\n4. Rank hurricanes based on their total travel distances and select the hurricane that ranks second.\n5. Output the coordinate changes for the movement of this hurricane\n6. Only retain the last coordinate of this hurricane",
        "special_function": [
            "geography-functions/ST_DISTANCE",
            "geography-functions/ST_GEOGPOINT",
            "geography-functions/ST_Y",
            "navigation-functions/LAG",
            "numbering-functions/DENSE_RANK"
        ]
    },
    {
        "instance_id": "bq119",
        "db": "bigquery-public-data.noaa_hurricanes\nbigquery-public-data.noaa_tsunami\nbigquery-public-data.noaa_significant_earthquakes\nbigquery-public-data.noaa_preliminary_severe_storms\nbigquery-public-data.noaa_pifsc_metadata\nbigquery-public-data.noaa_passive_bioacoustic\nbigquery-public-data.noaa_passive_acoustic_index\nbigquery-public-data.noaa_icoads\nbigquery-public-data.noaa_historic_severe_storms\nbigquery-public-data.noaa_gsod",
        "question": "Please show information of the hurricane with the third longest total travel distance in the North Atlantic during 2020, including its travel coordinates, the cumulative travel distance at each point, and the maximum sustained wind speed at those times.",
        "SQL": "WITH hurricane_geometry AS (\n  SELECT\n    * EXCEPT (longitude, latitude),\n    ST_GEOGPOINT(longitude, latitude) AS geom,\n  FROM\n    `bigquery-public-data.noaa_hurricanes.hurricanes`\n  WHERE\n    season = '2020'\n    AND basin = 'NA'\n    AND name != 'NOT NAMED'\n),\ndist_between_points AS (\n  SELECT\n    sid,\n    name,\n    season,\n    iso_time,\n    usa_wind,\n    geom,\n    ST_DISTANCE(geom, LAG(geom, 1) OVER (PARTITION BY sid ORDER BY iso_time ASC)) / 1000 AS dist\n  FROM\n    hurricane_geometry\n),\ntotal_distances AS (\n  SELECT\n    sid,\n    name,\n    season,\n    iso_time,\n    usa_wind,\n    geom,\n    SUM(dist) OVER (PARTITION BY sid ORDER BY iso_time ASC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_distance,\n    SUM(dist) OVER (PARTITION BY sid) AS total_dist\n  FROM\n    dist_between_points\n),\nranked_hurricanes AS (\n  SELECT\n    *,\n    DENSE_RANK() OVER (ORDER BY total_dist DESC) AS dense_rank\n  FROM\n    total_distances\n)\n\nSELECT\n  geom,cumulative_distance,usa_wind\nFROM\n  ranked_hurricanes\nWHERE\n  dense_rank = 3\nORDER BY\ncumulative_distance;",
        "external_knowledge": null,
        "plan": "1. **Filter Data**:\n   - Start with a dataset containing records of a specific event type for a particular year and region.\n   - Exclude entries that are not named.\n\n2. **Create Geometric Points**:\n   - For each record, create a geographic point from the given coordinates.\n\n3. **Calculate Distances**:\n   - For each event, calculate the distance between consecutive geographic points in kilometers.\n   - Use a window function to compute the distance between each point and the previous point, ordered by time.\n\n4. **Cumulative Distance Calculation**:\n   - For each event, calculate the cumulative distance traveled up to each point in time.\n   - Also, compute the total distance traveled by each event over its entire duration.\n\n5. **Rank Events**:\n   - Rank all events based on their total travel distance, with the longest distance receiving the highest rank.\n\n6. **Select Specific Rank**:\n   - Select the records for the event that has the third longest total travel distance.\n\n7. **Final Output**:\n   - From the selected event, extract and display the geographic points, cumulative distances at each point, and the maximum wind speed recorded at those points.\n   - Order the results by cumulative distance.",
        "special_function": [
            "geography-functions/ST_DISTANCE",
            "geography-functions/ST_GEOGPOINT",
            "navigation-functions/LAG",
            "numbering-functions/DENSE_RANK"
        ]
    },
    {
        "instance_id": "bq117",
        "db": "bigquery-public-data.noaa_historic_severe_storms\nbigquery-public-data.noaa_hurricanes\nbigquery-public-data.noaa_tsunami\nbigquery-public-data.noaa_significant_earthquakes\nbigquery-public-data.noaa_preliminary_severe_storms\nbigquery-public-data.noaa_pifsc_metadata\nbigquery-public-data.noaa_passive_bioacoustic\nbigquery-public-data.noaa_passive_acoustic_index\nbigquery-public-data.noaa_icoads\nbigquery-public-data.noaa_gsod",
        "question": "What is the total number of severe storm events that occurred in the most affected month over the past 15 years according to NOAA records, considering only the top 100 storm events with the highest property damage?",
        "SQL": "WITH base_info AS (\n  SELECT\n    episode_id, \n    CONCAT(CAST(EXTRACT(MONTH FROM MIN(event_begin_time)) AS STRING), \"-\", CAST(EXTRACT(year FROM MIN(event_begin_time)) AS STRING)) as episode_month,\n    EXTRACT(MONTH FROM MIN(event_begin_time)) AS month,\n    STRING_AGG(DISTINCT(cz_name) LIMIT 5) as counties, \n    STRING_AGG(DISTINCT(state)) as states, \n    STRING_AGG(DISTINCT(event_type) LIMIT 5) as event_types,\n    SUM(damage_property)/1000000000 as damage_property_in_billions\n  FROM\n    `bigquery-public-data.noaa_historic_severe_storms.storms_*`\n  WHERE\n    _TABLE_SUFFIX BETWEEN CAST((EXTRACT(YEAR from CURRENT_DATE())-15) AS STRING) AND CAST(EXTRACT(YEAR from CURRENT_DATE()) AS STRING)\n  GROUP BY\n    episode_id\n  ORDER BY\n    damage_property_in_billions desc\n  LIMIT 100\n)\n\nSELECT COUNT(*) AS month_count\nFROM base_info\nGROUP BY month\nORDER BY month_count DESC\nLIMIT 1",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. **Define a Subquery for Base Information:**\n   - Create a temporary table to aggregate storm event data.\n   - Extract unique event identifiers and calculate the earliest occurrence time for each event.\n   - Format the month and year of the earliest occurrence into a string.\n   - Aggregate unique county names, state names, and event types, limiting each to a certain number of unique values.\n   - Sum the property damage values and convert the total to billions.\n\n2. **Filter Data for the Past 15 Years:**\n   - Restrict the data to the past 15 years by comparing the year of the event to the current year, using a dynamic range calculated from the current date.\n\n3. **Group and Sort Data:**\n   - Group the data by unique event identifiers.\n   - Sort the aggregated results by the total property damage in descending order.\n   - Limit the results to the top 100 events with the highest property damage.\n\n4. **Count Events by Month:**\n   - From the aggregated results, count the number of events that occurred in each month.\n\n5. **Determine the Most Affected Month:**\n   - Group the counted results by month.\n   - Sort the counts in descending order to identify the month with the highest number of severe storm events.\n   - Limit the results to the top month (the one with the highest count).\n\n6. **Return the Final Result:**\n   - Output the total number of severe storm events that occurred in the most affected month over the past 15 years.",
        "special_function": [
            "aggregate-functions/STRING_AGG",
            "conversion-functions/CAST",
            "date-functions/CURRENT_DATE",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "json-functions/STRING",
            "string-functions/CONCAT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/STRING"
        ]
    },
    {
        "instance_id": "bq419",
        "db": "bigquery-public-data.noaa_historic_severe_storms\nbigquery-public-data.noaa_hurricanes\nbigquery-public-data.noaa_tsunami\nbigquery-public-data.noaa_significant_earthquakes\nbigquery-public-data.noaa_preliminary_severe_storms\nbigquery-public-data.noaa_pifsc_metadata\nbigquery-public-data.noaa_passive_bioacoustic\nbigquery-public-data.noaa_passive_acoustic_index\nbigquery-public-data.noaa_icoads\nbigquery-public-data.noaa_gsod",
        "question": "Which 5 states had the most storm events from 1980 to 1995, considering only the top 1000 states with the highest event counts each year? Please use state abbreviations.",
        "SQL": "WITH s80 as\n  (SELECT state, COUNT(event_id) as num_events\n  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1980` \n  GROUP BY state \n  ORDER BY num_events DESC\n  LIMIT 1000),\ns81 as\n(SELECT state, COUNT(event_id) as num_events\n  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1981` \n  GROUP BY state \n  ORDER BY num_events DESC\n  LIMIT 1000),\ns82 as\n  (SELECT state, COUNT(event_id) as num_events\n  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1982` \n  GROUP BY state \n  ORDER BY num_events DESC\n  LIMIT 1000),\n\ns83 as\n  (SELECT state, COUNT(event_id) as num_events\n  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1983` \n  GROUP BY state \n  ORDER BY num_events DESC\n  LIMIT 1000),\n\ns84 as\n  (SELECT state, COUNT(event_id) as num_events\n  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1984` \n  GROUP BY state \n  ORDER BY num_events DESC\n  LIMIT 1000),\n\ns85 as\n  (SELECT state, COUNT(event_id) as num_events\n  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1985` \n  GROUP BY state \n  ORDER BY num_events DESC\n  LIMIT 1000),\n\ns86 as\n  (SELECT state, COUNT(event_id) as num_events\n  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1986` \n  GROUP BY state \n  ORDER BY num_events DESC\n  LIMIT 1000),\n\ns87 as\n  (SELECT state, COUNT(event_id) as num_events\n  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1987` \n  GROUP BY state \n  ORDER BY num_events DESC\n  LIMIT 1000),\n\ns88 as\n  (SELECT state, COUNT(event_id) as num_events\n  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1988` \n  GROUP BY state \n  ORDER BY num_events DESC\n  LIMIT 1000),\n\ns89 as\n  (SELECT state, COUNT(event_id) as num_events\n  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1989` \n  GROUP BY state \n  ORDER BY num_events DESC\n  LIMIT 1000),\n\ns90 as\n  (SELECT state, COUNT(event_id) as num_events\n  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1990` \n  GROUP BY state \n  ORDER BY num_events DESC\n  LIMIT 1000),\n\ns91 as\n  (SELECT state, COUNT(event_id) as num_events\n  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1991` \n  GROUP BY state \n  ORDER BY num_events DESC\n  LIMIT 1000),\ns92 as\n  (SELECT state, COUNT(event_id) as num_events\n  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1992` \n  GROUP BY state \n  ORDER BY num_events DESC\n  LIMIT 1000),\n\ns93 as\n  (SELECT state, COUNT(event_id) as num_events\n  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1993` \n  GROUP BY state \n  ORDER BY num_events DESC\n  LIMIT 1000),\n\ns94 as\n  (SELECT state, COUNT(event_id) as num_events\n  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1994` \n  GROUP BY state \n  ORDER BY num_events DESC\n  LIMIT 1000),\n\ns95 as\n  (SELECT state, COUNT(event_id) as num_events\n  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1995` \n  GROUP BY state \n  ORDER BY num_events DESC\n  LIMIT 1000)\n\nSELECT s80.state, \ns80.num_events + s81.num_events +  s82.num_events +  s83.num_events +  s84.num_events +  s85.num_events +  s86.num_events +  s87.num_events + s88.num_events +  s89.num_events +  s90.num_events + s91.num_events + s92.num_events + s93.num_events + s94.num_events + s95.num_events as total_events \nFROM s80 FULL JOIN s81 ON s80.state = s81.state\nFULL JOIN s82 ON s82.state = s81.state\nFULL JOIN s83 ON s83.state = s81.state\nFULL JOIN s84 ON s84.state = s81.state\nFULL JOIN s85 ON s85.state = s81.state\nFULL JOIN s86 ON s86.state = s81.state\nFULL JOIN s87 ON s87.state = s81.state\nFULL JOIN s88 ON s88.state = s81.state\nFULL JOIN s89 ON s89.state = s81.state\nFULL JOIN s90 ON s90.state = s81.state\nFULL JOIN s91 ON s91.state = s81.state \nFULL JOIN s92 ON s92.state = s81.state\nFULL JOIN s93 ON s93.state = s81.state\nFULL JOIN s94 ON s94.state = s81.state\nFULL JOIN s95 ON s95.state = s81.state\n\nORDER BY total_events DESC\nLIMIT 5;",
        "external_knowledge": null,
        "plan": "1. **Data Aggregation for Each Year**:\n   - For each year from 1980 to 1995, create a subquery that:\n     - Counts the number of events for each state.\n     - Orders the results by the event count in descending order.\n     - Limits the result to the top 1000 states with the highest event counts.\n\n2. **Combine Yearly Data**:\n   - Perform a full join of the results from each yearly subquery based on the state abbreviation.\n   - This ensures that all states from each year's top 1000 lists are included, even if they do not appear in other years' top 1000 lists.\n\n3. **Calculate Total Event Counts**:\n   - For each state, sum the event counts from all the years (1980 to 1995) to get the total number of events across the specified period.\n\n4. **Rank States by Total Events**:\n   - Order the states by their total event counts in descending order.\n\n5. **Select Top 5 States**:\n   - Limit the final result to the top 5 states with the highest total event counts.",
        "special_function": null
    },
    {
        "instance_id": "bq071",
        "db": "bigquery-public-data.geo_us_boundaries\nbigquery-public-data.noaa_hurricanes\nbigquery-public-data.noaa_tsunami\nbigquery-public-data.noaa_significant_earthquakes\nbigquery-public-data.noaa_preliminary_severe_storms\nbigquery-public-data.noaa_pifsc_metadata\nbigquery-public-data.noaa_passive_bioacoustic\nbigquery-public-data.noaa_passive_acoustic_index\nbigquery-public-data.noaa_icoads\nbigquery-public-data.noaa_historic_severe_storms\nbigquery-public-data.noaa_gsod",
        "question": "What are the zip codes of the areas in the United States along with the number of times they have been affected by the named hurricanes, ordered by the number of occurences?",
        "SQL": "SELECT\n  z.city as city\n  ,z.zip_code as zip_code\n  ,z.state_name as state\n  ,COUNT(DISTINCT(h.name)) as count_hurricanes\n  ,STRING_AGG(DISTINCT(h.name)) as hurricanes\nFROM \n  `bigquery-public-data.geo_us_boundaries.zip_codes` as z\n  ,`bigquery-public-data.noaa_hurricanes.hurricanes` as h\nWHERE\n  ST_WITHIN(ST_GeogPoint(h.longitude,h.latitude), z.zip_code_geom)\n  AND h.name != \"NOT_NAMED\"\nGROUP BY \n  zip_code\n  ,city\n  ,state  \nORDER BY\n  count_hurricanes desc",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. **Select Required Columns**: Begin by selecting the necessary columns which include the city, zip code, state, the count of distinct hurricane names, and a concatenated string of distinct hurricane names.\n\n2. **Join Two Data Sources**: Combine data from the zip codes dataset and the hurricanes dataset. This will allow you to link hurricanes to the zip codes they have affected.\n\n3. **Filter Data**: Use a spatial function to filter and ensure that only hurricanes whose coordinates fall within the geometric boundaries of the zip codes are considered. Additionally, exclude hurricanes that are not named.\n\n4. **Group Data**: Group the filtered data by zip code, city, and state. This allows for aggregation operations to be performed on each group.\n\n5. **Count Distinct Hurricanes**: For each group, count the number of distinct hurricanes that have affected that area.\n\n6. **Concatenate Hurricane Names**: Create a concatenated string of the names of the distinct hurricanes for each group.\n\n7. **Order and Limit Results**: Order the results by the count of distinct hurricanes in descending order to identify the areas most affected by named hurricanes. Limit the output to the top 10 zip codes.\n\n8. **Return Final Output**: Display the final output with columns for city, zip code, state, the count of hurricanes, and the concatenated hurricane names.",
        "special_function": [
            "aggregate-functions/STRING_AGG",
            "conversion-functions/CAST",
            "geography-functions/ST_GEOGPOINT",
            "geography-functions/ST_WITHIN",
            "json-functions/STRING",
            "string-functions/CONCAT",
            "timestamp-functions/STRING"
        ]
    },
    {
        "instance_id": "bq236",
        "db": "bigquery-public-data.geo_us_boundaries\nbigquery-public-data.noaa_historic_severe_storms\nbigquery-public-data.noaa_hurricanes\nbigquery-public-data.noaa_tsunami\nbigquery-public-data.noaa_significant_earthquakes\nbigquery-public-data.noaa_preliminary_severe_storms\nbigquery-public-data.noaa_pifsc_metadata\nbigquery-public-data.noaa_passive_bioacoustic\nbigquery-public-data.noaa_passive_acoustic_index\nbigquery-public-data.noaa_icoads\nbigquery-public-data.noaa_gsod",
        "question": "What are the top 5 zip codes of the areas in the United States that have experienced the most hail storm events in the past 10 years?",
        "SQL": "SELECT\n  CONCAT(city,\", \", state_name) as city,\n  zip_code,\n  COUNT(event_id) as count_storms\nFROM\n  `bigquery-public-data.noaa_historic_severe_storms.storms_*`,\n  `bigquery-public-data.geo_us_boundaries.zip_codes` \nWHERE\n  _TABLE_SUFFIX BETWEEN CAST((EXTRACT(YEAR from CURRENT_DATE())-10) AS STRING) AND CAST(EXTRACT(YEAR from CURRENT_DATE()) AS STRING) AND\n  LOWER(event_type) = 'hail' AND\n  ST_WITHIN(event_point, zip_code_geom)\n  \nGROUP BY\n  event_type,\n  zip_code, \n  city\nORDER BY\n  count_storms desc\nLIMIT 5",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "What zip codes have experienced the most hail storms in the last 10 years?\nThis query combines the severe weather events dataset with the zip code boundary data  available as a BigQuery Public Dataset to group hail events by zip code over the last 10 years.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/CURRENT_DATE",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "geography-functions/ST_WITHIN",
            "interval-functions/EXTRACT",
            "json-functions/STRING",
            "string-functions/CONCAT",
            "string-functions/LOWER",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/STRING"
        ]
    },
    {
        "instance_id": "bq356",
        "db": "bigquery-public-data.noaa_gsod\nbigquery-public-data.noaa_historic_severe_storms\nbigquery-public-data.noaa_hurricanes\nbigquery-public-data.noaa_tsunami\nbigquery-public-data.noaa_significant_earthquakes\nbigquery-public-data.noaa_preliminary_severe_storms\nbigquery-public-data.noaa_pifsc_metadata\nbigquery-public-data.noaa_passive_bioacoustic\nbigquery-public-data.noaa_passive_acoustic_index\nbigquery-public-data.noaa_icoads",
        "question": "What is the number of weather stations where the valid temperature record days in 2019 reached 90% or more of the maximum number of recorded days, and have had tracking back to 1/1/2000 or before and through at least 6/30/2019 according to the field 'begin' and 'end'?",
        "SQL": "# Subquery to count # of dates w/ valid temperature data by station\nWITH\nNum2019TempDatesByStation AS\n(\n    SELECT\n    daily_weather.stn,\n\n    # Count # of distinct dates w/ temperature data for each station\n    COUNT(DISTINCT\n        # Convert year/month/day info into date\n        DATE(\n        CAST(daily_weather.year AS INT64),\n        CAST(daily_weather.mo AS INT64),\n        CAST(daily_weather.da AS INT64)\n        )) AS num_2019_temp_dates\n\n    FROM\n    bigquery-public-data.noaa_gsod.gsod2019 daily_weather\n\n    WHERE\n    daily_weather.temp IS NOT NULL AND\n    daily_weather.max IS NOT NULL AND\n    daily_weather.min IS NOT NULL AND\n    # Remove days w/ missing temps coded as 9999.9\n    daily_weather.temp != 9999.9 AND\n    daily_weather.max != 9999.9 AND\n    daily_weather.min != 9999.9\n\n    GROUP BY\n    daily_weather.stn\n),\n\n# Calculate max number of 2019 temperature dates across all stations\nMaxNum2019TempDates AS\n(\n    SELECT\n    MAX(num_2019_temp_dates) AS max_num_2019_temp_dates\n\n    FROM\n    Num2019TempDatesByStation\n)\n\nSELECT\n    COUNT(*)\nFROM\n    bigquery-public-data.noaa_gsod.stations Stations\n\n# Inner join to filter to only stations present in 2019 data\nINNER JOIN\n    Num2019TempDatesByStation ON (\n    stations.usaf = Num2019TempDatesByStation.stn\n    )\n\n# Cross join to get max number on each row, to use in filtering below\nCROSS JOIN\n    MaxNum2019TempDates\n\nWHERE\n    # Filter to stations that have had tracking since at least 1/1/2000\n    Stations.begin <= '20000101' AND\n    # Filter to stations that have had tracking through at least 6/30/2019\n    Stations.end >= '20190630' AND\n    # Filter to stations w/ >= 90% of the max number of dates for 2019\n    Num2019TempDatesByStation.num_2019_temp_dates >=\n    (0.90 * MaxNum2019TempDates.max_num_2019_temp_dates)",
        "external_knowledge": null,
        "plan": "1. **Subquery to Count Valid Temperature Record Days by Station in 2019:**\n    - Create a subquery to count the number of distinct dates with valid temperature data for each station.\n    - Convert year, month, and day information into a date format.\n    - Filter out records with missing temperature data and invalid temperature values.\n    - Group the results by each weather station.\n\n2. **Subquery to Calculate Maximum Number of Valid Temperature Record Days in 2019:**\n    - Create another subquery to find the maximum number of valid temperature record days across all stations for the year 2019.\n\n3. **Main Query to Count Qualifying Weather Stations:**\n    - Select the main weather stations table.\n    - Perform an inner join with the subquery from step 1 to filter only those stations that have temperature data in 2019.\n    - Cross join with the subquery from step 2 to make the maximum number of valid temperature record days available for filtering.\n    - Apply additional filtering conditions:\n        - Include only stations that have been tracking data since at least January 1, 2000.\n        - Include only stations that have continued tracking data through at least June 30, 2019.\n        - Include only stations where the number of valid temperature record days in 2019 is at least 90% of the maximum number of valid temperature record days across all stations for 2019.\n\n4. **Count and Return the Number of Stations Meeting All Criteria:**\n    - Count the number of stations that meet all the specified conditions and return this count as the final result.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "mathematical-functions/LEAST"
        ]
    },
    {
        "instance_id": "bq042",
        "db": "bigquery-public-data.noaa_gsod\nbigquery-public-data.noaa_historic_severe_storms\nbigquery-public-data.noaa_hurricanes\nbigquery-public-data.noaa_tsunami\nbigquery-public-data.noaa_significant_earthquakes\nbigquery-public-data.noaa_preliminary_severe_storms\nbigquery-public-data.noaa_pifsc_metadata\nbigquery-public-data.noaa_passive_bioacoustic\nbigquery-public-data.noaa_passive_acoustic_index\nbigquery-public-data.noaa_icoads",
        "question": "Help me analyze the weather conditions (including temperature, wind speed and precipitation) at NYC's airport LaGuardia for June 12, year over year, starting from 2011 to 2020.",
        "SQL": "SELECT\n  -- Create a timestamp from the date components.\n  TIMESTAMP(CONCAT(year,\"-\",mo,\"-\",da)) AS timestamp,\n  -- Replace numerical null values with actual null\n  AVG(IF (temp=9999.9,\n      null,\n      temp)) AS temperature,\n  AVG(IF (wdsp=\"999.9\",\n      null,\n      CAST(wdsp AS Float64))) AS wind_speed,\n  AVG(IF (prcp=99.99,\n      0,\n      prcp)) AS precipitation\nFROM\n  `bigquery-public-data.noaa_gsod.gsod20*`\nWHERE\n  CAST(YEAR AS INT64) > 2010\n  AND CAST(YEAR AS INT64) < 2021\n  AND CAST(MO AS INT64) = 6\n  AND CAST(DA AS INT64) = 12\n  AND stn = \"725030\" -- La Guardia\nGROUP BY\n  timestamp\nORDER BY\n  timestamp ASC;",
        "external_knowledge": null,
        "plan": "1. Filter Data for Specific Dates and Location: Extract the records for the specified date (June 12) across the given range of years (2011-2020) and for the specific location (LaGuardia airport).\n\n2. Handle Missing or Invalid Data: Replace any placeholder values indicating missing data with `NULL` for temperature and wind speed, and with `0` for precipitation.\n\n3. Calculate Averages: Compute the average values for temperature, wind speed, and precipitation on June 12 for each year.\n\n4. Organize Results: Create a timestamp for each record, group the results by this timestamp, and sort the output in ascending order of the timestamp for clear year-over-year analysis.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "string-functions/CONCAT",
            "string-functions/REPLACE",
            "timestamp-functions/TIMESTAMP",
            "conditional-functions/IF"
        ]
    },
    {
        "instance_id": "bq394",
        "db": "bigquery-public-data.noaa_icoads\nbigquery-public-data.noaa_historic_severe_storms\nbigquery-public-data.noaa_hurricanes\nbigquery-public-data.noaa_tsunami\nbigquery-public-data.noaa_significant_earthquakes\nbigquery-public-data.noaa_preliminary_severe_storms\nbigquery-public-data.noaa_pifsc_metadata\nbigquery-public-data.noaa_passive_bioacoustic\nbigquery-public-data.noaa_passive_acoustic_index\nbigquery-public-data.noaa_gsod",
        "question": "What are the top 3 months between 2010 and 2014 with the smallest sum of absolute differences between the average air temperature, wet bulb temperature, dew point temperature, and sea surface temperature, including respective years and sum of differences? Please present the year and month in numerical format.",
        "SQL": "WITH DailyAverages AS (\n    SELECT \n        year, month, day,\n        air_temperature,\n        wetbulb_temperature,\n        dewpoint_temperature,\n        sea_surface_temp\n    FROM \n        `bigquery-public-data.noaa_icoads.icoads_core_*`\n    WHERE\n        _TABLE_SUFFIX BETWEEN '2010' AND '2014'\n),\n\nMonthlyAverages AS (\n    SELECT \n        year,\n        month,\n        AVG(air_temperature) AS avg_air_temperature,\n        AVG(wetbulb_temperature) AS avg_wetbulb_temperature,\n        AVG(dewpoint_temperature) AS avg_dewpoint_temperature,\n        AVG(sea_surface_temp) AS avg_sea_surface_temp\n    FROM \n        DailyAverages\n    WHERE\n        air_temperature IS NOT NULL\n        AND wetbulb_temperature IS NOT NULL\n        AND dewpoint_temperature IS NOT NULL\n        AND sea_surface_temp IS NOT NULL\n    GROUP BY \n        year, month\n),\n\nDifferenceSums AS (\n    SELECT\n        year,\n        month,\n        (ABS(avg_air_temperature - avg_wetbulb_temperature) +\n        ABS(avg_air_temperature - avg_dewpoint_temperature) +\n        ABS(avg_air_temperature - avg_sea_surface_temp) +\n        ABS(avg_wetbulb_temperature - avg_dewpoint_temperature) +\n        ABS(avg_wetbulb_temperature - avg_sea_surface_temp) +\n        ABS(avg_dewpoint_temperature - avg_sea_surface_temp)) AS sum_of_differences\n    FROM \n        MonthlyAverages\n)\n\nSELECT\n    year,\n    month,\n    sum_of_differences\nFROM\n    DifferenceSums\nORDER BY\n    sum_of_differences ASC\nLIMIT 3;",
        "external_knowledge": null,
        "plan": "1. **Data Selection and Filtering**:\n   - Begin by selecting daily records from the dataset for the years within the specified range.\n   - Ensure that only records within the given years are included.\n\n2. **Daily Averages Extraction**:\n   - Extract daily measurements of specific attributes like temperature and other related metrics.\n\n3. **Monthly Aggregation**:\n   - Group the daily data by year and month to calculate monthly averages for each attribute.\n   - Ensure that only complete records, where all attributes are non-null, are considered.\n\n4. **Compute Absolute Differences**:\n   - For each month, calculate the absolute differences between every pair of the monthly average values of the attributes.\n   - Sum these absolute differences to get a cumulative difference measure for each month.\n\n5. **Ranking and Limiting Results**:\n   - Order the results by the cumulative difference measure in ascending order to find the months with the smallest differences.\n   - Limit the result to the top 3 entries to identify the three months with the smallest sum of differences.\n\n6. **Final Output**:\n   - Select and present the year, month, and the sum of differences for these top 3 entries. Ensure the year and month are in numerical format.",
        "special_function": [
            "mathematical-functions/ABS"
        ]
    },
    {
        "instance_id": "bq357",
        "db": "bigquery-public-data.noaa_icoads\nbigquery-public-data.noaa_historic_severe_storms\nbigquery-public-data.noaa_hurricanes\nbigquery-public-data.noaa_tsunami\nbigquery-public-data.noaa_significant_earthquakes\nbigquery-public-data.noaa_preliminary_severe_storms\nbigquery-public-data.noaa_pifsc_metadata\nbigquery-public-data.noaa_passive_bioacoustic\nbigquery-public-data.noaa_passive_acoustic_index\nbigquery-public-data.noaa_gsod",
        "question": "What are the latitude and longitude coordinates and dates between 2005 and 2015 with the top 5 highest daily average wind speeds, excluding records with missing wind speed values? Using data from tables start with prefix \"icoads_core\".",
        "SQL": "WITH DailyAverages AS (\n    SELECT \n        year, month, day, latitude, longitude,\n        AVG(wind_speed) AS avg_wind_speed,\n    FROM \n        `bigquery-public-data.noaa_icoads.icoads_core_*`\n    WHERE\n        _TABLE_SUFFIX BETWEEN '2005' AND '2015'\n    GROUP BY \n        year, month, day, latitude, longitude\n)\nSELECT \n    year, month, day, latitude, longitude,\n    avg_wind_speed,\n\nFROM \n    DailyAverages\nWHERE\n    avg_wind_speed IS NOT NULL\n\nORDER BY avg_wind_speed DESC LIMIT 5",
        "external_knowledge": null,
        "plan": "1. **Data Aggregation Preparation**:\n   - Create a temporary table to store aggregated daily average values.\n   - Select the necessary date and location columns.\n   - Calculate the average value of the target variable for each unique combination of date and location.\n\n2. **Filtering by Date Range**:\n   - Ensure that only the records within the specified date range are considered for aggregation.\n\n3. **Grouping Data**:\n   - Group the data by date and location to facilitate the calculation of daily averages.\n\n4. **Calculate Daily Averages**:\n   - Compute the average value of the target variable for each group (combination of date and location).\n\n5. **Exclude Missing Values**:\n   - Filter out any records where the computed average is missing.\n\n6. **Sort and Limit Results**:\n   - Sort the resulting dataset by the calculated average in descending order.\n   - Limit the results to the top 5 records with the highest daily averages.\n\n7. **Final Selection**:\n   - Select the required columns from the temporary table to present the final output, including date, location, and the calculated averages.",
        "special_function": null
    },
    {
        "instance_id": "bq181",
        "db": "bigquery-public-data.noaa_gsod\nbigquery-public-data.noaa_historic_severe_storms\nbigquery-public-data.noaa_hurricanes\nbigquery-public-data.noaa_tsunami\nbigquery-public-data.noaa_significant_earthquakes\nbigquery-public-data.noaa_preliminary_severe_storms\nbigquery-public-data.noaa_pifsc_metadata\nbigquery-public-data.noaa_passive_bioacoustic\nbigquery-public-data.noaa_passive_acoustic_index\nbigquery-public-data.noaa_icoads",
        "question": "How much percentage of weather stations recorded temperature data for at least 90% of the days in 2022?",
        "SQL": "WITH \nNum2022TempDatesByStation AS (\n  SELECT\n    daily_weather.stn,\n    COUNT(DISTINCT DATE(\n      CAST(daily_weather.year AS INT64),\n      CAST(daily_weather.mo AS INT64),\n      CAST(daily_weather.da AS INT64)\n    )) AS num_2022_temp_dates\n  FROM\n    `bigquery-public-data.noaa_gsod.gsod2022` daily_weather\n  WHERE\n    daily_weather.temp IS NOT NULL\n    AND daily_weather.max IS NOT NULL\n    AND daily_weather.min IS NOT NULL\n    AND daily_weather.temp != 9999.9\n    AND daily_weather.max != 9999.9\n    AND daily_weather.min != 9999.9\n  GROUP BY\n    daily_weather.stn\n),\n\nTotalStations AS (\n  SELECT\n    COUNT(*) AS total_stations\n  FROM\n    `bigquery-public-data.noaa_gsod.stations` Stations\n  WHERE\n    Stations.usaf != '999999'\n),\n\nStationsWith90PercentCoverage AS (\n  SELECT\n    COUNT(*) AS stations_with_90_percent_coverage\n  FROM\n    Num2022TempDatesByStation\n  WHERE\n    num_2022_temp_dates >= 0.90 * 365\n)\n\nSELECT\n  (stations_with_90_percent_coverage / total_stations) * 100 AS percentage_of_stations_with_90_percent_coverage\nFROM\n  TotalStations,\n  StationsWith90PercentCoverage",
        "external_knowledge": null,
        "plan": "1. Calculate the total number of days in 2022 that it records the temperature data (not missing) for each station.\n2. Find out the total number of valid stations.\n3. Leverage the knowledge that 2022 has 365 days.\n4. Join the table with the stations table to find out the stations which records for at least 90% of days in 2022.\n5. Calculate the percentage of stations with over 90% records .",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE"
        ]
    },
    {
        "instance_id": "bq045",
        "db": "bigquery-public-data.noaa_gsod\nbigquery-public-data.noaa_historic_severe_storms\nbigquery-public-data.noaa_hurricanes\nbigquery-public-data.noaa_tsunami\nbigquery-public-data.noaa_significant_earthquakes\nbigquery-public-data.noaa_preliminary_severe_storms\nbigquery-public-data.noaa_pifsc_metadata\nbigquery-public-data.noaa_passive_bioacoustic\nbigquery-public-data.noaa_passive_acoustic_index\nbigquery-public-data.noaa_icoads",
        "question": "Which weather stations in Washington State had more than 150 rainy days in 2023 but fewer rainy days than in 2022? Define a 'rainy day' as any day where the precipitation recorded is more than 0 millimeters.",
        "SQL": "WITH WashingtonStations2023 AS \n    (\n        SELECT \n            weather.stn AS station_id,\n            ANY_VALUE(station.name) AS name\n        FROM\n            `bigquery-public-data.noaa_gsod.stations` AS station\n        INNER JOIN\n            `bigquery-public-data.noaa_gsod.gsod2023` AS weather\n        ON\n            station.usaf = weather.stn\n        WHERE\n            station.state = 'WA' \n            AND \n            station.usaf != '999999'\n        GROUP BY\n            station_id\n    ),\nprcp2023 AS (\nSELECT\n    washington_stations.name,\n    (\n        SELECT \n            COUNT(*)\n        FROM\n            `bigquery-public-data.noaa_gsod.gsod2023` AS weather\n        WHERE\n            washington_stations.station_id = weather.stn\n            AND\n            prcp > 0\n            AND\n            prcp !=99.99\n    )\n    AS rainy_days\nFROM \n    WashingtonStations2023 AS washington_stations\nORDER BY\n    rainy_days DESC\n),\nWashingtonStations2022 AS \n    (\n        SELECT \n            weather.stn AS station_id,\n            ANY_VALUE(station.name) AS name\n        FROM\n            `bigquery-public-data.noaa_gsod.stations` AS station\n        INNER JOIN\n            `bigquery-public-data.noaa_gsod.gsod2022` AS weather\n        ON\n            station.usaf = weather.stn\n        WHERE\n            station.state = 'WA' \n            AND \n            station.usaf != '999999'\n        GROUP BY\n            station_id\n    ),\nprcp2022 AS (\nSELECT\n    washington_stations.name,\n    (\n        SELECT \n            COUNT(*)\n        FROM\n            `bigquery-public-data.noaa_gsod.gsod2022` AS weather\n        WHERE\n            washington_stations.station_id = weather.stn\n            AND\n            prcp > 0\n            AND\n            prcp != 99.99\n    )\n    AS rainy_days\nFROM \n    WashingtonStations2022 AS washington_stations\nORDER BY\n    rainy_days DESC\n)\n\nSELECT prcp2023.name\nFROM prcp2023\nJOIN prcp2022\non prcp2023.name = prcp2022.name\nWHERE prcp2023.rainy_days > 150\nAND prcp2023.rainy_days < prcp2022.rainy_days",
        "external_knowledge": null,
        "plan": "1. Extract data for each weather station for the years 2023 and 2022, focusing on entries that record precipitation levels.\n2. Filter these records to retain only the days with precipitation greater than zero as rainy days.\n3. Merge the records from 2023 and 2022, retaining only those stations where 2023 had more than 150 rainy days and less precipitation than in 2022.\n4. Query station information to obtain the names of these weather stations.",
        "special_function": [
            "aggregate-functions/ANY_VALUE"
        ]
    },
    {
        "instance_id": "bq358",
        "db": "bigquery-public-data.noaa_gsod\nbigquery-public-data.noaa_historic_severe_storms\nbigquery-public-data.noaa_hurricanes\nbigquery-public-data.noaa_tsunami\nbigquery-public-data.noaa_significant_earthquakes\nbigquery-public-data.noaa_preliminary_severe_storms\nbigquery-public-data.noaa_pifsc_metadata\nbigquery-public-data.noaa_passive_bioacoustic\nbigquery-public-data.noaa_passive_acoustic_index\nbigquery-public-data.noaa_icoads",
        "question": "Can you tell me which bike trip in New York City on July 15, 2015, started and ended in ZIP Code areas with the highest average temperature for that day, as recorded by the Central Park weather station '94728'? If there's more than one trip that meets these criteria, I'd like to know about the one that starts in the smallest ZIP Code and ends in the largest ZIP Code.",
        "SQL": "SELECT\n ZIPSTART.zip_code AS zip_code_start,\n ZIPEND.zip_code AS zip_code_end,\n -- WEA.temp\nFROM  \n `bigquery-public-data.new_york_citibike.citibike_trips` AS TRI\nINNER JOIN\n`bigquery-public-data.geo_us_boundaries.zip_codes` ZIPSTART\nON ST_WITHIN(\nST_GEOGPOINT(TRI.start_station_longitude, TRI.start_station_latitude),\n ZIPSTART.zip_code_geom)\nINNER JOIN\n`bigquery-public-data.geo_us_boundaries.zip_codes` ZIPEND\nON ST_WITHIN(\n ST_GEOGPOINT(TRI.end_station_longitude, TRI.end_station_latitude),\nZIPEND.zip_code_geom)\nINNER JOIN\n `bigquery-public-data.noaa_gsod.gsod2015` AS WEA\n ON PARSE_DATE(\"%Y%m%d\", CONCAT(WEA.year, WEA.mo, WEA.da)) = DATE(TRI.starttime)\nWHERE\n  WEA.wban = '94728'\nAND DATE(TRI.starttime) = DATE('2015-07-15')\nORDER BY WEA.temp DESC, ZIPSTART.zip_code ASC, ZIPEND.zip_code DESC LIMIT 1",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. **Select Relevant Data**: Retrieve the starting and ending ZIP Code regions for bike trips.\n   \n2. **Identify Starting and Ending Regions**: Use spatial functions to determine the ZIP Code regions based on the geographical coordinates of the bike trip's start and end locations.\n\n3. **Join Weather Data**: Link the bike trip data with weather data by matching the trip's start date with the weather data date.\n\n4. **Filter by Weather Station**: Ensure that the weather data is sourced from the specific Central Park weather station.\n\n5. **Filter by Date Range**: Limit the data to bike trips that occurred during the summer months (July, August, September) of 2015.\n\n6. **Sort by Temperature**: Order the results by the daily mean temperature in descending order to find the trip with the highest temperature on the start day.\n\n7. **Limit Results**: Restrict the output to the top result, which corresponds to the bike trip with the highest mean temperature on its start day.\n\n8. **Output Results**: Display the starting and ending ZIP Code regions for the identified bike trip.",
        "special_function": [
            "ST_WITHIN",
            "ST_GEOGPOINT",
            "PARSE_DATE",
            "CONCAT",
            "DATE"
        ]
    },
    {
        "instance_id": "bq290",
        "db": "bigquery-public-data.noaa_gsod\nbigquery-public-data.noaa_historic_severe_storms\nbigquery-public-data.noaa_hurricanes\nbigquery-public-data.noaa_tsunami\nbigquery-public-data.noaa_significant_earthquakes\nbigquery-public-data.noaa_preliminary_severe_storms\nbigquery-public-data.noaa_pifsc_metadata\nbigquery-public-data.noaa_passive_bioacoustic\nbigquery-public-data.noaa_passive_acoustic_index\nbigquery-public-data.noaa_icoads",
        "question": "Can you calculate the difference in maximum temperature, minimum temperature, and average temperature between US and UK weather stations for each day in October 2023, excluding records with missing temperature values?",
        "SQL": "with \n\nstations_selected as (\n  select\n    usaf,\n    wban,\n    country,\n    name\n  from\n    `bigquery-public-data.noaa_gsod.stations`\n  where\n    country in ('US', 'UK')\n),\n\ndata_filtered as (\n  select\n    gsod.*,\n    stations.country\n  from\n    `bigquery-public-data.noaa_gsod.gsod2023` gsod\n  join\n    stations_selected stations\n  on\n    gsod.stn = stations.usaf\n    and gsod.wban = stations.wban\n  where\n    date(gsod.date) between '2023-10-01' and '2023-10-31'\n    and gsod.temp != 9999.9\n),\n\n-- US Metrics\nus_metrics as (\n  select\n    date(date) as metric_date,\n    avg(temp) as avg_temp_us,\n    min(temp) as min_temp_us,\n    max(temp) as max_temp_us\n  from\n    data_filtered\n  where\n    country = 'US'\n  group by\n    metric_date\n),\n\n-- UK Metrics\nuk_metrics as (\n  select\n    date(date) as metric_date,\n    avg(temp) as avg_temp_uk,\n    min(temp) as min_temp_uk,\n    max(temp) as max_temp_uk\n  from\n    data_filtered\n  where\n    country = 'UK'\n  group by\n    metric_date\n),\n\n-- Temperature Differences\ntemp_differences as (\n  select\n    us.metric_date,\n    us.max_temp_us - uk.max_temp_uk as max_temp_diff,\n    us.min_temp_us - uk.min_temp_uk as min_temp_diff,\n    us.avg_temp_us - uk.avg_temp_uk as avg_temp_diff\n  from\n    us_metrics us\n  join\n    uk_metrics uk\n  on\n    us.metric_date = uk.metric_date\n)\n\nselect \n  metric_date, \n  max_temp_diff, \n  min_temp_diff, \n  avg_temp_diff\nfrom \n  temp_differences\norder by\n  metric_date;",
        "external_knowledge": null,
        "plan": "1. **Filter Relevant Stations**:\n   - Identify and select weather stations located in the specified countries.\n\n2. **Join Station Data with Weather Data**:\n   - Combine the weather station data with the weather observations for the specified year.\n   - Filter the combined data to include only the records within the specified date range.\n   - Exclude records with missing temperature values.\n\n3. **Calculate Daily Metrics for US**:\n   - For the filtered data, compute the average, minimum, and maximum temperatures for each day in the specified date range for stations in one of the specified countries.\n   - Group the results by date.\n\n4. **Calculate Daily Metrics for UK**:\n   - Repeat the previous step for the other specified country.\n\n5. **Calculate Temperature Differences**:\n   - For each day, compute the difference in maximum, minimum, and average temperatures between the two countries.\n   - Join the daily metrics from both countries on the date to perform these calculations.\n\n6. **Output Results**:\n   - Select the computed temperature differences and the corresponding dates.\n   - Order the results by date for a chronological output.",
        "special_function": [
            "date-functions/DATE"
        ]
    },
    {
        "instance_id": "bq031",
        "db": "bigquery-public-data.noaa_gsod\nbigquery-public-data.noaa_historic_severe_storms\nbigquery-public-data.noaa_hurricanes\nbigquery-public-data.noaa_tsunami\nbigquery-public-data.noaa_significant_earthquakes\nbigquery-public-data.noaa_preliminary_severe_storms\nbigquery-public-data.noaa_pifsc_metadata\nbigquery-public-data.noaa_passive_bioacoustic\nbigquery-public-data.noaa_passive_acoustic_index\nbigquery-public-data.noaa_icoads",
        "question": "Show me the daily weather data (temperature, precipitation, and wind speed) in Rochester for the first season of year 2019, converted to Celsius, centimeters, and meters per second, respectively. Also, include the moving averages (window size = 8) and the differences between the moving averages for up to 8 days prior (all values rounded to one decimal place, sorted by date in ascending order, and records starting from 2019-01-09).",
        "SQL": "WITH transrate AS (\n    SELECT\n        DATE(CAST(year AS INT64), CAST(mo AS INT64), CAST(da AS INT64)) AS observation_date\n        , ROUND((temp - 32.0) / 1.8, 1) AS temp_mean_c -- using Celsius instead of Fahrenheit\n        , ROUND(prcp * 2.54, 1) AS prcp_cm -- from inches to centimeters\n        , ROUND(CAST(wdsp AS FLOAT64) * 1.852 / 3.6, 1) AS wdsp_ms -- from knots to meters per second\n    FROM `bigquery-public-data.noaa_gsod.gsod*`\n    WHERE _TABLE_SUFFIX = \"2019\"\n        AND CAST(mo AS INT64) <= 3\n        AND stn in (SELECT usaf FROM `bigquery-public-data.noaa_gsod.stations` WHERE name = \"ROCHESTER\")\n),\n\nmoving_avg AS (\n    SELECT\n        observation_date\n        , temp_mean_c\n        , prcp_cm\n        , wdsp_ms\n        , AVG(temp_mean_c) OVER (ORDER BY observation_date ROWS 7 PRECEDING) AS temp_moving_avg\n        , AVG(prcp_cm) OVER (ORDER BY observation_date ROWS 7 PRECEDING) AS prcp_moving_avg\n        , AVG(wdsp_ms) OVER (ORDER BY observation_date ROWS 7 PRECEDING) AS wdsp_moving_avg\n    FROM transrate\n),\n\nlag_moving_avg AS (\n    SELECT\n        observation_date\n        , temp_mean_c\n        , prcp_cm\n        , wdsp_ms\n        , LAG(temp_moving_avg, 1) OVER (ORDER BY observation_date) AS lag1_temp_moving_avg\n        , LAG(prcp_moving_avg, 1) OVER (ORDER BY observation_date) AS lag1_prcp_moving_avg\n        , LAG(wdsp_moving_avg, 1) OVER (ORDER BY observation_date) AS lag1_wdsp_moving_avg\n\n        , LAG(temp_moving_avg, 2) OVER (ORDER BY observation_date) AS lag2_temp_moving_avg\n        , LAG(prcp_moving_avg, 2) OVER (ORDER BY observation_date) AS lag2_prcp_moving_avg\n        , LAG(wdsp_moving_avg, 2) OVER (ORDER BY observation_date) AS lag2_wdsp_moving_avg\n\n        , LAG(temp_moving_avg, 3) OVER (ORDER BY observation_date) AS lag3_temp_moving_avg\n        , LAG(prcp_moving_avg, 3) OVER (ORDER BY observation_date) AS lag3_prcp_moving_avg\n        , LAG(wdsp_moving_avg, 3) OVER (ORDER BY observation_date) AS lag3_wdsp_moving_avg\n\n        , LAG(temp_moving_avg, 4) OVER (ORDER BY observation_date) AS lag4_temp_moving_avg\n        , LAG(prcp_moving_avg, 4) OVER (ORDER BY observation_date) AS lag4_prcp_moving_avg\n        , LAG(wdsp_moving_avg, 4) OVER (ORDER BY observation_date) AS lag4_wdsp_moving_avg\n\n        , LAG(temp_moving_avg, 5) OVER (ORDER BY observation_date) AS lag5_temp_moving_avg\n        , LAG(prcp_moving_avg, 5) OVER (ORDER BY observation_date) AS lag5_prcp_moving_avg\n        , LAG(wdsp_moving_avg, 5) OVER (ORDER BY observation_date) AS lag5_wdsp_moving_avg\n\n        , LAG(temp_moving_avg, 6) OVER (ORDER BY observation_date) AS lag6_temp_moving_avg\n        , LAG(prcp_moving_avg, 6) OVER (ORDER BY observation_date) AS lag6_prcp_moving_avg\n        , LAG(wdsp_moving_avg, 6) OVER (ORDER BY observation_date) AS lag6_wdsp_moving_avg\n\n        , LAG(temp_moving_avg, 7) OVER (ORDER BY observation_date) AS lag7_temp_moving_avg\n        , LAG(prcp_moving_avg, 7) OVER (ORDER BY observation_date) AS lag7_prcp_moving_avg\n        , LAG(wdsp_moving_avg, 7) OVER (ORDER BY observation_date) AS lag7_wdsp_moving_avg\n\n        , LAG(temp_moving_avg, 8) OVER (ORDER BY observation_date) AS lag8_temp_moving_avg\n        , LAG(prcp_moving_avg, 8) OVER (ORDER BY observation_date) AS lag8_prcp_moving_avg\n        , LAG(wdsp_moving_avg, 8) OVER (ORDER BY observation_date) AS lag8_wdsp_moving_avg\n    FROM moving_avg\n)\n\nSELECT\n    observation_date\n    , temp_mean_c\n    , prcp_cm\n    , wdsp_ms\n\n    , ROUND(lag1_temp_moving_avg, 1) AS lag1_temp_moving_avg\n    , ROUND(lag1_prcp_moving_avg, 1) AS lag1_prcp_moving_avg\n    , ROUND(lag1_wdsp_moving_avg, 1) AS lag1_wdsp_moving_avg\n    \n    , ROUND(lag1_temp_moving_avg - lag2_temp_moving_avg, 1) AS diff2_temp_moving_avg\n    , ROUND(lag1_prcp_moving_avg - lag2_prcp_moving_avg, 1) AS diff2_prcp_moving_avg\n    , ROUND(lag1_wdsp_moving_avg - lag2_wdsp_moving_avg, 1) AS diff2_wdsp_moving_avg\n    , ROUND(lag2_temp_moving_avg, 1) AS lag2_temp_moving_avg\n    , ROUND(lag2_prcp_moving_avg, 1) AS lag2_prcp_moving_avg\n    , ROUND(lag2_wdsp_moving_avg, 1) AS lag2_wdsp_moving_avg\n    \n    , ROUND(lag2_temp_moving_avg - lag3_temp_moving_avg, 1) AS diff3_temp_moving_avg\n    , ROUND(lag2_prcp_moving_avg - lag3_prcp_moving_avg, 1) AS diff3_prcp_moving_avg\n    , ROUND(lag2_wdsp_moving_avg - lag3_wdsp_moving_avg, 1) AS diff3_wdsp_moving_avg\n    , ROUND(lag3_temp_moving_avg, 1) AS lag3_temp_moving_avg\n    , ROUND(lag3_prcp_moving_avg, 1) AS lag3_prcp_moving_avg\n    , ROUND(lag3_wdsp_moving_avg, 1) AS lag3_wdsp_moving_avg\n    \n    , ROUND(lag3_temp_moving_avg - lag4_temp_moving_avg, 1) AS diff4_temp_moving_avg\n    , ROUND(lag3_prcp_moving_avg - lag4_prcp_moving_avg, 1) AS diff4_prcp_moving_avg\n    , ROUND(lag3_wdsp_moving_avg - lag4_wdsp_moving_avg, 1) AS diff4_wdsp_moving_avg\n    , ROUND(lag4_temp_moving_avg, 1) AS lag4_temp_moving_avg\n    , ROUND(lag4_prcp_moving_avg, 1) AS lag4_prcp_moving_avg\n    , ROUND(lag4_wdsp_moving_avg, 1) AS lag4_wdsp_moving_avg\n    \n    , ROUND(lag4_temp_moving_avg - lag5_temp_moving_avg, 1) AS diff5_temp_moving_avg\n    , ROUND(lag4_prcp_moving_avg - lag5_prcp_moving_avg, 1) AS diff5_prcp_moving_avg\n    , ROUND(lag4_wdsp_moving_avg - lag5_wdsp_moving_avg, 1) AS diff5_wdsp_moving_avg\n    , ROUND(lag5_temp_moving_avg, 1) AS lag5_temp_moving_avg\n    , ROUND(lag5_prcp_moving_avg, 1) AS lag5_prcp_moving_avg\n    , ROUND(lag5_wdsp_moving_avg, 1) AS lag5_wdsp_moving_avg\n    \n    , ROUND(lag5_temp_moving_avg - lag6_temp_moving_avg, 1) AS diff6_temp_moving_avg\n    , ROUND(lag5_prcp_moving_avg - lag6_prcp_moving_avg, 1) AS diff6_prcp_moving_avg\n    , ROUND(lag5_wdsp_moving_avg - lag6_wdsp_moving_avg, 1) AS diff6_wdsp_moving_avg\n    , ROUND(lag6_temp_moving_avg, 1) AS lag6_temp_moving_avg\n    , ROUND(lag6_prcp_moving_avg, 1) AS lag6_prcp_moving_avg\n    , ROUND(lag6_wdsp_moving_avg, 1) AS lag6_wdsp_moving_avg\n    \n    , ROUND(lag6_temp_moving_avg - lag7_temp_moving_avg, 1) AS diff7_temp_moving_avg\n    , ROUND(lag6_prcp_moving_avg - lag7_prcp_moving_avg, 1) AS diff7_prcp_moving_avg\n    , ROUND(lag6_wdsp_moving_avg - lag7_wdsp_moving_avg, 1) AS diff7_wdsp_moving_avg\n    , ROUND(lag7_temp_moving_avg, 1) AS lag7_temp_moving_avg\n    , ROUND(lag7_prcp_moving_avg, 1) AS lag7_prcp_moving_avg\n    , ROUND(lag7_wdsp_moving_avg, 1) AS lag7_wdsp_moving_avg\n    \n    , ROUND(lag7_temp_moving_avg - lag8_temp_moving_avg, 1) AS diff8_temp_moving_avg\n    , ROUND(lag7_prcp_moving_avg - lag8_prcp_moving_avg, 1) AS diff8_prcp_moving_avg\n    , ROUND(lag7_wdsp_moving_avg - lag8_wdsp_moving_avg, 1) AS diff8_wdsp_moving_avg\n    , ROUND(lag8_temp_moving_avg, 1) AS lag8_temp_moving_avg\n    , ROUND(lag8_prcp_moving_avg, 1) AS lag8_prcp_moving_avg\n    , ROUND(lag8_wdsp_moving_avg, 1) AS lag8_wdsp_moving_avg\nFROM lag_moving_avg\nWHERE\n  lag8_temp_moving_avg IS NOT NULL\nORDER BY observation_date;\n-- all result rounded to 1 decimal place",
        "external_knowledge": null,
        "plan": "1. Data Transformation and Filtering:\n   - Convert the date components into a proper date format.\n   - Transform temperature from Fahrenheit to Celsius, precipitation from inches to centimeters, and wind speed from knots to meters per second.\n   - Filter the data to include only the records for the specified location and year.\n\n2. Calculation of Moving Averages:\n   - Calculate the moving averages for temperature, precipitation, and wind speed using a window size of 8 days.\n\n3. Lagging the Moving Averages:\n   - Create lagged versions of the moving averages for up to 8 days prior.\n\n4. Calculating Differences:\n   - Compute the differences between the current moving averages and their lagged versions for up to 8 days.\n\n5. Final Selection and Sorting:\n   - Select the relevant columns, round the values to one decimal place, ensure the lagged values are not null, and sort the results by date in ascending order.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "mathematical-functions/ROUND",
            "navigation-functions/LAG"
        ]
    },
    {
        "instance_id": "bq392",
        "db": "bigquery-public-data.noaa_gsod",
        "question": "What are the top 3 dates in October 2009 with the highest average temperature for station number 723758, in the format YYYY-MM-DD?",
        "SQL": "WITH\n  # FIRST CAST EACH YEAR, MONTH, DATE TO STRINGS\n  T AS (\n    SELECT\n      *,\n      CAST(year AS STRING) AS year_string,\n      CAST(mo AS STRING) AS month_string,\n      CAST(da AS STRING) AS day_string\n    FROM\n      `bigquery-public-data.noaa_gsod.gsod2009`\n    WHERE\n      stn = \"723758\"\n  ),\n\n  # SECOND, CONCAT ALL THE STRINGS TOGETHER INTO ONE COLUMN\n  TT AS (\n    SELECT\n      *,\n      CONCAT(year_string, \"-\", month_string, \"-\", day_string) AS date_string\n    FROM\n      T\n  ),\n\n  # THIRD, CAST THE DATE STRING INTO A DATE FORMAT\n  TTT AS (\n    SELECT\n      *,\n      CAST(date_string AS DATE) AS date_date\n    FROM\n      TT\n  ),\n\n  # FOURTH, CALCULATE THE MEAN TEMPERATURE FOR EACH DATE\n  Temp_Avg AS (\n    SELECT\n      date_date,\n      AVG(temp) AS avg_temp\n    FROM\n      TTT\n    WHERE\n      date_date BETWEEN '2009-10-01' AND '2009-10-31'\n    GROUP BY\n      date_date\n  )\n\n# FINAL SELECTION OF TOP 3 DATES WITH HIGHEST MEAN TEMPERATURE\nSELECT\n  date_date AS dates\nFROM\n  Temp_Avg\nORDER BY\n  avg_temp DESC\nLIMIT 3;",
        "external_knowledge": null,
        "plan": "1. **Filter Data for Specific Criteria**:\n   - Extract records for a specific station and year.\n\n2. **Convert Numeric Components to Strings**:\n   - Transform the year, month, and day components of the date into string format.\n\n3. **Construct Date Strings**:\n   - Concatenate the year, month, and day strings to form a complete date string in the format YYYY-MM-DD.\n\n4. **Convert Strings to Date Format**:\n   - Change the concatenated date strings into a proper date format recognized by the database.\n\n5. **Filter by Date Range and Calculate Averages**:\n   - Select records within a specific date range (October 2009) and compute the average temperature for each date.\n\n6. **Retrieve Top Results**:\n   - Order the dates by the computed average temperature in descending order and limit the results to the top three dates.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "json-functions/STRING",
            "string-functions/CONCAT",
            "string-functions/FORMAT",
            "timestamp-functions/STRING"
        ]
    },
    {
        "instance_id": "bq050",
        "db": "bigquery-public-data.geo_us_boundaries\nbigquery-public-data.noaa_gsod\nbigquery-public-data.new_york_citibike\nspider2-public-data.cyclistic",
        "question": "Help me look at the total number of bike trips, average trip duration (in minutes), average daily temperature, wind speed, and precipitation when trip starts (rounded to 1 decimal), as well as the month with the most trips (e.g., `4`), categorized by different starting and ending neighborhoods in New York City for the year 2014.",
        "SQL": "WITH data AS (\n    SELECT\n        ZIPSTARTNAME.borough AS borough_start,\n        ZIPSTARTNAME.neighborhood AS neighborhood_start,\n        ZIPENDNAME.borough AS borough_end,\n        ZIPENDNAME.neighborhood AS neighborhood_end, -- different combinations of start and end neighborhoods\n        CAST(TRI.tripduration / 60 AS NUMERIC) AS trip_minutes,\n        WEA.temp AS temperature,\n        CAST(WEA.wdsp AS NUMERIC) AS wind_speed,\n        WEA.prcp AS precipitation, -- weather conditoins\n        EXTRACT(month FROM DATE(TRI.starttime)) AS start_month\n    FROM\n        `bigquery-public-data.new_york_citibike.citibike_trips` AS TRI\n    INNER JOIN\n        `bigquery-public-data.geo_us_boundaries.zip_codes` ZIPSTART\n        ON ST_WITHIN(\n            ST_GEOGPOINT(TRI.start_station_longitude, TRI.start_station_latitude),\n            ZIPSTART.zip_code_geom)\n    INNER JOIN\n        `bigquery-public-data.geo_us_boundaries.zip_codes` ZIPEND\n        ON ST_WITHIN(\n            ST_GEOGPOINT(TRI.end_station_longitude, TRI.end_station_latitude),\n            ZIPEND.zip_code_geom)\n    INNER JOIN\n        `bigquery-public-data.noaa_gsod.gsod2014` AS WEA\n        ON PARSE_DATE(\"%Y%m%d\", CONCAT(WEA.year, WEA.mo, WEA.da)) = DATE(TRI.starttime)\n    INNER JOIN\n        `spider2-public-data.cyclistic.zip_codes` AS ZIPSTARTNAME\n        ON ZIPSTART.zip_code = CAST(ZIPSTARTNAME.zip AS STRING)\n    INNER JOIN\n        `spider2-public-data.cyclistic.zip_codes` AS ZIPENDNAME\n        ON ZIPEND.zip_code = CAST(ZIPENDNAME.zip AS STRING)\n    WHERE\n        -- get the weather data for New York Central Park station\n        WEA.wban = (\n            SELECT wban \n            FROM `bigquery-public-data.noaa_gsod.stations`\n            WHERE\n                state = 'NY'\n                AND LOWER(name) LIKE LOWER('%New York Central Park%')\n            LIMIT 1\n        )\n        AND EXTRACT(YEAR FROM DATE(TRI.starttime)) = 2014\n),\nagg_data AS (\n    SELECT\n        borough_start,\n        neighborhood_start,\n        borough_end,\n        neighborhood_end,\n        COUNT(*) AS num_trips,\n        ROUND(AVG(trip_minutes), 1) AS avg_trip_minutes,\n        ROUND(AVG(temperature), 1) AS avg_temperature,\n        ROUND(AVG(wind_speed), 1) AS avg_wind_speed,\n        ROUND(AVG(precipitation), 1) AS avg_precipitation\n    FROM data\n    GROUP BY 1, 2, 3, 4\n),\nmost_common_months AS (\n    SELECT\n        borough_start,\n        neighborhood_start,\n        borough_end,\n        neighborhood_end,\n        start_month,\n        ROW_NUMBER() OVER (PARTITION BY borough_start, neighborhood_start, borough_end, neighborhood_end ORDER BY COUNT(*) DESC) AS row_num\n    FROM data\n    GROUP BY 1, 2, 3, 4, 5\n)\nSELECT\n    a.*,\n    m.start_month AS most_common_month\nFROM\n    agg_data a\nJOIN\n    most_common_months m\nON\n    a.borough_start = m.borough_start AND\n    a.neighborhood_start = m.neighborhood_start AND\n    a.borough_end = m.borough_end AND\n    a.neighborhood_end = m.neighborhood_end AND\n    m.row_num = 1\nORDER BY a.neighborhood_start, a.neighborhood_end",
        "external_knowledge": null,
        "plan": "1. Data Preparation: extract necessary data for each trip, including start and end locations, trip duration, weather conditions, and the month of the trip start.\n   - Join multiple tables to gather geographical and weather information for each trip.\n   - Use function `ST_WITHIN` and `ST_GEOGPOINT` to determine the zip codes of the start and end station.\n   - Extract the borough and neighborhood based on the zip code.\n\n2. Filtering and Transformation:\n   - Filter the data to include only trips that started in the year 2014.\n   - Ensure the weather data corresponds to the specified weather station `New York Central Park`.\n   - Convert trip duration to minutes and round weather metrics to one decimal place.\n\n3. Aggregation:\n   - Group the data by starting and ending locations.\n   - Calculate aggregate metrics: total number of trips, average trip duration, average temperature, average wind speed, and average precipitation.\n\n4. Identify Most Common Month:\n   - Determine the month with the highest number of trips for each start and end location combination.\n   - Combine this information with the aggregated data to provide a comprehensive view.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "date-functions/PARSE_DATE",
            "datetime-functions/EXTRACT",
            "geography-functions/ST_GEOGPOINT",
            "geography-functions/ST_WITHIN",
            "interval-functions/EXTRACT",
            "json-functions/STRING",
            "mathematical-functions/ROUND",
            "numbering-functions/ROW_NUMBER",
            "string-functions/CONCAT",
            "string-functions/LOWER",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/STRING"
        ]
    },
    {
        "instance_id": "bq426",
        "db": "bigquery-public-data.new_york_citibike\nspider2-public-data.cyclistic\nbigquery-public-data.geo_us_boundaries\nbigquery-public-data.noaa_gsod",
        "question": "What user type recorded the highest average temperature for trips starting and ending in New York City's zip code 10019 during 2018? Include average precipitation, wind speed, and temperature for that user type based on weather data from the New York Central Park station.",
        "SQL": "SELECT \n  trip.usertype,\n  AVG(CAST(w.prcp AS FLOAT64)) AS avg_precipitation,  \n  AVG(CAST(w.wdsp AS FLOAT64)) AS avg_wind_speed,     \n  AVG(CAST(w.temp AS FLOAT64)) AS avg_temperature\nFROM `bigquery-public-data.new_york_citibike.citibike_trips` trip\nINNER JOIN `bigquery-public-data.geo_us_boundaries.zip_codes` zip_start\n  ON ST_WITHIN(\n    ST_GEOGPOINT(trip.start_station_longitude, trip.start_station_latitude),\n    zip_start.zip_code_geom) AND zip_start.zip_code in ('10019' )\nINNER JOIN `bigquery-public-data.geo_us_boundaries.zip_codes` zip_end\n  ON ST_WITHIN(\n    ST_GEOGPOINT(trip.end_station_longitude, trip.end_station_latitude),\n    zip_end.zip_code_geom) AND zip_end.zip_code in ('10019')\nINNER JOIN `bigquery-public-data.noaa_gsod.gsod201*` w\n  ON PARSE_DATE('%Y%m%d', CONCAT(w.year, w.mo, w.da)) = DATE(trip.starttime)\nWHERE \n  w.wban IN (SELECT DISTINCT wban FROM `bigquery-public-data.noaa_gsod.stations` WHERE name = 'NEW YORK CENTRAL PARK') \n  AND EXTRACT(YEAR FROM DATE(trip.starttime)) = 2018\nGROUP BY \n  trip.usertype\nORDER BY avg_temperature DESC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "1. **Filter Trips by Year and Location**:\n   - Select trips that both start and end within the specified city boundaries.\n   - Restrict the trips to those occurring in the year 2018.\n\n2. **Join Weather Data**:\n   - Match trips to corresponding weather data based on the trip start date.\n   - Ensure the weather data is from a specific weather station in Central Park.\n\n3. **Compute Averages**:\n   - Calculate the average precipitation, wind speed, and temperature for each user type by aggregating the respective weather data fields.\n\n4. **Identify Top User Type**:\n   - Order the user types by their average trip temperature in descending order.\n   - Select the user type with the highest average temperature.\n\n5. **Output Results**:\n   - Return the user type with the highest average temperature along with their average precipitation, average wind speed, and average temperature.",
        "special_function": null
    },
    {
        "instance_id": "bq291",
        "db": "spider2-public-data.geo_openstreetmap\nspider2-public-data.noaa_global_forecast_system",
        "question": "Can you provide a daily weather summary for July 2019 within a 5 km radius of latitude 26.75 and longitude 51.5? I need the maximum, minimum, and average temperatures; total precipitation; average cloud cover between 10 AM and 5 PM; total snowfall (when average temperature is below 32\u00b0F); and total rainfall (when average temperature is 32\u00b0F or above) for each forecast date. The data should correspond to forecasts created in July 2019 for the following day.",
        "SQL": "WITH daily_forecasts AS (\n  SELECT\n    creation_time,\n    DATE(DATETIME_ADD(forecast.time, INTERVAL 1 HOUR)) AS local_forecast_date,\n    MAX(IF(forecast.temperature_2m_above_ground IS NOT NULL, forecast.temperature_2m_above_ground, NULL)) AS max_temp,\n    MIN(IF(forecast.temperature_2m_above_ground IS NOT NULL, forecast.temperature_2m_above_ground, NULL)) AS min_temp,\n    AVG(IF(forecast.temperature_2m_above_ground IS NOT NULL, forecast.temperature_2m_above_ground, NULL)) AS avg_temp,\n    SUM(IF(forecast.total_precipitation_surface IS NOT NULL, forecast.total_precipitation_surface, 0)) AS total_precipitation,\n    AVG(IF(TIME(DATETIME_ADD(forecast.time, INTERVAL 1 HOUR)) BETWEEN '10:00:00' AND '17:00:00' AND forecast.total_cloud_cover_entire_atmosphere IS NOT NULL, forecast.total_cloud_cover_entire_atmosphere, NULL)) AS avg_cloud_cover,\n    CASE\n      WHEN AVG(forecast.temperature_2m_above_ground) < 32 THEN SUM(IF(forecast.total_precipitation_surface IS NOT NULL, forecast.total_precipitation_surface, 0))\n      ELSE 0\n    END AS total_snow,\n    CASE\n      WHEN AVG(forecast.temperature_2m_above_ground) >= 32 THEN SUM(IF(forecast.total_precipitation_surface IS NOT NULL, forecast.total_precipitation_surface, 0))\n      ELSE 0\n    END AS total_rain\n  FROM\n    `spider2-public-data.noaa_global_forecast_system.NOAA_GFS0P25`,\n    UNNEST(forecast) AS forecast\n  WHERE\n    creation_time BETWEEN '2019-07-01' AND '2021-07-31' \n    AND\n    ST_DWithin(geography, ST_GeogPoint(26.75, 51.5), 5000)\n    AND DATE(forecast.time) = DATE_ADD(DATE(creation_time), INTERVAL 1 DAY)\n  GROUP BY\n    creation_time,\n    local_forecast_date\n)\nSELECT\n  creation_time,\n  local_forecast_date AS forecast_date,\n  max_temp,\n  min_temp,\n  avg_temp,\n  total_precipitation,\n  avg_cloud_cover,\n  total_snow,\n  total_rain\nFROM\n  daily_forecasts\nORDER BY\n  creation_time,\n  local_forecast_date",
        "external_knowledge": null,
        "plan": "1. **Define Subquery for Daily Forecasts:**\n   - Create a subquery to gather relevant weather data for the specified location and date.\n   - Convert forecast times to local dates by adding 1 hour and extracting the date.\n\n2. **Calculate Temperature Statistics:**\n   - Compute the maximum temperature for the day by selecting the highest value of temperature data.\n   - Compute the minimum temperature for the day by selecting the lowest value of temperature data.\n   - Compute the average temperature for the day by averaging the temperature data.\n\n3. **Calculate Precipitation:**\n   - Sum up all the precipitation data to get the total precipitation for the day.\n\n4. **Calculate Cloud Cover:**\n   - Calculate the average cloud cover during daytime hours (10:00 AM to 5:00 PM).\n\n5. **Calculate Snow and Rain:**\n   - Calculate the total snow by summing up precipitation if the average temperature is below freezing.\n   - Calculate the total rain by summing up precipitation if the average temperature is above or equal to freezing.\n\n6. **Filter and Group Data:**\n   - Filter the data to include only the forecasts created on the specified date.\n   - Ensure the data is for the specified geographical location within a 5 km radius.\n   - Group the data by creation time and local forecast date.\n\n7. **Select Final Output:**\n   - Select the relevant columns from the subquery to be included in the final output.\n   - Order the results by creation time and local forecast date.\n\n8. **Return Results:**\n   - Return the daily weather forecast summary including temperature statistics, precipitation, cloud cover, snow, and rain for the specified location and date.",
        "special_function": [
            "date-functions/DATE",
            "date-functions/DATE_ADD",
            "datetime-functions/DATETIME_ADD",
            "geography-functions/ST_DWITHIN",
            "geography-functions/ST_GEOGPOINT",
            "time-functions/TIME",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE",
            "conditional-functions/IF",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq208",
        "db": "bigquery-public-data.new_york\nbigquery-public-data.noaa_gsod",
        "question": "Can you provide weather stations within a 20-mile radius of Chappaqua, New York (Latitude: 41.197, Longitude: -73.764), and tell me the number of valid temperature observations they have recorded from 2011 to 2020?",
        "SQL": "with my_location as (\n  SELECT  ST_GEOGPOINT(-73.764, 41.197) as my_location,\n          'Chappaqua' as home\n), stations as (\n  SELECT *, ST_GEOGPOINT(lon,lat) as latlon_geo\n  FROM `bigquery-public-data.noaa_gsod.stations` \n), get_closest as (\n  SELECT home,my_location, st.*, \n  FROM (\n    SELECT ST_ASTEXT(my_location) as my_location, \n           home,\n           ARRAY_AGG( # get the closest station\n              STRUCT(usaf,wban,name,lon,lat,country,state,\n                    ST_DISTANCE(my_location, b.latlon_geo)*0.00062137 as miles)\n           ) as stations\n    FROM my_location a, stations b\n    WHERE ST_DWITHIN(my_location, b.latlon_geo, 32187)\n    GROUP BY my_location, home\n  ), UNNEST(stations) as st\n)\n\nSELECT COUNT(temp) as Data_Points\nFROM get_closest gc, `bigquery-public-data.noaa_gsod.gsod20*` gs\nWHERE max != 9999.9 AND min != 9999.9 AND temp != 9999.9\nAND   _TABLE_SUFFIX BETWEEN '11' AND '20'\nAND   gc.usaf = gs.stn\nAND   gc.wban = gs.wban\nGROUP BY home, my_location, usaf, gc.wban, name, lon, lat, country, state, miles\nORDER BY miles ASC",
        "external_knowledge": null,
        "plan": "1. **Define Location and Home**:\n   - Create a temporary reference for the specific location (latitude and longitude) and assign a name to it.\n\n2. **Fetch and Enhance Weather Stations Data**:\n   - Retrieve data about weather stations, adding a geographical point (latitude and longitude) for each station.\n\n3. **Identify Nearby Stations**:\n   - Calculate the distance between the specified location and each weather station.\n   - Filter out stations that are within a 20-mile radius of the specified location.\n   - Group these stations by their proximity to the specified location.\n\n4. **Extract Closest Station Details**:\n   - Flatten the grouped data to get detailed information about each nearby station.\n\n5. **Count Valid Temperature Observations**:\n   - Join the detailed weather station information with temperature observation data.\n   - Filter out invalid temperature readings.\n   - Restrict the data to observations recorded between the years 2011 and 2020.\n   - Count the number of valid temperature observations for each station within the specified radius.\n   - Group the results by station details and order them by proximity.\n\nThis plan ensures that the query retrieves the required weather stations within the given radius and accurately counts their valid temperature observations over the specified period.",
        "special_function": [
            "aggregate-functions/ARRAY_AGG",
            "geography-functions/ST_ASTEXT",
            "geography-functions/ST_DISTANCE",
            "geography-functions/ST_DWITHIN",
            "geography-functions/ST_GEOGPOINT",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq047",
        "db": "bigquery-public-data.noaa_gsod\nbigquery-public-data.new_york",
        "question": "Could you help me analyze the relationship between each complaint type and daily temperature in New York city, focusing on data in airports LaGuardia and JFK over the 10 years starting from 2008? Calculate the total complaint count, the total day count, and the Pearson correlation coefficient (rounded to 4 decimals) between temperature and both the count and percentage of each common (>5000 occurrences) and strongly correlated (absolute value > 0.5) complaint type.",
        "SQL": "WITH nyc_weather AS (\n    SELECT\n    -- Create a timestamp from the date components.\n    timestamp(concat(year, \"-\", mo, \"-\", da)) as timestamp,\n    -- Replace numerical null values with actual nulls\n    AVG(IF (temp=9999.9, null, temp)) AS temperature\n    FROM\n        `bigquery-public-data.noaa_gsod.gsod20*`\n    WHERE\n        CAST(YEAR AS INT64) BETWEEN 2008 AND 2018\n    AND (stn=\"725030\" OR  -- La Guardia\n        stn=\"744860\")    -- JFK\n    GROUP BY timestamp\n)\nSELECT\n    complaint_type,\n    sum(complaint_count) as total_complaint_count,\n    count(temperature) as data_count,\n    ROUND(corr(temperature, avg_count), 4) AS corr_count,\n    ROUND(corr(temperature, avg_pct_count), 4) AS corr_pct\nFrom (\n    SELECT\n        avg(pct_count) as avg_pct_count,\n        avg(day_count) as avg_count,\n        sum(day_count) as complaint_count,\n        complaint_type,\n        temperature\n    FROM (\n        SELECT\n            DATE(timestamp) AS date,\n            temperature\n        FROM\n            nyc_weather ) a\n    JOIN (\n        SELECT x.date, complaint_type, day_count, day_count / all_calls_count as pct_count\n        FROM (\n            SELECT\n                DATE(created_date) AS date,\n                complaint_type,\n                COUNT(*) AS day_count\n            FROM\n                `bigquery-public-data.new_york.311_service_requests`\n            GROUP BY\n                date,\n                complaint_type) x\n        JOIN (\n            SELECT\n                DATE(timestamp) AS date,\n                COUNT(*) AS all_calls_count\n            FROM nyc_weather\n            GROUP BY date\n        ) y\n        ON x.date = y.date\n    ) b\n    ON a.date = b.date\n    GROUP BY\n        complaint_type,\n        temperature\n)\nGROUP BY complaint_type\nHAVING\n    ABS(corr_pct) > 0.5 AND\n    total_complaint_count > 5000\nORDER BY\n    ABS(corr_pct) DESC",
        "external_knowledge": null,
        "plan": "1. Temperature Data Preparation:\n    - Extract and clean weather data for the specified 10-year period, focusing on the relevant locations.\n    - Transform date components into a single timestamp and handle any placeholder values for temperature.\n    - Calculate the average temperature for each day from the cleaned weather data.\n\n2. Prepare Complaint Data:\n    - Extract and aggregate complaint data by type and date, counting the number of complaints per day for each type.\n    - Calculate the daily proportion of each complaint type relative to the total complaints on that day.\n\n3. Join Weather and Complaint Data:\n    - Combine the weather data with the complaint data based on matching dates.\n    - Aggregate this joined data by complaint type and temperature to compute the average daily counts and proportions of each complaint.\n\n4. Analyze and Filter Results:\n    - Compute the correlation between temperature and both the average daily count and proportion of each complaint type using function `CORR`.\n    - Filter the results to include only those complaint types that are both common (more than 5000 total occurrences) and strongly correlated (with absolute correlation value greater than 0.5).\n    - Sort the results by the strength of the correlation.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "mathematical-functions/ABS",
            "mathematical-functions/ROUND",
            "statistical-aggregate-functions/CORR",
            "string-functions/CONCAT",
            "string-functions/REPLACE",
            "timestamp-functions/TIMESTAMP",
            "conditional-functions/IF"
        ]
    },
    {
        "instance_id": "bq048",
        "db": "bigquery-public-data.noaa_gsod\nbigquery-public-data.new_york",
        "question": "Which common complaint types (more than 3000 complaints) have the strongest positive and negative correlation with wind speed respectively, given the data in NYC JFK Airport from year 2011 to 2020? Also, provide the corresponding correlation values (rounded to 4 decimals).",
        "SQL": "WITH nyc_weather AS (\n    SELECT\n    -- Create a timestamp from the date components.\n    timestamp(concat(year, \"-\", mo, \"-\", da)) as timestamp,\n    -- Replace numerical null values with actual nulls\n    AVG(IF (wdsp=\"999.9\", null, CAST(wdsp AS Float64))) AS wind_speed\n    FROM\n        `bigquery-public-data.noaa_gsod.gsod20*`\n    WHERE\n        CAST(YEAR AS INT64) BETWEEN 2011 AND 2020\n    AND stn = \"744860\" -- JFK Airport\n    GROUP BY timestamp\n),\ncomplaint_correlations AS (\n    SELECT\n        complaint_type,\n        sum(complaint_count) as total_complaint_count,\n        count(wind_speed) as data_count,\n        ROUND(corr(wind_speed, avg_count), 4) AS corr_count,\n        ROUND(corr(wind_speed, avg_pct_count), 4) AS corr_pct\n    From (\n        SELECT\n            avg(pct_count) as avg_pct_count,\n            avg(day_count) as avg_count,\n            sum(day_count) as complaint_count,\n            complaint_type,\n            wind_speed\n        FROM (\n            SELECT\n                DATE(timestamp) AS date,\n                wind_speed\n            FROM\n                nyc_weather) a\n        JOIN (\n            SELECT x.date, complaint_type, day_count, day_count / all_calls_count as pct_count\n            FROM (\n                SELECT\n                        DATE(created_date) AS date,\n                        complaint_type,\n                        COUNT(*) AS day_count\n                FROM\n                    `bigquery-public-data.new_york.311_service_requests`\n                GROUP BY\n                    date,\n                    complaint_type) x\n            JOIN (\n                SELECT\n                    DATE(timestamp) AS date,\n                    COUNT(*) AS all_calls_count\n                FROM nyc_weather\n                GROUP BY date\n            ) y\n            ON x.date=y.date\n        ) b\n        ON\n            a.date = b.date\n        GROUP BY\n            complaint_type,\n            wind_speed\n    )\n    GROUP BY complaint_type\n    HAVING\n        total_complaint_count > 3000\n    ORDER BY\n    corr_pct DESC\n)\nSELECT\n    'Positive' AS correlation_type,\n    complaint_type,\n    corr_pct AS correlation\nFROM\n    complaint_correlations\nWHERE\n    corr_pct = (SELECT MAX(corr_pct) FROM complaint_correlations)\nUNION ALL\nSELECT\n    'Negative' AS correlation_type,\n    complaint_type,\n    corr_pct AS correlation\nFROM\n    complaint_correlations\nWHERE\n    corr_pct = (SELECT MIN(corr_pct) FROM complaint_correlations);",
        "external_knowledge": null,
        "plan": "1. Aggregate Wind Speed Data: Create a dataset with daily average wind speed values for a specific location and time range, replacing erroneous values with nulls.\n\n2. Compute Daily Complaint Data: Generate daily complaint counts and their proportions relative to all complaints for different complaint types over the same time range.\n\n3. Join Weather and Complaint Data: Merge the weather dataset with the complaint dataset on their date fields to align daily wind speed data with daily complaint data.\n\n4. Calculate Correlations: For each complaint type, calculate the correlation between the average daily wind speed and the average daily complaint count, filtering out complaint types with insufficient data.\n\n5. Identify Extremes: Select the complaint types with the strongest positive and negative correlations with wind speed, and report these types along with their correlation values.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "mathematical-functions/ROUND",
            "statistical-aggregate-functions/CORR",
            "string-functions/CONCAT",
            "string-functions/REPLACE",
            "timestamp-functions/TIMESTAMP",
            "conditional-functions/IF"
        ]
    },
    {
        "instance_id": "bq293",
        "db": "bigquery-public-data.geo_us_boundaries\nbigquery-public-data.new_york",
        "question": "What were the top 5 busiest pickup times and locations (by ZIP code) for yellow taxi rides in New York City on January 1, 2015? Additionally, provide detailed metrics for each of these top 5 records, including the count of rides, hourly, daily, and weekly lagged counts, as well as 14-day and 21-day average and standard deviation of ride counts.",
        "SQL": "WITH base_data AS (\n    SELECT \n        nyc_taxi.*, \n        gis.* EXCEPT (zip_code_geom)\n    FROM (\n        SELECT * \n        FROM `bigquery-public-data.new_york.tlc_yellow_trips_2015`\n        WHERE DATE(pickup_datetime) = '2015-01-01'\n            AND pickup_latitude <= 90 \n            AND pickup_latitude >= -90\n    ) AS nyc_taxi\n    JOIN (\n        SELECT \n            zip_code, \n            state_code, \n            state_name, \n            city, \n            county, \n            zip_code_geom \n        FROM `bigquery-public-data.geo_us_boundaries.zip_codes`\n        WHERE state_code = 'NY'\n    ) AS gis \n    ON ST_CONTAINS(zip_code_geom, ST_GEOGPOINT(pickup_longitude, pickup_latitude))\n),\ndistinct_datetime AS (\n    SELECT DISTINCT \n        DATETIME_TRUNC(pickup_datetime, hour) AS pickup_hour\n    FROM base_data\n),\ndistinct_zip_code AS (\n    SELECT DISTINCT zip_code \n    FROM base_data\n),\nzip_code_datetime_join AS (\n    SELECT \n        *,\n        EXTRACT(MONTH FROM pickup_hour) AS month,\n        EXTRACT(DAY FROM pickup_hour) AS day,\n        CAST(FORMAT_DATETIME('%u', pickup_hour) AS INT64) - 1 AS weekday,\n        EXTRACT(HOUR FROM pickup_hour) AS hour,\n        CASE \n            WHEN CAST(FORMAT_DATETIME('%u', pickup_hour) AS INT64) IN (6, 7) THEN 1 \n            ELSE 0 \n        END AS is_weekend\n    FROM distinct_zip_code\n    CROSS JOIN distinct_datetime\n),\nagg_data AS (\n    SELECT \n        zip_code,\n        DATETIME_TRUNC(pickup_datetime, hour) AS pickup_hour,\n        COUNT(*) AS cnt\n    FROM base_data\n    GROUP BY zip_code, pickup_hour\n),\njoin_output AS (\n    SELECT \n        zip_code_datetime.*, \n        IFNULL(agg_data.cnt, 0) AS cnt\n    FROM zip_code_datetime_join AS zip_code_datetime\n    LEFT JOIN agg_data \n    ON zip_code_datetime.zip_code = agg_data.zip_code \n        AND zip_code_datetime.pickup_hour = agg_data.pickup_hour\n),\nfinal_output AS (\n    SELECT \n        *,\n        LAG(cnt, 1) OVER (PARTITION BY zip_code ORDER BY pickup_hour) AS lag_1h_cnt,\n        LAG(cnt, 24) OVER (PARTITION BY zip_code ORDER BY pickup_hour) AS lag_1d_cnt,\n        LAG(cnt, 168) OVER (PARTITION BY zip_code ORDER BY pickup_hour) AS lag_7d_cnt,\n        LAG(cnt, 336) OVER (PARTITION BY zip_code ORDER BY pickup_hour) AS lag_14d_cnt,\n        AVG(cnt) OVER (PARTITION BY zip_code ORDER BY pickup_hour ROWS BETWEEN 168 PRECEDING AND 1 PRECEDING) AS avg_14d_cnt,\n        AVG(cnt) OVER (PARTITION BY zip_code ORDER BY pickup_hour ROWS BETWEEN 336 PRECEDING AND 1 PRECEDING) AS avg_21d_cnt,\n        CAST(STDDEV(cnt) OVER (PARTITION BY zip_code ORDER BY pickup_hour ROWS BETWEEN 168 PRECEDING AND 1 PRECEDING) AS INT64) AS std_14d_cnt,\n        CAST(STDDEV(cnt) OVER (PARTITION BY zip_code ORDER BY pickup_hour ROWS BETWEEN 336 PRECEDING AND 1 PRECEDING) AS INT64) AS std_21d_cnt\n    FROM join_output\n)\nSELECT *\nFROM final_output\nORDER BY cnt DESC\nLIMIT 5;",
        "external_knowledge": null,
        "plan": "1. **Define `base_data` CTE:**\r\n   - **Select Data:** Fetch data from the `bigquery-public-data.new_york.tlc_yellow_trips_2015` table for trips on '2015-01-01' where `pickup_latitude` is between -90 and 90.\r\n   - **Filter and Join:** Join the filtered taxi data (`nyc_taxi`) with the `bigquery-public-data.geo_us_boundaries.zip_codes` table for New York state (`state_code = 'NY'`), using spatial containment (`ST_CONTAINS`) to match pickups to zip codes, excluding the `zip_code_geom` field from the `gis` table.\r\n\r\n2. **Define `distinct_datetime` CTE:**\r\n   - **Extract Unique Hours:** Select distinct `pickup_datetime` truncated to the hour from `base_data`.\r\n\r\n3. **Define `distinct_zip_code` CTE:**\r\n   - **Extract Unique Zip Codes:** Select distinct `zip_code` from `base_data`.\r\n\r\n4. **Define `zip_code_datetime_join` CTE:**\r\n   - **Cartesian Product:** Perform a CROSS JOIN between `distinct_zip_code` and `distinct_datetime` to create combinations of all zip codes and hours.\r\n   - **Extract Date Parts:** Add columns to extract month, day, weekday, hour, and a flag (`is_weekend`) indicating if the day is a weekend.\r\n\r\n5. **Define `agg_data` CTE:**\r\n   - **Aggregate Data:** Aggregate `base_data` by `zip_code` and `pickup_hour`, counting the number of pickups per hour per zip code.\r\n\r\n6. **Define `join_output` CTE:**\r\n   - **Left Join Aggregated Data:** Perform a LEFT JOIN between `zip_code_datetime_join` and `agg_data` to include pickup counts (`cnt`), filling missing counts with 0 using `IFNULL`.\r\n\r\n7. **Define `final_output` CTE:**\r\n   - **Calculate Lag and Rolling Stats:** Compute lag values and rolling averages/std deviations over different time windows (1 hour, 1 day, 7 days, 14 days, 21 days) for each `zip_code` and `pickup_hour`.\r\n\r\n8. **Final Selection and Ordering:**\r\n   - **Select and Limit:** Select all columns from `final_output`, order by `cnt` in descending order, and limit the result to the top 5 records.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "datetime-functions/DATETIME_TRUNC",
            "datetime-functions/EXTRACT",
            "datetime-functions/FORMAT_DATETIME",
            "geography-functions/ST_CONTAINS",
            "geography-functions/ST_GEOGPOINT",
            "interval-functions/EXTRACT",
            "mathematical-functions/ROUND",
            "navigation-functions/LAG",
            "statistical-aggregate-functions/STDDEV",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE",
            "conditional-functions/IFNULL"
        ]
    },
    {
        "instance_id": "bq017",
        "db": "spider2-public-data.geo_openstreetmap",
        "question": "What are the five longest types of highways within the multipolygon boundary of Denmark (as defined by Wikidata ID 'Q35') by total length?",
        "SQL": "WITH bounding_area AS (\n  SELECT geometry \n  FROM `spider2-public-data.geo_openstreetmap.planet_features`\n  WHERE feature_type = \"multipolygons\"\n    AND ('wikidata', 'Q35') IN (\n      SELECT (key, value) \n      FROM unnest(all_tags)\n    )\n),\n\nhighway_info AS (\n  SELECT \n    SUM(ST_LENGTH(planet_features.geometry)) AS highway_length,\n    format(\"%'d\", CAST(SUM(ST_LENGTH(planet_features.geometry)) AS INT64)) AS highway_length_formatted,\n    count(*) AS highway_count,\n    (\n      SELECT value \n      FROM unnest(all_tags) \n      WHERE key = 'highway'\n    ) AS highway_type  -- Extract value of \"highway\" tag\n  FROM \n    `spider2-public-data.geo_openstreetmap.planet_features` planet_features, \n    bounding_area\n  WHERE \n    feature_type = 'lines'\n    AND 'highway' IN (\n      SELECT key \n      FROM UNNEST(all_tags)\n    ) -- Select highways\n    AND ST_DWithin(bounding_area.geometry, planet_features.geometry, 0)  -- Filter only features within bounding_area\n  GROUP BY \n    highway_type\n)\n\nSELECT \n  highway_type\nFROM\n  highway_info\nORDER BY \n  highway_length DESC\nLIMIT 5",
        "external_knowledge": null,
        "plan": "1.Select records identified by Wikidata ID Q17 to capture the geographic area of Japan, retrieving geometries of the 'multipolygons' feature type.\n2.Choose records of the 'lines' feature type, which typically represent highways and other linear geographic features, ensuring that the highway data's geometry intersects with the boundaries of Japan.\n3.Extract the type of each highway and calculate the length of these highways.\n4.Group the results by highway type and then order them by the total length of the highways in descending order.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "geography-functions/ST_DWITHIN",
            "geography-functions/ST_LENGTH",
            "interval-functions/EXTRACT",
            "string-functions/FORMAT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq131",
        "db": "spider2-public-data.geo_openstreetmap",
        "question": "What is the number of bus stops for the bus network with the most stops within the multipolygon boundary of Denmark (as defined by Wikidata ID 'Q35')?",
        "SQL": "WITH bounding_area AS (\n  SELECT geometry\n  FROM `spider2-public-data.geo_openstreetmap.planet_features`\n  WHERE feature_type = \"multipolygons\"\n    AND ('wikidata', 'Q35') IN (\n      SELECT (key, value)\n      FROM unnest(all_tags)\n    )\n),\n\nINFO AS (\n  SELECT \n    COUNT(*) AS stops_count,\n    (\n      SELECT value \n      FROM unnest(all_tags) \n      WHERE key = 'network'\n    ) AS bus_network\n  FROM \n    `spider2-public-data.geo_openstreetmap.planet_features` planet_features,\n    bounding_area\n  WHERE \n    feature_type = 'points'\n    AND ('highway', 'bus_stop') IN (\n      SELECT (key, value)\n      FROM UNNEST(all_tags)\n    ) -- Select bus stops\n    AND ST_DWithin(bounding_area.geometry, planet_features.geometry, 0)  -- Filter only features within bounding_area\n  GROUP BY \n    bus_network\n  ORDER BY \n    stops_count DESC \n)\n\nSELECT stops_count\nFROM INFO\nORDER BY \n  stops_count DESC \nLIMIT 1",
        "external_knowledge": null,
        "plan": "1. Retrieve the 'multipolygons' geometry for the area identified by Wikidata ID Q62, setting the boundaries for the query.\n2. Choose point features tagged as 'highway' with 'bus_stop' values, ensuring they are within the defined area.\n3. Extract the bus network information from the 'network' key for each stop, and count the number of stops per network.\n4. Group the results by bus network and order them by the descending count of stops to identify the network with the most stops.",
        "special_function": [
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "geography-functions/ST_DWITHIN",
            "interval-functions/EXTRACT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq349",
        "db": "spider2-public-data.geo_openstreetmap",
        "question": "Which OpenStreetMap ID from the planet features corresponds to the administrative boundary, represented as multipolygons, whose total number of 'amenity'-tagged Points of Interest (POIs) is closest to the median count among all such boundaries?",
        "SQL": "WITH bounding_area AS (\n    SELECT \n        osm_id, \n        geometry,\n        ST_Area(geometry) AS area\n    FROM `spider2-public-data.geo_openstreetmap.planet_features`\n    WHERE \n        feature_type = \"multipolygons\"\n        AND ('boundary', 'administrative') IN (SELECT (key, value) FROM UNNEST(all_tags))\n),\n\npoi AS (\n    SELECT \n        nodes.id AS poi_id,\n        nodes.geometry AS poi_geometry,\n        tags.value AS poitype\n    FROM `spider2-public-data.geo_openstreetmap.planet_nodes` AS nodes\n    JOIN UNNEST(nodes.all_tags) AS tags\n    WHERE tags.key = 'amenity'\n),\n\npoi_counts AS (\n    SELECT\n        ba.osm_id,\n        COUNT(poi.poi_id) AS total_pois\n    FROM bounding_area ba\n    JOIN poi\n    ON ST_DWithin(ba.geometry, poi.poi_geometry, 0)\n    GROUP BY ba.osm_id\n),\n\nmedian_value AS (\n    SELECT\n        APPROX_QUANTILES(total_pois, 2)[OFFSET(1)] AS median_pois\n    FROM poi_counts\n),\n\nclosest_to_median AS (\n    SELECT\n        osm_id,\n        total_pois,\n        ABS(total_pois - median_pois) AS diff_from_median\n    FROM poi_counts, median_value\n)\n\nSELECT\n    osm_id\nFROM closest_to_median\nORDER BY diff_from_median\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "1. **Identify Multipolygon Boundaries**: Create a list of areas that are multipolygons and are top-level administrative boundaries. Calculate the area of each boundary.\n\n2. **Extract POIs with Specific Tags**: Identify all Points of Interest (POIs) that have a specific tag indicating they are amenities.\n\n3. **Count POIs within Boundaries**: For each multipolygon boundary identified in step 1, count how many POIs from step 2 are located within the boundary.\n\n4. **Determine Median POI Count**: Calculate the median number of POIs across all the boundaries identified in step 3.\n\n5. **Find Closest POI Count to Median**: For each boundary, compute the absolute difference between its POI count and the median POI count. Select the boundary with the smallest difference.\n\n6. **Output the Result**: Return the ID of the boundary whose POI count is closest to the median.",
        "special_function": [
            "approximate-aggregate-functions/APPROX_QUANTILES",
            "geography-functions/ST_AREA",
            "geography-functions/ST_DWITHIN",
            "mathematical-functions/ABS",
            "other-functions/UNNEST",
            "other-functions/ARRAY_SUBSCRIPT",
            "other-functions/STRUCT_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "bq007",
        "db": "bigquery-public-data.geo_us_boundaries\nbigquery-public-data.census_bureau_acs\nspider2-public-data.cyclistic",
        "question": "Identify the top 10 U.S. states with the highest vulnerable population, calculated based on a weighted sum of employment sectors using 2017 ACS 5-Year data, and determine their average median income change from 2015 to 2018 using zip code data. ",
        "SQL": "WITH median_income_diff_by_zipcode AS (\n    WITH acs_2018 AS (\n    SELECT\n      geo_id,\n      median_income AS median_income_2018\n    FROM\n      `bigquery-public-data.census_bureau_acs.zip_codes_2018_5yr` ),\n    acs_2015 AS (\n    SELECT\n      geo_id,\n      median_income AS median_income_2015\n    FROM\n      `bigquery-public-data.census_bureau_acs.zip_codes_2015_5yr` ),\n    acs_diff AS (\n    SELECT\n      a18.geo_id,\n      a18.median_income_2018,\n      a15.median_income_2015,\n      (a18.median_income_2018 - a15.median_income_2015) AS median_income_diff\n    FROM\n      acs_2018 a18\n    JOIN\n      acs_2015 a15\n    ON\n      a18.geo_id = a15.geo_id \n  )\n  SELECT\n    geo_id,\n    AVG(median_income_diff) AS avg_median_income_diff\n  FROM\n    acs_diff\n  WHERE\n    median_income_diff IS NOT NULL\n  GROUP BY geo_id\n),\nbase_census AS (\n  SELECT\n    geo.state_name,\n    AVG(i.avg_median_income_diff) AS avg_median_income_diff,\n    SUM(\n      employed_wholesale_trade * 0.38423645320197042 +\n      occupation_natural_resources_construction_maintenance * 0.48071410777129553 +\n      employed_arts_entertainment_recreation_accommodation_food * 0.89455676291236841 +\n      employed_information * 0.31315240083507306 +\n      employed_retail_trade * 0.51 +\n      employed_public_administration * 0.039299298394228743 +\n      occupation_services * 0.36555534476489654 +\n      employed_education_health_social * 0.20323178400562944 +\n      employed_transportation_warehousing_utilities * 0.3680506593618087 +\n      employed_manufacturing * 0.40618955512572535\n    ) AS total_vulnerable\n  FROM \n    `bigquery-public-data.census_bureau_acs.zip_codes_2017_5yr` AS census\n  JOIN \n    median_income_diff_by_zipcode i ON CAST(census.geo_id AS STRING) = i.geo_id\n  JOIN \n    `bigquery-public-data.geo_us_boundaries.zip_codes` geo ON census.geo_id = geo.zip_code\n  GROUP BY \n    geo.state_name\n)\n\nSELECT \n  state_name,\n  avg_median_income_diff,\n  total_vulnerable\nFROM \n  base_census\nORDER BY \n  total_vulnerable DESC\nLIMIT 10;",
        "external_knowledge": "total_vulnerable_weights.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq429",
        "db": "bigquery-public-data.geo_us_boundaries\nbigquery-public-data.census_bureau_acs\nspider2-public-data.cyclistic",
        "question": "What are the top 5 states with the highest average median income difference from 2015 to 2018? also provide the average number of vulnerable employees across various industries for these states, using data from the ACS 5-Year Estimates for 2017.",
        "SQL": "WITH median_income_diff_by_zipcode AS (\n  WITH acs_2018 AS (\n    SELECT\n      geo_id,\n      median_income AS median_income_2018\n    FROM\n      `bigquery-public-data.census_bureau_acs.zip_codes_2018_5yr`\n  ),\n  acs_2015 AS (\n    SELECT\n      geo_id,\n      median_income AS median_income_2015\n    FROM\n      `bigquery-public-data.census_bureau_acs.zip_codes_2015_5yr`\n  ),\n  acs_diff AS (\n    SELECT\n      a18.geo_id,\n      (a18.median_income_2018 - a15.median_income_2015) AS median_income_diff\n    FROM\n      acs_2018 a18\n    JOIN\n      acs_2015 a15 ON a18.geo_id = a15.geo_id\n  )\n  SELECT\n    geo_id,\n    AVG(median_income_diff) AS avg_median_income_diff\n  FROM\n    acs_diff\n  WHERE\n    median_income_diff IS NOT NULL\n  GROUP BY geo_id\n),\nbase_census AS (\n  SELECT\n    geo.state_name,\n    AVG(i.avg_median_income_diff) AS avg_median_income_diff,\n    AVG(\n      employed_wholesale_trade * 0.38423645320197042 +\n      occupation_natural_resources_construction_maintenance * 0.48071410777129553 +\n      employed_arts_entertainment_recreation_accommodation_food * 0.89455676291236841 +\n      employed_information * 0.31315240083507306 +\n      employed_retail_trade * 0.51\n    ) AS avg_vulnerable\n  FROM\n    `bigquery-public-data.census_bureau_acs.zip_codes_2017_5yr` AS census\n  JOIN\n    median_income_diff_by_zipcode i ON CAST(census.geo_id AS STRING) = i.geo_id\n  JOIN\n    `bigquery-public-data.geo_us_boundaries.zip_codes` geo ON census.geo_id = geo.zip_code\n  GROUP BY geo.state_name\n)\n\nSELECT \n  state_name,\n  avg_median_income_diff,\n  avg_vulnerable\nFROM \n  base_census\nORDER BY \n  avg_median_income_diff DESC\nLIMIT 5;",
        "external_knowledge": "avg_vulnerable_weights.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq073",
        "db": "bigquery-public-data.geo_us_boundaries\nbigquery-public-data.census_bureau_acs\nspider2-public-data.cyclistic",
        "question": "List states in order of the total number of vulnerable workers, including the state name, the number of vulnerable workers in wholesale trade (38% of workers in that sector), and the number of vulnerable workers in manufacturing (41% of workers in that sector), based on 2015-2018 median income differences by ZIP code.",
        "SQL": "WITH median_income_diff_by_zipcode AS (\n  WITH acs_2018 AS (\n    SELECT\n      geo_id,\n      median_income AS median_income_2018\n    FROM\n      `bigquery-public-data.census_bureau_acs.zip_codes_2018_5yr`),\n  acs_2015 AS (\n    SELECT\n      geo_id,\n      median_income AS median_income_2015\n    FROM\n      `bigquery-public-data.census_bureau_acs.zip_codes_2015_5yr`),\n  acs_diff AS (\n    SELECT\n      a18.geo_id,\n      (a18.median_income_2018 - a15.median_income_2015) AS median_income_diff\n    FROM\n      acs_2018 a18\n    JOIN\n      acs_2015 a15\n    ON\n      a18.geo_id = a15.geo_id\n    WHERE\n      a18.median_income_2018 IS NOT NULL\n      AND a15.median_income_2015 IS NOT NULL\n  )\n  SELECT\n    *\n  FROM\n    acs_diff\n),\nbase_census AS (\n  SELECT\n    geo.state_name,\n    census.geo_id,\n    i.median_income_diff,\n    employed_wholesale_trade * 0.38 AS employed_wholesale_trade_vulnerable,\n    employed_manufacturing * 0.41 AS employed_manufacturing_vulnerable\n  FROM\n    `bigquery-public-data.census_bureau_acs.zip_codes_2017_5yr` AS census\n  JOIN\n    median_income_diff_by_zipcode i ON CAST(census.geo_id AS STRING) = i.geo_id\n  JOIN\n    `bigquery-public-data.geo_us_boundaries.zip_codes` geo ON census.geo_id = geo.zip_code\n),\ncensus_summary AS (\n  SELECT\n    state_name,\n    SUM(employed_wholesale_trade_vulnerable) AS total_wholesale_vulnerable,\n    SUM(employed_manufacturing_vulnerable) AS total_manufacturing_vulnerable\n  FROM\n    base_census\n  GROUP BY\n    state_name\n)\n\nSELECT\n  state_name,\n  total_wholesale_vulnerable,\n  total_manufacturing_vulnerable,\n  (total_wholesale_vulnerable + total_manufacturing_vulnerable) AS total_vulnerable_workers\nFROM\n  census_summary\nORDER BY\n  total_vulnerable_workers DESC;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq410",
        "db": "bigquery-public-data.geo_us_boundaries\nbigquery-public-data.census_bureau_acs\nspider2-public-data.cyclistic",
        "question": "Please tell me the top three states with the least poeple not in labor force, the amount increase in their total median income from 2015 to 2018, the number of people not in the labor force, and the proportion they represent according to the censustract data in 2015 and 2018.",
        "SQL": "WITH median_income_diff_by_tract AS (\n  WITH acs_2018 AS (\n    SELECT\n      geo_id,\n      median_income AS median_income_2018\n    FROM\n      bigquery-public-data.census_bureau_acs.censustract_2018_5yr\n  ),\n  acs_2015 AS (\n    SELECT\n      geo_id,\n      median_income AS median_income_2015\n    FROM\n      bigquery-public-data.census_bureau_acs.censustract_2015_5yr\n  ),\n  acs_diff AS (\n    SELECT\n      a18.geo_id,\n      (a18.median_income_2018 - a15.median_income_2015) AS median_income_diff\n    FROM\n      acs_2018 a18\n    JOIN\n      acs_2015 a15 ON a18.geo_id = a15.geo_id\n  )\n  SELECT\n    *\n  FROM\n    acs_diff\n  WHERE\n    median_income_diff IS NOT NULL\n)\n\nSELECT\n  sf.postal_code AS state,\n  SUM(i.median_income_diff) AS total_median_income_diff,\n  SUM(\n    CASE\n      WHEN unemployed_pop + not_in_labor_force - group_quarters < 0 THEN 0\n      ELSE unemployed_pop + not_in_labor_force - group_quarters\n    END\n  ) AS total_not_in_labor_force,\n  SUM(\n    CASE\n      WHEN SAFE_DIVIDE(unemployed_pop + not_in_labor_force - group_quarters, total_pop) < 0 THEN 0\n      ELSE SAFE_DIVIDE(unemployed_pop + not_in_labor_force - group_quarters, total_pop)\n    END\n  ) / COUNT(*) AS percent_not_in_labor_force\nFROM\n  bigquery-public-data.census_bureau_acs.censustract_2017_5yr AS census\nJOIN\n  median_income_diff_by_tract i ON CAST(census.geo_id AS STRING) = i.geo_id\nJOIN\n  spider2-public-data.cyclistic.state_fips sf ON census.geo_id LIKE CONCAT(sf.fips, \"%\")\nGROUP BY\n  state\nORDER BY\n  total_not_in_labor_force ASC\nLIMIT 3;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq348",
        "db": "spider2-public-data.geo_openstreetmap",
        "question": "What are the top 3 usernames with the largest number of historical nodes of hospitals, clinics, or doctors' offices tagged as amenities within the geographic area bounded by the geogpoints `(31.1798246, 18.4519921)`, `(54.3798246, 18.4519921)`, `(54.3798246, 33.6519921)`, and `(31.1798246, 33.6519921)` that are no longer present in the latest dataset (`planet nodes`)?",
        "SQL": "WITH place AS (\n  SELECT \n      ST_MAKEPOLYGON(ST_MAKELINE(\n    [ST_GEOGPOINT(31.1798246, 18.4519921),ST_GEOGPOINT(54.3798246, 18.4519921),\n     ST_GEOGPOINT(54.3798246, 33.6519921),ST_GEOGPOINT(31.1798246, 33.6519921)\n    ]\n  )) AS boundingbox\n),\nhistorical_data AS (\n  SELECT \n      hist.username, \n      hist.id\n  FROM \n      `spider2-public-data.geo_openstreetmap.history_nodes` AS hist\n  INNER JOIN UNNEST(all_tags) AS tags\n  INNER JOIN place ON ST_INTERSECTS(place.boundingbox, hist.geometry)\n  WHERE \n      tags.key = 'amenity'\n    AND tags.value IN ('hospital', 'clinic', 'doctors')\n    AND hist.id NOT IN (\n      SELECT \n          nodes.id\n      FROM \n          `spider2-public-data.geo_openstreetmap.planet_nodes` AS nodes\n      INNER JOIN UNNEST(all_tags) AS tags\n      INNER JOIN place ON ST_INTERSECTS(place.boundingbox, nodes.geometry)\n      WHERE \n          tags.key = 'amenity'\n        AND tags.value IN ('hospital', 'clinic', 'doctors')\n    )\n)\nSELECT \n    username\nFROM \n    historical_data\nGROUP BY \n    username\nORDER BY \n    COUNT(*) DESC\nLIMIT 3;",
        "external_knowledge": null,
        "plan": "1. **Define Geographic Area**: Create a polygon representing the geographic area using the provided coordinates.\n\n2. **Filter Historical Data**: \n   - Select records from the historical dataset.\n   - Unnest the tags to access individual tag key-value pairs.\n   - Ensure the historical node's location intersects with the defined geographic area.\n   - Filter for nodes tagged as amenities with specific values (hospital, clinic, doctors).\n\n3. **Exclude Existing Nodes**:\n   - From the latest dataset, select nodes within the same geographic area and having the same specific amenity tags.\n   - Exclude these nodes from the historical dataset.\n\n4. **Aggregate and Rank Users**:\n   - Group the filtered historical data by username.\n   - Count the number of relevant historical nodes per user.\n   - Order the results to get the top 3 users with the highest counts.\n\n5. **Return Results**: Output the top 3 usernames.",
        "special_function": null
    },
    {
        "instance_id": "bq253",
        "db": "spider2-public-data.geo_openstreetmap",
        "question": "Find the name of the OpenStreetMap relation that encompasses the most features within the same geographic area as the multipolygon tagged with Wikidata item 'Q1095'. The relation should have a specified name without a 'wikidata' tag, and at least one of its included features must have a 'wikidata' tag. Return the name of this relation",
        "SQL": "WITH bounding_area AS (\n    SELECT geometry FROM `spider2-public-data.geo_openstreetmap.planet_features`\n    WHERE feature_type=\"multipolygons\"\n    AND ('wikidata', 'Q1095') IN (SELECT (key, value) FROM unnest(all_tags))\n),\nrelations_wo_wikidata as (\n    SELECT planet_relations.id, (SELECT value FROM unnest(planet_relations.all_tags) where key = 'name') as name, m.id as member_id\n    FROM `spider2-public-data.geo_openstreetmap.planet_relations` as planet_relations,\n      planet_relations.members as m,\n      bounding_area\n    WHERE 'wikidata' NOT IN (SELECT key FROM UNNEST(all_tags))\n    AND ST_DWithin(bounding_area.geometry, planet_relations.geometry, 0)\n),\nbounding_area_features AS (\n    SELECT * FROM `spider2-public-data.geo_openstreetmap.planet_features` as planet_features, bounding_area\n    WHERE ST_DWithin(bounding_area.geometry, planet_features.geometry, 0)\n)\nSELECT relations_wo_wikidata.name\nFROM relations_wo_wikidata \nJOIN bounding_area_features as planet_features \nON relations_wo_wikidata.member_id = planet_features.osm_id\nWHERE 'wikidata' IN \n(SELECT key FROM UNNEST(all_tags)) \nAND relations_wo_wikidata.name IS NOT NULL\nGROUP BY id, name\nORDER BY COUNT(planet_features.osm_id) DESC\nLIMIT 1\n;",
        "external_knowledge": null,
        "plan": "1. Extract the geometry of multipolygon features having a wikidata tag.\n2. Get planet_relations without a wikidata tag, but whose geometries are within the bounding_area.\n3. Count the number of OpenStreetMaps each planet relation has through osm_id.\n4. Sort and return the planet relation id which has most OSMs.\n",
        "special_function": [
            "geography-functions/ST_DWITHIN",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq254",
        "db": "spider2-public-data.geo_openstreetmap",
        "question": "Can you find the names of the multipolygons with valid ids that rank in the top two in terms of the number of points within their boundaries, among those multipolygons that do not have a Wikidata tag but are located within the same geographic area as the multipolygon associated with Wikidata item Q191?",
        "SQL": "WITH bounding_area AS (\n    SELECT geometry FROM `spider2-public-data.geo_openstreetmap.planet_features`\n    WHERE feature_type=\"multipolygons\"\n    AND ('wikidata', 'Q191') IN (SELECT (key, value) FROM unnest(all_tags))\n),\nbounding_area_features AS (\n    SELECT planet_features.osm_id, planet_features.feature_type, planet_features.geometry, planet_features.all_tags FROM `spider2-public-data.geo_openstreetmap.planet_features` as planet_features, bounding_area\n    WHERE ST_DWithin(bounding_area.geometry, planet_features.geometry, 0)\n),\npolygons_wo_wikidata as (\n    SELECT planet_multipolygons.osm_id as id, (SELECT value FROM unnest(planet_multipolygons.all_tags) where key = 'name') as name, planet_multipolygons.geometry as geometry\n    FROM bounding_area_features as planet_multipolygons,\n      bounding_area\n    WHERE feature_type = 'multipolygons'\n    AND osm_id IS NOT NULL\n    AND 'wikidata' NOT IN (SELECT key FROM UNNEST(all_tags))\n    AND ST_DWithin(bounding_area.geometry, planet_multipolygons.geometry, 0)\n)\nSELECT polygons_wo_wikidata.name as name\nFROM bounding_area_features as baf, polygons_wo_wikidata\nWHERE 'wikidata' IN (SELECT key FROM UNNEST(all_tags)) AND polygons_wo_wikidata.name IS NOT NULL\nAND baf.feature_type = \"points\"\nAND ST_DWithin(baf.geometry, polygons_wo_wikidata.geometry, 0)\nGROUP BY polygons_wo_wikidata.id, polygons_wo_wikidata.name\nORDER BY COUNT(baf.osm_id) DESC\nLIMIT 2;",
        "external_knowledge": null,
        "plan": "1. Define a bouding_area which has the geometry of a specific multipolygon with a Wikidata ID 'Q218'.\n2. Retrieve all the features that are within the bounding_area.\n3. Select the multipolygons within the bounding_area that do not have a Wikidata tag and have a non-null osm_id, and retrieve their osm_id, name, and geometry.\n4. Count the number of OpenStreetMaps each polygon has.\n5. Sort and return the name of the polygon which has the second highest OSM number.\n",
        "special_function": [
            "geography-functions/ST_DWITHIN",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq056",
        "db": "spider2-public-data.geo_openstreetmap\nbigquery-public-data.geo_us_boundaries",
        "question": "How many different pairs of roads classified as motorway, trunk, primary, secondary, or residential in California overlap each other without sharing nodes and do not have a bridge tag, where these roads are tagged with 'highway'",
        "SQL": "DECLARE var_State            STRING DEFAULT 'CA';\nDECLARE var_RoadTypes ARRAY<STRING> DEFAULT ['motorway', 'trunk','primary', 'secondary','residential'];\n\n\nwith t_state_geo as (\n    select State\n         , state_geom as StateGeography\n      from `bigquery-public-data.geo_us_boundaries.states`\n     where LOWER(State) = LOWER(var_State)\n)\n, t_roads as (\n    select distinct Id as WayId\n      from `spider2-public-data.geo_openstreetmap.planet_ways` pw, pw.all_tags as tag\n     where LOWER(tag.Key) = 'highway'\n       and LOWER(tag.Value) in UNNEST(var_RoadTypes)\n)\n, t_state_ways as (\n    select pw.Id       as WayId\n         , pw.geometry as WayGeography\n         , pw.Nodes    as WayNodes\n      from `spider2-public-data.geo_openstreetmap.planet_ways` pw\n      join t_state_geo ts\n        on ST_CONTAINS(ts.StateGeography, pw.geometry)\n      join t_roads tr\n        on pw.Id = tr.WayId\n)\n, t_touching_ways as (\n    select    LEAST(t1.WayId, t2.WayId) as WayId\n         , GREATEST(t1.WayId, t2.WayId) as TouchingWayId\n         , t1.WayNodes, t2.WayNodes as TouchingWayNodes\n      from t_state_ways t1\n      join t_state_ways t2\n        on ST_INTERSECTS(t1.WayGeography, t2.WayGeography)\n     where not t1.WayId = t2.WayId\n)\n, t_sharing_nodes as (\n    select distinct WayId, TouchingWayId\n      from t_touching_ways t, t.WayNodes as WayNode\n     where WayNode in UNNEST(TouchingWayNodes)\n)\n, t_overlapping_ways as (\n    select distinct WayId, TouchingWayId\n      from t_touching_ways tt\n      left join t_sharing_nodes ts\n     using (WayId, TouchingWayId)\n     where ts.WayId is NULL\n)\n, t_with_metadata as (\n    select WayId, TouchingWayId\n         , pw1.all_tags as WayTags\n         , pw2.all_tags as TouchingWayTags\n         , pw1.geometry as WayGeography\n         , pw2.geometry as TouchingWayGeography\n      from t_overlapping_ways tw\n      join `spider2-public-data.geo_openstreetmap.planet_ways` pw1\n        on tw.WayId = pw1.Id\n      join `spider2-public-data.geo_openstreetmap.planet_ways` pw2\n        on tw.TouchingWayId = pw2.Id\n)\n, t_has_bridge_tag as (\n    select distinct WayId, TouchingWayId\n      from t_with_metadata t, t.WayTags as WayTag, t.TouchingWayTags as TouchingWayTag\n     where (LOWER(TouchingWayTag.key) = 'bridge' and LOWER(TouchingWayTag.value) = 'yes')\n        or (LOWER(WayTag.key) = 'bridge' and LOWER(WayTag.value) = 'yes')\n)\n, filtered_results as (\n    select WayId, TouchingWayId\n    from t_with_metadata tm\n    left join t_has_bridge_tag tb\n    using (WayId, TouchingWayId)\n    where tb.WayId is NULL\n)\nSELECT COUNT(*) AS num_overlapping_ways\nFROM filtered_results;",
        "external_knowledge": null,
        "plan": "1. Filter by State and Road Type: Identify the specified state and road type from the geographical boundaries and road data tables.\n\n2. Select Relevant Roads: Extract unique identifiers for roads that match the specified type (e.g., motorway) from the road data.\n\n3. Identify Roads Within State: Match the roads with the geographical boundary of the specified state to ensure only roads within the state are considered.\n\n4. Find Overlapping Roads: Determine pairs of roads that geographically overlap each other but do not share nodes.\n\n5. Exclude Roads with Bridges: Filter out road pairs where either road has a bridge attribute.\n\n6. Count Overlapping Pairs: Count the number of unique road pairs that overlap but do not share nodes and do not have a bridge.",
        "special_function": [
            "geography-functions/ST_CONTAINS",
            "geography-functions/ST_INTERSECTS",
            "json-functions/STRING",
            "mathematical-functions/GREATEST",
            "mathematical-functions/LEAST",
            "string-functions/LOWER",
            "timestamp-functions/STRING",
            "other-functions/DECLARE",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq289",
        "db": "bigquery-public-data.geo_us_census_places\nspider2-public-data.geo_openstreetmap",
        "question": "Can you find the shortest distance between any two amenities (either a library, place of worship, or community center) located within Philadelphia?",
        "SQL": "WITH philadelphia AS (\n    SELECT \n        * \n    FROM \n        `bigquery-public-data.geo_us_census_places.places_pennsylvania` \n    WHERE \n        place_name = 'Philadelphia'\n),\namenities AS (\n    SELECT *, \n           (\n                SELECT \n                    tags.value \n                FROM \n                    UNNEST(all_tags) AS tags \n                WHERE \n                    tags.key = 'amenity'\n           ) AS amenity\n    FROM \n        `spider2-public-data.geo_openstreetmap.planet_features_points` AS features\n    CROSS JOIN philadelphia\n    WHERE ST_CONTAINS(philadelphia.place_geom, features.geometry)\n    AND \n    (\n        EXISTS (\n            SELECT 1 \n            FROM UNNEST(all_tags) AS tags \n            WHERE tags.key = 'amenity' \n            AND tags.value IN ('library', 'place_of_worship', 'community_centre')\n        )\n    )\n),\njoiin AS (\n    SELECT \n        a1.*, \n        a2.osm_id AS nearest_osm_id, \n        ST_DISTANCE(a1.geometry, a2.geometry) AS distance, \n        ROW_NUMBER() OVER (PARTITION BY a1.osm_id ORDER BY ST_Distance(a1.geometry, a2.geometry)) AS row_num\n    FROM amenities a1\n    CROSS JOIN amenities a2\n    WHERE a1.osm_id < a2.osm_id\n    ORDER BY a1.osm_id, distance\n) \nSELECT distance\nFROM joiin  \nWHERE row_num = 1\nORDER BY distance ASC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "1. Extract all records where the place name matches 'Philadelphia'.\n2. For each point feature from a global geographic dataset, check if the point lies within the Philadelphia area.\n3. From the point's tags, filter those records where the tag key indicates an 'amenity' and the value is one of the specified types (library, place of worship, community center).\n4. Extract the value of the 'amenity' tag for each qualifying point.\n5. Perform a self-join on the amenities dataset to compare each amenity with every other amenity.\n6. For each pair of amenities (ensuring pairs are unique and non-repeating by comparing IDs), calculate the geographical distance between them.\n7. Assign a row number to each paired record, partitioned by the ID of the first amenity and ordered by the calculated distance.\n8. From the results of the self-join, filter to retain those records where the row number is 1.\n9. Order these records by distance in ascending order to find the pair with the shortest distance among all.\n",
        "special_function": [
            "geography-functions/ST_CONTAINS",
            "geography-functions/ST_DISTANCE",
            "numbering-functions/ROW_NUMBER",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq250",
        "db": "spider2-public-data.geo_openstreetmap\nspider2-public-data.worldpop",
        "question": "What is the total population living on the geography grid which is the farthest from any hospital in Singapore, based on the most recent population data before 2023? Note that geographic grids and distances are calculated based on geospatial data and GIS related functions.",
        "SQL": "WITH country_name AS (\n  SELECT 'Singapore' AS value\n),\n\nlast_updated AS (\n  SELECT\n    MAX(last_updated) AS value\n  FROM `spider2-public-data.worldpop.population_grid_1km` AS pop\n    INNER JOIN country_name ON (pop.country_name = country_name.value)\n  WHERE last_updated < '2023-01-01'\n),\n\npopulation AS (\n  SELECT\n    SUM(sum_population) AS sum_population,\n    ST_CONVEXHULL(st_union_agg(centr)) AS boundingbox\n  FROM (\n    SELECT\n      SUM(population) AS sum_population,\n      ST_CENTROID_AGG(ST_GEOGPOINT(longitude_centroid, latitude_centroid)) AS centr\n    FROM\n      `spider2-public-data.worldpop.population_grid_1km` AS pop\n      INNER JOIN country_name ON (pop.country_name = country_name.value)\n      INNER JOIN last_updated ON (pop.last_updated = last_updated.value)\n    GROUP BY geo_id\n  )\n),\n\nhospitals AS (\n  SELECT\n    layer.geometry\n  FROM\n    `spider2-public-data.geo_openstreetmap.planet_layers` AS layer\n    INNER JOIN population ON ST_INTERSECTS(population.boundingbox, layer.geometry)\n  WHERE\n    layer.layer_code in (2110, 2120)\n),\n\ndistances AS (\n  SELECT\n    pop.geo_id,\n    pop.population,\n    MIN(ST_DISTANCE(pop.geog, hospitals.geometry)) AS distance\n  FROM\n    `spider2-public-data.worldpop.population_grid_1km` AS pop\n      INNER JOIN country_name ON pop.country_name = country_name.value\n      INNER JOIN last_updated ON pop.last_updated = last_updated.value  \n      CROSS JOIN hospitals\n  WHERE pop.population > 0\n  GROUP BY geo_id, population\n)\n\nSELECT\n  SUM(pd.population) AS population\nFROM\n  distances pd\nCROSS JOIN population p\nGROUP BY distance\nORDER BY distance DESC\nLIMIT 1",
        "external_knowledge": "OpenStreetMap_data_in_layered_GIS_format.md",
        "plan": "1. Simply define a single value 'Singapore' as the country of interest.\n2. Find the most recent date when the population data was last updated for Singapore.\n3. Calculate the total population of Singapore and create a bounding box that encompasses the entire country.\n4. Select the geometries of hospitals and doctors that are within the bounding box of Singapore.\n5. Calculate the minimum distance between each populated grid cell and the nearest hospital or doctor.\n6. Aggregate the total population of all grid cells and return the total population that lives farthest away from hospitals and doctors in Singapore. \n",
        "special_function": [
            "geography-functions/ST_CENTROID_AGG",
            "geography-functions/ST_CONVEXHULL",
            "geography-functions/ST_DISTANCE",
            "geography-functions/ST_GEOGPOINT",
            "geography-functions/ST_INTERSECTS",
            "geography-functions/ST_UNION_AGG"
        ]
    },
    {
        "instance_id": "bq083",
        "db": "spider2-public-data.crypto_ethereum\nspider2-public-data.crypto_bitcoin\nspider2-public-data.crypto_bitcoin_cash\nspider2-public-data.crypto_dash\nspider2-public-data.crypto_ethereum_classic\nspider2-public-data.crypto_band\nspider2-public-data.crypto_zilliqa",
        "question": "What is the daily change in the total market value (formatted as a string in USD currency format) of the USDC token (with a target address of \"0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48\") in 2023 , considering both Mint (the input starts with 0x42966c68) and Burn (the input starts with 0x40c10f19) transactions?",
        "SQL": "CREATE TEMP FUNCTION IFMINT(input STRING, ifTrue ANY TYPE, ifFalse ANY TYPE) AS (\n    CASE\n      WHEN input LIKE \"0x40c10f19%\" THEN ifTrue\n    ELSE\n    ifFalse\n  END\n);\n\nCREATE TEMP FUNCTION\n  USD(input FLOAT64) AS ( \n    CAST(input AS STRING FORMAT \"$999,999,999,999\") \n);\n\nSELECT\n  DATE(block_timestamp) AS `Date`,\n  USD(SUM(IFMINT(input,\n        1,\n        -1) * CAST(CONCAT(\"0x\", LTRIM(SUBSTRING(input, IFMINT(input,\n                75,\n                11), 64), \"0\")) AS FLOAT64) / 1000000)) AS `\u0394 Total Market Value`\nFROM\n  `spider2-public-data.crypto_ethereum.transactions`\nWHERE\n  DATE(block_timestamp) BETWEEN \"2023-01-01\"\n  AND \"2023-12-31\"\n  AND to_address = \"0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48\" -- USDC Token\n  AND (input LIKE \"0x42966c68%\" -- Burn\n    OR input LIKE \"0x40c10f19%\" -- Mint\n    )\nGROUP BY\n  `Date`\nORDER BY\n  `Date` DESC;",
        "external_knowledge": "Total_Market_Value_Change.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq184",
        "db": "spider2-public-data.crypto_ethereum\nspider2-public-data.crypto_bitcoin\nspider2-public-data.crypto_bitcoin_cash\nspider2-public-data.crypto_dash\nspider2-public-data.crypto_ethereum_classic\nspider2-public-data.crypto_band\nspider2-public-data.crypto_zilliqa",
        "question": "I want to compute and compare the cumulative count of Ethereum smart contracts created by users versus created by other contracts. Please list out the daily cumulative tallies between 2017 and 2021.",
        "SQL": "WITH a AS (\n    SELECT \n        DATE(block_timestamp) AS date, \n        COUNT(*) AS contracts_creation\n    FROM  \n        `spider2-public-data.crypto_ethereum.traces` AS traces\n    WHERE \n        block_timestamp < '2022-01-01 00:00:00'\n        AND trace_type = 'create'\n        AND trace_address IS null\n    GROUP BY \n        date\n),\nb AS (\n    SELECT \n        date, \n        SUM(contracts_creation) OVER (ORDER BY date) AS ccc, \n        LEAD(date, 1) OVER (ORDER BY date) AS next_date\n    FROM \n        a\n    ORDER BY \n        date\n),\ncalendar AS (\n    SELECT \n        date\n    FROM \n        UNNEST(generate_date_array('2017-01-01', '2021-12-31')) AS date\n),\nc AS (\n    SELECT \n        calendar.date, \n        b.ccc\n    FROM \n        b \n    JOIN \n        calendar \n    ON \n        b.date <= calendar.date\n        AND calendar.date < b.next_date\n    ORDER BY \n        calendar.date\n),\nd AS (\n    SELECT \n        DATE(block_timestamp) AS date1, \n        COUNT(*) AS contracts_creation1\n    FROM  \n        `spider2-public-data.crypto_ethereum.traces` AS traces\n    WHERE \n        block_timestamp < '2022-01-01 00:00:00'\n        AND trace_type = 'create'\n        AND trace_address IS NOT null\n    GROUP BY \n        date1\n),\ne AS (\n    SELECT \n        date1, \n        SUM(contracts_creation1) OVER (ORDER BY date1) AS ccc1, \n        LEAD(date1, 1) OVER (ORDER BY date1) AS next_date1\n    FROM \n        d\n    ORDER BY \n        date1\n),\ncalendar1 AS (\n    SELECT \n        date1\n    FROM \n        UNNEST(generate_date_array('2017-01-01', '2021-12-31')) AS date1\n),\nf AS (\n    SELECT \n        calendar1.date1, \n        e.ccc1\n    FROM \n        e \n    JOIN \n        calendar1 \n    ON \n        e.date1 <= calendar1.date1\n        AND calendar1.date1 < e.next_date1\n    ORDER BY \n        calendar1.date1\n)\nSELECT \n    f.date1, \n    f.ccc1 AS cumulative_contract_creation_by_contracts, \n    c.ccc AS cumulative_contract_creation_by_users\nFROM \n    c \nJOIN \n    f \nON \n    f.date1 = c.date\nORDER BY \n    f.date1\n;",
        "external_knowledge": null,
        "plan": "1. **Filter Data for User-Created Contracts:**\n   - Extract records where the entity type is contract creation and the creator is a user (not another contract).\n   - Group these records by date and count the number of creations per day.\n   \n2. **Calculate Cumulative Sum for User-Created Contracts:**\n   - Compute the running total of contract creations by users, ordered by date.\n   - Determine the next date for each record to help with calendar alignment.\n\n3. **Generate Calendar for August 2021:**\n   - Create a list of all dates in August 2021 to ensure all dates are covered in the final output, even if no contracts were created on some days.\n\n4. **Align User-Created Contracts with Calendar:**\n   - Join the cumulative sum data with the calendar dates to ensure there is a record for each day in August 2021.\n   - Ensure the cumulative sum is carried forward to the next date where appropriate.\n\n5. **Filter Data for Contract-Created Contracts:**\n   - Extract records where the entity type is contract creation and the creator is another contract.\n   - Group these records by date and count the number of creations per day.\n\n6. **Calculate Cumulative Sum for Contract-Created Contracts:**\n   - Compute the running total of contract creations by contracts, ordered by date.\n   - Determine the next date for each record to help with calendar alignment.\n\n7. **Generate Calendar for August 2021 (Again):**\n   - Create a list of all dates in August 2021 to ensure all dates are covered in the final output, even if no contracts were created on some days (separate calendar for the second cumulative sum).\n\n8. **Align Contract-Created Contracts with Calendar:**\n   - Join the cumulative sum data with the calendar dates to ensure there is a record for each day in August 2021.\n   - Ensure the cumulative sum is carried forward to the next date where appropriate.\n\n9. **Combine and Compare Results:**\n   - Join the two cumulative sum datasets (user-created and contract-created) on the date to get a consolidated view.\n   - Select the date, cumulative sum of user-created contracts, and cumulative sum of contract-created contracts for the final output.\n\n10. **Order Final Output:**\n    - Ensure the final result is ordered by date for clear comparison.",
        "special_function": [
            "array-functions/GENERATE_DATE_ARRAY",
            "date-functions/DATE",
            "navigation-functions/LEAD",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq195",
        "db": "spider2-public-data.crypto_ethereum\nspider2-public-data.crypto_bitcoin\nspider2-public-data.crypto_bitcoin_cash\nspider2-public-data.crypto_dash\nspider2-public-data.crypto_ethereum_classic\nspider2-public-data.crypto_band\nspider2-public-data.crypto_zilliqa",
        "question": "What are the top 10 Ethereum addresses by balance, considering both value transactions and gas fees, before September 1, 2021? Only keep successful transactions with no call type or where the call type is 'call'.",
        "SQL": "WITH value_table AS (\n    SELECT to_address AS address, value AS value\n    FROM `spider2-public-data.crypto_ethereum.traces`\n    WHERE to_address IS NOT null\n    AND block_timestamp < '2021-09-01 00:00:00'\n    AND status=1\n    AND (call_type NOT IN ('delegatecall', 'callcode', 'staticcall') OR call_type IS null)\n    \n    UNION ALL\n    \n    SELECT from_address AS address, -value AS value\n    FROM `spider2-public-data.crypto_ethereum.traces`\n    WHERE from_address IS NOT null\n    AND block_timestamp < '2021-09-01 00:00:00'\n    AND status=1\n    AND (call_type NOT IN ('delegatecall', 'callcode', 'staticcall') OR call_type IS null)\n    \n    UNION ALL\n    \n    SELECT miner as address, SUM(CAST(receipt_gas_used AS NUMERIC) * CAST(gas_price AS NUMERIC)) AS value\n    FROM `spider2-public-data.crypto_ethereum.transactions` AS transactions\n    JOIN `spider2-public-data.crypto_ethereum.blocks` AS blocks\n    ON blocks.number = transactions.block_number\n    WHERE block_timestamp < '2021-09-01 00:00:00'\n    GROUP BY blocks.miner\n    \n    UNION ALL\n    \n    SELECT from_address as address, -(CAST(receipt_gas_used AS NUMERIC) * CAST(gas_price AS NUMERIC)) AS value\n    FROM  `spider2-public-data.crypto_ethereum.transactions`\n    WHERE block_timestamp < '2021-09-01 00:00:00'\n)\nSELECT address, SUM(value) / power(10,18) AS balance\nFROM value_table\nGROUP BY address\nORDER BY balance DESC\nLIMIT 10",
        "external_knowledge": null,
        "plan": "1. **Define the Value Table:**\n   - Create a temporary table to aggregate all relevant balance changes for addresses.\n   \n2. **Select Incoming Transactions:**\n   - Extract all transactions where the address is the recipient and the transaction was successful before the specified date.\n   - Include only specific types of calls (exclude 'delegatecall', 'callcode', 'staticcall') or if the call type is null.\n\n3. **Select Outgoing Transactions:**\n   - Extract all transactions where the address is the sender and the transaction was successful before the specified date.\n   - Negate the transaction value to represent outgoing funds.\n   - Similar to incoming transactions, exclude certain call types or if the call type is null.\n\n4. **Calculate Miner Rewards:**\n   - Sum the gas fees received by miners for all blocks mined before the specified date.\n   - Group the results by miner address to aggregate total rewards.\n\n5. **Calculate Gas Fees Spent:**\n   - Extract all transactions where the address is the sender and calculate the total gas fees spent.\n   - Negate the value to represent the outgoing gas fees.\n\n6. **Aggregate All Values:**\n   - Combine the results of the above steps into the temporary table, ensuring all values are correctly signed (positive for incoming, negative for outgoing).\n\n7. **Calculate Final Balances:**\n   - Sum all values for each address from the temporary table.\n   - Convert the summed value from the base unit to the standard unit of measurement.\n\n8. **Order and Limit Results:**\n   - Sort the addresses by their calculated balance in descending order.\n   - Limit the results to the top 10 addresses.\n\nBy following these steps, the query identifies the top 10 Ethereum addresses by balance before the specified date, considering both transaction values and gas fees.",
        "special_function": [
            "conversion-functions/CAST",
            "mathematical-functions/FLOOR",
            "mathematical-functions/POWER"
        ]
    },
    {
        "instance_id": "bq256",
        "db": "spider2-public-data.crypto_ethereum\nspider2-public-data.crypto_bitcoin\nspider2-public-data.crypto_bitcoin_cash\nspider2-public-data.crypto_dash\nspider2-public-data.crypto_ethereum_classic\nspider2-public-data.crypto_band\nspider2-public-data.crypto_zilliqa",
        "question": "Determine the Ether balance of the Ethereum address that initiated the highest number of successful transactions before September 1, 2021. Exclude specific contract call types and account for all relevant incoming and outgoing transactions. Present the balance in Ether by converting from its native unit.",
        "SQL": "WITH value_table AS (\n    SELECT to_address AS address, value AS value\n    FROM `spider2-public-data.crypto_ethereum.traces`\n    WHERE to_address IS NOT null\n    AND block_timestamp < '2021-09-01 00:00:00' \n    AND status=1\n    AND (call_type NOT IN ('delegatecall', 'callcode', 'staticcall') OR call_type IS null)\n    \n    UNION ALL\n    \n    SELECT from_address AS address, -value AS value\n    FROM `spider2-public-data.crypto_ethereum.traces`\n    WHERE from_address IS NOT null\n    AND block_timestamp < '2021-09-01 00:00:00'\n    AND status=1\n    AND (call_type NOT IN ('delegatecall', 'callcode', 'staticcall') OR call_type IS null)\n    \n    UNION ALL\n    \n    SELECT miner as address, SUM(CAST(receipt_gas_used AS NUMERIC) * CAST(gas_price AS NUMERIC)) AS value\n    FROM `spider2-public-data.crypto_ethereum.transactions` AS transactions\n    JOIN `spider2-public-data.crypto_ethereum.blocks` AS blocks\n    ON blocks.number = transactions.block_number\n    WHERE block_timestamp < '2021-09-01 00:00:00'\n    GROUP BY blocks.miner\n    \n    UNION ALL\n    \n    SELECT from_address as address, -(CAST(receipt_gas_used AS NUMERIC) * CAST(gas_price AS NUMERIC)) AS value\n    FROM  `spider2-public-data.crypto_ethereum.transactions`\n    WHERE block_timestamp < '2021-09-01 00:00:00'\n),\na AS (\n    SELECT SUM(value)/POWER(10,18) AS balance, address\n    FROM value_table\n    GROUP BY address\n    ORDER BY balance DESC\n),\nb AS (\n    SELECT to_address, COUNT(transactions.hash) AS tx_recipient\n    FROM  `spider2-public-data.crypto_ethereum.transactions` AS transactions\n    WHERE block_timestamp < '2021-09-01 00:00:00'\n    GROUP BY to_address\n), \nc AS (\n    SELECT from_address, COUNT(transactions.hash) AS tx_sender\n    FROM  `spider2-public-data.crypto_ethereum.transactions` AS transactions\n    WHERE block_timestamp < '2021-09-01 00:00:00'\n    GROUP BY from_address\n)\nSELECT balance\nFROM c LEFT JOIN a ON (a.address = c.from_address)\nORDER BY tx_sender DESC\nLIMIT 1",
        "external_knowledge": null,
        "plan": "1. Gather transaction details like addresses, values, and gas information for Ethereum transactions up to September 1, 2021.\n2. Calculate the net balance for each address by aggregating incoming and outgoing transaction values.\n3. Count the number of transactions received by each address.\n4. Count the number of transactions sent by each address.\n5. Calculate the balance for senders with the most transactions and return.\n",
        "special_function": [
            "conversion-functions/CAST",
            "mathematical-functions/POWER"
        ]
    },
    {
        "instance_id": "bq080",
        "db": "spider2-public-data.crypto_ethereum\nspider2-public-data.crypto_bitcoin\nspider2-public-data.crypto_bitcoin_cash\nspider2-public-data.crypto_dash\nspider2-public-data.crypto_ethereum_classic\nspider2-public-data.crypto_band\nspider2-public-data.crypto_zilliqa",
        "question": "Can you provide a daily summary showing the cumulative number of Ethereum smart contracts created by users and other contracts, from August 30, 2018, to September 30, 2018, based on Ethereum transaction traces?",
        "SQL": "WITH a AS (\n    SELECT \n        DATE(block_timestamp) AS date, \n        COUNT(*) AS contracts_creation\n    FROM `spider2-public-data.crypto_ethereum.traces` AS traces\n    WHERE \n        block_timestamp < '2018-10-01 00:00:00'\n        AND trace_type = 'create'\n        AND trace_address IS NULL\n    GROUP BY date\n),\nb AS (\n    SELECT \n        date, \n        SUM(contracts_creation) OVER (ORDER BY date) AS ccc, \n        LEAD(date, 1) OVER (ORDER BY date) AS next_date\n    FROM a\n    ORDER BY date\n),\ncalendar AS (\n    SELECT \n        date\n    FROM UNNEST(generate_date_array('2018-08-30', '2018-09-30')) AS date\n),\nc AS (\n    SELECT \n        calendar.date, \n        b.ccc\n    FROM b\n    JOIN calendar \n        ON b.date <= calendar.date\n        AND (calendar.date < b.next_date OR b.next_date IS NULL)\n    ORDER BY calendar.date\n),\nd AS (\n    SELECT \n        DATE(block_timestamp) AS date1, \n        COUNT(*) AS contracts_creation1\n    FROM `spider2-public-data.crypto_ethereum.traces` AS traces\n    WHERE \n        block_timestamp < '2018-10-01 00:00:00'\n        AND trace_type = 'create'\n        AND trace_address IS NOT NULL\n    GROUP BY date1\n),\ne AS (\n    SELECT \n        date1, \n        SUM(contracts_creation1) OVER (ORDER BY date1) AS ccc1, \n        LEAD(date1, 1) OVER (ORDER BY date1) AS next_date1\n    FROM d\n    ORDER BY date1\n),\ncalendar1 AS (\n    SELECT \n        date1\n    FROM UNNEST(generate_date_array('2018-08-30', '2018-09-30')) AS date1\n),\nf AS (\n    SELECT \n        calendar1.date1, \n        e.ccc1\n    FROM e\n    JOIN calendar1 \n        ON e.date1 <= calendar1.date1\n        AND (calendar1.date1 < e.next_date1 OR e.next_date1 IS NULL)\n    ORDER BY calendar1.date1\n)\nSELECT \n    c.date, \n    f.ccc1 AS cumulative_contract_creation_by_contracts, \n    c.ccc AS cumulative_contract_creation_by_users\nFROM c\nJOIN f \n    ON f.date1 = c.date\nORDER BY f.date1;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq342",
        "db": "spider2-public-data.crypto_ethereum\nspider2-public-data.crypto_bitcoin\nspider2-public-data.crypto_bitcoin_cash\nspider2-public-data.crypto_dash\nspider2-public-data.crypto_ethereum_classic\nspider2-public-data.crypto_band\nspider2-public-data.crypto_zilliqa",
        "question": "What is the difference between the average hourly changes in transaction values for the Ethereum token 0x68e54af74b22acaccffa04ccaad13be16ed14eac, involving the addresses 0x8babf0ba311aab914c00e8fda7e8558a8b66de5d and 0xfbd6c6b112214d949dcdfb1217153bc0a742862f as either sender or receiver, between the years 2019 and 2020?",
        "SQL": "WITH transaction_addresses AS (\n  SELECT \n    from_address, \n    to_address, \n    block_timestamp, \n    CAST(value AS FLOAT64) AS value\n  FROM \n    `spider2-public-data.crypto_ethereum.token_transfers`\n  WHERE \n    token_address = '0x68e54af74b22acaccffa04ccaad13be16ed14eac'\n    AND (from_address = '0x8babf0ba311aab914c00e8fda7e8558a8b66de5d'\n         OR to_address = '0xfbd6c6b112214d949dcdfb1217153bc0a742862f')\n    AND DATE(block_timestamp) BETWEEN '2019-01-01' AND '2020-12-31'\n),\n\nout_addresses AS (\n  SELECT \n    from_address, \n    TIMESTAMP_TRUNC(block_timestamp, HOUR) AS hour, \n    SUM(value) AS hourly_change  \n  FROM \n    transaction_addresses\n  WHERE \n    from_address = '0x8babf0ba311aab914c00e8fda7e8558a8b66de5d'\n  GROUP BY \n    from_address, TIMESTAMP_TRUNC(block_timestamp, HOUR)\n),\n\nin_addresses AS (\n  SELECT \n    to_address, \n    TIMESTAMP_TRUNC(block_timestamp, HOUR) AS hour, \n    SUM(value) AS hourly_change\n  FROM \n    transaction_addresses\n  WHERE \n    to_address = '0xfbd6c6b112214d949dcdfb1217153bc0a742862f'\n  GROUP BY \n    to_address, TIMESTAMP_TRUNC(block_timestamp, HOUR)\n),\n\nall_addresses AS (\n  SELECT \n    from_address AS address, \n    hour, \n    hourly_change\n  FROM \n    out_addresses\n\n  UNION ALL\n\n  SELECT \n    to_address AS address, \n    hour, \n    hourly_change\n  FROM \n    in_addresses\n),\n\navg_hourly_change_2019 AS (\n  SELECT \n    AVG(hourly_change) AS avg_change_2019\n  FROM \n    all_addresses\n  WHERE \n    EXTRACT(YEAR FROM hour) = 2019\n),\n\navg_hourly_change_2020 AS (\n  SELECT \n    AVG(hourly_change) AS avg_change_2020\n  FROM \n    all_addresses\n  WHERE \n    EXTRACT(YEAR FROM hour) = 2020\n)\n\nSELECT \n  a.avg_change_2020 - b.avg_change_2019 AS diff_avg_hourly_change\nFROM \n  avg_hourly_change_2019 b,\n  avg_hourly_change_2020 a;",
        "external_knowledge": null,
        "plan": "1. **Filter Transactions**: Begin by filtering the dataset to include only transactions related to a specific token and involving either of two specified addresses, within the years 2019 and 2020.\n\n2. **Extract Outgoing Transactions**: From the filtered data, identify and group transactions where the first address is the sender. Aggregate the transaction values on an hourly basis.\n\n3. **Extract Incoming Transactions**: Similarly, identify and group transactions where the second address is the receiver. Aggregate these transaction values on an hourly basis.\n\n4. **Combine Transactions**: Combine the hourly aggregated outgoing and incoming transaction records into a single dataset, ensuring the address and hourly changes are included.\n\n5. **Calculate Average for 2019**: From the combined dataset, calculate the average of the hourly changes for all transactions occurring in the year 2019.\n\n6. **Calculate Average for 2020**: Similarly, calculate the average of the hourly changes for all transactions occurring in the year 2020.\n\n7. **Compute Difference**: Finally, compute the difference between the average hourly changes of 2020 and 2019 to find the required result.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/TIMESTAMP_TRUNC"
        ]
    },
    {
        "instance_id": "bq341",
        "db": "spider2-public-data.crypto_ethereum\nspider2-public-data.crypto_bitcoin\nspider2-public-data.crypto_bitcoin_cash\nspider2-public-data.crypto_dash\nspider2-public-data.crypto_ethereum_classic\nspider2-public-data.crypto_band\nspider2-public-data.crypto_zilliqa",
        "question": "Which Ethereum address has the top 3 smallest positive balance from transactions involving the token at address \"0xa92a861fc11b99b24296af880011b47f9cafb5ab\"?",
        "SQL": "WITH transaction_addresses as (\n     SELECT \n          from_address, \n          to_address, \n          CAST(value as numeric)/1000000 as value\n     FROM \n          `spider2-public-data.crypto_ethereum.token_transfers` \n     WHERE \n          token_address = \"0xa92a861fc11b99b24296af880011b47f9cafb5ab\"\n     ),\n\n     out_addresses as (\n     SELECT \n          from_address, SUM(-1*value) as total_value\n     FROM \n          transaction_addresses\n     GROUP BY \n          from_address\n     ),\n\n     in_addresses as (\n     SELECT \n          to_address, \n          SUM(value) as total_value\n     FROM \n          transaction_addresses\n     GROUP BY \n          to_address\n     ),\n\n     all_addresses as (\n     SELECT \n          from_address as address, \n          total_value\n     FROM \n          out_addresses\n\n     UNION ALL\n\n     SELECT \n          to_address as address, \n          total_value\n     FROM \n          in_addresses\n     )\n\n     SELECT \n          address\n     FROM \n          all_addresses\n     GROUP BY \n          address\n     HAVING \n          sum(total_value) > 0\n     ORDER BY \n          sum(total_value) ASC\n     LIMIT 3;",
        "external_knowledge": null,
        "plan": "1. **Filter Transactions**: Extract all transactions involving the specified token address that occurred after the given date. For each transaction, capture the sender, recipient, and value of the transaction.\n\n2. **Calculate Outgoing Balances**: Aggregate the negative values of all transactions sent from each address. This will represent the total value sent by each address.\n\n3. **Calculate Incoming Balances**: Aggregate the positive values of all transactions received by each address. This will represent the total value received by each address.\n\n4. **Combine Balances**: Create a unified list of addresses by combining the outgoing and incoming balances, ensuring that each address appears only once in the list with its corresponding balance.\n\n5. **Filter Positive Balances**: From the combined list, select only those addresses that have a positive balance.\n\n6. **Find Smallest Positive Balance**: Sort the addresses with positive balances in ascending order of their balances and select the address with the smallest positive balance. Limit the result to only one address.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE"
        ]
    },
    {
        "instance_id": "bq444",
        "db": "spider2-public-data.crypto_ethereum\nspider2-public-data.crypto_bitcoin\nspider2-public-data.crypto_bitcoin_cash\nspider2-public-data.crypto_dash\nspider2-public-data.crypto_ethereum_classic\nspider2-public-data.crypto_band\nspider2-public-data.crypto_zilliqa",
        "question": "Can you pull the blockchain timestamp, block number, and transaction hash for the first five mint and burn events from Ethereum logs for the address '0x8ad599c3a0ff1de082011efddc58f1908eb6e6d8'? Please include mint events identified by the topic '0x7a53080ba414158be7ec69b987b5fb7d07dee101fe85488f0853ae16239d0bde' and burn events by '0x0c396cd989a39f4459b5fa1aed6a9a8dcdbc45908acfd67e028cd568da98982c', and order them by block timestamp from the oldest to the newest.",
        "SQL": "CREATE TEMP FUNCTION\n  PARSE_BURN_LOG(data STRING, topics ARRAY<STRING>)\n  RETURNS STRUCT<`owner` STRING, `tickLower` STRING, `tickUpper` STRING, `amount` STRING, `amount0` STRING, `amount1` STRING>\n  LANGUAGE js AS \"\"\"\n    var parsedEvent = {\"anonymous\": false, \"inputs\": [{\"indexed\": true, \"internalType\": \"address\", \"name\": \"owner\", \"type\": \"address\"}, {\"indexed\": true, \"internalType\": \"int24\", \"name\": \"tickLower\", \"type\": \"int24\"}, {\"indexed\": true, \"internalType\": \"int24\", \"name\": \"tickUpper\", \"type\": \"int24\"}, {\"indexed\": false, \"internalType\": \"uint128\", \"name\": \"amount\", \"type\": \"uint128\"}, {\"indexed\": false, \"internalType\": \"uint256\", \"name\": \"amount0\", \"type\": \"uint256\"}, {\"indexed\": false, \"internalType\": \"uint256\", \"name\": \"amount1\", \"type\": \"uint256\"}], \"name\": \"Burn\", \"type\": \"event\"}\n    return abi.decodeEvent(parsedEvent, data, topics, false);\n\"\"\"\nOPTIONS\n  ( library=\"https://storage.googleapis.com/ethlab-183014.appspot.com/ethjs-abi.js\" );\n\nCREATE TEMP FUNCTION\n  PARSE_MINT_LOG(data STRING, topics ARRAY<STRING>)\n  RETURNS STRUCT<`sender` STRING, `owner` STRING, `tickLower` STRING, `tickUpper` STRING, `amount` STRING, `amount0` STRING, `amount1` STRING>\n  LANGUAGE js AS \"\"\"\n    var parsedEvent = {\"anonymous\": false, \"inputs\": [{\"indexed\": false, \"internalType\": \"address\", \"name\": \"sender\", \"type\": \"address\"}, {\"indexed\": true, \"internalType\": \"address\", \"name\": \"owner\", \"type\": \"address\"}, {\"indexed\": true, \"internalType\": \"int24\", \"name\": \"tickLower\", \"type\": \"int24\"}, {\"indexed\": true, \"internalType\": \"int24\", \"name\": \"tickUpper\", \"type\": \"int24\"}, {\"indexed\": false, \"internalType\": \"uint128\", \"name\": \"amount\", \"type\": \"uint128\"}, {\"indexed\": false, \"internalType\": \"uint256\", \"name\": \"amount0\", \"type\": \"uint256\"}, {\"indexed\": false, \"internalType\": \"uint256\", \"name\": \"amount1\", \"type\": \"uint256\"}], \"name\": \"Mint\", \"type\": \"event\"}\n    return abi.decodeEvent(parsedEvent, data, topics, false);\n\"\"\"\nOPTIONS\n  ( library=\"https://storage.googleapis.com/ethlab-183014.appspot.com/ethjs-abi.js\" );\n\nWITH parsed_burn_logs AS\n(SELECT\n    logs.block_timestamp AS block_timestamp\n    ,logs.block_number AS block_number\n    ,logs.transaction_hash AS transaction_hash\n    ,logs.log_index AS log_index\n    ,PARSE_BURN_LOG(logs.data, logs.topics) AS parsed\nFROM `spider2-public-data.crypto_ethereum.logs` AS logs\nWHERE address IN ('0x8ad599c3a0ff1de082011efddc58f1908eb6e6d8')\n  AND topics[SAFE_OFFSET(0)] = '0x0c396cd989a39f4459b5fa1aed6a9a8dcdbc45908acfd67e028cd568da98982c'\n)\n,parsed_mint_logs AS\n  (SELECT\n    logs.block_timestamp AS block_timestamp\n    ,logs.block_number AS block_number\n    ,logs.transaction_hash AS transaction_hash\n    ,logs.log_index AS log_index\n    ,PARSE_MINT_LOG(logs.data, logs.topics) AS parsed\nFROM `spider2-public-data.crypto_ethereum.logs` AS logs\nWHERE address IN ('0x8ad599c3a0ff1de082011efddc58f1908eb6e6d8')\n  AND topics[SAFE_OFFSET(0)] = '0x7a53080ba414158be7ec69b987b5fb7d07dee101fe85488f0853ae16239d0bde'\n)\nSELECT\n     block_timestamp\n     ,block_number\n     ,transaction_hash\nFROM parsed_mint_logs\nUNION ALL\nSELECT\n     block_timestamp\n     ,block_number\n     ,transaction_hash\nFROM parsed_burn_logs\n\nORDER BY block_timestamp LIMIT 5",
        "external_knowledge": "ethereum_logs_and_events_overview.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq340",
        "db": "spider2-public-data.crypto_ethereum\nspider2-public-data.crypto_bitcoin\nspider2-public-data.crypto_bitcoin_cash\nspider2-public-data.crypto_dash\nspider2-public-data.crypto_ethereum_classic\nspider2-public-data.crypto_band\nspider2-public-data.crypto_zilliqa",
        "question": "Which six Ethereum addresses, excluding '0x0000000000000000000000000000000000000000', have the largest absolute differences between their previous and current balances from the tokens at addresses '0x0d8775f648430679a709e98d2b0cb6250d2887ef0' and '0x1e15c05cbad367f044cbfbafda3d9a1510db5513'?",
        "SQL": "WITH token_transfers AS (\n  SELECT\n    from_address AS address,\n    -CAST(value AS NUMERIC) AS value,\n    transaction_hash,\n    block_number,\n    log_index\n  FROM `spider2-public-data.crypto_ethereum.token_transfers`\n  WHERE token_address = \"0x0d8775f648430679a709e98d2b0cb6250d2887ef0\"\n\n  UNION ALL\n\n  SELECT\n    to_address AS address,\n    CAST(value AS NUMERIC) AS value,\n    transaction_hash,\n    block_number,\n    log_index\n  FROM `spider2-public-data.crypto_ethereum.token_transfers`\n  WHERE token_address = \"0x1e15c05cbad367f044cbfbafda3d9a1510db5513\"\n),\n\nbalances AS (\n  SELECT\n    address,\n    block_number,\n    log_index,\n    SUM(value) OVER (PARTITION BY address ORDER BY block_number, log_index ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS balance\n  FROM token_transfers\n  WHERE address != \"0x0000000000000000000000000000000000000000\"\n),\n\nprevious_balances AS (\n  SELECT\n    address,\n    balance,\n    COALESCE(LAG(balance, 1) OVER (PARTITION BY address ORDER BY block_number, log_index), 0) AS prev_balance,\n    ABS(balance - COALESCE(LAG(balance, 1) OVER (PARTITION BY address ORDER BY block_number, log_index), 0)) AS abs_diff\n  FROM balances\n)\n\nSELECT\n  address\nFROM previous_balances\nORDER BY abs_diff DESC\nLIMIT 6;",
        "external_knowledge": null,
        "plan": "1. **Identify Relevant Transactions**:\n   - Create a temporary dataset combining transactions where the specified token is either sent or received.\n   - For outgoing transactions, record the sender's address with a negative token value.\n   - For incoming transactions, record the recipient's address with a positive token value.\n   - Ensure these records include transaction details like transaction hash, block number, and log index.\n\n2. **Calculate Running Balances**:\n   - For each address (excluding the specified null address), compute the running balance over time.\n   - Use the block number and log index to order the transactions chronologically.\n   - Sum the token values cumulatively to determine the balance at each point in time.\n\n3. **Determine Previous Balances**:\n   - For each balance entry, calculate the previous balance by looking at the immediately preceding balance entry.\n   - If there is no preceding entry, assume the previous balance was zero.\n   - Compute the absolute difference between the current balance and the previous balance.\n\n4. **Find Maximum Absolute Difference**:\n   - Order all entries by the computed absolute differences in descending order.\n   - Select the address associated with the largest absolute difference.\n\n5. **Output the Result**:\n   - Return the address with the largest absolute balance change.",
        "special_function": [
            "conversion-functions/CAST",
            "mathematical-functions/ABS",
            "navigation-functions/LAG",
            "conditional-functions/COALESCE"
        ]
    },
    {
        "instance_id": "bq005",
        "db": "spider2-public-data.crypto_bitcoin\nspider2-public-data.crypto_ethereum\nspider2-public-data.crypto_bitcoin_cash\nspider2-public-data.crypto_dash\nspider2-public-data.crypto_ethereum_classic\nspider2-public-data.crypto_band\nspider2-public-data.crypto_zilliqa",
        "question": "What are the average block intervals for Bitcoin blocks mined in 2023, broken down by date, and can you provide the first ten dates with their corresponding average intervals?",
        "SQL": "WITH block_rows AS (\n  SELECT *, \n         ROW_NUMBER() OVER (ORDER BY timestamp) AS row_num\n  FROM \n      `spider2-public-data.crypto_bitcoin.blocks` \n), \n\nminer_block_interval AS (\n    SELECT row_a.timestamp AS block_time, \n           row_a.number AS block_number,\n           TIMESTAMP_DIFF(row_a.timestamp,row_b.timestamp, SECOND) AS delta_block_time\n    FROM \n        block_rows row_b\n    JOIN block_rows row_a\n    ON row_b.row_num = row_a.row_num-1\n    ORDER BY \n        row_a.timestamp\n),\n\nblock_interval_by_date AS (\n  SELECT DATE(block_time) AS date, \n         AVG(delta_block_time) AS mean_block_interval\n  FROM \n      miner_block_interval\n  WHERE \n      block_number > 1 AND EXTRACT(YEAR FROM block_time) = 2023\n  GROUP BY \n      date\n)\n\nSELECT * \nFROM \n    block_interval_by_date\nORDER BY date\nLIMIT 10;",
        "external_knowledge": null,
        "plan": "1.Extract Rows with Row Numbers: First, retrieve all rows from the Bitcoin blocks dataset, assigning a sequential row number to each block based on its timestamp. \n2.Calculate Time Differences Between Consecutive Blocks: Join each row to its subsequent row using the previously assigned row numbers. Calculate the time difference in seconds between consecutive blocks. \n3.Aggregate Data by Date for 2023: Filter the time differences to include only those within the year 2023. For each date in 2023, calculate the average time interval between blocks. \n4.Order and Present the Data: Finally, the result is sorted by date, presenting a chronological view of the average block intervals for each day of 2023. ",
        "special_function": null
    },
    {
        "instance_id": "bq334",
        "db": "spider2-public-data.crypto_bitcoin\nspider2-public-data.crypto_ethereum\nspider2-public-data.crypto_bitcoin_cash\nspider2-public-data.crypto_dash\nspider2-public-data.crypto_ethereum_classic\nspider2-public-data.crypto_band\nspider2-public-data.crypto_zilliqa",
        "question": "In my Bitcoin database, there are discrepancies in transaction records. Can you determine the annual differences in average output values calculated from separate input and output records versus a consolidated transactions table, focusing only on the years common to both calculation methods?",
        "SQL": "WITH all_transactions AS (\n    SELECT \n        block_timestamp AS timestamp,\n        value,\n        'input' AS type\n    FROM \n        `spider2-public-data.crypto_bitcoin.inputs`\n    UNION ALL\n    SELECT \n        block_timestamp AS timestamp,\n        value,\n        'output' AS type\n    FROM \n        `spider2-public-data.crypto_bitcoin.outputs`\n),\nfiltered_transactions AS (\n    SELECT\n        EXTRACT(YEAR FROM timestamp) AS year,\n        value\n    FROM \n        all_transactions\n    WHERE type = 'output'\n),\naverage_output_values AS (\n    SELECT\n        year,\n        AVG(value) AS avg_value\n    FROM \n        filtered_transactions\n    GROUP BY year\n),\naverage_transaction_values AS (\n    SELECT \n    EXTRACT(YEAR FROM block_timestamp) AS year, \n    AVG(output_value) AS avg_transaction_value \n    FROM `spider2-public-data.crypto_bitcoin.transactions` \n    GROUP BY year ORDER BY year \n),\ncommon_years AS (\n    SELECT\n        ao.year,\n        ao.avg_value AS avg_output_value,\n        atv.avg_transaction_value\n    FROM\n        average_output_values ao\n    JOIN\n        average_transaction_values atv \n        ON ao.year = atv.year\n)\n\nSELECT\n    year,\n    avg_transaction_value - avg_output_value AS difference\nFROM\n    common_years\nORDER BY\n    year;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq335",
        "db": "spider2-public-data.crypto_bitcoin\nspider2-public-data.crypto_ethereum\nspider2-public-data.crypto_bitcoin_cash\nspider2-public-data.crypto_dash\nspider2-public-data.crypto_ethereum_classic\nspider2-public-data.crypto_band\nspider2-public-data.crypto_zilliqa",
        "question": "Which address, among those with the most recent transaction in October 2017, had the highest total transaction value, considering both inputs and outputs",
        "SQL": "WITH all_transactions AS (\n    -- Combine inputs and outputs\n    SELECT \n        ARRAY_TO_STRING(addresses, \",\") AS address,\n        value,\n        block_timestamp AS timestamp\n    FROM `spider2-public-data.crypto_bitcoin.inputs`\n    \n    UNION ALL\n    \n    SELECT \n        ARRAY_TO_STRING(addresses, \",\") AS address,\n        value,\n        block_timestamp AS timestamp\n    FROM `spider2-public-data.crypto_bitcoin.outputs`\n),\n\nfiltered_transactions AS (\n    -- Filter for transactions in October 2017\n    SELECT\n        address,\n        value,\n        timestamp\n    FROM all_transactions\n    WHERE EXTRACT(YEAR FROM timestamp) = 2017\n      AND EXTRACT(MONTH FROM timestamp) = 10\n),\n\naggregated_data AS (\n    -- Aggregate transaction data by address\n    SELECT\n        address,\n        SUM(value) AS total_value,\n        MAX(timestamp) AS last_transaction\n    FROM filtered_transactions\n    GROUP BY address\n),\n\n--Find the address with the highest total value and latest transaction in October 2017\nfinal_result AS (\n    SELECT\n        address\n    FROM aggregated_data\n    WHERE last_transaction = (\n        SELECT MAX(last_transaction)\n        FROM aggregated_data\n    )\n    ORDER BY total_value DESC\n    LIMIT 1\n)\n\n-- Return the result\nSELECT address\nFROM final_result;",
        "external_knowledge": null,
        "plan": "1. **Combine Data:**\n   - Retrieve all relevant records from two separate sources (inputs and outputs) and standardize the format.\n   - Combine these records into a single dataset.\n\n2. **Filter Data:**\n   - From the combined dataset, filter the records to include only those transactions that occurred in October 2017.\n\n3. **Aggregate Data:**\n   - Group the filtered transactions by address.\n   - For each address, calculate the total transaction value.\n   - Also, determine the most recent transaction date within the filtered period for each address.\n\n4. **Identify Target Address:**\n   - From the aggregated data, identify the address with the most recent transaction date in October 2017.\n   - Among these addresses, find the one with the highest total transaction value.\n\n5. **Return Result:**\n   - Extract and return the address that meets the criteria defined in the previous steps.",
        "special_function": [
            "array-functions/ARRAY_TO_STRING",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/TIMESTAMP"
        ]
    },
    {
        "instance_id": "bq057",
        "db": "spider2-public-data.crypto_bitcoin\nspider2-public-data.crypto_ethereum\nspider2-public-data.crypto_bitcoin_cash\nspider2-public-data.crypto_dash\nspider2-public-data.crypto_ethereum_classic\nspider2-public-data.crypto_band\nspider2-public-data.crypto_zilliqa",
        "question": "Which month (e.g., 3) in 2021 witnessed the highest percent of Bitcoin volume that took place in CoinJoin transactions? Also give me the percentage of CoinJoins transactions, the average input and output UTXOs ratio, and the proportion of CoinJoin transaction volume for that month (all 1 decimal).",
        "SQL": "WITH totals AS (\n-- Aggregate monthly totals for Bitcoin txs, input/output UTXOs,\n-- and input/output values (UTXO stands for Unspent Transaction Output)\n  SELECT\n    txs_tot.block_timestamp_month as tx_month,\n    count(txs_tot.hash) as tx_count,\n    sum(txs_tot.input_count) as tx_inputs,\n    sum(txs_tot.output_count) as tx_outputs,\n    sum(txs_tot.input_value) / 100000000 as tx_input_val,\n    sum(txs_tot.output_value) / 100000000 as tx_output_val\n  FROM `spider2-public-data.crypto_bitcoin.transactions` as txs_tot\n  WHERE txs_tot.block_timestamp_month BETWEEN cast('2021-01-01' as date) AND cast('2021-12-31' as date)\n  GROUP BY txs_tot.block_timestamp_month\n  ORDER BY txs_tot.block_timestamp_month desc\n),\ncoinjoinOuts AS(\n-- Builds a table where each row represents an output of a \n-- potential CoinJoin tx, defined as a tx that had more \n-- than two outputs and had a total output value less than its\n-- input value, per Adam Fiscor's description in this article: \n  SELECT \n    txs.hash,\n    txs.block_number,\n    txs.block_timestamp_month,\n    txs.input_count,\n    txs.output_count,\n    txs.input_value,\n    txs.output_value,\n    o.value as outputs_val\n  FROM `spider2-public-data.crypto_bitcoin.transactions` as txs, UNNEST(txs.outputs) as o\n  WHERE output_count > 2 AND \n    output_value <= input_value AND \n    block_timestamp_month BETWEEN cast('2021-01-01' as date) AND cast('2021-12-31' as date)\n  ORDER BY block_number, txs.hash desc\n),\ncoinjoinTxs AS(\n-- Builds a table of just the distinct CoinJoin tx hashes\n-- which had more than one equal-value output.\n  SELECT \n    STRING_AGG(DISTINCT coinjoinOuts.hash LIMIT 1) as cjHash,\n    CONCAT(coinjoinOuts.hash, \" \", cast(coinjoinOuts.outputs_val as string)) as outputVal,\n    count(*) as cjOuts\n  FROM coinjoinOuts\n  GROUP BY outputVal\n  having count(*) >1\n),\ncoinjoinsD AS(\n-- Filter out all potential CoinJoin txs that did not have\n-- more than one equal-value output. Do not list the\n-- outputs themselves, only the distinct tx hashes and\n-- their input/output counts and values.\n  SELECT \n    DISTINCT coinjoinOuts.hash, \n    coinjoinOuts.block_number, \n    coinjoinOuts.block_timestamp_month,\n    coinjoinOuts.input_count,\n    coinjoinOuts.output_count,\n    coinjoinOuts.input_value,\n    coinjoinOuts.output_value\n  FROM coinjoinOuts INNER JOIN coinjoinTxs ON coinjoinOuts.hash = coinjoinTxs.cjHash\n),\ncoinjoins AS (\n-- Aggregate monthly totals for CoinJoin txs, input/output UTXOs,\n-- and input/output values\n  SELECT \n    block_timestamp_month as cjs_month,\n    count(cjs.hash) as cjs_count,\n    sum(cjs.input_count) as cjs_inputs,\n    sum(cjs.output_count) as cjs_outputs,\n    sum(cjs.input_value)/100000000 as cjs_input_val,\n    sum(cjs.output_value)/100000000 as cjs_output_val\n  FROM coinjoinsD as cjs\n  GROUP BY cjs.block_timestamp_month\n  ORDER BY cjs.block_timestamp_month desc\n)\nSELECT extract(month from tx_month) as month,\n-- Calculate resulting CoinJoin percentages:\n-- tx_percent = percent of monthly Bitcoin txs that were CoinJoins\n  ROUND(coinjoins.cjs_count / totals.tx_count * 100, 1) as tx_percent,\n  \n-- utxos_percent = percent of monthly Bitcoin utxos that were CoinJoins\n  ROUND((coinjoins.cjs_inputs / totals.tx_inputs + coinjoins.cjs_outputs / totals.tx_outputs) / 2 * 100, 1) as utxos_percent,\n  \n-- value_percent = percent of monthly Bitcoin volume that took place\n-- in CoinJoined transactions\n  ROUND(coinjoins.cjs_input_val / totals.tx_input_val * 100, 1) as value_percent\nFROM totals\nINNER JOIN coinjoins\nON totals.tx_month = coinjoins.cjs_month\nORDER BY value_percent DESC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "1. Aggregate Monthly Totals:\n   - Calculate the monthly totals for the number of transactions, input/output counts, and input/output values for Bitcoin transactions within a specified year.\n\n2. Identify Potential CoinJoin Transactions:\n   - Identify potential CoinJoin transactions by selecting those with more than two outputs and an output value less than or equal to the input value, within the same year.\n\n3. Filter CoinJoin Transactions:\n   - Filter these potential CoinJoin transactions to only include those with more than one equal-value output, ensuring the transactions are distinct.\n\n4. Aggregate CoinJoin Monthly Totals:\n   - Aggregate the monthly totals specifically for the identified CoinJoin transactions, including their input/output counts and values.\n\n5. Calculate Percentages:\n   - For each month, calculate the percentage of transactions that were CoinJoin transactions, the average percentage of input and output UTXOs that were part of CoinJoin transactions, and the percentage of the total transaction volume that was involved in CoinJoin transactions.\n\n6. Determine the Month with Highest CoinJoin Volume:\n   - Select the month with the highest percentage of transaction volume involved in CoinJoin transactions, and return this month along with the calculated percentages, formatted to one decimal place.",
        "special_function": [
            "aggregate-functions/STRING_AGG",
            "conversion-functions/CAST",
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "json-functions/STRING",
            "mathematical-functions/ROUND",
            "string-functions/CONCAT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/STRING",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq068",
        "db": "spider2-public-data.crypto_bitcoin_cash\nspider2-public-data.crypto_bitcoin\nspider2-public-data.crypto_ethereum\nspider2-public-data.crypto_dash\nspider2-public-data.crypto_ethereum_classic\nspider2-public-data.crypto_band\nspider2-public-data.crypto_zilliqa",
        "question": "What are the maximum and minimum balances across all addresses for different address types on Bitcoin Cash during March 2014?",
        "SQL": "WITH double_entry_book AS (\n    -- debits\n    SELECT\n      ARRAY_TO_STRING(inputs.addresses, \",\") AS address,\n      inputs.type,\n      - inputs.value AS value\n    FROM\n        `spider2-public-data.crypto_bitcoin_cash.transactions`\n    JOIN\n        UNNEST(inputs) AS inputs\n    WHERE block_timestamp >= '2014-03-01' AND block_timestamp < '2014-04-01'\n\n    UNION ALL\n \n    -- credits\n    SELECT\n      ARRAY_TO_STRING(outputs.addresses, \",\") AS address,\n      outputs.type,\n      outputs.value AS value\n    FROM\n        `spider2-public-data.crypto_bitcoin_cash.transactions` JOIN UNNEST(outputs) AS outputs\n    WHERE block_timestamp >= '2014-03-01' AND block_timestamp < '2014-04-01'\n),\naddress_balances AS (\n    SELECT \n        address,\n        type,\n        SUM(value) AS balance\n    FROM double_entry_book\n    GROUP BY 1, 2\n),\nmax_min_balances AS (\n    SELECT\n        type,\n        MAX(balance) AS max_balance,\n        MIN(balance) AS min_balance\n    FROM address_balances\n    GROUP BY type\n)\nSELECT\n    type,\n    max_balance,\n    min_balance\nFROM max_min_balances\nORDER BY type;",
        "external_knowledge": null,
        "plan": "1. Setup Temporary Data:\n    - Create a Common Table Expression (CTE) to consolidate debits and credits for each address on January 1st, 2019.\n    - Debits Calculation:\n        - Select addresses and types from the `inputs` array within transactions.\n        - Negate the `inputs.value` to represent the debit.\n        - Filter for transactions that occurred on '2019-01-01'.\n    - Credits Calculation:\n        - Select addresses and types from the `outputs` array within transactions.\n        - Use `outputs.value` directly to represent the credit.\n        - Filter for transactions that occurred on '2019-01-01'.\n    - Combine both debits and credits using `UNION ALL`.\n\n2. Calculate Address Balances:\n    - Create another CTE to calculate the net balance for each address and type.\n    - Select address, type, and the sum of values from the table defined in Step 1.\n    - Group the results by address and type.\n\n3. Determine Maximum and Minimum Balances:\n    - Create a third CTE to determine the maximum and minimum balances for each address type.\n    - Select type, maximum balance, and minimum balance from  it.\n    - Group and return the results by type.",
        "special_function": [
            "array-functions/ARRAY_TO_STRING",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq092",
        "db": "spider2-public-data.crypto_dash\nspider2-public-data.crypto_bitcoin_cash\nspider2-public-data.crypto_bitcoin\nspider2-public-data.crypto_ethereum\nspider2-public-data.crypto_ethereum_classic\nspider2-public-data.crypto_band\nspider2-public-data.crypto_zilliqa",
        "question": "Tell me the highest and lowest net changes among all addresses and types on Bitcoin Cash as of April, 2023?",
        "SQL": "WITH double_entry_book AS (\n    -- debits\n    SELECT\n      ARRAY_TO_STRING(inputs.addresses, \",\") AS address,\n      inputs.type,\n      - inputs.value AS value\n    FROM\n        `spider2-public-data.crypto_dash.transactions`\n    JOIN\n        UNNEST(inputs) AS inputs\n    WHERE block_timestamp_month = '2023-04-01'\n\n    UNION ALL\n \n    -- credits\n    SELECT\n      ARRAY_TO_STRING(outputs.addresses, \",\") AS address,\n      outputs.type,\n      outputs.value AS value\n    FROM\n        `spider2-public-data.crypto_dash.transactions` JOIN UNNEST(outputs) AS outputs\n    WHERE block_timestamp_month = '2023-04-01'\n),\naddress_balances AS (\n    SELECT \n        address,\n        type,\n        SUM(value) AS balance\n    FROM double_entry_book\n    GROUP BY 1, 2\n)\nSELECT\n    MAX(balance) AS max_balance,\n    MIN(balance) AS min_balance\nFROM address_balances;",
        "external_knowledge": null,
        "plan": "1. Create a Common Table Expression (CTE) to combine all debit and credit transactions for the specified date, January 1st, 2019.\n    - Debits:\n        - Extract addresses and types from the `inputs` field of the transactions.\n        - Negate the value to represent the debit.\n        - Filter transactions where `block_timestamp_month` equals '2019-01-01'.\n    - Credits:\n        - Extract addresses and types from the `outputs` field of the transactions.\n        - Use the value as-is to represent the credit.\n        - Filter transactions where `block_timestamp_month` equals '2019-01-01'.\n    - Use `UNION ALL` to combine both debit and credit data into a single table.\n\n2. Create a second CTE to calculate the net balance for each unique address and type combination.\n    - Group the data by address and type.\n    - Sum the values to get the net balance for each group.\n\n3. Select the maximum and minimum balances:\n    - Compute the highest and lowest balances.\n    - These values represent the highest and lowest net changes among all addresses and types for the specified date.",
        "special_function": [
            "array-functions/ARRAY_TO_STRING",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq093",
        "db": "spider2-public-data.crypto_ethereum_classic\nspider2-public-data.crypto_dash\nspider2-public-data.crypto_bitcoin_cash\nspider2-public-data.crypto_bitcoin\nspider2-public-data.crypto_ethereum\nspider2-public-data.crypto_band\nspider2-public-data.crypto_zilliqa",
        "question": "Tell me the maximum and minimum net changes in balances for Ethereum Classic addresses on October 14, 2016, considering debits, credits, and gas fees, while excluding internal calls like 'delegatecall', 'callcode', and 'staticcall'.",
        "SQL": "WITH double_entry_book AS (\n    -- Debits\n    SELECT \n        to_address AS address, \n        value AS value\n    FROM \n        `bigquery-public-data.crypto_ethereum_classic.traces`\n    WHERE \n        to_address IS NOT NULL\n        AND status = 1\n        AND (call_type NOT IN ('delegatecall', 'callcode', 'staticcall') OR call_type IS NULL)\n        AND EXTRACT(DATE FROM block_timestamp) = '2016-10-14'\n\n    UNION ALL\n    \n    -- Credits\n    SELECT \n        from_address AS address, \n        -value AS value\n    FROM \n        `bigquery-public-data.crypto_ethereum_classic.traces`\n    WHERE \n        from_address IS NOT NULL\n        AND status = 1\n        AND (call_type NOT IN ('delegatecall', 'callcode', 'staticcall') OR call_type IS NULL)\n        AND EXTRACT(DATE FROM block_timestamp) = '2016-10-14'\n\n    UNION ALL\n\n    -- Transaction Fees Debits\n    SELECT \n        miner AS address, \n        SUM(CAST(receipt_gas_used AS NUMERIC) * CAST(gas_price AS NUMERIC)) AS value\n    FROM \n        `bigquery-public-data.crypto_ethereum_classic.transactions` AS transactions\n    JOIN \n        `bigquery-public-data.crypto_ethereum_classic.blocks` AS blocks \n        ON blocks.number = transactions.block_number\n    WHERE \n        EXTRACT(DATE FROM block_timestamp) = '2016-10-14'\n    GROUP BY \n        blocks.miner, \n        block_timestamp\n\n    UNION ALL\n    \n    -- Transaction Fees Credits\n    SELECT \n        from_address AS address, \n        -(CAST(receipt_gas_used AS NUMERIC) * CAST(gas_price AS NUMERIC)) AS value\n    FROM \n        `bigquery-public-data.crypto_ethereum_classic.transactions`\n    WHERE \n        EXTRACT(DATE FROM block_timestamp) = '2016-10-14'\n),\nnet_changes AS (\n    SELECT \n        address,\n        SUM(value) AS net_change\n    FROM \n        double_entry_book\n    GROUP BY \n        address\n)\nselect \n    MAX(net_change) AS max_net_change,\n    MIN(net_change) AS min_net_change\nFROM\n    net_changes;",
        "external_knowledge": null,
        "plan": "1. Create a temporary dataset that combines different types of financial transactions (debits, credits, and fees) affecting account balances.\n- Extract Debits: Identify and include transactions where value is being added to an account. Ensure these transactions are successful and exclude certain internal calls. Limit the data to a specific date.\n- Extract Credits: Identify and include transactions where value is being deducted from an account. Similar to debits, ensure these transactions are successful and exclude specific internal calls. Again, limit the data to the same specific date.\n- Calculate Transaction Fee Debits: Identify and calculate the total transaction fees credited to miners. Ensure the data is limited to the specific date.\n- Calculate Transaction Fee Credits: Identify and calculate the transaction fees debited from the originating accounts. Ensure the data is limited to the specific date.\n\n2. Combine Data: Merge the results of debits, credits, and transaction fees into a single dataset representing changes to account balances.\n\n3. Aggregate Net Changes: Group the combined data by account and calculate the net change in balance for each account.\n\n4. Identify Extremes: Determine the maximum and minimum net changes in balances across all accounts for the specific date.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT"
        ]
    },
    {
        "instance_id": "bq292",
        "db": "spider2-public-data.crypto_bitcoin\nspider2-public-data.crypto_ethereum_classic\nspider2-public-data.crypto_dash\nspider2-public-data.crypto_bitcoin_cash\nspider2-public-data.crypto_ethereum\nspider2-public-data.crypto_band\nspider2-public-data.crypto_zilliqa",
        "question": "Please analyse the monthly aggregated statistics and percentages of potential CoinJoin transactions within the Bitcoin network since July 1, 2023 as below. How much percent of monthly Bitcoin txs that were CoinJoins and what about the Bitcoin utxos? Also please show me the percent of monthly Bitcoin volume that took place in CoinJoined transactions.",
        "SQL": "WITH totals AS (\n  SELECT\n    txs_tot.block_timestamp_month as tx_month,\n    count(txs_tot.hash) as tx_count,\n    sum(txs_tot.input_count) as tx_inputs,\n    sum(txs_tot.output_count) as tx_outputs,\n    sum(txs_tot.input_value)/100000000 as tx_input_val,\n    sum(txs_tot.output_value)/100000000 as tx_output_val\n  FROM `spider2-public-data.crypto_bitcoin.transactions` as txs_tot\n  WHERE txs_tot.block_timestamp_month > cast('2023-06-30' as date)\n  GROUP BY txs_tot.block_timestamp_month\n  ORDER BY txs_tot.block_timestamp_month desc\n),\ncoinjoinOuts AS(\n  SELECT \n    txs.hash,\n    txs.block_number,\n    txs.block_timestamp_month,\n    txs.input_count,\n    txs.output_count,\n    txs.input_value,\n    txs.output_value,\n    o.value as outputs_val\n  FROM `spider2-public-data.crypto_bitcoin.transactions` as txs, UNNEST(txs.outputs) as o\n  WHERE output_count > 2 AND \n    output_value <= input_value AND \n    block_timestamp_month > cast('2023-06-30' as date)\n  ORDER BY block_number, txs.hash desc\n),\ncoinjoinTxs AS(\n  SELECT \n    STRING_AGG(DISTINCT coinjoinOuts.hash LIMIT 1) as cjHash,\n    CONCAT(coinjoinOuts.hash, \" \", cast(coinjoinOuts.outputs_val as string)) as outputVal,\n    count(*) as cjOuts\n  FROM coinjoinOuts\n  GROUP BY outputVal\n  having count(*) >1\n),\ncoinjoinsD AS(\n  SELECT \n    DISTINCT coinjoinOuts.hash, \n    coinjoinOuts.block_number, \n    coinjoinOuts.block_timestamp_month,\n    coinjoinOuts.input_count,\n    coinjoinOuts.output_count,\n    coinjoinOuts.input_value,\n    coinjoinOuts.output_value\n  FROM coinjoinOuts INNER JOIN coinjoinTxs ON coinjoinOuts.hash = coinjoinTxs.cjHash\n),\ncoinjoins AS (\n  SELECT \n    block_timestamp_month as cjs_month,\n    count(cjs.hash) as cjs_count,\n    sum(cjs.input_count) as cjs_inputs,\n    sum(cjs.output_count) as cjs_outputs,\n    sum(cjs.input_value)/100000000 as cjs_input_val,\n    sum(cjs.output_value)/100000000 as cjs_output_val\n  FROM coinjoinsD as cjs\n  GROUP BY cjs.block_timestamp_month\n  ORDER BY cjs.block_timestamp_month desc\n)\nSELECT \n  coinjoins.cjs_count / totals.tx_count * 100 as tx_percent,\n  (coinjoins.cjs_inputs / totals.tx_inputs + coinjoins.cjs_outputs / totals.tx_outputs) / 2 * 100 as utxos_percent,\n  coinjoins.cjs_input_val / totals.tx_input_val * 100 as value_percent\n  \nFROM totals INNER JOIN coinjoins ON totals.tx_month = coinjoins.cjs_month\nORDER BY tx_month desc",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq135",
        "db": "spider2-public-data.crypto_zilliqa\nspider2-public-data.crypto_bitcoin\nspider2-public-data.crypto_ethereum_classic\nspider2-public-data.crypto_dash\nspider2-public-data.crypto_bitcoin_cash\nspider2-public-data.crypto_ethereum\nspider2-public-data.crypto_band",
        "question": "Which date before 2022 had the highest total transaction amount in the Zilliqa blockchain data?",
        "SQL": "WITH all_transactions AS (\n  SELECT block_timestamp, amount\n  FROM `spider2-public-data.crypto_zilliqa.transactions`\n  UNION ALL\n  SELECT block_timestamp, amount\n  FROM `spider2-public-data.crypto_zilliqa.transitions` \n)\nSELECT \n  DATE(block_timestamp) AS date\nFROM all_transactions\nWHERE block_timestamp < TIMESTAMP('2022-01-01')\nGROUP BY date\nORDER BY SUM(amount) / 1e12 DESC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "1. Combine Data Sources: Merge data from two different sources into a single dataset that includes the relevant fields, ensuring all transactions are considered.\n2. Filter Data: Extract transactions that occurred before a specific date, focusing only on those within the desired timeframe.\n3. Aggregate and Identify: Group the filtered transactions by date, calculate the total transaction amount for each date, and identify the date with the highest total transaction amount.",
        "special_function": [
            "date-functions/DATE",
            "timestamp-functions/TIMESTAMP"
        ]
    },
    {
        "instance_id": "bq136",
        "db": "spider2-public-data.crypto_zilliqa\nspider2-public-data.crypto_bitcoin\nspider2-public-data.crypto_ethereum_classic\nspider2-public-data.crypto_dash\nspider2-public-data.crypto_bitcoin_cash\nspider2-public-data.crypto_ethereum\nspider2-public-data.crypto_band",
        "question": "Tell me all 2-hop transaction paths on the Zilliqa blockchain from the address zil1jrpjd8pjuv50cfkfr7eu6yrm3rn5u8rulqhqpz to the address zil19nmxkh020jnequql9kvqkf3pkwm0j0spqtd26e. Exclude intermediary addresses with over 50 outgoing transactions to avoid exchanges and active wallets. Display each path in the following format: <from_address> --(tx <first5_chars_of_transaction_id>..)--> <intermediate_address> --(tx <first5_chars_of_transaction_id>..)--> <end_address>",
        "SQL": "DECLARE start_address STRING DEFAULT 'zil1jrpjd8pjuv50cfkfr7eu6yrm3rn5u8rulqhqpz';\nDECLARE end_address STRING DEFAULT 'zil19nmxkh020jnequql9kvqkf3pkwm0j0spqtd26e';\n\nDECLARE max_fan_out INT64 DEFAULT 50;\n\nWITH all_transactions AS (\n  SELECT \n    id AS transaction_id, \n    block_number, \n    block_timestamp, \n    sender AS from_address, \n    to_addr AS to_address\n  FROM `spider2-public-data.crypto_zilliqa.transactions`\n  \n  UNION ALL\n  \n  SELECT \n    transaction_id, \n    block_number, \n    block_timestamp, \n    addr AS from_address, \n    recipient AS to_address\n  FROM `spider2-public-data.crypto_zilliqa.transitions` \n),\n\naddresses_with_high_fan_out AS (\n  SELECT \n    from_address AS address\n  FROM all_transactions\n  GROUP BY from_address\n  HAVING COUNT(*) > max_fan_out \n),\n\ntransactions_0_hops AS (\n  SELECT\n    0 AS hops,\n    transactions.from_address,\n    transactions.to_address,\n    transactions.block_timestamp,\n    CONCAT(\n      transactions.from_address,\n      ' --(tx ', \n      SUBSTR(transactions.transaction_id, 0, 5), \n      '..)--> ', \n      transactions.to_address\n    ) AS path\n  FROM all_transactions AS transactions\n  WHERE transactions.from_address = start_address \n),\n\ntransactions_1_hops AS (\n  SELECT\n    1 AS hops,\n    transactions.from_address,\n    transactions.to_address,\n    transactions.block_timestamp,\n    CONCAT(\n      transactions_0_hops.path,\n      ' --(tx ', \n      SUBSTR(transactions.transaction_id, 0, 5), \n      '..)--> ', \n      transactions.to_address\n    ) AS path\n  FROM all_transactions AS transactions\n  INNER JOIN transactions_0_hops \n    ON transactions_0_hops.to_address = transactions.from_address\n    AND transactions_0_hops.block_timestamp <= transactions.block_timestamp\n  LEFT JOIN addresses_with_high_fan_out\n    ON addresses_with_high_fan_out.address = transactions.from_address\n  WHERE addresses_with_high_fan_out.address IS NULL \n),\n\ntransactions_2_hops AS (\n  SELECT\n    2 AS hops,\n    transactions.from_address,\n    transactions.to_address,\n    transactions.block_timestamp,\n    CONCAT(\n      transactions_1_hops.path, \n      ' --(tx ', \n      SUBSTR(transactions.transaction_id, 0, 5), \n      '..)--> ', \n      transactions.to_address\n    ) AS path\n  FROM all_transactions AS transactions\n  INNER JOIN transactions_1_hops \n    ON transactions_1_hops.to_address = transactions.from_address\n    AND transactions_1_hops.block_timestamp <= transactions.block_timestamp\n  LEFT JOIN addresses_with_high_fan_out \n    ON addresses_with_high_fan_out.address = transactions.from_address\n  WHERE addresses_with_high_fan_out.address IS NULL \n),\n\ntransactions_two_hops AS (\n  SELECT \n    * \n  FROM transactions_2_hops \n  WHERE to_address = end_address\n)\n\nSELECT\n  path\nFROM transactions_two_hops;",
        "external_knowledge": null,
        "plan": "What is the shortest path between two Zilliqa addresses in the transactions graph?\nThis query returns the shortest path between two Zilliqa addresses in the transactions graph, which has at most 3 hops.",
        "special_function": null
    },
    {
        "instance_id": "bq065",
        "db": "spider2-public-data.crypto_band\nspider2-public-data.crypto_zilliqa\nspider2-public-data.crypto_bitcoin\nspider2-public-data.crypto_ethereum_classic\nspider2-public-data.crypto_dash\nspider2-public-data.crypto_bitcoin_cash\nspider2-public-data.crypto_ethereum",
        "question": "Provide the most recent 10 results of symbols and their corresponding rates, adjusted for the multiplier, from oracle requests with the script ID 3.",
        "SQL": "WITH parsed_aggregator_oracle_requests AS (\n    SELECT ARRAY(\n        SELECT JSON_EXTRACT_SCALAR(symbol_as_json, '$')\n        FROM UNNEST(JSON_EXTRACT_ARRAY(decoded_result.calldata, \"$.symbols\")) AS symbol_as_json\n    ) AS symbols,\n    CAST(JSON_EXTRACT_SCALAR(decoded_result.calldata, \"$.multiplier\") AS FLOAT64) AS multiplier,\n    ARRAY(\n        SELECT CAST(JSON_EXTRACT_SCALAR(rate_as_json, '$') AS FLOAT64)\n        FROM UNNEST(JSON_EXTRACT_ARRAY(decoded_result.result, \"$.rates\")) AS rate_as_json\n    ) AS rates,\n    block_timestamp,\n    oracle_request_id,\n    FROM `spider2-public-data.crypto_band.oracle_requests`\n    WHERE request.oracle_script_id = 3\n),\n-- zip symbols and rates\nzipped_rates AS (\n    SELECT block_timestamp,\n        oracle_request_id,\n        struct(symbol, rates[OFFSET(off)] AS rate) AS zipped,\n        multiplier,\n    FROM parsed_aggregator_oracle_requests,\n        UNNEST(symbols) AS symbol WITH OFFSET off\n),\n-- adjust for multiplier\nadjusted_rates AS (\n    SELECT \n        block_timestamp,\n        oracle_request_id,\n        struct(zipped.symbol, IEEE_DIVIDE(zipped.rate, multiplier) AS rate) AS zipped,\n    FROM zipped_rates\n)\nSELECT \n    block_timestamp,\n    oracle_request_id,\n    zipped.symbol,\n    zipped.rate,\nFROM adjusted_rates\nORDER BY block_timestamp DESC\nLIMIT 10",
        "external_knowledge": null,
        "plan": "1. **Parse Aggregator Oracle Requests:**\n   - **Objective:** Extract relevant data (symbols, multiplier, rates) from JSON fields within the `oracle_requests` table for requests with `oracle_script_id = 3`.\n   - **Steps:**\n     - Use `JSON_EXTRACT_ARRAY` to get arrays of symbols and rates from the `calldata` and `result` fields, respectively.\n     - Convert the `symbols` array elements to strings and the `rates` array elements to floating-point numbers.\n     - Extract the `multiplier` as a floating-point number from the `calldata`.\n     - Filter out rows where `calldata` or `result` is NULL.\n     - Collect the extracted arrays and additional fields (`block_timestamp`, `oracle_request_id`) into a temporary table `parsed_aggregator_oracle_requests`.\n\n2. **Zip Symbols and Rates:**\n   - **Objective:** Pair each symbol with its corresponding rate.\n   - **Steps:**\n     - Use `UNNEST` to break down the `symbols` array and pair each symbol with its corresponding rate using the `OFFSET` index.\n     - Ensure the lengths of the `symbols` and `rates` arrays match before proceeding.\n     - Create a struct `zipped` containing the symbol and its associated rate.\n     - Include additional fields (`block_timestamp`, `oracle_request_id`, `multiplier`) in the resulting temporary table `zipped_rates`.\n\n3. **Adjust Rates for Multiplier:**\n   - **Objective:** Adjust each rate by dividing it by the `multiplier`.\n   - **Steps:**\n     - Iterate over the `zipped_rates` table.\n     - For each row, create a new struct `zipped` with the symbol and the adjusted rate (rate divided by multiplier using `IEEE_DIVIDE`).\n     - Store the results along with `block_timestamp` and `oracle_request_id` in the temporary table `adjusted_rates`.\n\n4. **Select Final Adjusted Rates:**\n   - **Objective:** Retrieve and display the final adjusted rates.\n   - **Steps:**\n     - Select `block_timestamp`, `oracle_request_id`, `zipped.symbol`, and `zipped.rate` from the `adjusted_rates` table.\n     - Optionally, a filter can be applied to select specific symbols (commented out in the provided query).\n     - Order the results by `block_timestamp` in descending order.\n     - Limit the output to the top 10 rows.\n\nThis reference plan outlines the logical flow and transformations applied to the data to achieve the intended result of parsing, zipping, adjusting, and selecting specific oracle request data.",
        "special_function": [
            "array-functions/ARRAY",
            "conversion-functions/CAST",
            "json-functions/JSON_EXTRACT_ARRAY",
            "json-functions/JSON_EXTRACT_SCALAR",
            "mathematical-functions/IEEE_DIVIDE",
            "other-functions/UNNEST",
            "other-functions/ARRAY_SUBSCRIPT",
            "other-functions/STRUCT_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "bq037",
        "db": "spider2-public-data.human_genome_variants",
        "question": "About the refined human genetic variations collected in phase 3 on 2015-02-20, I want to know the minimum and maximum start positions as well as the proportions of these two respectively for reference bases 'AT' and 'TA'.",
        "SQL": "WITH A AS (\n    SELECT\n        reference_bases,\n        start_position\n    FROM\n        `spider2-public-data.human_genome_variants.1000_genomes_phase_3_optimized_schema_variants_20150220`\n    WHERE\n        reference_bases IN ('AT', 'TA')\n),\nB AS (\n    SELECT\n        reference_bases,\n        MIN(start_position) AS min_start_position,\n        MAX(start_position) AS max_start_position,\n        COUNT(1) AS total_count\n    FROM\n        A\n    GROUP BY\n        reference_bases\n),\nmin_counts AS (\n    SELECT\n        reference_bases,\n        start_position AS min_start_position,\n        COUNT(1) AS min_count\n    FROM\n        A\n    GROUP BY\n        reference_bases, start_position\n    HAVING\n        start_position = (SELECT MIN(start_position) FROM A AS sub WHERE sub.reference_bases = A.reference_bases)\n),\nmax_counts AS (\n    SELECT\n        reference_bases,\n        start_position AS max_start_position,\n        COUNT(1) AS max_count\n    FROM\n        A\n    GROUP BY\n        reference_bases, start_position\n    HAVING\n        start_position = (SELECT MAX(start_position) FROM A AS sub WHERE sub.reference_bases = A.reference_bases)\n)\nSELECT\n    B.reference_bases,\n    B.min_start_position,\n    CAST(min_counts.min_count AS FLOAT64) / B.total_count AS min_position_ratio,\n    B.max_start_position,\n    CAST(max_counts.max_count AS FLOAT64) / B.total_count AS max_position_ratio\nFROM\n    B\nLEFT JOIN\n    min_counts ON B.reference_bases = min_counts.reference_bases AND B.min_start_position = min_counts.min_start_position\nLEFT JOIN\n    max_counts ON B.reference_bases = max_counts.reference_bases AND B.max_start_position = max_counts.max_start_position\nORDER BY\n    B.reference_bases;",
        "external_knowledge": null,
        "plan": "1. Filter and extract relevant data:\n   - Use a Common Table Expression (CTE) to select the `reference_bases` and `start_position` columns from the `1000_genomes_phase_3_optimized_schema_variants_20150220` table.\n   - Filter the rows where `reference_bases` is either 'AT' or 'TA'.\n\n2. Compute minimum and maximum start positions:\n   - Use another CTE to compute the minimum (`MIN`) and maximum (`MAX`) start positions for each `reference_bases`.\n   - Also, count the total number of occurrences for each `reference_bases`.\n\n3. Count occurrences of minimum start position:\n   - Use a CTE named to find the number of occurrences of the minimum start position for each `reference_bases`.\n   - Group by `reference_bases` and `start_position`.\n   - Use a subquery in the `HAVING` clause to ensure only rows with the minimum start position for each `reference_bases` are selected.\n\n4. Similarly, count occurrences of maximum start position:\n\n5. Combine results and calculate proportions for the minimum and maximum positions per reference base.",
        "special_function": [
            "conversion-functions/CAST"
        ]
    },
    {
        "instance_id": "bq012",
        "db": "spider2-public-data.ethereum_blockchain",
        "question": "What is the average balance of the top 10 addresses with the most balance on the Ethereum blockchain, considering both incoming and outgoing transactions with valid addresses, but only those recorded as used on receipt, as well as transaction fees? Only keep successful transactions with no call type or where the call type is 'call'. The average balance, expressed in quadrillions (10^15), is rounded to two decimal places.",
        "SQL": "WITH double_entry_book AS (\n  -- Debits\n  SELECT \n    to_address AS address,\n    value AS value\n  FROM `spider2-public-data.ethereum_blockchain.traces`\n  WHERE to_address IS NOT NULL\n    AND status = 1\n    AND (call_type NOT IN ('delegatecall', 'callcode', 'staticcall') OR call_type IS NULL)\n  UNION ALL\n  -- Credits\n  SELECT \n    from_address AS address,\n    -value AS value\n  FROM `spider2-public-data.ethereum_blockchain.traces`\n  WHERE from_address IS NOT NULL\n    AND status = 1\n    AND (call_type NOT IN ('delegatecall', 'callcode', 'staticcall') OR call_type IS NULL)\n  UNION ALL\n  -- Transaction fees debits\n  SELECT \n    miner AS address,\n    SUM(CAST(receipt_gas_used AS NUMERIC) * CAST(gas_price AS NUMERIC)) AS value\n  FROM `spider2-public-data.ethereum_blockchain.transactions` AS transactions\n  JOIN `spider2-public-data.ethereum_blockchain.blocks` AS blocks\n  ON blocks.number = transactions.block_number\n  GROUP BY blocks.miner\n  UNION ALL\n  -- Transaction fees credits\n  SELECT \n    from_address AS address,\n    -(CAST(receipt_gas_used AS NUMERIC) * CAST(gas_price AS NUMERIC)) AS value\n  FROM `spider2-public-data.ethereum_blockchain.transactions`\n),\ntop_10_balances AS (\n  SELECT\n    address,\n    SUM(value) AS balance\n  FROM double_entry_book\n  GROUP BY address\n  ORDER BY balance DESC\n  LIMIT 10\n)\nSELECT \n    ROUND(AVG(balance) / 1e15, 2) AS average_balance_trillion\nFROM top_10_balances;",
        "external_knowledge": null,
        "plan": "Calculate and identify the average balance of the top 10 Ethereum addresses with the highest balances using the Ethereum blockchain data.\nFinally, sum all values for each address in this constructed ledger to determine the balance and order the result by the highest balance to identify the top 10 addresses. Then calculate the average result.\nNote that 1. **Credits**: You add the value of Ether sent to an address (`to_address` from the `traces` table) considering only successful transactions (`status = 1`) and excluding special call types (`delegatecall`, `callcode`, `staticcall`). \n2. **Debits**: You subtract the value of Ether sent from an address (`from_address` from the `traces` table) under the same conditions as for credits. \n3. **Transaction Fees as Debits**: You calculate and add transaction fees (computed as `receipt_gas_used * gas_price`) that miners earn (`miner` from the `blocks` table) by joining `transactions` and `blocks` tables. \n4. **Transaction Fees as Credits**: You subtract the transaction fees from the addresses that initiated the transactions (`from_address` from the `transactions` table).",
        "special_function": null
    },
    {
        "instance_id": "bq187",
        "db": "spider2-public-data.ethereum_blockchain",
        "question": "What is the total circulating supply balances of the 'BNB' token for all addresses (excluding the zero address), based on the amount they have received (converted by dividing by 10^18) minus the amount they have sent?",
        "SQL": "WITH tokenInfo AS (\n    SELECT address\n    FROM `spider2-public-data.ethereum_blockchain.tokens`\n    WHERE name = 'BNB'\n),\n\nreceivedTx AS (\n    SELECT tx.to_address as addr, \n           tokens.name as name, \n           SUM(CAST(tx.value AS float64) / POWER(10, 18)) as amount_received\n    FROM `spider2-public-data.ethereum_blockchain.token_transfers` as tx\n    JOIN tokenInfo ON tx.token_address = tokenInfo.address,\n         `spider2-public-data.ethereum_blockchain.tokens` as tokens\n    WHERE tx.token_address = tokens.address \n        AND tx.to_address <> '0x0000000000000000000000000000000000000000'\n    GROUP BY 1, 2\n),\n\nsentTx AS (\n    SELECT tx.from_address as addr, \n           tokens.name as name, \n           SUM(CAST(tx.value AS float64) / POWER(10, 18)) as amount_sent\n    FROM `spider2-public-data.ethereum_blockchain.token_transfers` as tx\n    JOIN tokenInfo ON tx.token_address = tokenInfo.address,\n         `spider2-public-data.ethereum_blockchain.tokens` as tokens\n    WHERE tx.token_address = tokens.address \n        AND tx.from_address <> '0x0000000000000000000000000000000000000000'\n    GROUP BY 1, 2\n),\n\nwalletBalances AS (\n    SELECT r.addr,\n           COALESCE(SUM(r.amount_received), 0) - COALESCE(SUM(s.amount_sent), 0) as balance\n    FROM \n        receivedTx as r\n    LEFT JOIN \n        sentTx as s\n    ON \n        r.addr = s.addr\n    GROUP BY \n        r.addr\n)\n\nSELECT \n    SUM(balance) as circulating_supply\nFROM \n    walletBalances;",
        "external_knowledge": null,
        "plan": "1. **Identify the Token:** Select the address of the specified token from the token details table.\n   \n2. **Calculate Received Amounts:**\n   - Filter transactions where the token matches the specified token.\n   - Exclude transactions to the zero address.\n   - Sum the received amounts for each address, converting the values to a standard unit.\n\n3. **Calculate Sent Amounts:**\n   - Filter transactions where the token matches the specified token.\n   - Exclude transactions from the zero address.\n   - Sum the sent amounts for each address, converting the values to a standard unit.\n\n4. **Compute Balances:**\n   - Combine the received and sent amounts for each address.\n   - Subtract the total sent amounts from the total received amounts to determine the balance for each address.\n\n5. **Aggregate Total Circulating Supply:**\n   - Sum the balances of all addresses to compute the total circulating supply of the token.",
        "special_function": [
            "conversion-functions/CAST",
            "mathematical-functions/POWER",
            "conditional-functions/COALESCE"
        ]
    },
    {
        "instance_id": "bq450",
        "db": "spider2-public-data.ethereum_blockchain",
        "question": "Comprehensively analyze Ethereum blockchain addresses by calculating their transaction activities, balances, token exchanges, and other related metrics up to January 1, 2017, to provide detailed insights into their behaviors and interactions.",
        "SQL": "WITH\n  address_book AS (\n  SELECT\n    address, \n    case when count(1) > 24 then sqrt(pow(sum(cos(extract(hour from ts)/12*acos(-1))), 2)+pow(sum(sin(extract(hour from ts)/12*acos(-1))), 2))/count(1) else null end as R_active_hour,\n    count(distinct date(ts)) as active_days\n  FROM (\n    SELECT\n      from_address AS address,\n      block_timestamp AS ts\n    FROM\n      `spider2-public-data.ethereum_blockchain.traces`\n    UNION ALL\n    SELECT\n      to_address AS address,\n      block_timestamp AS ts\n    FROM\n      `spider2-public-data.ethereum_blockchain.traces`)\n  WHERE\n    ts < TIMESTAMP('2017-01-01')\n  GROUP BY\n    address ),\n  -- balance\n  balance_book AS (\n  WITH\n    value_records AS (\n      -- debits\n    SELECT\n      to_address AS address,\n      value AS value,\n      block_timestamp AS ts\n    FROM\n      `spider2-public-data.ethereum_blockchain.traces`\n    WHERE\n      to_address IS NOT NULL\n      AND status = 1\n      AND (call_type NOT IN ('delegatecall',\n          'callcode',\n          'staticcall')\n        OR call_type IS NULL)\n    UNION ALL\n      -- credits\n    SELECT\n      from_address AS address,\n      -value AS value,\n      block_timestamp AS ts\n    FROM\n      `spider2-public-data.ethereum_blockchain.traces`\n    WHERE\n      from_address IS NOT NULL\n      AND status = 1\n      AND (call_type NOT IN ('delegatecall',\n          'callcode',\n          'staticcall')\n        OR call_type IS NULL)\n    UNION ALL\n      -- transaction fees debits\n    SELECT\n      miner AS address,\n      CAST(receipt_gas_used AS numeric) * CAST(gas_price AS numeric) AS value,\n      block_timestamp AS ts\n    FROM\n      `spider2-public-data.ethereum_blockchain.transactions` AS transactions\n    JOIN\n      `spider2-public-data.ethereum_blockchain.blocks` AS blocks\n    ON\n      blocks.number = transactions.block_number\n    UNION ALL\n      -- transaction fees credits\n    SELECT\n      from_address AS address,\n      -(CAST(receipt_gas_used AS numeric) * CAST(gas_price AS numeric)) AS value,\n      block_timestamp AS ts\n    FROM\n      `spider2-public-data.ethereum_blockchain.transactions` )\n  SELECT\n    address,\n    SUM(value) AS balance\n  FROM\n    value_records\n  WHERE\n    ts < TIMESTAMP('2017-01-01')\n  GROUP BY\n    address ),\n  -- token transfer in count and accumulative token types\n  token_in_book AS (\n    SELECT\n      to_address AS address,\n      count(1) as token_in_tnx,\n      count(distinct token_address) as token_in_type,\n      count(distinct from_address) as token_from_addr\n    FROM\n      `spider2-public-data.ethereum_blockchain.token_transfers`\n  WHERE\n    block_timestamp < TIMESTAMP('2017-01-01')\n  GROUP BY\n    address),\n  token_out_book AS (\n    SELECT\n      from_address AS address,\n      count(1) as token_out_tnx,\n      count(distinct token_address) as token_out_type,\n      count(distinct to_address) as token_to_addr\n    FROM\n      `spider2-public-data.ethereum_blockchain.token_transfers`\n  WHERE\n    block_timestamp < TIMESTAMP('2017-01-01')\n  GROUP BY\n    address),\n  -- Out count\n  out_book AS (\n  SELECT\n    from_address AS address,\n    COUNT(to_address) AS out_trace_count,\n    COUNT(DISTINCT to_address) AS out_addr_count,\n    countif(value > 0) as out_transfer_count,\n    avg(nullif(value, 0)) as out_avg_amount\n  FROM\n    `spider2-public-data.ethereum_blockchain.traces`\n  WHERE\n    from_address IS NOT NULL\n    AND status = 1\n    AND block_timestamp < TIMESTAMP('2017-01-01')\n  GROUP BY\n    address ),\n  -- In count\n  in_book AS (\n  SELECT\n    to_address AS address,\n    COUNT(from_address) AS in_trace_count,\n    COUNT(DISTINCT from_address) AS in_addr_count,\n    countif(value > 0) as in_transfer_count,\n    avg(nullif(value, 0)) as in_avg_amount,\n    \n    avg(if(trace_type=\"call\", nullif(gas_used, 0), null)) as avg_gas_used,\n    stddev(if(trace_type=\"call\", nullif(gas_used, 0), null)) as std_gas_used\n  FROM\n    `spider2-public-data.ethereum_blockchain.traces`\n  WHERE\n    -- keep or not? contract creation with null to_address\n    to_address IS NOT NULL\n    AND status = 1\n    AND block_timestamp < TIMESTAMP('2017-01-01')\n  GROUP BY\n    address ),\n  -- Mining Reward (exclude gas fee)\n  reward_book AS (\n  SELECT\n    to_address AS address,\n    SUM(value) AS reward_amount\n  FROM\n    `spider2-public-data.ethereum_blockchain.traces`\n  WHERE\n    trace_type = 'reward'\n    AND block_timestamp < TIMESTAMP('2017-01-01')\n  GROUP BY\n    address ),\n  -- Contract Creation Count\n  contract_create_book AS (\n  SELECT\n    from_address AS address,\n    COUNT(from_address) AS contract_create_count\n  FROM\n    `spider2-public-data.ethereum_blockchain.traces`\n  WHERE\n    trace_type = 'create'\n    AND block_timestamp < TIMESTAMP('2017-01-01')\n  GROUP BY\n    from_address ),\n  -- Failure Count\n  failed_trace_book AS (\n  SELECT\n    from_address AS address,\n    COUNT(from_address) AS failure_count\n  FROM\n    `spider2-public-data.ethereum_blockchain.traces`\n  WHERE\n    status <> 1\n    AND block_timestamp < TIMESTAMP('2017-01-01')\n  GROUP BY\n    from_address ),\n    -- Bytecode Length\n    bytecode_book as (\nselect address, cast((length(bytecode)-2)/2 as int64) as bytecode_size from `spider2-public-data.ethereum_blockchain.contracts` where block_timestamp < TIMESTAMP('2017-01-01')\n)\n  -- MAIN QUERY\nSELECT\n  address_book.address AS address,\n  balance/1E18 as balance,\n  R_active_hour,\n  active_days,\n  in_trace_count,\n  in_addr_count,\n  in_transfer_count,\n  in_avg_amount/1E18 as in_avg_amount,\n  avg_gas_used,\n  std_gas_used,\n  out_trace_count,\n  out_addr_count,\n  out_transfer_count,\n  out_avg_amount/1E18 as out_avg_amount,\n  token_in_tnx,\n  token_in_type,\n  token_from_addr,\n  token_out_tnx,\n  token_out_type,\n  token_to_addr,\n  reward_amount/1E18 as reward_amount,\n  contract_create_count,\n  failure_count,\n  bytecode_size\nFROM\n  address_book\nLEFT JOIN\n  balance_book\nON\n  address_book.address = balance_book.address\nLEFT JOIN\n  token_in_book\nON\n  address_book.address = token_in_book.address\n  LEFT JOIN\n  token_out_book\nON\n  address_book.address = token_out_book.address\nLEFT JOIN\n  out_book\nON\n  address_book.address = out_book.address\nLEFT JOIN\n  in_book\nON\n  address_book.address = in_book.address\nLEFT JOIN\n  reward_book\nON\n  address_book.address = reward_book.address\nLEFT JOIN\n  contract_create_book\nON\n  address_book.address = contract_create_book.address\nLEFT JOIN\n  failed_trace_book\nON\n  address_book.address = failed_trace_book.address\n  left join\n  bytecode_book\n  on address_book.address = bytecode_book.address\nWHERE\n  coalesce(in_trace_count + out_trace_count,\n    in_trace_count,\n    out_trace_count,\n    0) > 0",
        "external_knowledge": "ethereum_data_transformation.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq034",
        "db": "bigquery-public-data.ghcn_d",
        "question": "I want to know the IDs, names of weather stations within a 50 km straight-line distance from the center of Chicago (41.8319\u00b0N, 87.6847\u00b0W)",
        "SQL": "WITH params AS (\n  SELECT ST_GeogPoint(-87.6847, 41.8319) AS center,\n         50 AS maxdist_km\n),\ndistance_from_center AS (\n  SELECT\n    id,\n    name,\n    state,\n    ST_GeogPoint(longitude, latitude) AS loc,\n    ST_Distance(ST_GeogPoint(longitude, latitude), params.center) AS dist_meters\n  FROM\n    `bigquery-public-data.ghcn_d.ghcnd_stations`,\n    params\n  WHERE ST_DWithin(ST_GeogPoint(longitude, latitude), params.center, params.maxdist_km*1000)\n),\nnearest_stations AS (\n  SELECT \n    *, \n    RANK() OVER (ORDER BY dist_meters ASC) AS rank\n  FROM \n    distance_from_center\n),\nnearest_nstations AS (\n  SELECT \n    station.* \n  FROM \n    nearest_stations AS station, params\n)\nSELECT * from nearest_nstations",
        "external_knowledge": null,
        "plan": "1.Initializes parameters for the center of Chicago, maximum number of stations, and search radius.\n2.Calculates distance for each station from the center and filters by maximum distance.\n3.Ranks stations by proximity.\n4.Selects the top five nearest stations.",
        "special_function": null
    },
    {
        "instance_id": "bq383",
        "db": "bigquery-public-data.ghcn_d",
        "question": "Could you provide the highest recorded precipitation, minimum temperature, and maximum temperature from the last 15 days of each year from 2013 to 2016 at weather station USW00094846? Ensure each value represents the peak measurement for that period, with precipitation in millimeters and temperatures in degrees Celsius, including only valid and high-quality data.",
        "SQL": "WITH data AS (\n  SELECT\n    EXTRACT(YEAR FROM wx.date) AS year,\n    MAX(IF(wx.element = 'PRCP', wx.value/10, NULL)) AS max_prcp,\n    MAX(IF(wx.element = 'TMIN', wx.value/10, NULL)) AS max_tmin,\n    MAX(IF(wx.element = 'TMAX', wx.value/10, NULL)) AS max_tmax\n  FROM\n    `bigquery-public-data.ghcn_d.ghcnd_2013` AS wx\n  WHERE\n    wx.id = 'USW00094846' AND\n    wx.qflag IS NULL AND\n    wx.value IS NOT NULL AND\n    DATE_DIFF(DATE('2013-12-31'), wx.date, DAY) < 15\n  GROUP BY\n    year\n\n  UNION ALL\n\n  SELECT\n    EXTRACT(YEAR FROM wx.date) AS year,\n    MAX(IF(wx.element = 'PRCP', wx.value/10, NULL)) AS max_prcp,\n    MAX(IF(wx.element = 'TMIN', wx.value/10, NULL)) AS max_tmin,\n    MAX(IF(wx.element = 'TMAX', wx.value/10, NULL)) AS max_tmax\n  FROM\n    `bigquery-public-data.ghcn_d.ghcnd_2014` AS wx\n  WHERE\n    wx.id = 'USW00094846' AND\n    wx.qflag IS NULL AND\n    wx.value IS NOT NULL AND\n    DATE_DIFF(DATE('2014-12-31'), wx.date, DAY) < 15\n  GROUP BY\n    year\n\n  UNION ALL\n\n  SELECT\n    EXTRACT(YEAR FROM wx.date) AS year,\n    MAX(IF(wx.element = 'PRCP', wx.value/10, NULL)) AS max_prcp,\n    MAX(IF(wx.element = 'TMIN', wx.value/10, NULL)) AS max_tmin,\n    MAX(IF(wx.element = 'TMAX', wx.value/10, NULL)) AS max_tmax\n  FROM\n    `bigquery-public-data.ghcn_d.ghcnd_2015` AS wx\n  WHERE\n    wx.id = 'USW00094846' AND\n    wx.qflag IS NULL AND\n    wx.value IS NOT NULL AND\n    DATE_DIFF(DATE('2015-12-31'), wx.date, DAY) < 15\n  GROUP BY\n    year\n\n  UNION ALL\n\n  SELECT\n    EXTRACT(YEAR FROM wx.date) AS year,\n    MAX(IF(wx.element = 'PRCP', wx.value/10, NULL)) AS max_prcp,\n    MAX(IF(wx.element = 'TMIN', wx.value/10, NULL)) AS max_tmin,\n    MAX(IF(wx.element = 'TMAX', wx.value/10, NULL)) AS max_tmax\n  FROM\n    `bigquery-public-data.ghcn_d.ghcnd_2016` AS wx\n  WHERE\n    wx.id = 'USW00094846' AND\n    wx.qflag IS NULL AND\n    wx.value IS NOT NULL AND\n    DATE_DIFF(DATE('2016-12-31'), wx.date, DAY) < 15\n  GROUP BY\n    year\n)\n\nSELECT\n  year,\n  MAX(max_prcp) AS annual_max_prcp,\n  MAX(max_tmin) AS annual_max_tmin,\n  MAX(max_tmax) AS annual_max_tmax\nFROM data\nGROUP BY year\nORDER BY year ASC;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq051",
        "db": "bigquery-public-data.ghcn_d\nbigquery-public-data.new_york",
        "question": "Get the average number of trips on rainy and non-rainy days in New York City during 2016, using data from the closest weather station located near the coordinates (longitude: -74.0060, latitude: 40.7128). Define a 'rainy day' as any day where the precipitation recorded is more than 0.5 millimeters.",
        "SQL": "WITH params AS (\n  SELECT ST_GeogPoint(-74.0060, 40.7128) AS center,\n         50 AS maxn_stations,\n         50 AS maxdist_km\n),\ndistance_from_center AS (\n  SELECT\n    id,\n    name,\n    state,\n    ST_GeogPoint(longitude, latitude) AS loc,\n    ST_Distance(ST_GeogPoint(longitude, latitude), params.center) AS dist_meters\n  FROM\n    `bigquery-public-data.ghcn_d.ghcnd_stations`,\n    params\n  WHERE ST_DWithin(ST_GeogPoint(longitude, latitude), params.center, params.maxdist_km*1000)\n),\nnearest_stations AS (\n  SELECT \n    *, \n    RANK() OVER (ORDER BY dist_meters ASC) AS rank\n  FROM \n    distance_from_center\n),\nnearest_nstations AS (\n  SELECT \n    station.* \n  FROM \n    nearest_stations AS station, params\n  WHERE \n    rank <= params.maxn_stations\n),\nstation_ids AS (\nSELECT id, dist_meters from nearest_nstations\nORDER BY dist_meters ASC\nLIMIT 1000\n),\nbicycle_rentals AS (\n  SELECT\n    COUNT(starttime) as num_trips,\n    EXTRACT(DATE from starttime) as trip_date\n  FROM `bigquery-public-data.new_york_citibike.citibike_trips`\n  GROUP BY trip_date\n),\nclosest AS (\n  SELECT\n    station_ids.id as id,\n    ANY_VALUE(station_ids.dist_meters) as dist\n  FROM\n    `bigquery-public-data.ghcn_d.ghcnd_2016` AS wx\n  JOIN station_ids\n  on wx.id=station_ids.id\n  GROUP BY station_ids.id\n  ORDER BY dist ASC\n  LIMIT 1\n),\nrainy_days AS\n(\nSELECT\n  date,\n  (COALESCE(MAX(prcp), 0) > 5) AS rainy\nFROM (\n  SELECT\n    wx.date AS date,\n    IF (wx.element = 'PRCP', wx.value/10, NULL) AS prcp\n  FROM\n    `bigquery-public-data.ghcn_d.ghcnd_2016` AS wx\n  WHERE\n    wx.id in (SELECT id FROM closest)\n)\nGROUP BY\n date\n)\n\nSELECT\n  ROUND(AVG(bk.num_trips)) AS num_trips,\n  wx.rainy\nFROM bicycle_rentals AS bk\nJOIN rainy_days AS wx\nON wx.date = bk.trip_date\nGROUP BY wx.rainy",
        "external_knowledge": null,
        "plan": "1. Which days were rainy in 2016, and how can we obtain weather information?\n2. The GHCN-D database allows us to access weather data from each weather station.\n3. Given that the central coordinates of New York City are (-74.0060, 40.7128), we need to select a weather station to represent the weather data for New York City.\n4. Calculate the 20 weather stations closest to the center of New York City based on their distance.\n5. Obtain the precipitation data from that weather station.\n6. Use the precipitation data to classify the days in 2016 as either rainy or non-rainy.\n7. The New York Citibike database stores daily bike rental data, which can be grouped based on whether it was a rainy day and then averaged.\n8. Compare the differences in the average number of bike rentals on rainy days versus non-rainy days.",
        "special_function": null
    },
    {
        "instance_id": "bq038",
        "db": "bigquery-public-data.new_york",
        "question": "Can you list the top 10 station id with the highest proportion of group rides? The group rides are trips that start and end at the same station within a 2-minute window.",
        "SQL": "WITH grouped_trips AS (\n  SELECT\n    COUNTIF(group_size = 1) AS single_trips,\n    COUNTIF(group_size != 1) AS group_trips,\n    end_station_id\n  FROM (\n    SELECT\n      ROUND(UNIX_SECONDS(starttime) / 120) AS start,\n      ROUND(UNIX_SECONDS(stoptime) / 120) AS stop,\n      start_station_id,\n      end_station_id,\n      COUNT(*) AS group_size\n    FROM\n      `bigquery-public-data.new_york.citibike_trips`\n    GROUP BY\n      start,\n      stop,\n      start_station_id,\n      end_station_id )\n  GROUP BY\n    end_station_id\n)\n\nSELECT\nend_station_id\nFROM\ngrouped_trips\nORDER BY\ngroup_trips / (single_trips + group_trips) DESC\nLIMIT 10",
        "external_knowledge": null,
        "plan": "1. Calculate the number of single and group trips for each end station\n2. For each station, compute the proportion of group trips to the total number of trips\n3. Rank the stations by the proportion of group trips and select the top 10.\n4. Join the top stations with trip data to get distinct station names for the top-ranked stations by group trip proportion",
        "special_function": [
            "aggregate-functions/COUNTIF",
            "mathematical-functions/ROUND",
            "timestamp-functions/UNIX_SECONDS"
        ]
    },
    {
        "instance_id": "bq053",
        "db": "bigquery-public-data.new_york",
        "question": "How has the number of trees of each fall color in New York City changed from 1995 to 2015, considering only trees that were still alive in 2015 and excluding those marked as dead in 1995?",
        "SQL": "SELECT\n  c.fall_color,\n  SUM(d.count_growth) AS change\nFROM (\n  SELECT\n    fall_color,\n    UPPER(species_scientific_name) AS latin\n  FROM\n    `bigquery-public-data.new_york.tree_species`)c\nJOIN (\n  SELECT\n    IFNULL(a.upper_latin,\n      b.upper_latin) AS latin,\n    (IFNULL(count_2015,\n        0)-IFNULL(count_1995,\n        0)) AS count_growth\n  FROM (\n    SELECT\n      UPPER(spc_latin) AS upper_latin,\n      spc_common,\n      COUNT(*) AS count_2015\n    FROM\n      `bigquery-public-data.new_york.tree_census_2015`\n    WHERE\n      status=\"Alive\"\n    GROUP BY\n      spc_latin,\n      spc_common)a\n  FULL OUTER JOIN (\n    SELECT\n      UPPER(spc_latin) AS upper_latin,\n      COUNT(*) AS count_1995\n    FROM\n      `bigquery-public-data.new_york.tree_census_1995`\n    WHERE\n      status !=\"Dead\"\n    GROUP BY\n      spc_latin)b\n  ON\n    a.upper_latin=b.upper_latin\n  ORDER BY\n    count_growth DESC)d\nON\n  d.latin=c.latin\nGROUP BY\n  fall_color\nORDER BY\n  change DESC",
        "external_knowledge": null,
        "plan": "1. Retrieve fall colors and scientific names from tree species data.\n2. Calculate the number of trees for 1995 and 2015, focusing on 'Alive' for 2015 and excluding 'Dead' for 1995.\n3. Determine the net change in tree counts by comparing 1995 and 2015 data.\n4. Combine changes with fall color data, summarize changes by color, and sort by significance.",
        "special_function": null
    },
    {
        "instance_id": "bq054",
        "db": "bigquery-public-data.new_york",
        "question": "Can you provide the top 10 tree species in New York with their species latin not empty, based on the growth in their population from 1995 to 2015, including the count of trees, the count of alive and dead trees for both years, and the respective growth values?",
        "SQL": "#standardsql\nSELECT\n  IFNULL(a.upper_latin, b.upper_latin) as upper_latin,\n  IFNULL(count_2015, 0) as count_2015,\n  IFNULL(count_1995, 0) as count_1995,\n  (IFNULL(count_2015, 0)-IFNULL(count_1995, 0)) AS count_growth,\n  (IFNULL(alive_2015, 0)-IFNULL(alive_1995, 0)) as alive_growth,\n  (IFNULL(dead_2015, 0)-IFNULL(dead_1995, 0)) as dead_growth\nFROM (\n  SELECT\n    UPPER(spc_latin) AS upper_latin,\n    spc_common,\n    COUNT(*) AS count_2015,\n    COUNTIF(status=\"Alive\") AS alive_2015,\n    COUNTIF(status=\"Dead\") AS dead_2015\n  FROM\n    `bigquery-public-data.new_york.tree_census_2015`\n  WHERE spc_latin != \"\"\n  GROUP BY\n    spc_latin,\n    spc_common)a\nFULL OUTER JOIN (\n  SELECT\n    UPPER(spc_latin) AS upper_latin,\n    COUNT(*) AS count_1995,\n    COUNTIF(status!=\"Dead\" AND status!=\"Unknown\") AS alive_1995,\n    COUNTIF(status=\"Dead\") AS dead_1995\n  FROM\n    `bigquery-public-data.new_york.tree_census_1995`\n  GROUP BY\n    spc_latin)b\nON\n  a.upper_latin=b.upper_latin\nORDER BY\n  count_growth DESC\nLIMIT 10",
        "external_knowledge": null,
        "plan": "1. **Subquery A: Tree Census 2015 Data Aggregation**\n   - **Select and Transform Data**: \n     - Convert `spc_latin` to uppercase (`UPPER(spc_latin) AS upper_latin`).\n     - Select species common name (`spc_common`).\n   - **Aggregate Data**:\n     - Count total trees (`COUNT(*) AS count_2015`).\n     - Count trees with status \"Alive\" (`COUNTIF(status=\"Alive\") AS alive_2015`).\n     - Count trees with status \"Dead\" (`COUNTIF(status=\"Dead\") AS dead_2015`).\n   - **Filter and Group**:\n     - Exclude rows where `spc_latin` is an empty string.\n     - Group by `spc_latin` and `spc_common`.\n\n2. **Subquery B: Tree Census 1995 Data Aggregation**\n   - **Select and Transform Data**:\n     - Convert `spc_latin` to uppercase (`UPPER(spc_latin) AS upper_latin`).\n   - **Aggregate Data**:\n     - Count total trees (`COUNT(*) AS count_1995`).\n     - Count trees with status not equal to \"Dead\" (`COUNTIF(status!=\"Dead\") AS alive_1995`).\n     - Count trees with status \"Dead\" (`COUNTIF(status=\"Dead\") AS dead_1995`).\n   - **Group**:\n     - Group by `spc_latin`.\n\n3. **Full Outer Join on Subqueries A and B**\n   - Join `a` and `b` on the `upper_latin` column to ensure all records from both subqueries are included, even if there's no match in the other subquery.\n\n4. **Select and Calculate Final Fields**\n   - **Select `upper_latin`**:\n     - Use `IFNULL` to handle cases where the species name might be missing in either subquery.\n   - **Calculate Counts**:\n     - Use `IFNULL` to handle missing counts and default them to 0.\n     - Select and alias `count_2015` and `count_1995`.\n   - **Calculate Growth Metrics**:\n     - Calculate `count_growth` as the difference between `count_2015` and `count_1995`.\n     - Calculate `alive_growth` as the difference between `alive_2015` and `alive_1995`.\n     - Calculate `dead_growth` as the difference between `dead_2015` and `dead_1995`.\n\n5. **Order and Limit the Result**\n   - Order the results by `count_growth` in descending order.\n   - Limit the result to the top 10 records.",
        "special_function": [
            "aggregate-functions/COUNTIF",
            "string-functions/UPPER",
            "conditional-functions/IFNULL"
        ]
    },
    {
        "instance_id": "bq021",
        "db": "bigquery-public-data.new_york",
        "question": "For the top 20 Citi Bike routes in 2016, which route is faster than yellow taxis and among those, which one has the longest average bike duration? Please provide the start station name of this route. The coordinates are rounded to three decimals.",
        "SQL": "WITH top20route AS (\nSELECT\n  start_station_name, end_station_name, avg_bike_duration, avg_taxi_duration\nFROM (\n  SELECT\n    start_station_name,\n    end_station_name,\n    ROUND(start_station_latitude, 3) AS ss_lat,\n    ROUND(start_station_longitude, 3) AS ss_long,\n    ROUND(end_station_latitude, 3) AS es_lat,\n    ROUND(end_station_longitude, 3) AS es_long,\n    AVG(tripduration) AS avg_bike_duration,\n    COUNT(*) AS bike_trips\n  FROM\n    `bigquery-public-data.new_york.citibike_trips`\n  WHERE \n  EXTRACT(YEAR from starttime) = 2016 AND\n    start_station_name != end_station_name\n  GROUP BY\n    start_station_name, end_station_name, ss_lat, ss_long, es_lat, es_long\n  ORDER BY\n    bike_trips DESC\n  LIMIT\n    20\n) a\nJOIN (\n  SELECT\n    ROUND(pickup_latitude, 3) AS pu_lat,\n    ROUND(pickup_longitude, 3) AS pu_long,\n    ROUND(dropoff_latitude, 3) AS do_lat,\n    ROUND(dropoff_longitude, 3) AS do_long,\n    AVG(UNIX_SECONDS(dropoff_datetime)-UNIX_SECONDS(pickup_datetime)) AS avg_taxi_duration,\n    COUNT(*) AS taxi_trips\n  FROM\n    `bigquery-public-data.new_york.tlc_yellow_trips_2016`\n  GROUP BY\n    pu_lat, pu_long, do_lat, do_long\n) b\nON\n  a.ss_lat = b.pu_lat AND\n  a.es_lat = b.do_lat AND\n  a.ss_long = b.pu_long AND\n  a.es_long = b.do_long\n)\n\nSELECT start_station_name\nFROM top20route\nWHERE avg_bike_duration < avg_taxi_duration\nORDER BY\navg_bike_duration\nDESC\nLIMIT 1",
        "external_knowledge": null,
        "plan": "1. Focus on 2016 data to determine the top 20 most popular bike routes based on start and end stations, noting their latitude and longitude. 2. Calculate the average bike duration and count the number of bike trips for each route. 3. Extract the average duration for corresponding taxi routes using the same latitude and longitude for start and end points. 4. Calculate the average taxi duration for the matching routes. 5. Filter the results to find the bike route where the average bike duration is shorter than the average taxi duration. 6. Order the results by average bike duration in descending order and limit the output to one record.",
        "special_function": null
    },
    {
        "instance_id": "bq202",
        "db": "bigquery-public-data.new_york",
        "question": "For the station with the most citibike trips in 2018, what are the peak day of the week (as a numeric value) and the peak hour of the day?",
        "SQL": "WITH popular_station AS (\n  SELECT\n    start_station_name,\n    start_station_id,\n    COUNT(*) AS number_trips\n  FROM\n    `bigquery-public-data.new_york_citibike.citibike_trips`\n  WHERE \n    starttime >= '2018-01-01T00:00:00'\n  GROUP BY\n    start_station_name, start_station_id\n  ORDER BY number_trips DESC\n  LIMIT 1\n),\n\ndaily_data AS (\n  SELECT\n    extract(DAYOFWEEK FROM starttime) AS day_of_week,\n    COUNT(*) AS trips_count\n  FROM\n    `bigquery-public-data.new_york_citibike.citibike_trips`\n  WHERE \n    start_station_id = (SELECT start_station_id FROM popular_station)\n    AND starttime >= '2018-01-01'\n  GROUP BY\n    day_of_week\n  ORDER BY trips_count DESC\n  LIMIT 1\n),\n\nhourly_data AS (\n  SELECT\n    extract(HOUR FROM starttime) AS hour,\n    COUNT(*) AS trips_count\n  FROM\n    `bigquery-public-data.new_york_citibike.citibike_trips`\n  WHERE \n    start_station_id = (SELECT start_station_id FROM popular_station)\n    AND starttime >= '2018-01-01'\n  GROUP BY\n    hour\n  ORDER BY trips_count DESC\n  LIMIT 1\n)\n\nSELECT\n  (SELECT start_station_name FROM popular_station) AS station_name,\n  (SELECT day_of_week FROM daily_data) AS peak_day_of_week,\n  (SELECT hour FROM hourly_data) AS peak_hour",
        "external_knowledge": null,
        "plan": "1. Calculate the number of trips for each station in 2018 and identify the station with the highest number\n2. For the most popular station, aggregate the number of trips by day of the week throughout 2018, find the day with the highest trip count.\n3. Similarly, for the same station, aggregate the number of trips by hour and identify the hour with the most trips.\n4. Combine the results to display the most popular station's name along with its peak day of the week and peak hour.",
        "special_function": null
    },
    {
        "instance_id": "bq185",
        "db": "bigquery-public-data.new_york_taxi_trips\nbigquery-public-data.new_york_trees\nbigquery-public-data.new_york_subway\nbigquery-public-data.new_york_mv_collisions\nbigquery-public-data.new_york_citibike\nbigquery-public-data.new_york_311",
        "question": "What is the average valid trip duration (in minutes) for yellow taxi rides in Brooklyn with more than 3 passengers and a trip distance of at least 10 miles between February 1 and February 7, 2016?",
        "SQL": "SELECT \n    AVG(TIMESTAMP_DIFF(dropoff_datetime, pickup_datetime, SECOND) / 60.0) AS average_trip_duration_in_minutes\nFROM\n(\n    SELECT *\n    FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2016` t\n    WHERE \n        pickup_datetime BETWEEN '2016-02-01' AND '2016-02-07' AND \n        dropoff_datetime BETWEEN '2016-02-01' AND '2016-02-07' AND\n        TIMESTAMP_DIFF(dropoff_datetime, pickup_datetime, SECOND) > 0 AND \n        passenger_count > 3 AND \n        trip_distance >= 10\n) t\nINNER JOIN `bigquery-public-data.new_york_taxi_trips.taxi_zone_geom` tz\nON t.pickup_location_id = tz.zone_id\nINNER JOIN `bigquery-public-data.new_york_taxi_trips.taxi_zone_geom` tz1\nON t.dropoff_location_id = tz1.zone_id\nWHERE \n    tz.borough = \"Brooklyn\" AND\n    tz1.borough = \"Brooklyn\";",
        "external_knowledge": null,
        "plan": "1. **Filter Initial Records**:\n   - Select records where the pickup and dropoff times are both between February 1, 2016, and February 7, 2016.\n   - Ensure the trip duration is positive by calculating the difference between dropoff and pickup times.\n   - Filter for trips with more than 3 passengers.\n   - Filter for trips with a distance of at least 10 miles.\n\n2. **Join with Location Data**:\n   - Perform an inner join with the location data to get the borough information for the pickup location.\n   - Perform another inner join with the location data to get the borough information for the dropoff location.\n\n3. **Filter for Manhattan**:\n   - Ensure that both the pickup and dropoff locations are within the borough of Manhattan.\n\n4. **Calculate Average Trip Duration**:\n   - Compute the duration of each trip in seconds.\n   - Convert the duration from seconds to minutes.\n   - Calculate the average of these trip durations.\n\n5. **Return Result**:\n   - Output the average trip duration in minutes.",
        "special_function": null
    },
    {
        "instance_id": "bq040",
        "db": "bigquery-public-data.new_york_taxi_trips\nbigquery-public-data.new_york_trees\nbigquery-public-data.new_york_subway\nbigquery-public-data.new_york_mv_collisions\nbigquery-public-data.new_york_citibike\nbigquery-public-data.new_york_311",
        "question": "For NYC yellow taxi trips between January 1-7, 2016, excluding pickups from 'EWR' and 'Staten Island', calculate the proportion of trips by tip category for each pickup borough. Show the borough, tip category, and proportion, ensuring trips where the dropoff occurs after the pickup, the passenger count is greater than 0, and trip distance, tip, tolls, MTA tax, fare, and total amount are non-negative.",
        "SQL": "WITH t2 AS\n(\nSELECT \n    t.*,\n    t.pickup_location_id as pickup_zone_id,\n    tz.borough as pickup_borough\nFROM\n(\nSELECT *,\n    TIMESTAMP_DIFF(dropoff_datetime,pickup_datetime,SECOND) as time_duration_in_secs,\n    (CASE WHEN total_amount=0 THEN 0\n    ELSE (tip_amount*100/total_amount) END) as tip_rate\nFROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2016`\n) t\nINNER JOIN `bigquery-public-data.new_york_taxi_trips.taxi_zone_geom` tz\nON t.pickup_location_id = tz.zone_id\nWHERE \n    pickup_datetime BETWEEN '2016-01-01' AND '2016-01-07' \n    AND dropoff_datetime BETWEEN '2016-01-01' AND '2016-01-07'\n    AND TIMESTAMP_DIFF(dropoff_datetime,pickup_datetime,SECOND) > 0\n    AND passenger_count > 0\n    AND trip_distance >= 0 \n    AND tip_amount >= 0 \n    AND tolls_amount >= 0 \n    AND mta_tax >= 0 \n    AND fare_amount >= 0\n    AND total_amount >= 0\n),\nt3 AS\n(SELECT \npickup_borough,\n(CASE \n    WHEN tip_rate = 0 THEN 'no tip'\n    WHEN tip_rate <= 5 THEN 'Less than 5%'\n    WHEN tip_rate <= 10 THEN '5% to 10%'\n    WHEN tip_rate <= 15 THEN '10% to 15%'\n    WHEN tip_rate <= 20 THEN '15% to 20%'\n    WHEN tip_rate <= 25 THEN '20% to 25%'\n    ELSE 'More than 25%' END)as tip_category,\nCOUNT(*) as no_of_trips\nFROM t2\nGROUP BY 1,2\nORDER BY pickup_borough ASC),\nINFO AS (\nSELECT pickup_borough\n     , tip_category\n     , Sum(no_of_trips) as no_of_trips,\n     (CASE \n          WHEN pickup_borough is null THEN (select sum(no_of_trips)\n          FROM t3)\n          \n          WHEN pickup_borough is not null and tip_category is null THEN (select sum(no_of_trips)\n          FROM t3)\n          \n          WHEN pickup_borough is not null and tip_category is not null THEN (select sum(no_of_trips)\n          FROM t3\n          WHERE pickup_borough = m.pickup_borough)\n          END) as parent_sum,\n       (\n          Sum(no_of_trips)\n            /\n          (\n            CASE \n          WHEN pickup_borough is null THEN (select sum(no_of_trips)\n          FROM t3)\n          \n          WHEN pickup_borough is not null and tip_category is null THEN (select sum(no_of_trips)\n          FROM t3)\n          \n          WHEN pickup_borough is not null and tip_category is not null THEN (select sum(no_of_trips)\n          FROM t3\n          WHERE pickup_borough = m.pickup_borough)\n          END\n          )\n        ) as percentage\nFROM t3 m\nGROUP BY ROLLUP(pickup_borough, tip_category)\norder by 1, 2\n)\n\nSELECT pickup_borough, tip_category, percentage FROM INFO\nWHERE pickup_borough is not null and tip_category is not null\nAND pickup_borough not in ('EWR','Staten Island')",
        "external_knowledge": "taxi_tip_rate.md",
        "plan": "1.Select trips from 2016, calculate trip duration, speed, and tip rate, and extract date-time features.\n2.Attach geographic details to each trip by joining with taxi zone data based on coordinates.\n3.Classify trips into tipping categories and count the number of trips per category in each borough.\n4.Use ROLLUP to aggregate trip data by borough and tipping category, calculating the total and percentage contributions.\n5.Determine the weighted average tip rate for each borough by assigning numerical values to tipping categories.\n6.Select boroughs and their average tip rates, ordering the results to show areas with the highest average tips.",
        "special_function": [
            "timestamp-functions/TIMESTAMP_DIFF",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq098",
        "db": "bigquery-public-data.new_york_taxi_trips\nbigquery-public-data.new_york_trees\nbigquery-public-data.new_york_subway\nbigquery-public-data.new_york_mv_collisions\nbigquery-public-data.new_york_citibike\nbigquery-public-data.new_york_311",
        "question": "For NYC yellow taxi trips between January 1-7, 2016, could you tell me the percentage of no tips in each borough. Ensure trips where the dropoff occurs after the pickup, the passenger count is greater than 0, and trip distance, tip, tolls, MTA tax, fare, and total amount are non-negative.",
        "SQL": "WITH t2 AS\n(\nSELECT \n    t.*,\n    t.pickup_location_id as pickup_zone_id,\n    tz.borough as pickup_borough\nFROM\n(\nSELECT *,\n    TIMESTAMP_DIFF(dropoff_datetime,pickup_datetime,SECOND) as time_duration_in_secs,\n    (CASE WHEN total_amount=0 THEN 0\n    ELSE (tip_amount*100/total_amount) END) as tip_rate\nFROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2016`\n) t\nINNER JOIN `bigquery-public-data.new_york_taxi_trips.taxi_zone_geom` tz\nON t.pickup_location_id = tz.zone_id\nWHERE \n    pickup_datetime BETWEEN '2016-01-01' AND '2016-01-07' \n    AND dropoff_datetime BETWEEN '2016-01-01' AND '2016-01-07'\n    AND TIMESTAMP_DIFF(dropoff_datetime,pickup_datetime,SECOND) > 0\n    AND passenger_count > 0\n    AND trip_distance >= 0 \n    AND tip_amount >= 0 \n    AND tolls_amount >= 0 \n    AND mta_tax >= 0 \n    AND fare_amount >= 0\n    AND total_amount >= 0\n),\nt3 AS\n(SELECT \npickup_borough,\n(CASE \n    WHEN tip_rate = 0 THEN 'no tip'\n    WHEN tip_rate <= 5 THEN 'Less than 5%'\n    WHEN tip_rate <= 10 THEN '5% to 10%'\n    WHEN tip_rate <= 15 THEN '10% to 15%'\n    WHEN tip_rate <= 20 THEN '15% to 20%'\n    WHEN tip_rate <= 25 THEN '20% to 25%'\n    ELSE 'More than 25%' END)as tip_category,\nCOUNT(*) as no_of_trips\nFROM t2\nGROUP BY 1,2\nORDER BY pickup_borough ASC),\nINFO AS (\nSELECT pickup_borough\n     , tip_category\n     , Sum(no_of_trips) as no_of_trips,\n     (CASE \n          WHEN pickup_borough is null THEN (select sum(no_of_trips)\n          FROM t3)\n          \n          WHEN pickup_borough is not null and tip_category is null THEN (select sum(no_of_trips)\n          FROM t3)\n          \n          WHEN pickup_borough is not null and tip_category is not null THEN (select sum(no_of_trips)\n          FROM t3\n          WHERE pickup_borough = m.pickup_borough)\n          END) as parent_sum,\n       (\n          Sum(no_of_trips)\n            /\n          (\n            CASE \n          WHEN pickup_borough is null THEN (select sum(no_of_trips)\n          FROM t3)\n          \n          WHEN pickup_borough is not null and tip_category is null THEN (select sum(no_of_trips)\n          FROM t3)\n          \n          WHEN pickup_borough is not null and tip_category is not null THEN (select sum(no_of_trips)\n          FROM t3\n          WHERE pickup_borough = m.pickup_borough)\n          END\n          )\n        ) as percentage\nFROM t3 m\nGROUP BY ROLLUP(pickup_borough, tip_category)\norder by 1, 2\n)\n\nSELECT \n    pickup_borough,\n    (SUM(CASE WHEN tip_category = 'no tip' THEN no_of_trips ELSE 0 END) * 100.0 / SUM(no_of_trips)) AS percentage_no_tip\nFROM t3\nGROUP BY pickup_borough\nORDER BY pickup_borough;",
        "external_knowledge": "taxi_tip_rate.md",
        "plan": "1. Filter taxi trip records from January 2016, ensuring data quality by validating coordinates and calculating trip durations and amounts.\n2. Merge trip data with borough details by matching each trip's pickup location coordinates to taxi zone geometries.\n3. Calculate the tip rate for each trip as a percentage of the total amount and classify trips into tip categories.\n4.Count the number of trips in each tip category per borough.\n5. For each borough, calculate the percentage of trips with no tips relative to the total trips in that borough.",
        "special_function": [
            "timestamp-functions/TIMESTAMP_DIFF",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq039",
        "db": "bigquery-public-data.new_york_taxi_trips\nbigquery-public-data.new_york_trees\nbigquery-public-data.new_york_subway\nbigquery-public-data.new_york_mv_collisions\nbigquery-public-data.new_york_citibike\nbigquery-public-data.new_york_311",
        "question": "Which are the top 10 taxi trips in New York City from July 1 to July 7, 2016, with more than 5 passengers, a trip distance of at least 10 miles, and a positive fare, ranked by total fare amount? Display the pickup and dropoff zones, trip duration, driving speed in miles per hour, and tip rate. Note that you should avoid invalid items.",
        "SQL": "SELECT \n    tz.zone_name AS pickup_zone,\n    tz1.zone_name AS dropoff_zone, \n    time_duration_in_secs,\n    driving_speed_miles_per_hour,\n    tip_rate\nFROM\n(\nSELECT *,\n    TIMESTAMP_DIFF(dropoff_datetime,pickup_datetime,SECOND) as time_duration_in_secs,\n    ROUND(trip_distance / (TIMESTAMP_DIFF(dropoff_datetime, pickup_datetime, SECOND) / 3600), 2) AS driving_speed_miles_per_hour,\n    (CASE WHEN total_amount=0 THEN 0\n          ELSE (tip_amount*100/total_amount) END) as tip_rate\nFROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2016`\n) t\nINNER JOIN `bigquery-public-data.new_york_taxi_trips.taxi_zone_geom` tz\nON t.pickup_location_id = tz.zone_id\nINNER JOIN `bigquery-public-data.new_york_taxi_trips.taxi_zone_geom` tz1\nON t.dropoff_location_id = tz1.zone_id\nWHERE \n    pickup_datetime BETWEEN '2016-07-01' AND '2016-07-07' \n    AND dropoff_datetime BETWEEN '2016-07-01' AND '2016-07-07'\n    AND TIMESTAMP_DIFF(dropoff_datetime,pickup_datetime,SECOND) > 0\n    AND passenger_count > 5\n    AND trip_distance >= 10\n    AND tip_amount >= 0 \n    AND tolls_amount >= 0 \n    AND mta_tax >= 0 \n    AND fare_amount >= 0\n    AND total_amount >= 0\nORDER BY total_amount DESC\nLIMIT 10;",
        "external_knowledge": null,
        "plan": "Retrieve detailed data for New York taxi trips between January 1 and January 2, 2016. Calculate trip duration in seconds, driving speed in miles per hour, and the rate of tips given. Include time-related attributes such as year, month, day, and hour for both pickups and dropoffs. \nAugment each record with geographic information like zone IDs, zone names, and boroughs for both pickup and dropoff locations. \nEnsure data accuracy by filtering for valid geographic coordinates and positive financial values for fares, tips, and other charges.",
        "special_function": [
            "mathematical-functions/ROUND",
            "timestamp-functions/TIMESTAMP_DIFF",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq203",
        "db": "bigquery-public-data.new_york_subway\nbigquery-public-data.new_york_taxi_trips\nbigquery-public-data.new_york_trees\nbigquery-public-data.new_york_mv_collisions\nbigquery-public-data.new_york_citibike\nbigquery-public-data.new_york_311",
        "question": "What percentage of subway stations in each New York borough have at least one ADA-compliant entrance?",
        "SQL": "WITH stations_n_entrances AS (\n      SELECT borough_name,s.station_name,entry,ada_compliant\n      FROM `bigquery-public-data.new_york_subway.stations` s\n      JOIN `bigquery-public-data.new_york_subway.station_entrances` se\n      ON s.station_name = se.station_name\n      )\n\nSELECT se.borough_name, COUNT(DISTINCT se.station_name) num_stations,\n      COUNT(DISTINCT adas.station_name) num_stations_w_compliant_entrance, \n      (100*COUNT(DISTINCT adas.station_name))/(COUNT(DISTINCT se.station_name)) percent_compliant_stations\nFROM `stations_n_entrances` se\nLEFT JOIN `stations_n_entrances` adas\nON se.station_name = adas.station_name\nAND adas.entry AND adas.ada_compliant\nGROUP BY 1\nORDER BY 4 DESC",
        "external_knowledge": null,
        "plan": "1. Combine the station and station entrance tables to get borough names, station names, and ADA compliance details\n2. For each borough, count the total number of unique stations and the number of stations with at least one ADA compliant entrance.\n3. Calculate the percentage of stations with compliant entrances relative to the total number of stations in each borough.",
        "special_function": null
    },
    {
        "instance_id": "bq035",
        "db": "bigquery-public-data.san_francisco",
        "question": "What is the total distance traveled by each bike in the San Francisco Bikeshare program? Use data from bikeshare trips and stations to calculate this.",
        "SQL": "SELECT\n  bike_number, \n  AVG(dist_in_m) AS avg_dist_m, \n  SUM(dist_in_m) AS total_dist_m\nFROM (\n  SELECT\n    ST_DISTANCE(\n      ST_GEOGPOINT(start_lon, start_lat),\n      ST_GEOGPOINT(end_lon, end_lat)\n    ) AS dist_in_m,\n    starts.bike_number\n  FROM (\n    SELECT \n      latitude AS start_lat,\n      longitude AS start_lon,\n      bike_number,\n      trip_id\n    FROM `bigquery-public-data.san_francisco.bikeshare_trips` trips\n    LEFT JOIN `bigquery-public-data.san_francisco.bikeshare_stations` stations\n      ON trips.start_station_id = stations.station_id\n  ) starts\n  LEFT JOIN (\n    SELECT \n      latitude AS end_lat,\n      longitude AS end_lon,\n      bike_number,\n      trip_id\n    FROM `bigquery-public-data.san_francisco.bikeshare_trips` trips\n    LEFT JOIN `bigquery-public-data.san_francisco.bikeshare_stations` stations\n      ON trips.end_station_id = stations.station_id\n  ) ends ON ends.trip_id = starts.trip_id\n)\nGROUP BY bike_number\nORDER BY total_dist_m DESC",
        "external_knowledge": null,
        "plan": "1. Get start latitude, longitude, and bike number for each trip.\n2. Get end latitude, longitude, and bike number for each trip.\n3. Calculate the geographic distance between start and end points for each trip.\n4. Groups results by bike number and total distances traveled, order the results by total distance traveled in descending order.",
        "special_function": null
    },
    {
        "instance_id": "bq186",
        "db": "bigquery-public-data.san_francisco",
        "question": "Please help me calculate the first, last, highest, and lowest bike share trip durations in minutes for each month in San Francisco.",
        "SQL": "WITH aaa AS (\n    SELECT *,                 \n        CONCAT(CAST(EXTRACT(YEAR FROM start_date) AS STRING),\n               LPAD(CAST(EXTRACT(MONTH FROM start_date) AS STRING), 2, '0')) AS trip_date\n    FROM `bigquery-public-data.san_francisco.bikeshare_trips` AS bt\n    -- WHERE EXTRACT(YEAR FROM start_date) = 2015\n),\nbbb AS (\n    SELECT \n        trip_date AS date, \n        FIRST_VALUE(duration_sec) OVER (\n            PARTITION BY trip_date\n            ORDER BY trip_date\n        ) / 60.0 AS open,\n        LAST_VALUE(duration_sec) OVER (\n            PARTITION BY trip_date\n            ORDER BY trip_date\n        ) / 60.0 AS close,\n        MAX(duration_sec) OVER (\n            PARTITION BY trip_date\n            ORDER BY trip_date\n        ) / 60.0 AS high,\n        MIN(duration_sec) OVER (\n            PARTITION BY trip_date\n            ORDER BY trip_date\n        ) / 60.0 AS low\n    FROM aaa\n    ORDER BY start_date\n)\nSELECT date, open, high, close, low\nFROM bbb\nGROUP BY date, open, high, close, low\nORDER BY date;",
        "external_knowledge": null,
        "plan": "1. **Create a Temporary Dataset**:\n    - Generate a temporary dataset where each record is associated with a date string representing the year and month of the start date of the trip.\n    - This dataset includes all original columns plus the newly created date string.\n\n2. **Calculate Trip Durations**:\n    - From the temporary dataset, for each unique month, calculate:\n        - The first trip duration.\n        - The last trip duration.\n        - The highest trip duration.\n        - The lowest trip duration.\n    - Convert these durations from seconds to minutes.\n\n3. **Retrieve and Format Results**:\n    - Select the date string (representing the month) and the calculated trip durations (first, last, highest, and lowest) from the results of the previous step.\n    - Ensure the results are ordered by the date string to maintain chronological order.\n\n4. **Final Output**:\n    - Output the date string and the calculated trip durations (in minutes) for each month, ensuring that the data is grouped and ordered by the date string.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "json-functions/STRING",
            "navigation-functions/FIRST_VALUE",
            "navigation-functions/LAST_VALUE",
            "string-functions/CONCAT",
            "string-functions/LPAD",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/STRING"
        ]
    },
    {
        "instance_id": "bq081",
        "db": "bigquery-public-data.san_francisco_bikeshare\nbigquery-public-data.san_francisco_neighborhoods\nbigquery-public-data.san_francisco_311\nbigquery-public-data.san_francisco_film_locations\nbigquery-public-data.san_francisco_sffd_service_calls\nbigquery-public-data.san_francisco_sfpd_incidents\nbigquery-public-data.san_francisco_transit_muni\nbigquery-public-data.san_francisco_trees",
        "question": "Find the latest ride data for each region between 2014 and 2017. I want to know the name of each region, the trip ID of this ride, the ride duration, the start time, the starting station, and the gender of the rider.",
        "SQL": "SELECT t1.*\n  FROM \n  (SELECT Trips.trip_id TripId,\n               Trips.duration_sec TripDuration,\n               Trips.start_date TripStartDate,\n               Trips.start_station_name TripStartStation,\n               Trips.member_gender Gender,\n               Regions.name RegionName\n          FROM `bigquery-public-data.san_francisco_bikeshare.bikeshare_trips` Trips\n         INNER JOIN `bigquery-public-data.san_francisco_bikeshare.bikeshare_station_info` StationInfo\n            ON CAST(Trips.start_station_id AS STRING) = CAST(StationInfo.station_id AS STRING)\n         INNER JOIN `bigquery-public-data.san_francisco_bikeshare.bikeshare_regions` Regions\n            ON StationInfo.region_id = Regions.region_id\n         WHERE (EXTRACT(YEAR from Trips.start_date)) BETWEEN 2014 AND 2017\n           ) \n           t1\n RIGHT JOIN (SELECT MAX(start_date) TripStartDate,\n                   Regions.name RegionName\n              FROM `bigquery-public-data.san_francisco_bikeshare.bikeshare_station_info` StationInfo\n             INNER JOIN `bigquery-public-data.san_francisco_bikeshare.bikeshare_trips` Trips\n                ON CAST(StationInfo.station_id AS STRING) = CAST(Trips.start_station_id AS STRING)\n             INNER JOIN `bigquery-public-data.san_francisco_bikeshare.bikeshare_regions` Regions\n                ON Regions.region_id = StationInfo.region_id\n                 WHERE (EXTRACT(YEAR from Trips.start_date) BETWEEN 2014 AND 2017\n           AND Regions.name IS NOT NULL)\n             GROUP BY RegionName) \n             t2\n    ON t1.RegionName = t2.RegionName AND t1.TripStartDate = t2.TripStartDate",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq294",
        "db": "bigquery-public-data.san_francisco_bikeshare\nbigquery-public-data.san_francisco_neighborhoods\nbigquery-public-data.san_francisco_311\nbigquery-public-data.san_francisco_film_locations\nbigquery-public-data.san_francisco_sffd_service_calls\nbigquery-public-data.san_francisco_sfpd_incidents\nbigquery-public-data.san_francisco_transit_muni\nbigquery-public-data.san_francisco_trees",
        "question": "Can you provide the details of the top 5 longest bike share trips that started during the second half of 2017, including the trip ID, duration in seconds, start date, start station name, route (start station to end station), bike number, subscriber type, member's birth year, current age, age classification, gender, and the region name of the start station? Please exclude trips where the start station name, member's birth year, or member's gender is not specified.",
        "SQL": "SELECT\n  trip_id,\n  duration_sec,\n  DATE(start_date) AS star_date,\n  start_station_name,\n  CONCAT(start_station_name, \" - \", end_station_name) AS route,\n  bike_number,\n  subscriber_type,\n  member_birth_year,\n  (EXTRACT(YEAR FROM CURRENT_DATE()) - member_birth_year) AS age,\n  CASE\n    WHEN (EXTRACT(YEAR FROM CURRENT_DATE()) - member_birth_year) < 40 THEN 'Young (<40 Y.O)'\n    WHEN (EXTRACT(YEAR FROM CURRENT_DATE()) - member_birth_year) BETWEEN 40 AND 60 THEN 'Adult (40-60 Y.O)'\n    ELSE 'Senior Adult (>60 Y.O)'\n  END AS age_class,\n  member_gender,\n  c.name AS region_name\nFROM `bigquery-public-data.san_francisco_bikeshare.bikeshare_trips` a\nLEFT JOIN `bigquery-public-data.san_francisco_bikeshare.bikeshare_station_info` b \n  ON a.start_station_name = b.name\nLEFT JOIN `bigquery-public-data.san_francisco_bikeshare.bikeshare_regions` c \n  ON b.region_id = c.region_id\nWHERE start_date BETWEEN '2017-07-01' AND '2017-12-31'\n  AND b.name IS NOT NULL\n  AND member_birth_year IS NOT NULL\n  AND member_gender IS NOT NULL\nORDER BY duration_sec DESC\nLIMIT 5;",
        "temporal": "Yes",
        "external_knowledge": "trip_info.md",
        "plan": "1. **Select Columns**:\r\n   - `trip_id`: Unique identifier for each trip.\r\n   - `duration_sec`: Duration of the trip in seconds.\r\n   - `date(start_date) as star_date`: Extracts the date part from `start_date` and aliases it as `star_date`.\r\n   - `start_station_name`: Name of the station where the trip started.\r\n   - `concat(start_station_name, \" - \", end_station_name) as route`: Concatenates the start and end station names to form a route string.\r\n   - `bike_number`: Identifier for the bike used.\r\n   - `subscriber_type`: Type of subscription (e.g., Subscriber, Customer).\r\n   - `member_birth_year`: Birth year of the member.\r\n   - `(2022 - member_birth_year) as age`: Calculates the age of the member by subtracting birth year from 2022.\r\n   - `case` statement to classify age into categories:\r\n     - 'Young (<40 Y.O)' if age is less than 40.\r\n     - 'Adult (40-60 Y.O)' if age is between 40 and 60.\r\n     - 'Senior Adult (>60 Y.O)' if age is greater than 60.\r\n   - `member_gender`: Gender of the member.\r\n   - `c.name as region_name`: Name of the region associated with the start station.\r\n\r\n2. **From and Joins**:\r\n   - `from bigquery-public-data.san_francisco_bikeshare.bikeshare_trips a`: Main table containing bike trip data.\r\n   - `left join bigquery-public-data.san_francisco_bikeshare.bikeshare_station_info b on a.start_station_name = b.name`: Left join with station info to get details of the start station.\r\n   - `left join bigquery-public-data.san_francisco_bikeshare.bikeshare_regions c on b.region_id = c.region_id`: Left join with regions table to get the region name.\r\n\r\n3. **Where Clause**:\r\n   - `start_date between '2017-07-01' and '2017-12-31'`: Filters trips that occurred in the second half of 2017.\r\n   - `b.name is not null`: Ensures that the start station name is not null.\r\n   - `member_birth_year is not null`: Ensures that the member's birth year is not null.\r\n   - `member_gender is not null`: Ensures that the member's gender is not null.\r\n   - `member_gender != 'Other'`: Excludes trips where the gender is listed as 'Other'.\r\n\r\n4. **Order By and Limit**:\r\n   - `order by duration_sec desc`: Orders the results by trip duration in descending order.\r\n   - `limit 5`: Limits the result set to the top 5 longest trips.",
        "special_function": [
            "date-functions/CURRENT_DATE",
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "string-functions/CONCAT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq339",
        "db": "bigquery-public-data.san_francisco_bikeshare\nbigquery-public-data.san_francisco_neighborhoods\nbigquery-public-data.san_francisco_311\nbigquery-public-data.san_francisco_film_locations\nbigquery-public-data.san_francisco_sffd_service_calls\nbigquery-public-data.san_francisco_sfpd_incidents\nbigquery-public-data.san_francisco_transit_muni\nbigquery-public-data.san_francisco_trees",
        "question": "Which month in 2017 had the largest absolute difference between cumulative bike usage minutes for customers and subscribers?",
        "SQL": "WITH monthly_totals AS (\n  SELECT\n    SUM(CASE WHEN subscriber_type = 'Customer' THEN duration_sec / 60 ELSE NULL END) AS customer_minutes_sum,\n    SUM(CASE WHEN subscriber_type = 'Subscriber' THEN duration_sec / 60 ELSE NULL END) AS subscriber_minutes_sum,\n    EXTRACT(MONTH FROM end_date) AS end_month\n  FROM\n    `bigquery-public-data.san_francisco_bikeshare.bikeshare_trips`\n  WHERE\n    EXTRACT(YEAR FROM end_date) = 2017\n  GROUP BY\n    end_month\n),\n\ncumulative_totals AS (\n  SELECT\n    end_month,\n    SUM(customer_minutes_sum) OVER (ORDER BY end_month ROWS UNBOUNDED PRECEDING) / 1000 AS cumulative_minutes_cust,\n    SUM(subscriber_minutes_sum) OVER (ORDER BY end_month ROWS UNBOUNDED PRECEDING) / 1000 AS cumulative_minutes_sub\n  FROM\n    monthly_totals\n),\n\ndifferences AS (\n  SELECT\n    end_month,\n    ABS(cumulative_minutes_cust - cumulative_minutes_sub) AS abs_diff\n  FROM\n    cumulative_totals\n)\n\nSELECT\n  end_month\nFROM\n  differences\nORDER BY\n  abs_diff DESC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "1. **Create a Subquery for Monthly Totals**:\n   - Calculate the total usage minutes for each user type (e.g., type A and type B) for each month in the specified year.\n   - Group the results by month to get monthly totals for each user type.\n\n2. **Calculate Cumulative Totals**:\n   - Using the monthly totals, compute the cumulative sum of usage minutes for each user type up to each month.\n   - This involves summing the monthly totals in an ordered manner, from the beginning of the year to the end of each month.\n\n3. **Calculate Absolute Differences**:\n   - Determine the absolute difference between the cumulative totals of the two user types for each month.\n   - This provides a measure of how the cumulative usage minutes differ between the two user types at the end of each month.\n\n4. **Identify the Month with the Largest Difference**:\n   - Sort the months based on the absolute difference calculated in the previous step, in descending order.\n   - Select the month with the highest absolute difference.\n\nBy following these steps, the query identifies the month in the specified year that had the largest absolute difference in cumulative bike usage minutes between the two user types.",
        "special_function": [
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "mathematical-functions/ABS",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq400",
        "db": "bigquery-public-data.san_francisco_transit_muni\nbigquery-public-data.san_francisco_bikeshare\nbigquery-public-data.san_francisco_neighborhoods\nbigquery-public-data.san_francisco_311\nbigquery-public-data.san_francisco_film_locations\nbigquery-public-data.san_francisco_sffd_service_calls\nbigquery-public-data.san_francisco_sfpd_incidents\nbigquery-public-data.san_francisco_trees",
        "question": "What are the start and end times of trips from 'Clay St & Drumm St' to 'Sacramento St & Davis St' (one direction only), in the format of HH:MM:SS? I also want the trip headsign for each route.",
        "SQL": "WITH SelectedStops AS (\n  SELECT \n      stop_id,\n      stop_name\n  FROM \n      `bigquery-public-data.san_francisco_transit_muni.stops`\n  WHERE \n      stop_name IN ('Clay St & Drumm St', 'Sacramento St & Davis St')\n),\nFilteredStopTimes AS (\n  SELECT \n      st.trip_id, \n      st.stop_id, \n      st.arrival_time, \n      st.departure_time, \n      st.stop_sequence, \n      ss.stop_name\n  FROM \n      `bigquery-public-data.san_francisco_transit_muni.stop_times` st\n  JOIN \n      SelectedStops ss ON CAST(st.stop_id AS STRING) = ss.stop_id\n)\nSELECT\n    t.trip_headsign,\n    MIN(st1.departure_time) AS start_time,\n    MAX(st2.arrival_time) AS end_time\nFROM \n    `bigquery-public-data.san_francisco_transit_muni.trips` t\nJOIN FilteredStopTimes st1 ON t.trip_id = CAST(st1.trip_id AS STRING) AND st1.stop_name = 'Clay St & Drumm St'\nJOIN FilteredStopTimes st2 ON t.trip_id = CAST(st2.trip_id AS STRING) AND st2.stop_name = 'Sacramento St & Davis St'\nWHERE \n    st1.stop_sequence < st2.stop_sequence\nGROUP BY \n    t.trip_headsign;",
        "external_knowledge": null,
        "plan": "1. **Identify Relevant Stops**:\n   - Select and label the stops of interest based on their names, ensuring only the stops specified in the instruction are considered.\n\n2. **Retrieve Stop Times**:\n   - Collect the trip details such as trip ID, stop ID, arrival and departure times, and stop sequence for the selected stops, ensuring the data is filtered to include only these stops.\n\n3. **Join with Trip Information**:\n   - Combine the filtered stop times with trip details using the trip ID to link the data, ensuring the stops are matched by name.\n\n4. **Filter Trips by Sequence**:\n   - Ensure that the trips are in the correct direction by checking the sequence of the stops, ensuring that the starting stop occurs before the ending stop.\n\n5. **Select Desired Fields**:\n   - For each trip, determine the earliest arrival time at the starting stop and the latest departure time at the ending stop, ensuring the times are correctly aggregated.\n\n6. **Group and Output**:\n   - Group the results by trip headsign and output the trip headsign along with the formatted start and end times, ensuring the results meet the specified format.",
        "special_function": null
    },
    {
        "instance_id": "bq059",
        "db": "bigquery-public-data.san_francisco_bikeshare\nbigquery-public-data.san_francisco_neighborhoods\nbigquery-public-data.san_francisco_311\nbigquery-public-data.san_francisco_film_locations\nbigquery-public-data.san_francisco_sffd_service_calls\nbigquery-public-data.san_francisco_sfpd_incidents\nbigquery-public-data.san_francisco_transit_muni\nbigquery-public-data.san_francisco_trees",
        "question": "What is the highest average speed (rounded to 1 decimal, in metric m/s) for bike trips in Berkeley with trip distance greater than 1000 meters?",
        "SQL": "WITH stations AS (\n  SELECT station_id\n  FROM\n    `bigquery-public-data.san_francisco_bikeshare.bikeshare_station_info` AS stainfo\n  WHERE stainfo.region_id = (\n    SELECT region.region_id\n    FROM `bigquery-public-data.san_francisco_bikeshare.bikeshare_regions` AS region\n    WHERE region.name = \"Berkeley\"\n  )\n),\nmeta_data AS (\n    SELECT\n        round(st_distance(start_station_geom, end_station_geom), 1) as distancia_metros,\n        round(st_distance(start_station_geom, end_station_geom) / duration_sec, 1) as velocidade_media\n    FROM\n        `bigquery-public-data.san_francisco_bikeshare.bikeshare_trips` AS trips\n    WHERE\n        cast(trips.start_station_id as string) IN (SELECT station_id FROM stations)\n        AND cast(trips.end_station_id as string) IN (SELECT station_id FROM stations)\n        AND start_station_latitude IS NOT NULL\n        AND start_station_longitude IS NOT NULL\n        AND end_station_latitude IS NOT NULL\n        AND end_station_longitude IS NOT NULL\n        AND st_distance(start_station_geom, end_station_geom) > 1000\n    ORDER BY\n        velocidade_media DESC\n    LIMIT 1\n)\n\nSELECT velocidade_media as max_velocity\nFROM meta_data;",
        "external_knowledge": null,
        "plan": "1. Identify Relevant Locations: Select station identifiers for bike stations located in a specific region (Berkeley) by filtering based on the region's name.\n\n2. Calculate Distances and Speeds: For trips starting and ending at the identified stations, calculate the trip distance and the average speed. Ensure the coordinates for start and end stations are available.\n\n3. Filter Trips: Only include trips that meet the distance requirement (greater than 1000 meters) and start/end station restriction (within the region Berkeley).\n\n4. Find Maximum Speed: From the filtered trips, determine the highest average speed, rounded to one decimal place, and return this value.",
        "special_function": [
            "conversion-functions/CAST",
            "geography-functions/ST_DISTANCE",
            "json-functions/STRING",
            "mathematical-functions/ROUND",
            "timestamp-functions/STRING"
        ]
    },
    {
        "instance_id": "bq376",
        "db": "bigquery-public-data.san_francisco_bikeshare\nbigquery-public-data.san_francisco_neighborhoods\nbigquery-public-data.san_francisco_311\nbigquery-public-data.san_francisco_film_locations\nbigquery-public-data.san_francisco_sffd_service_calls\nbigquery-public-data.san_francisco_sfpd_incidents\nbigquery-public-data.san_francisco_transit_muni\nbigquery-public-data.san_francisco_trees",
        "question": "For each neighborhood in San Francisco, list the number of bike share stations and the total number of crime incidents.",
        "SQL": "WITH station_neighborhoods AS (\n   SELECT\n       bs.station_id,\n       bs.name AS station_name,\n       nb.neighborhood\n   FROM `bigquery-public-data.san_francisco.bikeshare_stations` bs\n   JOIN\n       bigquery-public-data.san_francisco_neighborhoods.boundaries nb\n   ON \n       ST_Intersects(ST_GeogPoint(bs.longitude, bs.latitude), nb.neighborhood_geom)\n),\n\nneighborhood_crime_counts AS (\n   SELECT\n       neighborhood,\n       COUNT(*) AS crime_count\n   FROM (\n       SELECT\n           n.neighborhood\n       FROM\n           bigquery-public-data.san_francisco.sfpd_incidents i\n       JOIN\n           bigquery-public-data.san_francisco_neighborhoods.boundaries n\n       ON\n           ST_Intersects(ST_GeogPoint(i.longitude, i.latitude), n.neighborhood_geom)\n   ) AS incident_neighborhoods\n   GROUP BY\n       neighborhood\n)\n\nSELECT\n  sn.neighborhood,\n  COUNT(station_name) AS station_number,\n  ANY_VALUE(ncc.crime_count) AS crime_number\nFROM\n  station_neighborhoods sn\nJOIN\n  neighborhood_crime_counts ncc\nON\n  sn.neighborhood = ncc.neighborhood\nGROUP BY sn.neighborhood\nORDER BY\n  crime_number ASC",
        "external_knowledge": null,
        "plan": "1. **Identify Neighborhoods with Bike Share Stations**:\n   - Create a list of neighborhoods that contain bike share stations by determining which stations fall within the geographic boundaries of each neighborhood.\n\n2. **Count Crimes per Neighborhood**:\n   - Generate a count of crimes for each neighborhood by identifying which crime incidents fall within the geographic boundaries of each neighborhood.\n\n3. **Calculate Average Crime Count**:\n   - Compute the average number of crimes across all neighborhoods.\n\n4. **Calculate Distance from Average Crime Count**:\n   - For each neighborhood, calculate the absolute difference between its crime count and the average crime count.\n\n5. **Find Neighborhoods Closest to Average Crime Count**:\n   - Identify the neighborhoods whose crime counts are closest to the average crime count. If there are multiple neighborhoods with the same minimum distance, include all of them.\n\n6. **Rank by Bike Share Stations**:\n   - From the neighborhoods identified in the previous step, rank them based on the number of bike share stations they contain, in descending order.\n\n7. **Select Top Neighborhood**:\n   - Return the neighborhood with the highest number of bike share stations among those whose crime counts are closest to the average.",
        "special_function": null
    },
    {
        "instance_id": "bq014",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Can you help me figure out the revenue for the product category that has the highest number of customers making a purchase in their first non-cancelled and non-returned order?",
        "SQL": "WITH first_orders AS (\n  SELECT\n    o.order_id,\n    o.user_id,\n    ROW_NUMBER() OVER (PARTITION BY o.user_id ORDER BY o.created_at ASC) order_sequence\n  FROM\n    `bigquery-public-data.thelook_ecommerce.orders` o\n  WHERE\n    o.status NOT IN ('Cancelled', 'Returned')\n  QUALIFY order_sequence = 1\n), \ncategory_stats AS (\n  SELECT\n    p.category,\n    SUM(oi.sale_price) AS revenue,\n    COUNT(DISTINCT fo.user_id) AS user_count\n  FROM\n    first_orders fo\n  LEFT JOIN\n    `bigquery-public-data.thelook_ecommerce.order_items` oi ON oi.order_id = fo.order_id\n  LEFT JOIN\n    `bigquery-public-data.thelook_ecommerce.products` p ON p.id = oi.product_id\n  GROUP BY\n    p.category\n),\n\ntop_category AS (\n  SELECT\n    category\n  FROM\n    category_stats\n  ORDER BY\n    user_count DESC\n  LIMIT 1\n)\n\nSELECT\n  c.revenue\nFROM\n  category_stats c\nJOIN\n  top_category t ON c.category = t.category;",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Get the first order of each user, excluding the cancelled and returned orders. (from 'orders' table)\n2. Calculate the total revenue and distinct user count for each product category. (combine the 'orders' 'order_items' and 'products' tables)\n3. Select the top category with the most user count.\n4. Based on the top category, get its revenue.",
        "special_function": [
            "numbering-functions/ROW_NUMBER"
        ]
    },
    {
        "instance_id": "bq188",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "What is the average time in minutes that users spend per visit on the product category with the highest total quantity purchased?",
        "SQL": "WITH table1 AS (\n  SELECT\n    created_at,\n    session_id,\n    sequence_number,\n    CASE WHEN event_type = 'product' THEN CAST(REPLACE(uri, '/product/', '') AS INT) ELSE NULL END product_uri_id,\n    LEAD(created_at) OVER (PARTITION BY session_id ORDER BY sequence_number) next_event\n  FROM `bigquery-public-data.thelook_ecommerce.events` \n),\n\ntable2 AS (\n  SELECT \n    p.category,\n    COUNT(o.id) times_bought\n  FROM `bigquery-public-data.thelook_ecommerce.products` p\n  LEFT JOIN `bigquery-public-data.thelook_ecommerce.order_items` o\n    ON p.id = o.product_id\n  GROUP BY 1\n),\n\ncategory_stats AS (\n  SELECT \n    p.category,  \n    COUNT(table1.product_uri_id) number_of_visits,\n    ROUND(AVG(DATE_DIFF(table1.next_event, table1.created_at, SECOND) / 60), 2) avg_time_spent,\n    table2.times_bought total_quantity_bought\n  FROM table1\n  LEFT JOIN bigquery-public-data.thelook_ecommerce.products p\n    ON table1.product_uri_id = p.id\n  LEFT JOIN table2\n    ON p.category = table2.category\n  WHERE table1.product_uri_id IS NOT NULL\n  GROUP BY 1, 4\n)\n\nSELECT \n  CAST(avg_time_spent AS STRING) \nFROM category_stats\nORDER BY category_stats.total_quantity_bought DESC\nLIMIT 1",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. **Create an Initial Dataset:**\n   - Extract event details including timestamps and session information.\n   - Identify events related to a specific type by transforming a specific URI component into an identifier.\n   - Calculate the timestamp for the next event in the same session.\n\n2. **Aggregate Purchase Data:**\n   - Determine the total number of times products in various categories have been purchased.\n   - Group this data by category.\n\n3. **Calculate Visit Statistics:**\n   - Compute visit details for each category by joining the initial dataset with product data.\n   - Count the number of visits for each category.\n   - Calculate the average time spent per visit by determining the time difference between consecutive events and converting it to minutes.\n   - Include the total quantity of purchases for each category from the aggregated purchase data.\n   - Filter out irrelevant events to ensure only meaningful data is included.\n\n4. **Determine the Desired Metric:**\n   - Select the average time spent per visit for the category with the highest total quantity purchased.\n   - Sort the results based on the total quantity purchased in descending order.\n   - Limit the results to get the top category.\n\n5. **Output the Result:**\n   - Convert the average time spent per visit to a string format for the final output.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE_DIFF",
            "json-functions/STRING",
            "mathematical-functions/ROUND",
            "navigation-functions/LEAD",
            "string-functions/REPLACE",
            "timestamp-functions/STRING",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq258",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Generate a monthly report for each product category detailing the month-over-month percentage growth in revenue and orders, along with the monthly total cost, profit, and profit-to-cost ratio for orders that were completed and delivered before 2022",
        "SQL": "WITH monthly_summary AS (\n  SELECT \n    FORMAT_DATE('%m', oi.delivered_at) AS Month,\n    FORMAT_DATE('%Y', oi.delivered_at) AS Year,\n    p.category AS Product_category,\n    SUM(oi.sale_price) AS TPV,\n    COUNT(DISTINCT oi.order_id) AS TPO\n  FROM \n    bigquery-public-data.thelook_ecommerce.order_items oi\n  JOIN \n    bigquery-public-data.thelook_ecommerce.products p ON oi.product_id = p.id\n  JOIN \n    bigquery-public-data.thelook_ecommerce.orders o ON oi.order_id = o.order_id\n  WHERE \n    oi.status ='Complete'\n  AND\n    FORMAT_DATE('%Y', oi.delivered_at) < '2022'\n  GROUP BY\n    Month, Year, Product_category\n  ORDER BY \n    Year, Month\n),\n\nlagged_summary AS (\n  SELECT \n    month,\n    year,\n    Product_category,\n    TPV,\n    TPO,\n    LAG(TPV) OVER(PARTITION BY Product_category ORDER BY year, month) AS Lagged_TPV,\n    LAG(TPO) OVER(PARTITION BY Product_category ORDER BY year, month) AS Lagged_TPO\n  FROM \n    monthly_summary\n)\n\nSELECT \n  month,\n  year,\n  Product_category,\n  TPV,\n  TPO,\n  Lagged_TPV,\n  Lagged_TPO,\n  ROUND((TPV - Lagged_TPV) / NULLIF(Lagged_TPV, 0) * 100, 2) AS Revenue_growth,\n  ROUND((TPO - Lagged_TPO) / NULLIF(Lagged_TPO, 0) * 100, 2) AS Order_growth,\n  ROUND(SUM(p.cost), 2) AS Total_cost,\n  ROUND(TPV - SUM(p.cost), 2) AS Total_profit,\n  ROUND(TPV / NULLIF(SUM(p.cost), 0) * 100, 2) AS Profit_to_cost_ratio\nFROM \n  lagged_summary ls\nJOIN \n  bigquery-public-data.thelook_ecommerce.products p ON ls.Product_category = p.category \nGROUP BY \n  month, year, Product_category, TPV, TPO, Lagged_TPV, Lagged_TPO;",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Figure out metrics like Total Purchase Value (TPV) and Total Number of Orders (TPO) per month and product category.\n2. Get lagged values of TPV and TPO for each product category and compare with the current month's values.\n3. Calculates targeted metrics through the lagged values and latest values of TPV and TPO.\n",
        "special_function": [
            "date-functions/FORMAT_DATE",
            "mathematical-functions/ROUND",
            "navigation-functions/LAG",
            "conditional-functions/NULLIF"
        ]
    },
    {
        "instance_id": "bq259",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Can you provide the percentage of users who made a purchase in the first, second, third and four months after their initial purchase, organized by the month of their first purchase, using data from until the end of 2022?",
        "SQL": "with a as(\n  select\n    user_id,\n    format_date('%Y-%m',first_purchase_date) as cohort_date,\n    created_at,\n    (extract(year from created_at) - extract(year from first_purchase_date))*12\n    + (extract(month from created_at) - extract(month from first_purchase_date)) + 1 as index\n  from (\n  select\n    user_id,\n    min(created_at) over(partition by user_id) as first_purchase_date,\n    created_at,\n    from bigquery-public-data.thelook_ecommerce.order_items\n    where created_at < TIMESTAMP \"2023-01-01 00:00:00 UTC\"\n  ) b\n),\n\nxxx as (\n  select \n        cohort_date,\n        index,\n        count(distinct user_id) as total_user,\n  from a \n  group by cohort_date, index\n), \n\nuser_cohort as(\nselect\n  cohort_date,\n  sum(case when index=1 then total_user else 0 end) as index_0,\n  sum(case when index=2 then total_user else 0 end) as index_1,\n  sum(case when index=3 then total_user else 0 end) as index_2,\n  sum(case when index=4 then total_user else 0 end) as index_3,\n  sum(case when index=5 then total_user else 0 end) as index_4,\nfrom xxx\ngroup by cohort_date\norder by cohort_date)\n\nselect cohort_date,\n100.00 * index_1 / index_0 as First,\n100.00 * index_2 / index_0 as Second,\n100.00 * index_3 / index_0 as Third,\n100.00 * index_4 / index_0 as Fourth\nfrom user_cohort",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Determine the cohort date and index based on the time difference between the creation date and the user's first purchase date.\n2. Aggregate the total number of unique users for each cohort date and purchase index.\n3. Summarize the total number of users at each purchase index (0, 1, 2, 3) for every cohort date.\n4. Calculate the percentage distribution of users at each purchase index level relative to the initial cohort size for each cohort date.\n",
        "special_function": [
            "date-functions/EXTRACT",
            "date-functions/FORMAT_DATE",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/TIMESTAMP",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq189",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "What is the average monthly revenue growth rate for the product category with the highest average monthly order growth rate based on completed orders?",
        "SQL": "WITH \nmonthly_summary AS (\n  SELECT \n    FORMAT_DATE('%m', oi.delivered_at) AS Month,\n    FORMAT_DATE('%Y', oi.delivered_at) AS Year,\n    p.category AS Product_category,\n    SUM(oi.sale_price) AS TPV,\n    COUNT(DISTINCT oi.order_id) AS TPO\n  FROM \n    `bigquery-public-data.thelook_ecommerce.order_items` oi\n  JOIN \n    `bigquery-public-data.thelook_ecommerce.products` p ON oi.product_id = p.id\n  JOIN \n    `bigquery-public-data.thelook_ecommerce.orders` o ON oi.order_id = o.order_id\n  WHERE\n    oi.status = 'Complete'\n  GROUP BY\n    Month, Year, Product_category\n  ORDER BY \n    Year, Month\n),\n\nlagged_summary AS (\n  SELECT \n    month,\n    year,\n    Product_category,\n    TPV,\n    TPO,\n    LAG(TPV) OVER(PARTITION BY Product_category ORDER BY year, month) AS Lagged_TPV,\n    LAG(TPO) OVER(PARTITION BY Product_category ORDER BY year, month) AS Lagged_TPO\n  FROM \n    monthly_summary\n),\n\ngrowth_summary AS (\n  SELECT \n    month,\n    year,\n    Product_category,\n    TPV,\n    TPO,\n    Lagged_TPV,\n    Lagged_TPO,\n    (TPV - Lagged_TPV) / NULLIF(Lagged_TPV, 0) * 100 AS Revenue_growth,\n    (TPO - Lagged_TPO) / NULLIF(Lagged_TPO, 0) * 100 AS Order_growth\n  FROM \n    lagged_summary\n),\n\nmax_order_growth AS (\n  SELECT \n    Product_category,\n    AVG(Order_growth) AS avg_order_growth\n  FROM \n    growth_summary\n  GROUP BY \n    Product_category\n  ORDER BY \n    avg_order_growth DESC\n  LIMIT 1\n)\n\nSELECT \n  AVG(gs.Revenue_growth) AS average_revenue_growth\nFROM \n  growth_summary gs\nJOIN \n  max_order_growth mog ON gs.Product_category = mog.Product_category;",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Calculate the total sale prices and total order amount (TPV) for each product category (TPO) every month.\n2. Get the TPV and TPO in the previous month as the 'Lagged_TPV' and 'Lagged_TPO'.\n3. Calculate the revenue growth rate and the order growth rate for each product monthly.\n4. Find out the product category with the max average order growth rate.\n5. Calculate the average revenue growth rate for that product category.",
        "special_function": [
            "date-functions/FORMAT_DATE",
            "mathematical-functions/ROUND",
            "navigation-functions/LAG",
            "conditional-functions/NULLIF"
        ]
    },
    {
        "instance_id": "bq260",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Find the total number of youngest and oldest users separately for each gender in the e-commerce platform created from January 1, 2019, to April 30, 2022.",
        "SQL": "WITH filtered_users AS (\n    SELECT \n        first_name, \n        last_name, \n        gender, \n        age,\n        created_at\n    FROM \n        bigquery-public-data.thelook_ecommerce.users\n    WHERE \n        created_at BETWEEN '2019-01-01' AND '2022-04-30'\n),\nyoungest_ages AS (\n    SELECT \n        gender, \n        MIN(age) AS age\n    FROM \n        filtered_users\n    GROUP BY \n        gender\n),\noldest_ages AS (\n    SELECT \n        gender, \n        MAX(age) AS age\n    FROM \n        filtered_users\n    GROUP BY \n        gender\n),\nyoungest_oldest AS (\n    SELECT \n        u.first_name, \n        u.last_name, \n        u.gender, \n        u.age, \n        'youngest' AS tag\n    FROM \n        filtered_users u\n    JOIN \n        youngest_ages y\n    ON \n        u.gender = y.gender AND u.age = y.age\n    UNION ALL\n    SELECT \n        u.first_name, \n        u.last_name, \n        u.gender, \n        u.age, \n        'oldest' AS tag\n    FROM \n        filtered_users u\n    JOIN \n        oldest_ages o\n    ON \n        u.gender = o.gender AND u.age = o.age\n)\nSELECT COUNT(*) AS num \nFROM youngest_oldest\nGROUP BY tag, gender",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Filter user data within the specified date range.\n2. Determine the youngest and oldest ages for each gender group.\n3. Identify users who are the youngest and oldest within their respective gender groups based on the age comparison.\n4. Count the number of users classified as the youngest and oldest within their gender groups.\n",
        "special_function": null
    },
    {
        "instance_id": "bq261",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "What is the total cost and profit of products whose profit rank first per month sorted chronologically? Only consider data before 2024.",
        "SQL": "WITH product_sales AS (\n    SELECT \n        FORMAT_TIMESTAMP('%Y-%m', i.created_at) AS month_year,\n        p.id AS product_id,\n        p.name AS product_name,\n        SUM(i.sale_price) AS sales,\n        SUM(p.cost) AS cost,\n        SUM(i.sale_price - p.cost) AS profit\n    FROM \n        bigquery-public-data.thelook_ecommerce.order_items AS i\n    JOIN \n        bigquery-public-data.thelook_ecommerce.products AS p \n    ON \n        i.product_id = p.id\n    WHERE created_at < TIMESTAMP \"2024-01-01 00:00:00 UTC\"\n    GROUP BY \n        month_year, product_id, product_name\n),\n\nranked_products AS (\n    SELECT \n        month_year,\n        product_id,\n        product_name,\n        sales,\n        cost,\n        profit,\n        RANK() OVER (PARTITION BY month_year ORDER BY profit DESC) AS rank_per_month\n    FROM \n        product_sales\n)\n\nSELECT \n    month_year,\n    product_id,\n    product_name,\n    sales,\n    cost,\n    profit\nFROM \n    ranked_products\nWHERE \n    rank_per_month = 1\nORDER BY \n    month_year;",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Calculate sales information for products, including the cost and profit.\n2. Assign a ranking to products within each month based on profit.\n3. Select and return the cost and profit figures for the top-ranking product for each month, sorting the results chronologically by month-year.\n",
        "special_function": [
            "numbering-functions/RANK",
            "timestamp-functions/FORMAT_TIMESTAMP",
            "timestamp-functions/TIMESTAMP"
        ]
    },
    {
        "instance_id": "bq262",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Help me generate a monthly analysis report on e-commerce sales of the second half of 2019, which should contain the total sum of order count/revenue/profit as well as their growth rates for each product category monthly. Please sort the results by months (in format \"2019-07\") and product categories in ascending order. Note that, for month July, please use the result of month June as the basis.",
        "SQL": "WITH orders_data AS (\n    SELECT\n        FORMAT_TIMESTAMP('%Y-%m', o.created_at) AS month,\n        i.product_id,\n        COUNT(o.order_id) AS order_count,\n        SUM(i.sale_price) AS total_sales\n    FROM \n        bigquery-public-data.thelook_ecommerce.orders AS o\n    JOIN \n        bigquery-public-data.thelook_ecommerce.order_items AS i \n    ON\n        o.order_id = i.order_id\n    WHERE\n        o.created_at BETWEEN TIMESTAMP('2019-06-01') AND TIMESTAMP('2019-12-31')\n    GROUP BY \n        month, i.product_id\n),\nproduct_data AS (\n    SELECT \n        p.id AS product_id,\n        p.category AS product_category,\n        p.cost\n    FROM \n        bigquery-public-data.thelook_ecommerce.products AS p\n),\nmonthly_metrics AS (\n    SELECT \n        o.month,\n        p.product_category,\n        SUM(o.order_count) AS total_order,\n        SUM(o.total_sales) AS total_revenue, \n        SUM(o.total_sales - (p.cost * o.order_count)) AS total_profit \n    FROM \n        orders_data AS o\n    JOIN \n        product_data AS p\n    ON \n        o.product_id = p.product_id\n    GROUP BY \n        o.month, p.product_category\n),\ngrowth_metrics AS (\n    SELECT \n        month,\n        product_category,\n        total_order,\n        (total_order - LAG(total_order) OVER (PARTITION BY product_category ORDER BY month)) / LAG(total_order) OVER (PARTITION BY product_category ORDER BY month) * 100 AS order_growth,\n        total_revenue,\n        (total_revenue - LAG(total_revenue) OVER (PARTITION BY product_category ORDER BY month)) / LAG(total_revenue) OVER (PARTITION BY product_category ORDER BY month) * 100 AS revenue_growth,\n        total_profit,\n        (total_profit - LAG(total_profit) OVER (PARTITION BY product_category ORDER BY month)) / LAG(total_profit) OVER (PARTITION BY product_category ORDER BY month) * 100 AS profit_growth\n    FROM \n        monthly_metrics\n)\n\nSELECT\n    *\nFROM \n    growth_metrics\nWHERE\n    month <> '2019-06'\nORDER BY\n    month, product_category ASC;",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Filter and Aggregate Orders Data:\n   - Objective: Gather monthly order data for each product.\n   - Action: Create a CTE `orders_data` to:\n     - Extract the year-month from `created_at` timestamp in the `orders` table.\n     - Count the total orders and sum the sales price for each product per month.\n     - Filter orders within the months from 2019-06 to 2019-12.\n\n2. Fetch Product Category and Cost:\n   - Objective: Associate each product with its category and cost.\n   - Action: Create a CTE `product_data` to:\n     - Select product ID, category, and cost from the `products` table.\n\n3. Calculate Monthly Metrics:\n   - Objective: Compute monthly order count, total revenue, and total profit for each product category.\n   - Action: Create a CTE `monthly_metrics` to:\n     - Join `orders_data` with `product_data` on product ID.\n     - Aggregate data by month and product category.\n     - Calculate total orders, total revenue, and total profit (revenue minus cost of goods sold).\n\n4. Calculate Growth Rates:\n   - Objective: Determine month-over-month growth rates for order count, revenue, and profit.\n   - Action: Create a CTE `growth_metrics` to:\n     - Use window functions (`LAG`) to calculate the previous month's values for each metric within the same product category.\n     - Compute growth rates for orders, revenue, and profit as percentages.\n\n5. Generate Final Report:\n   - Objective: Present the monthly analysis report with growth rates.\n   - Action: Select all columns from `growth_metrics` except the month `2019-06`.\n     - Order the result by month and product category for organized viewing.",
        "special_function": [
            "navigation-functions/LAG",
            "timestamp-functions/FORMAT_TIMESTAMP",
            "timestamp-functions/TIMESTAMP"
        ]
    },
    {
        "instance_id": "bq190",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "What is the count of the youngest and oldest users respectively for male and female users from January 2019 to April 2022?",
        "SQL": "WITH \nfemale_age AS (\n  SELECT \n    MIN(age) AS min_age, \n    MAX(age) AS max_age\n  FROM \n    `bigquery-public-data.thelook_ecommerce.users`\n  WHERE \n    gender = 'F' \n    AND created_at BETWEEN '2019-01-01 00:00:00' AND '2022-05-01 00:00:00'\n),\nmale_age AS (\n  SELECT \n    MIN(age) AS min_age, \n    MAX(age) AS max_age\n  FROM \n    `bigquery-public-data.thelook_ecommerce.users`\n  WHERE \n    gender = 'M' \n    AND created_at BETWEEN '2019-01-01 00:00:00' AND '2022-05-01 00:00:00'\n),\nyoung_old_group AS (\n  SELECT t1.first_name, t1.last_name, t1.gender, t1.age\n  FROM `bigquery-public-data.thelook_ecommerce.users` AS t1\n  JOIN female_age AS t2 ON t1.age = t2.min_age OR t1.age = t2.max_age\n  WHERE t1.gender = 'F' AND t1.created_at BETWEEN '2019-01-01 00:00:00' AND '2022-05-01 00:00:00'\n  UNION ALL\n  SELECT t3.first_name, t3.last_name, t3.gender, t3.age\n  FROM `bigquery-public-data.thelook_ecommerce.users` AS t3\n  JOIN male_age AS t4 ON t3.age = t4.min_age OR t3.age = t4.max_age\n  WHERE t3.gender = 'M' AND t3.created_at BETWEEN '2019-01-01 00:00:00' AND '2022-05-01 00:00:00'\n),\nage_tag AS (\n  SELECT *, \n    CASE \n      WHEN age IN (SELECT MIN(age) \n                   FROM `bigquery-public-data.thelook_ecommerce.users`\n                   WHERE gender = 'F' AND created_at BETWEEN '2019-01-01 00:00:00' AND '2022-05-01 00:00:00') \n           OR age IN (SELECT MIN(age) \n                      FROM `bigquery-public-data.thelook_ecommerce.users`\n                      WHERE gender = 'M' AND created_at BETWEEN '2019-01-01 00:00:00' AND '2022-05-01 00:00:00') \n           THEN 'Youngest'\n      ELSE 'Oldest'\n    END AS tag\n  FROM young_old_group \n),\ncount_summary AS (\n  SELECT \n    gender, \n    tag, \n    COUNT(*) AS user_count\n  FROM \n    age_tag\n  GROUP BY \n    gender, tag\n)\nSELECT \n  gender, \n  tag, \n  user_count\nFROM \n  count_summary\nORDER BY \n  gender, tag;",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. **Determine Age Ranges**:\n   - Create two subsets to find the minimum and maximum ages for each gender within the specified date range.\n\n2. **Identify Users with Extreme Ages**:\n   - For each gender, select users whose ages match the previously identified minimum or maximum ages within the specified date range.\n\n3. **Combine Users**:\n   - Combine the users from both genders into a single set, ensuring that only users with the extreme ages (youngest and oldest) are included.\n\n4. **Tag Users**:\n   - Add a tag to each user indicating whether they are in the 'Youngest' or 'Oldest' age group based on their age.\n\n5. **Count Users by Group**:\n   - Group the tagged users by gender and tag, then count the number of users in each group.\n\n6. **Output the Results**:\n   - Select and order the results by gender and tag to present a clear summary of the counts for each group.",
        "special_function": [
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq263",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Produce a 2023 monthly report for the 'Sleep & Lounge' category detailing total sales, costs, completed order counts, profits, and profit margins, ensuring accurate cost alignment with sales data.",
        "SQL": "with d as(\n    select\n        a.order_id, \n        format_date('%Y-%m', a.created_at) as month,\n        format_date('%Y', a.created_at) as year,\n        b.product_id, b.sale_price, c.category, c.cost\n    from bigquery-public-data.thelook_ecommerce.orders as a\n    JOIN\n        bigquery-public-data.thelook_ecommerce.order_items as b\n    on a.order_id = b.order_id\n    JOIN bigquery-public-data.thelook_ecommerce.products as c\n    on b.order_id = c.id\n    where a.status = 'Complete'\n    and a.created_at BETWEEN TIMESTAMP('2023-01-01') AND TIMESTAMP('2023-12-31')\n    and c.category = 'Sleep & Lounge'\n),\n\ne as (\n    select month, year, sale_price, category, cost,\n        sum(sale_price) over(partition by month, category) as TPV,\n        sum(cost) over(partition by month, category) as total_cost,\n        count(distinct order_id) over(partition by month, category) as TPO,\n        sum(sale_price-cost) over(partition by month, category) as total_profit,\n        sum((sale_price-cost)/cost) over(partition by month, category) as Profit_to_cost_ratio\n    from d\n)\n\nselect distinct month, category, TPV, total_cost, TPO, total_profit, Profit_to_cost_ratio\nfrom e\norder by month",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Consolidate data from the order items, orders, and products tables\n2. Computes key sales metrics including Total Purchase Value (TPV), total cost, Total Number of Orders (TPO), total profit, and Profit-to-Cost Ratio per month.\n3. Sort by month and return the result.\n",
        "special_function": [
            "date-functions/FORMAT_DATE",
            "timestamp-functions/TIMESTAMP"
        ]
    },
    {
        "instance_id": "bq264",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Identify the difference in the number of the oldest and youngest users registered between January 1, 2019, and April 30, 2022, from our e-commerce platform data.",
        "SQL": "with youngest as (SELECT\n        gender, id,\n        first_name,\n        last_name,\n        age,\n        'youngest' AS tag,\n  FROM `bigquery-public-data.thelook_ecommerce.users`\n  WHERE age = (SELECT MIN(age) FROM `bigquery-public-data.thelook_ecommerce.users`)\n        AND (created_at BETWEEN '2019-01-01' AND '2022-04-30')\n  GROUP BY gender, id, first_name, last_name, age\n  order by gender),\n\n  oldest as (SELECT\n        gender, id,\n        first_name,\n        last_name,\n        age,\n        'oldest' AS tag\n  FROM `bigquery-public-data.thelook_ecommerce.users`\n  WHERE age = (SELECT MAX(age) FROM `bigquery-public-data.thelook_ecommerce.users`)\n        AND (created_at BETWEEN '2019-01-01' AND '2022-04-30')\n  GROUP BY gender, id, first_name, last_name, age\n  order by gender),\n\n  TEMP_record as (\n  select *\n  from youngest \n  union all \n  select *\n  from oldest)\n \nselect \n      sum( CASE\n        WHEN age = (SELECT MAX(age) FROM `bigquery-public-data.thelook_ecommerce.users`) THEN 1\n      END)- sum( CASE\n        WHEN age = (SELECT MIN(age) FROM `bigquery-public-data.thelook_ecommerce.users`) THEN 1\n      END) as diff\n from TEMP_record",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Select the youngest users in terms of age for each gender within the specified date range.\n2. Identify the oldest users for each gender.\n3. Calculate the difference in the count of the oldest and youngest users.",
        "special_function": [
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq197",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "What is the top-selling product by sales volume and revenue for June 2024 and each month before, considering only completed orders?",
        "SQL": "WITH \nreport_monthly_orders_product_agg AS\n(\nSELECT \n    p.name AS product_name,\n    p.brand,\n    p.category,\n    FORMAT_TIMESTAMP('%Y-%m', o.created_at) AS month,\n    o.status,\n    COUNT(o.product_id) AS total_sales,\n    SUM(o.sale_price) AS total_revenue \nFROM \n    `bigquery-public-data.thelook_ecommerce.products` p\nLEFT JOIN \n    `bigquery-public-data.thelook_ecommerce.order_items` o\nON \n    p.id = o.product_id\nWHERE\n    o.status = 'Complete'\n    AND o.created_at < TIMESTAMP('2024-07-01') \nGROUP BY \n    product_name, \n    month,\n    o.status,\n    p.brand,\n    p.category        \nHAVING\n    month IS NOT NULL AND total_revenue IS NOT NULL AND p.brand IS NOT NULL\n)\n\nSELECT \n    month, \n    product_name, \n    brand,\n    category,\n    total_sales,\n    ROUND(total_revenue, 2) AS total_revenue,\n    status    \nFROM \n    (\n        SELECT \n            month, \n            product_name,\n            brand,\n            category, \n            total_sales,\n            total_revenue,\n            status,\n            ROW_NUMBER() OVER (PARTITION BY month ORDER BY total_sales DESC, total_revenue DESC) as row_num\n        FROM \n           report_monthly_orders_product_agg\n    ) \nWHERE \n    row_num = 1\nORDER BY month ASC;",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. **Data Aggregation for Each Month**:\n    - **Extract and Format Data**: For each record, extract the product details and the order details, including the timestamp of the order, and format the timestamp to year and month.\n    - **Filter Data**: Consider only the orders that are marked as completed and occurred before July 2024.\n    - **Aggregate Data**: Group the data by product details and month to calculate the total number of sales and the total revenue for each product within each month.\n\n2. **Filter Non-relevant Data**:\n    - **Ensure Valid Data**: Ensure that the month, total revenue, and product brand are not null to maintain the integrity of the data.\n\n3. **Identify Top-Selling Products**:\n    - **Rank Products**: For each month, assign a rank to each product based on the total number of sales, with ties broken by total revenue.\n    - **Select Top Product**: Select the product with the highest rank (i.e., the top-selling product) for each month.\n\n4. **Final Output**:\n    - **Organize and Format**: Retrieve the month, product details, total sales, and total revenue for the top-selling product of each month.\n    - **Order Results**: Order the final results in ascending order of the month to provide a chronological view of the top-selling products over the specified period.",
        "special_function": [
            "mathematical-functions/ROUND",
            "numbering-functions/ROW_NUMBER",
            "timestamp-functions/FORMAT_TIMESTAMP",
            "timestamp-functions/TIMESTAMP"
        ]
    },
    {
        "instance_id": "bq265",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Can you provide me with the emails of the top 10 users who have the highest average order value, considering only those users who registered in 2019 and made purchases within the same year?",
        "SQL": "WITH\n  main AS (\n  SELECT\n    id AS user_id,\n    email,\n    gender,\n    country,\n    traffic_source\n  FROM\n    `bigquery-public-data.thelook_ecommerce.users` \n  WHERE\n    created_at BETWEEN '2019-01-01' AND '2019-12-31'),\n\n  daate AS (\n  SELECT\n    user_id,\n    order_id,\n    EXTRACT (date\n    FROM\n      created_at) AS order_date,\n    num_of_item\n  FROM\n    `bigquery-public-data.thelook_ecommerce.orders` \n  WHERE\n    created_at BETWEEN '2019-01-01' AND '2019-12-31' ),\n\n  orders AS (\n  SELECT\n    user_id,\n    order_id,\n    product_id,\n    sale_price,\n    status\n  FROM\n    `bigquery-public-data.thelook_ecommerce.order_items` \n  WHERE \n    created_at BETWEEN '2019-01-01' AND '2019-12-31' ),\n\n  nest AS (\n  SELECT\n    o.user_id,\n    o.order_id,\n    o.product_id,\n    d.order_date,\n    d.num_of_item,\n    ROUND(o.sale_price,2)AS sale_price,\n    ROUND(d.num_of_item*o.sale_price,2) AS total_sale,\n  FROM\n    orders o\n  INNER JOIN\n    daate d\n  ON\n    o.order_id = d.order_id\n  ORDER BY\n    o.user_id ),\n\n  type AS (\n  SELECT\n    user_id,\n    MIN(nest.order_date) AS cohort_date,\n    MAX(nest.order_date) AS latest_shopping_date,\n    DATE_DIFF(MAX(nest.order_date),MIN(nest.order_date),month) AS lifespan_months,\n    ROUND(SUM(total_sale),2) AS ltv,\n    COUNT(order_id) AS no_of_order\n  FROM\n    nest\n  GROUP BY\n    user_id ),\n\n  kite AS (\n  SELECT\n    m.user_id,\n    m.email,\n    m.gender,\n    m.country,\n    m.traffic_source,\n    extract(year from n.cohort_date) as cohort_year,\n    n.latest_shopping_date,\n    n.lifespan_months,\n    n.ltv,\n    n.no_of_order,\n    ROUND(n.ltv/n.no_of_order, 2) as avg_order_value\n  FROM\n    main m\n  INNER JOIN\n    type n\n  ON\n    m.user_id = n.user_id )\n\nSELECT\n  email\nFROM\n  kite\nORDER BY avg_order_value DESC\nLIMIT 10",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Get the date level information.\n2. Get all the order level data including the sales value and number of orders.\n3. Combine date level data with the previously captured orders and sales level data.\n4. Calculate the Lifetime value (LTV) of the customer and number of orders placed by them.\n5. Figure out the average order value per user by dividing the LTV by the number of orders placed by that user.\n6. Sort to find the top 10 average order value users.\n",
        "special_function": [
            "date-functions/DATE",
            "date-functions/DATE_DIFF",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "mathematical-functions/ROUND",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT"
        ]
    },
    {
        "instance_id": "bq266",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Can you provide me with the names of the products that had the lowest profit margin each month throughout the year 2020, excluding any months where this data isn't available? Please list them in chronological order based on the month.",
        "SQL": "WITH cte AS (\n  SELECT \n    t1.id AS product_id,\n    t1.name,\n    t1.cost,\n    t1.retail_price,\n    t2.created_at,\n   CONCAT(EXTRACT(YEAR FROM t2.created_at), '-', LPAD(CAST(EXTRACT(MONTH FROM t2.created_at) AS STRING), 2, '0')) AS year_month,\n    (t1.retail_price - t1.cost) AS profit\n  FROM \n    `bigquery-public-data.thelook_ecommerce.products` AS t1\n  LEFT JOIN \n    `bigquery-public-data.thelook_ecommerce.order_items` AS t2\n  ON \n    t1.id = t2.product_id\n  WHERE\n    t2.created_at BETWEEN '2020-01-01' AND '2020-12-31'\n),\n\nranked_cte AS (\n  SELECT\n    year_month,\n    product_id,\n    name,\n    cost,\n    retail_price AS sale,\n    profit,\n    DENSE_RANK() OVER (PARTITION BY year_month ORDER BY profit asc) AS rank\n  FROM \n    cte\n)\nSELECT\n  name\nFROM \n  ranked_cte\nWHERE \n  rank = 1 and year_month is not null\nORDER BY year_month",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Calculate profit as the difference between retail price and cost.\n2. Assign a rank to each product within a month based on profit margin and partition the ranking by year and month and order products by profit in ascending order within each month.\n3. Retrieve the names of products that have the rank of 1 and return.\n",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "json-functions/STRING",
            "numbering-functions/DENSE_RANK",
            "numbering-functions/RANK",
            "string-functions/CONCAT",
            "string-functions/LPAD",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/STRING"
        ]
    },
    {
        "instance_id": "bq333",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Which are the top 3 browsers with the shortest average session duration, and what are their average session times? Only include browsers with more than 10 sessions.",
        "SQL": "WITH\n  main AS (\n    SELECT\n      browser,\n      user_id,\n      session_id,\n      DATE_DIFF(MAX(created_at), MIN(created_at), SECOND) AS user_duration\n    FROM\n      bigquery-public-data.thelook_ecommerce.events\n    GROUP BY\n      user_id,\n      session_id,\n      browser\n  ),\n  \n  browser_stats AS (\n    SELECT\n      browser,\n      AVG(user_duration) AS avg_user_duration,\n      COUNT(session_id) AS total_sessions\n    FROM\n      main\n    GROUP BY\n      browser\n    HAVING\n      COUNT(session_id) > 10\n  )\n\nSELECT\n  browser,\n  avg_user_duration\nFROM\n  browser_stats\nORDER BY\n  avg_user_duration ASC\nLIMIT 3;",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. **Define Main Data Aggregation**:\n   - Create a common table expression (CTE) to aggregate session data.\n   - Calculate the session duration for each session by subtracting the earliest event time from the latest event time within each session for each user and browser.\n   - Group the data by user, session, and browser.\n\n2. **Calculate Browser Statistics**:\n   - Create a second CTE to compute statistics for each browser.\n   - Calculate the average session duration for each browser.\n   - Count the total number of sessions for each browser.\n   - Filter out browsers with 10 or fewer sessions.\n\n3. **Select and Order Results**:\n   - Query the second CTE to select the browser and its average session duration.\n   - Order the results by average session duration in ascending order to find the browsers with the shortest average session durations.\n   - Limit the results to the top 3 browsers.\n\nThis plan achieves the user's intention by first aggregating session data, then filtering and computing statistics for browsers with sufficient session counts, and finally selecting and ordering the top 3 browsers based on the shortest average session duration.",
        "special_function": [
            "date-functions/DATE_DIFF"
        ]
    },
    {
        "instance_id": "bq361",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "For the user cohort with a first purchase date in January 2020, what proportion of users returned in the subsequent months of 2020?",
        "SQL": "-- group each customer based on their cohort\nWITH cohorts AS (\n  SELECT\n    user_id,\n    DATE(created_at) AS order_date,\n    DATE(DATE_TRUNC(FIRST_VALUE(created_at) OVER(PARTITION BY user_id ORDER BY created_at), month)) AS cohort\n  FROM\n    `bigquery-public-data.thelook_ecommerce.orders`\n  WHERE \n    DATE(created_at) BETWEEN '2020-01-01' AND '2020-12-31'\n),\n-- calculate the number of months after the first month \nactivity AS (\n  SELECT\n    user_id,\n    cohort,\n    DATE_DIFF(order_date, cohort, month) AS month_since_first_order\n  FROM \n    cohorts\n  WHERE \n    DATE(cohort) = '2020-01-01'\n),\n-- counting the number of unique users for each cohort and month_since_first_order\nnew_users AS ( \n  SELECT\n    cohort,\n    month_since_first_order,\n    COUNT(DISTINCT user_id) AS new_user\n  FROM\n    activity\n  GROUP BY\n    cohort,\n    month_since_first_order\n),\n-- calculate the total customer on each cohort\ncohort_users AS (\n  SELECT\n    cohort,\n    month_since_first_order,\n    new_user,\n    FIRST_VALUE(new_user) OVER(PARTITION BY cohort ORDER BY month_since_first_order) AS cohort_user\n  FROM\n    new_users\n)\n-- calculate the cohort users percentage\nSELECT \n  cohort,\n  month_since_first_order,\n  new_user,\n  cohort_user,\n  new_user / cohort_user AS cohort_users_percentage\nFROM \n  cohort_users\nWHERE\n  month_since_first_order > 0 -- Exclude January data (month 0)\nORDER BY\n  cohort, \n  month_since_first_order;",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "",
        "special_function": [
            "DATE",
            "DATE_TRUNC",
            "FIRST_VALUE",
            "PARTITION BY",
            "DATE_DIFF"
        ]
    },
    {
        "instance_id": "bq271",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Could you generate a report that, for each month in 2021, provides the number of orders, number of unique purchasers, and profit (calculated as total product retail price minus total cost) grouped by country, product department, and product category?",
        "SQL": "WITH\norders_x_order_items AS (\n  SELECT orders.*,\n         order_items.inventory_item_id,\n         order_items.sale_price\n  FROM `bigquery-public-data.thelook_ecommerce.orders` AS orders\n  LEFT JOIN `bigquery-public-data.thelook_ecommerce.order_items` AS order_items\n  ON orders.order_id = order_items.order_id\n  WHERE orders.created_at BETWEEN '2021-01-01' AND '2021-12-31'\n),\n\norders_x_inventory AS (\n  SELECT orders_x_order_items.*,\n         inventory_items.product_category,\n         inventory_items.product_department,\n         inventory_items.product_retail_price,\n         inventory_items.product_distribution_center_id,\n         inventory_items.cost,\n         distribution_centers.name\n  FROM orders_x_order_items\n  LEFT JOIN `bigquery-public-data.thelook_ecommerce.inventory_items` AS inventory_items\n  ON orders_x_order_items.inventory_item_id = inventory_items.id\n  LEFT JOIN `bigquery-public-data.thelook_ecommerce.distribution_centers` AS distribution_centers\n  ON inventory_items.product_distribution_center_id = distribution_centers.id\n  WHERE inventory_items.created_at BETWEEN '2021-01-01' AND '2021-12-31'\n),\n\norders_x_users AS (\n  SELECT orders_x_inventory.*,\n         users.country AS users_country,\n  FROM orders_x_inventory \n  LEFT JOIN `bigquery-public-data.thelook_ecommerce.users` AS users\n  ON orders_x_inventory.user_id = users.id\n  WHERE users.created_at BETWEEN '2021-01-01' AND '2021-12-31'\n)\n\nSELECT DATE_TRUNC(DATE(created_at),MONTH) AS reporting_month,\n        users_country,\n        product_department,\n        product_category,\n        COUNT(DISTINCT order_id) AS n_order,\n        COUNT(DISTINCT user_id) AS n_purchasers,\n        SUM(product_retail_price) - SUM(cost) AS profit\nFROM orders_x_users\nGROUP BY 1,2,3,4",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "Extract the inventory item ID and sale price for each order.\n1. Augment the data with the country associated with each user who made a purchase.\n2. Calculate the total product retail price, and total cost.\n3. Figure out the profit through subtracting the total product retail price from the total cost.\n4. Sort the results by the profit.\n5. Return the country name which has the highest monthly profit.\n",
        "special_function": [
            "date-functions/DATE",
            "date-functions/DATE_TRUNC"
        ]
    },
    {
        "instance_id": "bq272",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Please provide me with the names of the top three most profitable products for each month between January 2019 and August 2022, excluding any products that were either canceled or returned.",
        "SQL": "WITH product_profit AS (\n  SELECT\n    DATE(DATE_TRUNC(oi.created_at, month)) AS month,\n    oi.product_id,\n    p.name,\n    p.category,\n    (SUM(oi.sale_price)) AS sum_sales,\n    (SUM(p.cost)) AS sum_cost,\n    (SUM(oi.sale_price)-SUM(p.cost)) AS profit\n  FROM\n    `bigquery-public-data.thelook_ecommerce.order_items` oi\n    LEFT JOIN `bigquery-public-data.thelook_ecommerce.products` p ON oi.product_id = p.id\n  WHERE\n    DATE(oi.created_at) BETWEEN '2019-01-01' AND '2022-08-30'\n    AND oi.status NOT IN ('Cancelled', 'Returned')\n  GROUP BY\n    month,\n    product_id,\n    name,\n    category\n), \n\nprofit_rank AS (\n  SELECT\n  month,\n  product_id,\n  name,\n  category,\n  sum_sales,\n  sum_cost,\n  profit,\n  RANK() OVER(PARTITION BY month ORDER BY profit DESC) AS profit_rank_per_month\nFROM \n  product_profit\nQUALIFY\n  profit_rank_per_month <= 3\nORDER BY \n  month,\n  profit DESC\n)\n\nSELECT name\nFROM profit_rank",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Calculate monthly profit for each product by truncating the creation date to the month level.\n2. Only include data between specific dates and exclude records where the status indicates the item was either canceled or returned.\n3. For each month, rank products based on their profit in descending order. This ranking is calculated within each month's dataset.\n4. From the ranked list, select only the top 5 products by profit for each month.\n5. Count the total number of records that have been identified as top performers across all months in the filtered dataset.\n",
        "special_function": [
            "date-functions/DATE",
            "date-functions/DATE_TRUNC",
            "mathematical-functions/ROUND",
            "numbering-functions/RANK"
        ]
    },
    {
        "instance_id": "bq273",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Can you list the top 5 months from August 2022 to November 2023 where the profit from Facebook-sourced completed orders showed the largest month-over-month increase? Calculate profit as sales minus costs.",
        "SQL": "with \norders as (\n  select\n    order_id, \n    user_id, \n    created_at,\n    date_trunc(delivered_at, MONTH) as delivery_month,\n    status \n  from bigquery-public-data.thelook_ecommerce.orders \n),\n\norder_items as (\n  select \n    order_id, \n    product_id, \n    sale_price \n  from bigquery-public-data.thelook_ecommerce.order_items \n),\n\nproducts as (\n  select \n    id, \n    cost\n  from bigquery-public-data.thelook_ecommerce.products\n),\n\nusers as (\n  select\n    id, \n    traffic_source \n  from bigquery-public-data.thelook_ecommerce.users \n),\n\nfilter_join as (\n  select \n    orders.order_id,\n    orders.user_id,\n    order_items.product_id,\n    orders.delivery_month,\n    orders.status,\n    order_items.sale_price,\n    products.cost,\n    users.traffic_source\n  from orders\n  join order_items on orders.order_id = order_items.order_id\n  join products on order_items.product_id = products.id\n  join users on orders.user_id = users.id\n  where orders.status = 'Complete' \n      and users.traffic_source = 'Facebook'\n      and orders.created_at between '2022-08-01' and '2023-11-30'\n),\n\nmonthly_sales as (\n select \n    delivery_month,\n    traffic_source,\n    sum(sale_price) as total_revenue,\n    sum(sale_price) - sum(cost) as total_profit,\n    count(distinct product_id) as product_quantity,\n    count(distinct order_id) as orders_quantity,\n    count(distinct user_id) as users_quantity\n  from filter_join\n  group by delivery_month, traffic_source\n)\nselect \n  monthly_sales.delivery_month,\n  monthly_sales.total_profit - lag(monthly_sales.total_profit, 1) over(partition by monthly_sales.traffic_source order by monthly_sales.delivery_month) \n    as profit_vs_prior_month\nfrom monthly_sales\norder by profit_vs_prior_month DESC\nlimit 5;",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Convert delivery dates to the month granularity.\n2. Retrieve order-related details, product cost information, user information.\n3. Apply filters to keep only records where the order status is 'Complete', the user traffic source is \u2018Facebook\u2019 and the order created time in the specific period.\n4. Compute total revenue, total profit, and count unique products, orders, and users for each month.\n5. For both revenue and profit, compute a 3-month moving average.\n6. Calculate the difference between the current month's revenue and profit against their respective moving averages.\n7. Calculate month-over-month changes in revenue and profit by comparing with the previous month's figures.\n8. Select the delivery month with the highest increase in profit compared to the prior month.\n",
        "special_function": [
            "date-functions/DATE_TRUNC",
            "navigation-functions/LAG"
        ]
    },
    {
        "instance_id": "bq020",
        "db": "spider2-public-data.genomics_cannabis",
        "question": "What is the name of the reference sequence with the highest variant density in the given cannabis genome dataset?",
        "SQL": "SELECT\n  reference_name\nFROM (\n  SELECT\n    reference_name,\n    COUNT(reference_name) / r.length AS variant_density,\n    COUNT(reference_name) AS variant_count,\n    r.length AS reference_length\n  FROM\n    `spider2-public-data.genomics_cannabis.MNPR01_201703` v,\n    `spider2-public-data.genomics_cannabis.MNPR01_reference_201703` r\n  WHERE\n    v.reference_name = r.name\n    AND EXISTS (\n    SELECT\n      1\n    FROM\n      UNNEST(v.call) AS call\n    WHERE\n      EXISTS (\n      SELECT\n        1\n      FROM\n        UNNEST(call.genotype) AS gt\n      WHERE\n        gt > 0))\n  GROUP BY\n    reference_name,\n    r.length ) AS d\nORDER BY\n  variant_density DESC\nLIMIT \n  1;",
        "external_knowledge": null,
        "plan": "1. **Identify Relevant Tables**:\n   - Utilize two tables: one containing variant data and another containing reference sequence data.\n\n2. **Join Tables**:\n   - Perform an inner join on the tables based on matching reference sequence names to combine variant information with reference sequence details.\n\n3. **Filter Variants**:\n   - Include only those variants where there is at least one genotype value greater than zero within the variant calls. This ensures that only relevant variants are considered.\n\n4. **Calculate Variant Density**:\n   - For each reference sequence, calculate the density of variants by dividing the count of variants by the length of the reference sequence. Also, compute the total count of variants and store the reference sequence length for later use.\n\n5. **Group and Aggregate Data**:\n   - Group the results by reference sequence name and reference sequence length to compute the variant counts and densities for each reference sequence.\n\n6. **Order by Density**:\n   - Sort the grouped results in descending order of variant density to prioritize sequences with the highest density of variants.\n\n7. **Select Top Result**:\n   - Limit the results to a single entry to identify the reference sequence with the highest variant density.\n\n8. **Output**:\n   - Extract and display the name of the reference sequence with the highest variant density.",
        "special_function": [
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq107",
        "db": "spider2-public-data.genomics_cannabis",
        "question": "What is the variant density of the cannabis reference with the longest reference length? Pay attention that a variant is present if there is at least one variant call with a genotype greater than 0.",
        "SQL": "WITH density_data AS (\n  SELECT \n    reference_name, \n    COUNT(reference_name) / r.length AS variant_density, \n    COUNT(reference_name) AS variant_count, \n    r.length AS reference_length \n  FROM \n    `spider2-public-data.genomics_cannabis.MNPR01_201703` v, \n    `spider2-public-data.genomics_cannabis.MNPR01_reference_201703` r \n  WHERE \n    v.reference_name= r.name \n    AND EXISTS ( \n      SELECT \n        1 \n      FROM \n        UNNEST(v.call) AS call \n      WHERE \n        EXISTS ( \n          SELECT \n            1 \n          FROM \n            UNNEST(call.genotype) AS gt \n          WHERE \n            gt > 0)) \n  GROUP BY \n    reference_name, \n    r.length\n) \nSELECT \n  *\nFROM\n  density_data\nORDER BY \n  reference_length DESC, \n  reference_name\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "1. **Data Preparation (CTE Setup)**:\n   - Create a common table expression (CTE) to compute necessary metrics for each reference.\n   \n2. **Join Data**:\n   - Join two datasets: one containing variant data and another containing reference length information based on matching reference names.\n\n3. **Filter Data**:\n   - Apply a filter to ensure only those variants are considered where at least one genotype call is greater than zero.\n\n4. **Aggregation**:\n   - Group the data by reference name and reference length.\n   - Calculate the total count of variants for each reference.\n   - Compute the variant density by dividing the count of variants by the reference length.\n\n5. **Final Selection**:\n   - Select all data from the aggregated CTE.\n\n6. **Sorting and Limiting**:\n   - Order the results by reference length in descending order.\n   - In case of ties in reference length, order by reference name.\n   - Limit the result to one row to get the reference with the longest length and its variant density.",
        "special_function": [
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq025",
        "db": "bigquery-public-data.census_bureau_international",
        "question": "Provide a list of the top 10 countries for the year 2020, ordered by the highest percentage of their population under 20 years old. For each country, include the total population under 20 years old, the total midyear population, and the percentage of the population that is under 20 years old.",
        "SQL": "SELECT\n  age.country_name,\n  SUM(age.population) AS under_25,\n  pop.midyear_population AS total,\n  ROUND((SUM(age.population) / pop.midyear_population) * 100,2) AS pct_under_25\nFROM (\n  SELECT\n    country_name,\n    population,\n    country_code\n  FROM\n    `bigquery-public-data.census_bureau_international.midyear_population_agespecific`\n  WHERE\n    year =2020\n    AND age < 20) age\nINNER JOIN (\n  SELECT\n    midyear_population,\n    country_code\n  FROM\n    `bigquery-public-data.census_bureau_international.midyear_population`\n  WHERE\n    year = 2020) pop\nON\n  age.country_code = pop.country_code\nGROUP BY\n  1,\n  3\nORDER BY\n  4 DESC\n/* Remove limit for visualization */\nLIMIT\n  10",
        "external_knowledge": null,
        "plan": "1. **Define the Main Query Objective**:\r\n   - Retrieve and analyze the population data for countries, specifically focusing on the population under the age of 20 and its percentage relative to the total population.\r\n\r\n2. **Subquery 1 (Alias `age`)**:\r\n   - **Data Source**: `bigquery-public-data.census_bureau_international.midyear_population_agespecific`\r\n   - **Filters**:\r\n     - `year = 2020`: Only consider data from the year 2020.\r\n     - `age < 20`: Only include population data for individuals under the age of 20.\r\n   - **Selected Columns**:\r\n     - `country_name`: Name of the country.\r\n     - `population`: Population of the specific age group.\r\n     - `country_code`: Country code for joining purposes.\r\n\r\n3. **Subquery 2 (Alias `pop`)**:\r\n   - **Data Source**: `bigquery-public-data.census_bureau_international.midyear_population`\r\n   - **Filters**:\r\n     - `year = 2020`: Only consider data from the year 2020.\r\n   - **Selected Columns**:\r\n     - `midyear_population`: Total midyear population for the country.\r\n     - `country_code`: Country code for joining purposes.\r\n\r\n4. **Join Operation**:\r\n   - **Type**: INNER JOIN\r\n   - **Condition**: `age.country_code = pop.country_code`\r\n   - **Purpose**: Combine age-specific population data with total population data for each country based on matching country codes.\r\n\r\n5. **Aggregation and Calculations**:\r\n   - **Grouped By**: \r\n     - `age.country_name` (Column index 1 in the SELECT clause)\r\n     - `pop.midyear_population` (Column index 3 in the SELECT clause)\r\n   - **Aggregations**:\r\n     - `SUM(age.population) AS under_25`: Calculate the total population under the age of 20 for each country.\r\n     - `ROUND((SUM(age.population) / pop.midyear_population) * 100,2) AS pct_under_25`: Calculate and round to two decimal places the percentage of the population under 20 relative to the total midyear population for each country.\r\n\r\n6. **Ordering**:\r\n   - **Order By**: `pct_under_25 DESC` (Column index 4 in the SELECT clause)\r\n   - **Purpose**: Sort the results in descending order based on the percentage of the population under 20.\r\n\r\n7. **Limit**:\r\n   - **Limit**: `10`\r\n   - **Purpose**: Restrict the result set to the top 10 countries based on the sorted percentage of the population under 20.",
        "special_function": [
            "mathematical-functions/ROUND"
        ]
    },
    {
        "instance_id": "bq115",
        "db": "bigquery-public-data.census_bureau_international",
        "question": "Which country has the highest percentage of population under the age of 25 in 2017?",
        "SQL": "SELECT\ncountry_name\nFROM\n(SELECT\n  age.country_name,\n  SUM(age.population) AS under_25,\n  pop.midyear_population AS total,\n  ROUND((SUM(age.population) / pop.midyear_population) * 100,2) AS pct_under_25\nFROM (\n  SELECT\n    country_name,\n    population,\n    country_code\n  FROM\n    `bigquery-public-data.census_bureau_international.midyear_population_agespecific`\n  WHERE\n    year =2017\n    AND age < 25) age\nINNER JOIN (\n  SELECT\n    midyear_population,\n    country_code\n  FROM\n    `bigquery-public-data.census_bureau_international.midyear_population`\n  WHERE\n    year = 2017) pop\nON\n  age.country_code = pop.country_code\nGROUP BY\n  1,\n  3\nORDER BY\n  4 DESC\n)\nLIMIT\n1",
        "external_knowledge": null,
        "plan": "1. **Filter Data by Year and Age**: Begin by selecting records from the dataset that match the specific year (2017) and where the age is less than 25. This filters the population data to only include those under 25 years old.\n\n2. **Aggregate Population Under 25**: Sum the population values for the filtered records to get the total population under 25 for each country.\n\n3. **Fetch Total Population**: Select the total midyear population for each country for the same year (2017) from another dataset.\n\n4. **Combine Datasets**: Perform an inner join between the two datasets on the country code to combine the total population under 25 with the total midyear population for each country.\n\n5. **Calculate Percentage**: Compute the percentage of the population under 25 by dividing the summed population under 25 by the total midyear population, then multiply by 100 and round to two decimal places.\n\n6. **Group and Order**: Group the results by country and total population, then order the results by the calculated percentage of the population under 25 in descending order.\n\n7. **Select Top Country**: Limit the result to the top entry, which represents the country with the highest percentage of the population under 25.\n\n8. **Return Country Name**: Finally, select and return the name of the country with the highest percentage of the population under 25 in 2017.",
        "special_function": [
            "mathematical-functions/ROUND"
        ]
    },
    {
        "instance_id": "bq030",
        "db": "bigquery-public-data.covid19_open_data",
        "question": "As of May 10, 2020, which three countries with over 50,000 confirmed COVID-19 cases had the highest recovery rates? Please provide the list of these countries along with their respective recovery rates.",
        "SQL": "WITH cases_by_country AS (\n  SELECT\n    country_name AS country,\n    SUM(cumulative_confirmed) AS cases,\n    SUM(cumulative_recovered) AS recovered_cases\n  FROM\n    `bigquery-public-data.covid19_open_data.covid19_open_data`\n  WHERE\n    date=\"2020-05-10\"\n  GROUP BY\n    country_name\n)\n\n, recovered_rate AS (\n  SELECT\n    country, cases, recovered_cases,\n    (recovered_cases * 100)/cases AS recovery_rate\n  FROM\n    cases_by_country\n)\n\nSELECT country, recovery_rate\nFROM\n   recovered_rate\nWHERE\n   cases > 50000\nORDER BY recovery_rate DESC\nLIMIT 3",
        "external_knowledge": null,
        "plan": "1. Summarize confirmed and recovered cases for each country as of May 10, 2020.\n2. Calculate the percentage of recovered cases from confirmed cases for each country.\n3. Select countries with over 50,000 confirmed cases and rank them by their recovery rates.\n4. Show the top country with the highest recovery rates along with their case statistics.",
        "special_function": null
    },
    {
        "instance_id": "bq018",
        "db": "bigquery-public-data.covid19_open_data",
        "question": "Which day in March and April had the highest COVID-19 confirmed case growth rate in the United States? The format is MM-DD.",
        "SQL": "WITH us_cases_by_date AS (\n  SELECT\n    date,\n    SUM( cumulative_confirmed ) AS cases\n  FROM\n    `bigquery-public-data.covid19_open_data.covid19_open_data`\n  WHERE\n    country_name=\"United States of America\"\n    AND date between '2020-03-01' and '2020-04-30'\n  GROUP BY\n    date\n  ORDER BY\n    date ASC\n )\n\n, us_previous_day_comparison AS\n(SELECT\n  date,\n  cases,\n  LAG(cases) OVER(ORDER BY date) AS previous_day,\n  cases - LAG(cases) OVER(ORDER BY date) AS net_new_cases,\n  (cases - LAG(cases) OVER(ORDER BY date))*100/LAG(cases) OVER(ORDER BY date) AS percentage_increase\nFROM us_cases_by_date\n)\nSELECT\n  FORMAT_DATE('%m-%d', Date) \nFROM\n  us_previous_day_comparison\nORDER BY  \n  percentage_increase\nDESC\nLIMIT 1",
        "external_knowledge": null,
        "plan": "1. Collect and sum up the confirmed COVID-19 cases for each day within the specified range (March 22 to April 20, 2020) for the United States.\n2. Compute the net new cases for each day by comparing it to the previous day\u2019s case count.\n3. Calculate the percentage increase in cases from the previous day.\n4. Identify the day with the highest percentage increase in cases.",
        "special_function": null
    },
    {
        "instance_id": "bq086",
        "db": "bigquery-public-data.covid19_open_data\nbigquery-public-data.world_bank_intl_debt\nbigquery-public-data.world_bank_wdi\nbigquery-public-data.world_bank_intl_education\nbigquery-public-data.world_bank_health_population\nbigquery-public-data.world_bank_global_population",
        "question": "What percentage of each country\u2019s population was confirmed to have COVID-19 as of June 30, 2020?",
        "SQL": "WITH\n  country_pop AS (\n  SELECT\n    country_code AS iso_3166_1_alpha_3,\n    year_2018 AS population_2018\n  FROM\n    `bigquery-public-data.world_bank_global_population.population_by_country`)\nSELECT\n  country_code,\n  country_name,\n  cumulative_confirmed AS june_confirmed_cases,\n  population_2018,\n  ROUND(cumulative_confirmed/population_2018 * 100,2) AS case_percent\nFROM\n  `bigquery-public-data.covid19_open_data.covid19_open_data`\nJOIN\n  country_pop\nUSING\n  (iso_3166_1_alpha_3)\nWHERE\n  date = '2020-06-30'\n  AND aggregation_level = 0\nORDER BY\n  case_percent DESC",
        "external_knowledge": null,
        "plan": "1. Retrieve 2018 population data for each country from the World Bank global population dataset.\n2. Select COVID-19 case data as of June 30, 2020\n3. Compute the percentage of the population with confirmed cases",
        "special_function": [
            "date-functions/DATE",
            "mathematical-functions/ROUND"
        ]
    },
    {
        "instance_id": "bq085",
        "db": "bigquery-public-data.covid19_jhu_csse\nbigquery-public-data.world_bank_intl_debt\nbigquery-public-data.world_bank_wdi\nbigquery-public-data.world_bank_intl_education\nbigquery-public-data.world_bank_health_population\nbigquery-public-data.world_bank_global_population",
        "question": "Could you provide the total number of confirmed COVID-19 cases and the number of cases per 100,000 people, based on the 2020 population, on April 20, 2020, for the US, France, China, Italy, Spain, Germany, and Iran?",
        "SQL": "SELECT\n  c.country,\n  c.total_confirmed_cases,\n  (c.total_confirmed_cases / p.population) * 100000 AS cases_per_100k\nFROM\n  (\n    SELECT\n      CASE\n        WHEN country_region = 'US' THEN 'United States'\n        WHEN country_region = 'Iran' THEN 'Iran, Islamic Rep.'\n        ELSE country_region\n      END AS country,\n      SUM(confirmed) AS total_confirmed_cases\n    FROM\n      `bigquery-public-data.covid19_jhu_csse.summary`\n    WHERE\n      date = '2020-04-20'\n      AND country_region IN ('US', 'France', 'China', 'Italy', 'Spain', 'Germany', 'Iran')\n    GROUP BY\n      country\n  ) AS c\nJOIN\n  (\n    SELECT\n      country_name AS country,\n      SUM(value) AS population\n    FROM\n      `bigquery-public-data.world_bank_wdi.indicators_data`\n    WHERE\n      indicator_code = 'SP.POP.TOTL'\n      AND year = 2020\n    GROUP BY\n      country_name\n  ) AS p\nON\n  c.country = p.country\nORDER BY\n  cases_per_100k DESC",
        "external_knowledge": null,
        "plan": "1. **Define Population Data Subquery**: Create a subquery to standardize country names and select the population data for each country.\n   - Use conditional logic to rename specific country names to match those used in the COVID-19 cases data.\n   - Select the population data for the year 2018 for these countries.\n\n2. **Join COVID-19 Data with Population Data**: Combine the COVID-19 cases data with the population data.\n   - Use the country names as the joining condition.\n\n3. **Filter Data for Specific Countries and Date**: Restrict the data to only the specified countries and the specific date.\n   - Ensure the countries are in the list: US, France, China, Italy, Spain, Germany, and Iran.\n   - Ensure the date is April 20, 2021.\n\n4. **Aggregate and Calculate Metrics**: Compute the required metrics for each country.\n   - Sum the total confirmed COVID-19 cases for each country.\n   - Calculate the number of confirmed cases per 100,000 people using the population data.\n\n5. **Organize and Rank the Results**: Group and order the results to meet the specified requirements.\n   - Group the data by country and date.\n   - Order the results by the number of confirmed cases per 100,000 people in descending order.\n\n6. **Select Final Output**: Choose the relevant columns for the final output.\n   - Select the date, country, total confirmed cases, and confirmed cases per 100,000 people.",
        "special_function": null
    },
    {
        "instance_id": "bq130",
        "db": "bigquery-public-data.covid19_nyt",
        "question": "Analyze daily new COVID-19 case counts from March to May 2020, identifying the top five states by daily increases. Please compile a ranking based on how often each state appears in these daily top fives. Then, examine the state that ranks fourth overall and identify its top five counties based on their frequency of appearing in the daily top five new case counts.",
        "SQL": "WITH StateCases AS (\n    SELECT\n        b.state_name,\n        b.date,\n        b.confirmed_cases - a.confirmed_cases AS daily_new_cases\n    FROM \n        (SELECT\n            state_name,\n            state_fips_code,\n            confirmed_cases,\n            DATE_ADD(date, INTERVAL 1 DAY) AS date_shift\n        FROM\n            `bigquery-public-data.covid19_nyt.us_states`\n        WHERE\n            date >= '2020-02-29' AND date <= '2020-05-30'\n        ) a\n    JOIN\n        `bigquery-public-data.covid19_nyt.us_states` b \n        ON a.state_fips_code = b.state_fips_code AND a.date_shift = b.date\n    WHERE\n        b.date >= '2020-03-01' AND b.date <= '2020-05-31'\n),\nRankedStatesPerDay AS (\n    SELECT\n        state_name,\n        date,\n        daily_new_cases,\n        RANK() OVER (PARTITION BY date ORDER BY daily_new_cases DESC) as rank\n    FROM\n        StateCases\n),\nTopStates AS (\n    SELECT\n        state_name,\n        COUNT(*) AS appearance_count\n    FROM\n        RankedStatesPerDay\n    WHERE\n        rank <= 5\n    GROUP BY\n        state_name\n    ORDER BY\n        appearance_count DESC\n),\nFourthState AS (\n    SELECT\n        state_name\n    FROM\n        TopStates\n    LIMIT 1\n    OFFSET 3\n),\nCountyCases AS (\n    SELECT\n        b.county,\n        b.date,\n        b.confirmed_cases - a.confirmed_cases AS daily_new_cases\n    FROM \n        (SELECT\n            county,\n            county_fips_code,\n            confirmed_cases,\n            DATE_ADD(date, INTERVAL 1 DAY) AS date_shift\n        FROM\n            `bigquery-public-data.covid19_nyt.us_counties`\n        WHERE\n            date >= '2020-02-29' AND date <= '2020-05-30'\n        ) a\n    JOIN\n        `bigquery-public-data.covid19_nyt.us_counties` b \n        ON a.county_fips_code = b.county_fips_code AND a.date_shift = b.date\n    WHERE\n        b.date >= '2020-03-01' AND b.date <= '2020-05-31'\n        AND b.state_name = (SELECT state_name FROM FourthState)\n),\nRankedCountiesPerDay AS (\n    SELECT\n        county,\n        date,\n        daily_new_cases,\n        RANK() OVER (PARTITION BY date ORDER BY daily_new_cases DESC) as rank\n    FROM\n        CountyCases\n),\nTopCounties AS (\n    SELECT\n        county,\n        COUNT(*) AS appearance_count\n    FROM\n        RankedCountiesPerDay\n    WHERE\n        rank <= 5\n    GROUP BY\n        county\n    ORDER BY\n        appearance_count DESC\n    LIMIT 5\n)\nSELECT\n    county\nFROM\n    TopCounties;",
        "external_knowledge": null,
        "plan": "1. **Create a Subquery for Previous Day's Data**:\n   - Select the relevant fields such as state identifier, state name, confirmed cases, and the date incremented by one day.\n   - Filter the records where the sum of confirmed cases and deaths is greater than zero to ensure only valid data is considered.\n\n2. **Join Current and Previous Day's Data**:\n   - Perform a self-join on the main dataset where the state identifier matches and the date in the subquery matches the current date in the main dataset.\n\n3. **Calculate New Cases**:\n   - Compute the number of new confirmed cases by subtracting the previous day's confirmed cases from the current day's confirmed cases.\n\n4. **Filter for the Specific Date**:\n   - Restrict the records to only those that match the specified date (March 23, 2023).\n\n5. **Group and Aggregate Data**:\n   - Group the results by state name and date to ensure aggregation is performed correctly.\n   - Use the MAX function to get the highest number of new confirmed cases per state for the given date.\n\n6. **Sort and Limit Results**:\n   - Sort the results in descending order based on the number of new confirmed cases.\n   - Limit the output to the top 3 states with the highest number of new confirmed cases.\n\n7. **Select Required Fields**:\n   - Select the state name and the calculated number of new confirmed cases for the final output.\n\nThe result provides the state names and their respective number of new COVID-19 cases for the top 3 states with the highest new cases on the specified date.",
        "special_function": [
            "date-functions/DATE",
            "date-functions/DATE_ADD"
        ]
    },
    {
        "instance_id": "bq087",
        "db": "bigquery-public-data.covid19_symptom_search",
        "question": "Can you assess the collective percentage change in average search frequency for Anosmia symptoms across the five major boroughs of New York City from 2019 to 2020?",
        "SQL": "SELECT\n  table_2019.avg_symptom_Anosmia_2019,\n  table_2020.avg_symptom_Anosmia_2020,\n  ((table_2020.avg_symptom_Anosmia_2020 - table_2019.avg_symptom_Anosmia_2019) / table_2019.avg_symptom_Anosmia_2019) * 100 AS avg_increase\nFROM (\n  SELECT\n    AVG(SAFE_CAST(symptom_Anosmia AS FLOAT64)) AS avg_symptom_Anosmia_2020\n  FROM\n    `bigquery-public-data.covid19_symptom_search.symptom_search_sub_region_2_weekly`\n  WHERE\n    sub_region_1 = \"New York\"\n    AND sub_region_2 IN (\"Bronx County\", \"Queens County\", \"Kings County\", \"New York County\", \"Richmond County\")\n    AND date >= '2020-01-01'\n    AND date < '2021-01-01'\n) AS table_2020,\n(\n  SELECT\n    AVG(SAFE_CAST(symptom_Anosmia AS FLOAT64)) AS avg_symptom_Anosmia_2019\n  FROM\n    `bigquery-public-data.covid19_symptom_search.symptom_search_sub_region_2_weekly`\n  WHERE\n    sub_region_1 = \"New York\"\n    AND sub_region_2 IN (\"Bronx County\", \"Queens County\", \"Kings County\", \"New York County\", \"Richmond County\")\n    AND date >= '2019-01-01'\n    AND date < '2020-01-01'\n) AS table_2019",
        "external_knowledge": null,
        "plan": "1. Compute the average searches for Anosmia symptoms in 2020 for specific New York counties (Bronx, Queens, Kings, New York, and Richmond).\n2. Similarly, compute the average for 2019 for the same set of counties.\n3. Calculate Change: Determine the percentage increase or decrease in average searches from 2019 to 2020 by comparing the two averages.",
        "special_function": null
    },
    {
        "instance_id": "bq088",
        "db": "bigquery-public-data.covid19_symptom_search",
        "question": "Can you provide the average levels of anxiety and depression symptoms from the weekly country data in the United States for the years 2019 and 2020, and calculate the percentage increase in these symptoms from 2019 to 2020?",
        "SQL": "SELECT\n  table_2019.avg_symptom_Anxiety_2019,\n  table_2020.avg_symptom_Anxiety_2020,\n  ((table_2020.avg_symptom_Anxiety_2020 - table_2019.avg_symptom_Anxiety_2019)/table_2019.avg_symptom_Anxiety_2019) * 100 AS percent_increase_anxiety,\n  table_2019.avg_symptom_Depression_2019,\n  table_2020.avg_symptom_Depression_2020,\n  ((table_2020.avg_symptom_Depression_2020 - table_2019.avg_symptom_Depression_2019)/table_2019.avg_symptom_Depression_2019) * 100 AS percent_increase_depression\nFROM (\n  SELECT\n    AVG(CAST(symptom_Anxiety AS FLOAT64)) AS avg_symptom_Anxiety_2020,\n    AVG(CAST(symptom_Depression AS FLOAT64)) AS avg_symptom_Depression_2020,\n  FROM\n    `bigquery-public-data.covid19_symptom_search.symptom_search_country_weekly`\n  WHERE\n    country_region_code = \"US\"\n    AND date >= '2020-01-01'\n    AND date <'2021-01-01') AS table_2020,\n  (\n  SELECT\n    AVG(CAST(symptom_Anxiety AS FLOAT64)) AS avg_symptom_Anxiety_2019,\n    AVG(CAST(symptom_Depression AS FLOAT64)) AS avg_symptom_Depression_2019,\n  FROM\n    `bigquery-public-data.covid19_symptom_search.symptom_search_country_weekly`\n  WHERE\n    country_region_code = \"US\"\n    AND date >= '2019-01-01'\n    AND date <'2020-01-01') AS table_2019",
        "external_knowledge": null,
        "plan": "1. **Data Source Selection**:\r\n   - The query utilizes the `bigquery-public-data.covid19_symptom_search.symptom_search_country_weekly` dataset from BigQuery's public data.\r\n\r\n2. **Subquery for 2020 Data**:\r\n   - A subquery (`table_2020`) is executed to calculate the average levels of anxiety and depression symptoms for the year 2020.\r\n   - The `WHERE` clause filters the records to include only those from the US (`country_region_code = \"US\"`) and within the date range from January 1, 2020, to December 31, 2020.\r\n   - The `AVG` function calculates the average for `symptom_Anxiety` and `symptom_Depression`, casting these values to `FLOAT64` for precision.\r\n\r\n3. **Subquery for 2019 Data**:\r\n   - Another subquery (`table_2019`) is executed to calculate the average levels of anxiety and depression symptoms for the year 2019.\r\n   - Similar to the 2020 subquery, the `WHERE` clause filters the records to include only those from the US and within the date range from January 1, 2019, to December 31, 2019.\r\n   - The `AVG` function calculates the average for `symptom_Anxiety` and `symptom_Depression`, casting these values to `FLOAT64` for precision.\r\n\r\n4. **Main Query**:\r\n   - The main query selects the average values calculated in the subqueries:\r\n     - `table_2019.avg_symptom_Anxiety_2019`\r\n     - `table_2020.avg_symptom_Anxiety_2020`\r\n     - `table_2019.avg_symptom_Depression_2019`\r\n     - `table_2020.avg_symptom_Depression_2020`\r\n   - Additionally, it calculates the percentage increase in anxiety and depression symptoms from 2019 to 2020 using the formula:\r\n     - `((table_2020.avg_symptom_Anxiety_2020 - table_2019.avg_symptom_Anxiety_2019)/table_2019.avg_symptom_Anxiety_2019) * 100` as `percent_increase_anxiety`\r\n     - `((table_2020.avg_symptom_Depression_2020 - table_2019.avg_symptom_Depression_2019)/table_2019.avg_symptom_Depression_2019) * 100` as `percent_increase_depression`\r\n\r\n5. **Output**:\r\n   - The final output includes the average levels of anxiety and depression for both 2019 and 2020, along with the calculated percentage increases for both symptoms between the two years.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE"
        ]
    },
    {
        "instance_id": "bq089",
        "db": "bigquery-public-data.covid19_usafacts\nbigquery-public-data.covid19_vaccination_access\nbigquery-public-data.census_bureau_acs",
        "question": "Given the latest population estimates from the 2018 five-year American Community Survey, what is the number of vaccine sites per 1000 people for counties in California?",
        "SQL": "WITH\n  num_vaccine_sites_per_county AS (\n  SELECT\n    facility_sub_region_1 AS us_state,\n    facility_sub_region_2 AS us_county,\n    facility_sub_region_2_code AS us_county_fips,\n    COUNT(DISTINCT facility_place_id) AS num_vaccine_sites\n  FROM\n    bigquery-public-data.covid19_vaccination_access.facility_boundary_us_all\n  WHERE\n    STARTS_WITH(facility_sub_region_2_code, \"06\")\n  GROUP BY\n    facility_sub_region_1,\n    facility_sub_region_2,\n    facility_sub_region_2_code ),\n  total_population_per_county AS (\n  SELECT\n    LEFT(geo_id, 5) AS us_county_fips,\n    ROUND(SUM(total_pop)) AS total_population\n  FROM\n    bigquery-public-data.census_bureau_acs.censustract_2018_5yr\n  WHERE\n    STARTS_WITH(LEFT(geo_id, 5), \"06\")\n  GROUP BY\n    LEFT(geo_id, 5) )\nSELECT\n  * EXCEPT(us_county_fips),\n  ROUND((num_vaccine_sites * 1000) / total_population, 2) AS sites_per_1k_ppl\nFROM\n  num_vaccine_sites_per_county\nINNER JOIN\n  total_population_per_county\nUSING\n  (us_county_fips)\nORDER BY\n  sites_per_1k_ppl ASC\nLIMIT\n  100;",
        "external_knowledge": null,
        "plan": "1. Collect the number of vaccine sites for each county in California and calculate the population of each county.\n2. Compute the number of vaccine sites per 1000 people for each county.",
        "special_function": [
            "mathematical-functions/ROUND",
            "string-functions/LEFT",
            "string-functions/STARTS_WITH"
        ]
    },
    {
        "instance_id": "bq407",
        "db": "bigquery-public-data.covid19_usafacts\nbigquery-public-data.covid19_vaccination_access\nbigquery-public-data.census_bureau_acs",
        "question": "Find the top three counties with populations over 50,000, using the 2020 5-year census data, that had the highest COVID-19 case fatality rates on August 27, 2020. For these counties, provide the name, state, median age, total population, number of confirmed COVID-19 cases per 100,000 people, number of deaths per 100,000 people, and the case fatality rate as a percentage",
        "SQL": "WITH population_data AS (\n  SELECT\n    geo_id,\n    median_age,\n    total_pop\n  FROM\n    `bigquery-public-data.census_bureau_acs.county_2020_5yr`\n  WHERE\n    total_pop > 50000\n),\ncovid_data AS (\n  SELECT\n    county_fips_code,\n    county_name,\n    state,\n    SUM(confirmed_cases) AS total_cases,\n    SUM(deaths) AS total_deaths\n  FROM\n    `bigquery-public-data.covid19_usafacts.summary`\n  WHERE\n    date = '2020-08-27'\n  GROUP BY\n    county_fips_code, county_name, state\n)\nSELECT\n  covid.county_name,\n  covid.state,\n  pop.median_age,\n  pop.total_pop,\n  (covid.total_cases / pop.total_pop * 100000) AS confirmed_cases_per_100000,\n  (covid.total_deaths / pop.total_pop * 100000) AS deaths_per_100000,\n  (covid.total_deaths / covid.total_cases * 100) AS case_fatality_rate\nFROM\n  covid_data covid\nJOIN\n  population_data pop ON covid.county_fips_code = pop.geo_id\nORDER BY\n  case_fatality_rate DESC\nLIMIT 3;",
        "external_knowledge": null,
        "plan": "1. **Filter Population Data**: Create a temporary dataset containing regions with populations greater than 50,000. For each region, retain the region identifier, median age, and total population.\n\n2. **Aggregate COVID-19 Data**: Create another temporary dataset by aggregating COVID-19 data for each region on a specific date (first day of 2022). Ensure the aggregation includes only regions with reported cases or deaths and exclude any invalid region codes. For each region, sum the total confirmed cases and deaths.\n\n3. **Join Datasets**: Combine the two temporary datasets based on the region identifiers to pair population data with corresponding COVID-19 data.\n\n4. **Calculate Metrics**: For each combined entry, calculate:\n   - Confirmed cases per 100,000 people.\n   - Deaths per 100,000 people.\n   - Case fatality rate (percentage of confirmed cases that resulted in death).\n\n5. **Order and Limit Results**: Sort the combined entries primarily by deaths per 100,000 people (in descending order) and secondarily by confirmed cases per 100,000 people (also in descending order). Limit the results to the top 3 entries.\n\n6. **Select and Output Fields**: For the top 3 regions, output the region's name, state, median age, total population, confirmed cases per 100,000, deaths per 100,000, and case fatality rate.",
        "special_function": null
    },
    {
        "instance_id": "bq137",
        "db": "bigquery-public-data.census_bureau_usa\nbigquery-public-data.utility_us",
        "question": "Find details about zip code areas within 10 kilometers of the coordinates (-122.3321, 47.6062), including their geographic polygons, land and water area in meters, latitude and longitude points, state code, state name, city, county, and population from the 2010 census data.",
        "SQL": "with zip_pop AS (\n  SELECT\n    zip_census.zipcode AS zipcode,\n    population\n  FROM\n    `bigquery-public-data.census_bureau_usa.population_by_zip_2010` AS zip_census\n  WHERE\n    (gender LIKE 'male'\n      OR gender LIKE 'female')\n    AND minimum_age IS NULL\n    AND maximum_age IS NULL )\nSELECT\n  zip_area.zipcode_geom AS zipcode_polygon,\n  zip_area.zipcode AS zipcode,\n  area_land_meters,\n  area_water_meters,\n  ST_GeogPoint(longitude,\n    latitude) AS lat_lon,\n  state_code,\n  state_name,\n  city,\n  county,\n  population\nFROM\n  `bigquery-public-data.utility_us.zipcode_area` AS zip_area\nJOIN\n  zip_pop\nON\n  zip_area.zipcode = zip_pop.zipcode\nWHERE\n  ST_DWITHIN(ST_GeogPoint(longitude,latitude),\n  ST_GeogPoint(-122.3321,47.6062),10000)",
        "external_knowledge": null,
        "plan": "1. **Common Table Expression (CTE) - `zip_pop` Creation**:\r\n   - Query `bigquery-public-data.census_bureau_usa.population_by_zip_2010` table.\r\n   - Select `zipcode` and `population` columns.\r\n   - Filter rows where `gender` is either 'male' or 'female'.\r\n   - Ensure `minimum_age` and `maximum_age` are `NULL` (considering the entire population without age restrictions).\r\n\r\n2. **Main Query - Data Selection and Joining**:\r\n   - Query `bigquery-public-data.utility_us.zipcode_area` table.\r\n   - Select the following columns:\r\n     - `zipcode_geom` as `zipcode_polygon` (geometry of the zip code area),\r\n     - `zipcode` (zip code),\r\n     - `area_land_meters` (land area in square meters),\r\n     - `area_water_meters` (water area in square meters),\r\n     - `ST_GeogPoint(longitude, latitude)` as `lat_lon` (geographical point of the zip code),\r\n     - `state_code` (state abbreviation),\r\n     - `state_name` (full state name),\r\n     - `city` (city name),\r\n     - `county` (county name),\r\n     - `population` (population data from the CTE).\r\n\r\n3. **Join Operation**:\r\n   - Perform an inner join between `zip_area` and `zip_pop` on the `zipcode` column to combine area and population data.\r\n\r\n4. **Distance Filtering**:\r\n   - Filter results where the geographical point of the zip code (`ST_GeogPoint(longitude, latitude)`) is within 10,000 meters (10 kilometers) of the point (-122.3321, 47.6062) using the `ST_DWITHIN` function.\r\n\r\n5. **Final Output**:\r\n   - The final result includes data about zip code areas, such as their geographical polygons, land and water areas, geographical points, and associated population information, filtered to only include zip codes within 10 kilometers of the specified coordinates in Seattle, WA.",
        "special_function": [
            "geography-functions/ST_DWITHIN",
            "geography-functions/ST_GEOGPOINT"
        ]
    },
    {
        "instance_id": "bq060",
        "db": "bigquery-public-data.census_bureau_international",
        "question": "Which top 3 countries had the highest net migration in 2017 among those with an area greater than 500 square kilometers? And what are their migration rates?",
        "SQL": "WITH results AS (\n    SELECT\n        growth.country_name,\n        growth.net_migration,\n        CAST(area.country_area as INT64) as country_area\n    FROM (\n        SELECT\n            country_name,\n            net_migration,\n            country_code\n        FROM\n            `bigquery-public-data.census_bureau_international.birth_death_growth_rates`\n        WHERE\n            year = 2017\n    ) growth\n    INNER JOIN (\n        SELECT\n            country_area,\n            country_code\n        FROM\n            `bigquery-public-data.census_bureau_international.country_names_area`\n        WHERE\n            country_area > 500\n    ) area\n    ON\n        growth.country_code = area.country_code\n    ORDER BY\n        net_migration DESC\n    LIMIT 3\n)\nSELECT country_name, net_migration\nFROM results;",
        "external_knowledge": null,
        "plan": "1. **Filter and Select Data for 2017**: First, select the relevant columns (country name, net migration, and country code) from the dataset containing information about growth rates for the year 2017.\n\n2. **Filter and Join on Country Area**: Next, filter another dataset to include only countries with an area greater than 500 square kilometers. Join this filtered dataset with the previous one on the country code to combine the net migration data with the area data.\n\n3. **Sort and Retrieve Top Country**: Sort the combined data by net migration in descending order and limit the result to the top 3 entry. Finally, select the country name and net migration fields from this result to identify the country with the highest net migration.",
        "special_function": [
            "conversion-functions/CAST"
        ]
    },
    {
        "instance_id": "bq338",
        "db": "bigquery-public-data.geo_census_tracts\nbigquery-public-data.geo_us_boundaries\nbigquery-public-data.census_bureau_acs",
        "question": "Can you find the census tracts in the 36047 area that made both the top 20 lists for biggest population and median income increases from 2011 to 2018, and had over 1000 residents each year?",
        "SQL": "WITH population_change AS (\n    SELECT\n        a.geo_id,\n        a.total_pop AS pop_2011,\n        b.total_pop AS pop_2018,\n        ((b.total_pop - a.total_pop) / a.total_pop) * 100 AS population_change_percentage\n    FROM\n        bigquery-public-data.census_bureau_acs.censustract_2011_5yr a\n    JOIN\n        bigquery-public-data.census_bureau_acs.censustract_2018_5yr b\n    ON\n        a.geo_id = b.geo_id\n    WHERE \n        a.total_pop > 1000\n        AND b.total_pop > 1000\n        AND a.geo_id LIKE '36047%'\n        AND b.geo_id LIKE '36047%'\n    ORDER BY \n        population_change_percentage DESC\n    LIMIT 20\n),\n\nacs_2018 AS (\n    SELECT \n        geo_id, \n        median_income AS median_income_2018\n    FROM \n        bigquery-public-data.census_bureau_acs.censustract_2018_5yr  \n    WHERE \n        geo_id LIKE '36047%'\n        AND total_pop > 1000\n),\n\nacs_2011 AS (\n    SELECT \n        geo_id, \n        median_income AS median_income_2011\n    FROM \n        bigquery-public-data.census_bureau_acs.censustract_2011_5yr \n    WHERE \n        geo_id LIKE '36047%'\n    AND total_pop > 1000\n),\n\nacs_diff AS (\n    SELECT\n        a18.geo_id, \n        a18.median_income_2018, \n        a11.median_income_2011,\n        (a18.median_income_2018 - a11.median_income_2011) AS median_income_diff\n    FROM \n        acs_2018 a18\n    JOIN \n        acs_2011 a11\n        ON a18.geo_id = a11.geo_id\n    WHERE \n        (a18.median_income_2018 - a11.median_income_2011) IS NOT NULL\n    ORDER BY \n        (a18.median_income_2018 - a11.median_income_2011) DESC\n    LIMIT 20\n),\n\ncommon_geoids AS (\n    SELECT population_change.geo_id\n    FROM population_change\n    JOIN acs_diff ON population_change.geo_id = acs_diff.geo_id\n)\n\nSELECT geo_id FROM common_geoids;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq061",
        "db": "bigquery-public-data.geo_census_tracts\nbigquery-public-data.geo_us_boundaries\nbigquery-public-data.census_bureau_acs",
        "question": "Which census tract has witnessed the largest increase in median income between 2015 and 2018 in California? Tell me the tract code.",
        "SQL": "WITH acs_2018 AS (\n    SELECT\n      geo_id,\n      median_income AS median_income_2018\n    FROM\n      `bigquery-public-data.census_bureau_acs.censustract_2018_5yr` \n),\nacs_2015 AS (\n    SELECT\n      geo_id,\n      median_income AS median_income_2015\n    FROM\n      `bigquery-public-data.census_bureau_acs.censustract_2015_5yr` ),\nacs_diff AS (\n    SELECT\n      a18.geo_id,\n      a18.median_income_2018,\n      a15.median_income_2015,\n      (a18.median_income_2018 - a15.median_income_2015) AS median_income_diff,\n    FROM\n      acs_2018 a18\n    JOIN\n      acs_2015 a15\n    ON\n      a18.geo_id = a15.geo_id\n),\nmax_geo_id AS (\n    SELECT\n      geo_id\n    FROM\n      acs_diff\n    WHERE\n      median_income_diff IS NOT NULL\n      AND acs_diff.geo_id in (\n        SELECT\n          geo_id\n        FROM\n          `bigquery-public-data.geo_census_tracts.census_tracts_california`\n      )\n    ORDER BY\n      median_income_diff DESC\n    LIMIT 1\n)\nSELECT\n    tracts.tract_ce as tract_code\nFROM\n    max_geo_id\nJOIN\n    `bigquery-public-data.geo_census_tracts.census_tracts_california` AS tracts\nON\n    max_geo_id.geo_id = tracts.geo_id;",
        "external_knowledge": null,
        "plan": "1. **Extract Data for 2018**: Create a temporary dataset containing the geographical IDs and median incomes for the year 2018.\n\n2. **Extract Data for 2015**: Create another temporary dataset containing the geographical IDs and median incomes for the year 2015.\n\n3. **Calculate Income Difference**: Join the 2018 and 2015 datasets based on geographical IDs and calculate the difference in median income between these two years for each geographical ID.\n\n4. **Identify Maximum Increase**: From the dataset of income differences, filter out records that are not relevant to California and then identify the geographical ID with the highest increase in median income.\n\n5. **Retrieve Tract Code**: Join the geographical ID with the highest income increase back to the dataset containing California tract codes to retrieve the specific tract code associated with that geographical ID.",
        "special_function": null
    },
    {
        "instance_id": "bq064",
        "db": "bigquery-public-data.geo_census_tracts\nbigquery-public-data.geo_us_boundaries\nbigquery-public-data.census_bureau_acs",
        "question": "Could you calculate the population and average individual income (both rounded to 1 decimal) for each zip code based on U.S. census tract data in 2017? Only include those zip codes within a 5-mile radius of a specific geographic point (47.685833\u00b0N, -122.191667\u00b0W) in Washington and sort the results according to income in descending order.",
        "SQL": "WITH all_zip_tract_join AS (\n  SELECT \n    zips.zip_code, \n    zips.functional_status as zip_functional_status,\n    tracts.tract_ce, \n    tracts.geo_id as tract_geo_id, \n    tracts.functional_status as tract_functional_status,\n    ST_Area(ST_Intersection(tracts.tract_geom, zips.zip_code_geom))\n        / ST_Area(tracts.tract_geom) as tract_pct_in_zip_code\n  FROM  \n    `bigquery-public-data.geo_census_tracts.us_census_tracts_national` tracts,\n    `bigquery-public-data.geo_us_boundaries.zip_codes` zips\n  WHERE \n    ST_Intersects(tracts.tract_geom, zips.zip_code_geom)\n),\nzip_tract_join AS (\n  SELECT * FROM all_zip_tract_join WHERE tract_pct_in_zip_code > 0\n),\ncensus_totals AS (\n  -- convert averages to additive totals\n  SELECT \n    geo_id,\n    total_pop,\n    total_pop * income_per_capita AS total_income \n  FROM \n    `bigquery-public-data.census_bureau_acs.censustract_2017_5yr` \n),\njoined AS ( \n  -- join with precomputed census/zip pairs,\n  -- compute zip's share of tract\n  SELECT \n    zip_code, \n    total_pop * tract_pct_in_zip_code    AS zip_pop,\n    total_income * tract_pct_in_zip_code AS zip_income\n  FROM census_totals c\n  JOIN zip_tract_join ztj\n  ON c.geo_id = ztj.tract_geo_id\n),\nsums AS ( \n  -- aggregate all \"pieces\" of zip code\n  SELECT\n    zip_code, \n    SUM(zip_pop) AS zip_pop,\n    SUM(zip_income) AS zip_total_inc\n  FROM joined \n  GROUP BY zip_code\n),\nzip_pop_income AS (\n    SELECT \n        zip_code, zip_pop, \n        -- convert to averages\n        zip_total_inc / zip_pop AS income_per_capita\n    FROM sums\n),\nzipcodes_within_distance as (\n    SELECT \n        zip_code, zip_code_geom\n    FROM \n        `bigquery-public-data.geo_us_boundaries.zip_codes`\n    WHERE\n        state_code = 'WA'  -- Washington state code\n        AND\n        ST_DWithin(\n            ST_GeogPoint(-122.191667, 47.685833),\n            zip_code_geom,\n            8046.72\n        )\n)\nselect \n  stats.zip_code,\n  ROUND(stats.zip_pop, 1) as zip_population,\n  ROUND(stats.income_per_capita, 1) as average_income\nfrom \n  zipcodes_within_distance area\njoin \n  zip_pop_income stats\non area.zip_code = stats.zip_code\nORDER BY\n    average_income DESC;",
        "external_knowledge": null,
        "plan": "1. Find intersections between zip codes and census tracts, calculating the percentage of each tract within a zip code.\n2. Convert census tract data from averages to totals (e.g., total income).\n3. Join these totals with the intersection data to compute the portion of each tract's population and income that belongs to each zip code.\n4. Aggregate these values by zip code to calculate total population and income, then convert back to averages.\n5. Filter zip codes within a 5-mile radius of a specific geographic point in Washington and retrieve their statistics.",
        "special_function": [
            "geography-functions/ST_AREA",
            "geography-functions/ST_DWITHIN",
            "geography-functions/ST_GEOGPOINT",
            "geography-functions/ST_INTERSECTION",
            "geography-functions/ST_INTERSECTS",
            "mathematical-functions/ROUND"
        ]
    },
    {
        "instance_id": "bq461",
        "db": "bigquery-public-data.ncaa_basketball",
        "question": "Could you provide a summary of the scoring plays for the Wildcats and the Fighting Irish in their 2014 season game? I'd like to see details like the period, game clock, team scores, which team scored, and a description of each scoring event, all arranged chronologically.",
        "SQL": "SELECT\n  game_clock,\n  SUM(\n    CASE\n      WHEN team_name = 'Wildcats' THEN points_scored\n    END\n  ) OVER(ORDER BY timestamp ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS wildcats_score,\n  SUM(\n    CASE\n      WHEN team_name = 'Fighting Irish' THEN points_scored\n    END\n  ) OVER(ORDER BY timestamp ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS fighting_irish_score,\n  team_name,\n  event_description\nFROM\n  `bigquery-public-data.ncaa_basketball.mbb_pbp_sr`\nWHERE\n  season = 2014\n  AND home_name = 'Wildcats'\n  AND away_name = 'Fighting Irish'\n  AND points_scored IS NOT NULL\nORDER BY\n  fighting_irish_score ASC, wildcats_score ASC, timestamp DESC;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq198",
        "db": "bigquery-public-data.ncaa_basketball",
        "question": "What are the top 5 most successful college basketball teams over the seasons from 1900 to 2000, based on the number of times they had the maximum wins in a season?",
        "SQL": "SELECT\n  team_name,\n  COUNT(*) AS top_performer_count\nFROM (\n  SELECT\n    DISTINCT c2.season,\n    c2.market AS team_name\n  FROM (\n    SELECT\n      season AS a,\n      MAX(wins) AS win_max\n    FROM\n      `bigquery-public-data.ncaa_basketball.mbb_historical_teams_seasons`\n    WHERE\n      season<=2000\n      AND season >=1900\n    GROUP BY\n      season ),\n    `bigquery-public-data.ncaa_basketball.mbb_historical_teams_seasons` c2\n  WHERE\n    win_max = c2.wins\n    AND a = c2.season\n    AND c2.market IS NOT NULL\n  ORDER BY\n    c2.season)\nGROUP BY\n  team_name\nORDER BY\n  top_performer_count DESC,\n  team_name\nLIMIT\n  5",
        "external_knowledge": null,
        "plan": "1. **Identify Maximum Wins per Season:**\n   - Select each season within the specified range (1900 to 2000).\n   - For each season, determine the maximum number of wins achieved by any team.\n\n2. **Identify Teams with Maximum Wins:**\n   - For each season, find the teams that achieved the maximum number of wins identified in the previous step.\n   - Ensure the team names are not null.\n\n3. **Create a Distinct List:**\n   - Create a distinct list of seasons and corresponding teams who had the maximum wins in those seasons.\n\n4. **Count Top Performances:**\n   - Group the distinct list by team names.\n   - Count the number of times each team appears in the list, representing how often they had the maximum wins in a season.\n\n5. **Order and Limit Results:**\n   - Sort the teams by the count of top performances in descending order.\n   - If there is a tie in the count, sort the tied teams alphabetically by their names.\n   - Limit the results to the top 5 teams.\n\nBy following these steps, the query identifies and ranks the top 5 most successful college basketball teams based on the number of seasons they had the maximum wins.",
        "special_function": null
    },
    {
        "instance_id": "bq462",
        "db": "bigquery-public-data.ncaa_basketball",
        "question": "Please provide the top five records since 2010 for NCAA basketball in four categories: largest venues (Date N/A), biggest championship margins since 2015, highest scoring games, and most three-pointers in a matchup (\"Team A vs Team B\"). Organize the results into a table with columns for Category, Date, Matchup or Venue, and Key Metric.",
        "SQL": "WITH TotalThrees AS (\n  SELECT\n    'Total Threes' AS Category,\n    CAST(scheduled_date AS STRING) AS Date,\n    CONCAT(name, ' vs ', opp_name) AS Matchup,\n    CAST(three_points_made + opp_three_points_made AS STRING) AS KeyMetric\n  FROM `bigquery-public-data.ncaa_basketball.mbb_teams_games_sr`\n  WHERE season > 2010\n  ORDER BY (three_points_made + opp_three_points_made) DESC\n  LIMIT 5\n),\nTopVenues AS (\n  SELECT\n    'Top Venues' AS Category,\n    'N/A' AS Date, \n    venue_name AS Matchup,\n    CAST(venue_capacity AS STRING) AS KeyMetric,\n  FROM `bigquery-public-data.ncaa_basketball.mbb_teams_games_sr`\n  GROUP BY venue_name, venue_capacity, venue_city, venue_state\n  ORDER BY venue_capacity DESC\n  LIMIT 5\n),\nHighestScoringGames AS (\n  SELECT\n    'Highest Scoring Games' AS Category,\n    CAST(scheduled_date AS STRING) AS Date,\n    CONCAT(name, ' vs ', opp_name) AS Matchup,\n    CAST(points_game + opp_points_game AS STRING) AS KeyMetric\n  FROM `bigquery-public-data.ncaa_basketball.mbb_teams_games_sr`\n  WHERE season > 2010\n  ORDER BY (points_game + opp_points_game) DESC\n  LIMIT 5\n),\nBiggestChampionshipMargins AS (\n  SELECT\n    'Biggest Championship Margins' AS Category,\n    CAST(scheduled_date AS STRING) AS Date,\n    CONCAT(name, ' vs ', opp_name) AS Matchup,\n    CAST(ABS(points_game - opp_points_game) AS STRING) AS KeyMetric\n  FROM `bigquery-public-data.ncaa_basketball.mbb_teams_games_sr`\n  WHERE season > 2015 AND tournament_type = 'National Championship'\n  ORDER BY ABS(points_game - opp_points_game) DESC\n  LIMIT 5\n)\n\nSELECT * FROM TotalThrees\nUNION ALL\nSELECT * FROM TopVenues\nUNION ALL\nSELECT * FROM HighestScoringGames\nUNION ALL\nSELECT * FROM BiggestChampionshipMargins;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq427",
        "db": "bigquery-public-data.ncaa_basketball",
        "question": "Can you find the average x and y coordinates, the average number of shot attempts, and the average number of successful shots for the most frequent score delta interval in each shot type, considering only shots taken before March 15, 2018, and ensuring that the shots are on the correct side of the court based on the team's basket?",
        "SQL": "WITH ShotsWithScoreDelta AS (\n  SELECT \n    shot_type,\n    CASE \n      WHEN score_delta < -20 THEN '<-20'\n      WHEN score_delta >= -20 AND score_delta < -10 THEN '-20 \u2014 -11'\n      WHEN score_delta >= -10 AND score_delta < 0 THEN '-10 \u2014 -1'\n      WHEN score_delta = 0 THEN '0'\n      WHEN score_delta >= 1 AND score_delta <= 10 THEN '1 \u2014 10'\n      WHEN score_delta > 10 AND score_delta <= 20 THEN '11 \u2014 20'\n      WHEN score_delta > 20 THEN '>20'\n    END AS score_delta_interval,\n    IF(event_coord_x < 564, CAST(event_coord_x AS INT64), 1128 - CAST(event_coord_x AS INT64)) AS x,\n    IF(event_coord_x < 564, 600 - CAST(event_coord_y AS INT64), CAST(event_coord_y AS INT64)) AS y,\n    COUNT(*) AS attempts,\n    COUNTIF(points_scored IS NOT NULL) AS successes\n  FROM (\n    SELECT \n      event_coord_x, event_coord_y, points_scored, shot_type, team_basket, scheduled_date,\n      SUM(IFNULL(CAST(points_scored AS INT64), 0)) OVER (PARTITION BY game_id, team_id ORDER BY timestamp) AS team_score,\n      SUM(IFNULL(CAST(points_scored AS INT64), 0)) OVER (PARTITION BY game_id ORDER BY timestamp) AS game_score,\n      -- Calculate score_delta\n      SUM(IFNULL(CAST(points_scored AS INT64), 0)) OVER (PARTITION BY game_id, team_id ORDER BY timestamp)\n        - SUM(IFNULL(CAST(points_scored AS INT64), 0)) OVER (PARTITION BY game_id ORDER BY timestamp) AS score_delta\n    FROM `bigquery-public-data.ncaa_basketball.mbb_pbp_sr`\n  )\n  WHERE \n    shot_type IS NOT NULL\n    AND event_coord_x IS NOT NULL\n    AND event_coord_y IS NOT NULL\n    AND scheduled_date < '2018-03-15'\n    AND IF(event_coord_x < 564, 'left', 'right') = team_basket\n  GROUP BY shot_type, score_delta_interval, x, y\n),\n\nMostFrequentScoreDelta AS (\n  SELECT \n    shot_type,\n    score_delta_interval,\n    COUNT(*) AS attempts\n  FROM ShotsWithScoreDelta\n  GROUP BY shot_type, score_delta_interval\n  ORDER BY shot_type, attempts DESC\n),\n\nFinalResult AS (\n  SELECT \n    S.shot_type,\n    AVG(S.x) AS avg_x,\n    AVG(S.y) AS avg_y,\n    AVG(S.attempts) AS avg_attempts,\n    AVG(S.successes) AS avg_successes\n  FROM ShotsWithScoreDelta S\n  INNER JOIN MostFrequentScoreDelta M \n    ON S.shot_type = M.shot_type\n    AND S.score_delta_interval = M.score_delta_interval\n  GROUP BY S.shot_type\n)\n\nSELECT * FROM FinalResult\nORDER BY avg_attempts DESC;",
        "external_knowledge": "basketball.md",
        "plan": "1. **Define Score Delta Intervals**:\n   - Create a temporary dataset that includes a new column categorizing score deltas into predefined intervals.\n\n2. **Transform Coordinates**:\n   - Adjust the shot coordinates to ensure they are based on the correct side of the court relative to the team's basket.\n\n3. **Count Shot Attempts and Successes**:\n   - Calculate the total number of shot attempts and successful shots for each combination of shot type and score delta interval.\n\n4. **Filter and Aggregate Data**:\n   - Filter the dataset to include only relevant shots taken before a specific date.\n   - Ensure shots are on the correct side of the court.\n\n5. **Identify Most Frequent Score Delta Interval**:\n   - From the filtered dataset, determine the most frequent score delta interval for each shot type.\n\n6. **Calculate Averages**:\n   - Join the dataset with the most frequent score delta intervals.\n   - Calculate the average x and y coordinates, the average number of shot attempts, and the average number of successful shots for each shot type within the most frequent score delta interval.\n\n7. **Return Final Results**:\n   - Compile the final results with the averages calculated in the previous step.\n   - Order the final results based on the average number of shot attempts in descending order.",
        "special_function": null
    },
    {
        "instance_id": "bq428",
        "db": "bigquery-public-data.ncaa_basketball",
        "question": "For the top five team markets with the highest number of distinct players who scored at least 15 points during the second period of games between 2010 and 2018, provide details of each game they played in NCAA basketball historical tournament matches during the same period, as specified in the data model document.",
        "SQL": "WITH top_teams AS (\n  SELECT\n    team_market\n  FROM (\n    SELECT\n      team_market,\n      player_id AS id,\n      SUM(points_scored)\n    FROM\n      `bigquery-public-data.ncaa_basketball.mbb_pbp_sr`\n    WHERE\n      season >= 2010 AND season <=2018 AND period = 2\n    GROUP BY\n      game_id,\n      team_market,\n      player_id\n    HAVING\n      SUM(points_scored) >= 15) C\n  GROUP BY\n    team_market\n  HAVING\n    COUNT(DISTINCT id) > 5\n  ORDER BY\n    COUNT(DISTINCT id) DESC\n  LIMIT 5\n)\n\n\nSELECT\n  season,\n  round,\n  days_from_epoch,\n  game_date,\n  day,\n  'win' AS label,\n  win_seed AS seed,\n  win_market AS market,\n  win_name AS name,\n  win_alias AS alias,\n  win_school_ncaa AS school_ncaa,\n  lose_seed AS opponent_seed,\n  lose_market AS opponent_market,\n  lose_name AS opponent_name,\n  lose_alias AS opponent_alias,\n  lose_school_ncaa AS opponent_school_ncaa\nFROM `bigquery-public-data.ncaa_basketball.mbb_historical_tournament_games`\nJOIN top_teams ON top_teams.team_market = win_market\nWHERE season >= 2010 AND season <=2018\n\nUNION ALL\n\nSELECT\n  season,\n  round,\n  days_from_epoch,\n  game_date,\n  day,\n  'loss' AS label,\n  lose_seed AS seed,\n  lose_market AS market,\n  lose_name AS name,\n  lose_alias AS alias,\n  lose_school_ncaa AS school_ncaa,\n  win_seed AS opponent_seed,\n  win_market AS opponent_market,\n  win_name AS opponent_name,\n  win_alias AS opponent_alias,\n  win_school_ncaa AS opponent_school_ncaa\nFROM `bigquery-public-data.ncaa_basketball.mbb_historical_tournament_games`\nJOIN top_teams ON top_teams.team_market = lose_market\nWHERE season >= 2010 AND season <=2018",
        "external_knowledge": "ncaa_data_model.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq144",
        "db": "data-to-insights.ncaa",
        "question": "I would like to merge NCAA basketball historical tournament games outcomes with additional pace and efficiency performance metrics to enable comprehensive analysis of team and opponent dynamics from the 2014 season onwards (2018 included). Please refer to the Query Variable Guide to provide all the data.",
        "SQL": "WITH outcomes AS (\nSELECT\n\n  season, # 1994\n  \"win\" AS label, # our label\n  win_seed AS seed, # ranking # this time without seed even\n  win_school_ncaa AS school_ncaa,\n  lose_seed AS opponent_seed, # ranking\n  lose_school_ncaa AS opponent_school_ncaa\nFROM `data-to-insights.ncaa.mbb_historical_tournament_games` t\nWHERE season >= 2014\nUNION ALL\n\nSELECT\n\n  season, # 1994\n  \"loss\" AS label, # our label\n  lose_seed AS seed, # ranking\n  lose_school_ncaa AS school_ncaa,\n  win_seed AS opponent_seed, # ranking\n  win_school_ncaa AS opponent_school_ncaa\nFROM\n`data-to-insights.ncaa.mbb_historical_tournament_games` t\nWHERE season >= 2014\nUNION ALL\n\nSELECT\n  season,\n  label,\n  seed,\n  school_ncaa,\n  opponent_seed,\n  opponent_school_ncaa\nFROM\n  `data-to-insights.ncaa.2018_tournament_results`\n)\nSELECT\no.season,\nlabel,\n  seed,\n  school_ncaa,\n  team.pace_rank,\n  team.poss_40min,\n  team.pace_rating,\n  team.efficiency_rank,\n  team.pts_100poss,\n  team.efficiency_rating,\n  opponent_seed,\n  opponent_school_ncaa,\n  opp.pace_rank AS opp_pace_rank,\n  opp.poss_40min AS opp_poss_40min,\n  opp.pace_rating AS opp_pace_rating,\n  opp.efficiency_rank AS opp_efficiency_rank,\n  opp.pts_100poss AS opp_pts_100poss,\n  opp.efficiency_rating AS opp_efficiency_rating,\n  opp.pace_rank - team.pace_rank AS pace_rank_diff,\n  opp.poss_40min - team.poss_40min AS pace_stat_diff,\n  opp.pace_rating - team.pace_rating AS pace_rating_diff,\n  opp.efficiency_rank - team.efficiency_rank AS eff_rank_diff,\n  opp.pts_100poss - team.pts_100poss AS eff_stat_diff,\n  opp.efficiency_rating - team.efficiency_rating AS eff_rating_diff\nFROM outcomes AS o\nLEFT JOIN `data-to-insights.ncaa.feature_engineering` AS team\nON o.school_ncaa = team.team AND o.season = team.season\nLEFT JOIN `data-to-insights.ncaa.feature_engineering` AS opp\nON o.opponent_school_ncaa = opp.team AND o.season = opp.season",
        "external_knowledge": "NCAA_Basketball_Tournament_SQL_Query_Variable_Guide.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq113",
        "db": "bigquery-public-data.bls_qcew\nbigquery-public-data.bls\nbigquery-public-data.geo_us_boundaries",
        "question": "Which Utah county has witnessed the greatest percentage increase of construction jobs from 2000 to 2018? And what is the corresponding increase rate?",
        "SQL": "WITH utah_code AS (\n  SELECT DISTINCT geo_id\n  FROM bigquery-public-data.geo_us_boundaries.states\n  WHERE state_name = 'Utah'\n),\ne2000 as(\n  SELECT\n    AVG(month3_emplvl_23_construction) AS construction_employees_2000,\n    geoid\n  FROM\n    `bigquery-public-data.bls_qcew.2000_*`\n  WHERE\n    geoid LIKE CONCAT((SELECT geo_id FROM utah_code), '%')\n  GROUP BY\n    geoid),\n\ne2018 AS (\n  SELECT\n    AVG(month3_emplvl_23_construction) AS construction_employees_2018,\n    geoid,\n  FROM\n    `bigquery-public-data.bls_qcew.2018_*` e2018\n  WHERE\n    geoid LIKE CONCAT((SELECT geo_id FROM utah_code), '%')\n  GROUP BY\n    geoid)\n\nSELECT\n  c.county_name AS county,\n  (construction_employees_2018 - construction_employees_2000) / construction_employees_2000 * 100 AS increase_rate\nFROM\n  e2000\nJOIN\n  e2018 USING (geoid)\nJOIN \n  `bigquery-public-data.geo_us_boundaries.counties` c ON c.geo_id = e2018.geoid\nWHERE\n  c.state_fips_code = (SELECT geo_id FROM utah_code)\nORDER BY\n  increase_rate desc\nLIMIT 1",
        "external_knowledge": null,
        "plan": "1. Compute the average number of construction employees for each county in Utah (state FIPS code 49) in the year 2000.\n2. Compute the average number of construction employees for each county in Utah (state FIPS code 49) in the year 2018.\n3. Combine the data from the years 2000 and 2018 to compare the average number of construction employees.\n4. Add the county names for better readability and filtering.\n5. Identify the county with the highest increase rate.",
        "special_function": [
            "string-functions/CONCAT"
        ]
    },
    {
        "instance_id": "bq112",
        "db": "bigquery-public-data.bls_qcew\nbigquery-public-data.bls\nbigquery-public-data.geo_us_boundaries",
        "question": "Did the increase on average annual wages for all industries in Allegheny County, Pittsburgh keep pace with inflation of all consumer items between 1998 and 2017? Tell me their growth rates respectively (2 decimals).",
        "SQL": "WITH geo AS (\n  SELECT DISTINCT geo_id\n  FROM `bigquery-public-data.geo_us_boundaries.counties`\n  WHERE county_name = \"Allegheny\" \n),\navg_wage_1998 AS(\n  SELECT\n    ROUND(AVG(avg_wkly_wage_10_total_all_industries) * 52, 2) AS wages_1998\n  FROM\n    `bigquery-public-data.bls_qcew.1998*`\n  WHERE\n    geoid = (SELECT geo_id FROM geo) --Selecting Allgeheny County\n),\n    \navg_wage_2017 AS (\n  SELECT\n    ROUND(AVG(avg_wkly_wage_10_total_all_industries) * 52, 2) AS wages_2017\n  FROM\n    `bigquery-public-data.bls_qcew.2017*`\n  WHERE\n    geoid = (SELECT geo_id FROM geo) --Selecting Allgeheny County\n),\n\navg_cpi_1998 AS (\n  SELECT\n    AVG(value) AS cpi_1998\n  FROM\n    `bigquery-public-data.bls.cpi_u` c\n  WHERE\n    year = 1998\n    AND item_code in (\n      SELECT DISTINCT item_code FROM `bigquery-public-data.bls.cpi_u` WHERE LOWER(item_name) = \"all items\"\n    )\n    AND area_code = (\n      SELECT DISTINCT area_code FROM `bigquery-public-data.bls.cpi_u` WHERE area_name LIKE '%Pittsburgh%'\n    )\n), \n-- A104 is the code for Pittsburgh, PA\n-- SA0 is the code for all items\n    \navg_cpi_2017 AS(\n  SELECT\n    AVG(value) AS cpi_2017\n  FROM\n    `bigquery-public-data.bls.cpi_u` c\n  WHERE\n    year = 2017\n    AND item_code in (\n      SELECT DISTINCT item_code FROM `bigquery-public-data.bls.cpi_u` WHERE LOWER(item_name) = \"all items\"\n    )\n    AND area_code = (\n      SELECT DISTINCT area_code FROM `bigquery-public-data.bls.cpi_u` WHERE area_name LIKE '%Pittsburgh%'\n    )\n)\n-- A104 is the code for Pittsburgh, PA\n-- SA0 is the code for all items\n\nSELECT\n  ROUND((wages_2017 - wages_1998) / wages_1998 * 100, 2) AS wages_percent_change,\n  ROUND((cpi_2017 - cpi_1998) / cpi_1998 * 100, 2) AS cpi_percent_change\nFROM\n  avg_wage_2017,\n  avg_wage_1998,\n  avg_cpi_2017,\n  avg_cpi_1998",
        "external_knowledge": null,
        "plan": "1. **Calculate Average Annual Wages for 1998 and 2017**: Aggregate the weekly wage data for all industries in the specified county for the years 1998 and 2017, and convert them to annual wages. This involves averaging the weekly wages and multiplying by 52 to get the annual figure.\n\n2. **Calculate Average Consumer Price Index (CPI) for 1998 and 2017**: Aggregate the CPI data for all consumer items in the specified area for the years 1998 and 2017. This involves averaging the CPI values for each year.\n\n3. **Compute and Compare Growth Rates**: Calculate the percentage change in average annual wages and CPI from 1998 to 2017. This is done by finding the difference between the values in 2017 and 1998, dividing by the 1998 values, and then converting to a percentage. Return the growth rates for wages and CPI to compare them.",
        "special_function": [
            "mathematical-functions/ROUND",
            "string-functions/LOWER"
        ]
    },
    {
        "instance_id": "bq055",
        "db": "bigquery-public-data.google_dei\nbigquery-public-data.bls\nbigquery-public-data.bls_qcew",
        "question": "Can you provide the top 3 races with the largest percentage differences between Google's 2021 hiring data and the average percentages from the 2021 BLS reports, along with their respective differences? Focus the analysis on the technology sectors specifically defined as Internet content broadcast, software publishing, data management, hosting, and associated services, or within the industry group of 'Computer systems design and related services.' ",
        "SQL": "WITH google_data AS (\n    SELECT\n        'Google Hiring' AS data_source,\n        race_asian,\n        race_black,\n        race_hispanic_latinx,\n        race_native_american,\n        race_white,\n    FROM `bigquery-public-data.google_dei.dar_non_intersectional_hiring`\n    WHERE\n        workforce = 'overall'\n        AND report_year = 2021\n),\n\nbls_data AS (\n    SELECT\n        percent_asian AS race_asian,\n        percent_black_or_african_american AS race_black,\n        percent_hispanic_or_latino AS race_hispanic_latinx,\n        NULL AS race_native_american,\n        percent_white AS race_white,\n    FROM `bigquery-public-data.bls.cpsaat18`\n    WHERE year = 2021\n          AND (\n                  (\n                    subsector IN (\n                        'Internet publishing and broadcasting and web search portals',\n                        'Software publishers',\n                        'Data processing, hosting, and related services'\n                          )  \n                  )\n                 OR \n                  (\n                    industry_group = 'Computer systems design and related services'\n                  )\n              )\n),\n\nbls_averages AS (\n    SELECT\n        AVG(race_asian) AS avg_race_asian,\n        AVG(race_black) AS avg_race_black,\n        AVG(race_hispanic_latinx) AS avg_race_hispanic_latinx,\n        AVG(race_native_american) AS avg_race_native_american,\n        AVG(race_white) AS avg_race_white\n    FROM bls_data\n),\n\ngoogle_bl_difference AS (\n    SELECT\n        'asian' AS race,\n        ABS(google.race_asian - bls.avg_race_asian) AS diff\n    FROM google_data google, bls_averages bls\n    UNION ALL\n    SELECT\n        'black' AS race,\n        ABS(google.race_black - bls.avg_race_black) AS diff\n    FROM google_data google, bls_averages bls\n    UNION ALL\n    SELECT\n        'hispanic_latinx' AS race,\n        ABS(google.race_hispanic_latinx - bls.avg_race_hispanic_latinx) AS diff\n    FROM google_data google, bls_averages bls\n    UNION ALL\n    SELECT\n        'native_american' AS race,\n        ABS(google.race_native_american - bls.avg_race_native_american) AS diff\n    FROM google_data google, bls_averages bls\n    UNION ALL\n    SELECT\n        'white' AS race,\n        ABS(google.race_white - bls.avg_race_white) AS diff\n    FROM google_data google, bls_averages bls\n)\n\nSELECT\n    race,\n    diff\nFROM google_bl_difference\nORDER BY diff DESC",
        "external_knowledge": null,
        "plan": "When set side by side, how does Google\u2019s hiring and representation compare to related industries?\nThis query looks at the hiring and representation of employees at Google in the U.S. in 2022 by race and gender compared to the representation of U.S. employed persons in the internet publishing and broadcasting and web search portals industry; the software publishers industry; the data processing, hosting, and related services industry; and the computer systems designs and related services industry as reported by the U.S. Bureau of Labor Statistics",
        "special_function": [
            "mathematical-functions/ABS",
            "search-functions/SEARCH"
        ]
    },
    {
        "instance_id": "bq075",
        "db": "bigquery-public-data.google_dei\nbigquery-public-data.bls\nbigquery-public-data.bls_qcew",
        "question": "Can you provide a consolidated report that compares the racial and gender distribution across various data sources for the year 2021, focusing specifically on the overall workforce? This should include data from Google's hiring and workforce representation initiatives, as well as from BLS reports that target the technology sectors defined as Internet content broadcasting and 'Computer systems design and related services\u2018.",
        "SQL": "SELECT *\nFROM\n (\n   SELECT\n     '2021 Google Hiring' AS data_source,\n     race_asian,\n     race_black,\n     race_hispanic_latinx,\n     race_white,\n     gender_us_women,\n     gender_us_men\n   FROM `bigquery-public-data.google_dei.dar_non_intersectional_hiring`\n   WHERE\n     workforce = 'overall'\n     AND report_year = 2021\n )\nUNION ALL  (\n   SELECT\n     '2021 Google Workforce Representation' AS data_source,\n     race_asian,\n     race_black,\n     race_hispanic_latinx,\n     race_white,\n     gender_us_women,\n     gender_us_men\n   FROM\n     `bigquery-public-data.google_dei.dar_non_intersectional_representation`\n   WHERE\n     workforce = 'overall'\n     AND report_year = 2021\n )\nUNION ALL\n (\n   SELECT\n     CONCAT(\"2021 BLS Industry - \",IFNULL(industry_group, subsector)) AS data_source,\n     percent_asian AS race_asian,\n     percent_black_or_african_american AS race_black,\n     percent_hispanic_or_latino AS race_hispanic_latinx,\n     percent_white AS race_white,\n     percent_women AS gender_us_women,\n     1-percent_women AS gender_us_men\n\n   FROM `bigquery-public-data.bls.cpsaat18`\n   WHERE year = 2021\n          AND (\n                  (\n                    subsector IN (\n                        'Internet publishing and broadcasting and web search portals'\n                          )  \n                  )\n                 OR \n                  (\n                    industry_group = 'Computer systems design and related services'\n                  )\n              )\n )\n;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq406",
        "db": "bigquery-public-data.google_dei\nbigquery-public-data.bls\nbigquery-public-data.bls_qcew",
        "question": "Please calculate the growth rates for Asians, Black people, Latinx people, Native Americans, White people, US women, US men, global women, and global men from 2014 to 2024 concerning the overall workforce.",
        "SQL": "CREATE TEMP FUNCTION GrowthRate(end_value FLOAT64, begin_value FLOAT64)\nRETURNS FLOAT64\nAS ((end_value - begin_value) / begin_value);\n\nSELECT\n  GrowthRate(SUM(IF(report_year=2024, race_asian, 0)),\n             SUM(IF(report_year=2014, race_asian, 0))) AS race_asian_growth,\n  GrowthRate(SUM(IF(report_year=2024, race_black, 0)),\n             SUM(IF(report_year=2014, race_black, 0))) AS race_black_growth,\n  GrowthRate(SUM(IF(report_year=2024, race_hispanic_latinx, 0)),\n             SUM(IF(report_year=2014, race_hispanic_latinx, 0))) AS race_hispanic_growth,\n  GrowthRate(SUM(IF(report_year=2024, race_native_american, 0)),\n             SUM(IF(report_year=2014, race_native_american, 0))) AS race_native_american_growth,\n  GrowthRate(SUM(IF(report_year=2024, race_white, 0)),\n             SUM(IF(report_year=2014, race_white, 0))) AS race_white_growth,\n  GrowthRate(SUM(IF(report_year=2024, gender_us_women, 0)),\n             SUM(IF(report_year=2014, gender_us_women, 0))) AS gender_us_women_growth,\n  GrowthRate(SUM(IF(report_year=2024, gender_us_men, 0)),\n             SUM(IF(report_year=2014, gender_us_men, 0))) AS gender_us_men_growth,\n  GrowthRate(SUM(IF(report_year=2024, gender_global_women, 0)),\n             SUM(IF(report_year=2014, gender_global_women, 0))) AS gender_global_women_growth,\n  GrowthRate(SUM(IF(report_year=2024, gender_global_men, 0)),\n             SUM(IF(report_year=2014, gender_global_men, 0))) AS gender_global_men_growth\nFROM `bigquery-public-data.google_dei.dar_non_intersectional_representation`\nWHERE report_year IN (2014, 2024)\n  AND workforce = 'overall';",
        "external_knowledge": null,
        "plan": "1. **Define Growth Rate Calculation**:\n   - Create a temporary function that calculates the growth rate. This function takes two inputs: the value at the end year and the value at the beginning year.\n   - The growth rate is computed using the formula: \\((\\text{end value} - \\text{begin value}) / \\text{begin value}\\).\n\n2. **Select and Calculate Growth Rates**:\n   - For each demographic group (e.g., Asians, Black people, Latinx people, Native Americans, White people, US women, US men, global women, global men), calculate the sum of their values for the specified end year.\n   - Similarly, calculate the sum of their values for the specified beginning year.\n   - Use the previously defined growth rate function to compute the growth rate for each demographic group by passing the sums from the end year and the beginning year as arguments.\n\n3. **Filter Data by Year and Workforce Type**:\n   - Restrict the data to include only the records for the years 2014 and 2024.\n   - Further, filter the data to include only records that pertain to the overall workforce.\n\n4. **Output the Results**:\n   - The final output includes the calculated growth rates for each demographic group as separate columns in the result set.",
        "special_function": null
    },
    {
        "instance_id": "bq084",
        "db": "spider2-public-data.goog_blockchain_arbitrum_one_us\nspider2-public-data.goog_blockchain_avalanche_contract_chain_us\nspider2-public-data.goog_blockchain_cronos_mainnet_us\nspider2-public-data.goog_blockchain_ethereum_goerli_us\nspider2-public-data.goog_blockchain_ethereum_mainnet_us\nspider2-public-data.goog_blockchain_fantom_opera_us\nspider2-public-data.goog_blockchain_optimism_mainnet_us\nspider2-public-data.goog_blockchain_polygon_mainnet_us\nspider2-public-data.goog_blockchain_tron_mainnet_us",
        "question": "Please count the monthly transaction numbers and transactions per second for each month in 2023, and arrange them in descending order of monthly transaction count.",
        "SQL": "SELECT\n  COUNT(*) AS TXN_COUNT_PER_MONTH,\n  COUNT(*) / \n    CASE \n      WHEN EXTRACT(MONTH FROM MIN(txn.block_timestamp)) IN (1, 3, 5, 7, 8, 10, 12) THEN 2678400\n      WHEN EXTRACT(MONTH FROM MIN(txn.block_timestamp)) = 2 THEN \n        CASE \n          WHEN MOD(EXTRACT(YEAR FROM MIN(txn.block_timestamp)), 4) = 0 AND (MOD(EXTRACT(YEAR FROM MIN(txn.block_timestamp)), 100) != 0 OR MOD(EXTRACT(YEAR FROM MIN(txn.block_timestamp)), 400) = 0) THEN 2505600\n          ELSE 2419200\n        END\n      ELSE 2592000\n    END AS TXN_PER_SECOND,\n  EXTRACT(YEAR FROM MIN(txn.block_timestamp)) AS YEAR,\n  EXTRACT(MONTH FROM MIN(txn.block_timestamp)) AS MONTH\nFROM\n  `spider2-public-data.goog_blockchain_polygon_mainnet_us.transactions` AS txn\nWHERE EXTRACT(YEAR FROM txn.block_timestamp) = 2023\nGROUP BY\n  EXTRACT(YEAR FROM txn.block_timestamp),\n  EXTRACT(MONTH FROM txn.block_timestamp)\nORDER BY TXN_COUNT_PER_MONTH DESC;",
        "external_knowledge": null,
        "plan": "What\u2019s the TPS - Transactions Per Second on Polygon per month in 2024? ",
        "special_function": [
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "mathematical-functions/MOD",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq058",
        "db": "spider2-public-data.goog_blockchain_arbitrum_one_us\nspider2-public-data.goog_blockchain_avalanche_contract_chain_us\nspider2-public-data.goog_blockchain_cronos_mainnet_us\nspider2-public-data.goog_blockchain_ethereum_goerli_us\nspider2-public-data.goog_blockchain_ethereum_mainnet_us\nspider2-public-data.goog_blockchain_fantom_opera_us\nspider2-public-data.goog_blockchain_optimism_mainnet_us\nspider2-public-data.goog_blockchain_polygon_mainnet_us\nspider2-public-data.goog_blockchain_tron_mainnet_us",
        "question": "Retrieve all finalized deposits into Optimism at block 29815485 using the Optimism Standard Bridge, including transaction hash, an Etherscan link (the complete URL), L1 and L2 token addresses, sender and receiver addresses (with leading zeroes stripped), and the deposited amount (converted from hex to decimal). Ensure data is properly formatted and parsed according to Optimism's address and token standards, and remove the prefix '0x' except transaction hash. Note that, the keccak-256 hash of the Ethereum event signature for DepositFinalized is \"0x3303facd24627943a92e9dc87cfbb34b15c49b726eec3ad3487c16be9ab8efe8\".",
        "SQL": "CREATE TEMPORARY FUNCTION hex_to_bignumeric(hex_str STRING)\n  RETURNS BIGNUMERIC\n  LANGUAGE js AS '''\n    return BigInt('0x' + hex_str);\n  ''';\nSELECT \n  transaction_hash,\n  CONCAT('https://optimistic.etherscan.io/tx/', transaction_hash) AS txn_optimistic_etherscan,\n  LOWER(REGEXP_REPLACE(SUBSTR(topics[OFFSET(1)], -40), r'^0+', '')) AS L1Token,\n  LOWER(REGEXP_REPLACE(SUBSTR(topics[OFFSET(2)], -40), r'^0+', '')) AS L2Token,\n  LOWER(REGEXP_REPLACE(SUBSTR(topics[OFFSET(3)], -40), r'^0+', '')) AS from_address,\n  LOWER(REGEXP_REPLACE(SUBSTR(SUBSTR(data, 3, 64), -40), r'^0+', '')) AS to_address,\n  hex_to_bignumeric(SUBSTR(data, 67, 64)) AS amount_deposited\nFROM `spider2-public-data.goog_blockchain_optimism_mainnet_us.logs`\nWHERE \n  block_number = 29815485 AND\n  topics[SAFE_OFFSET(0)] = '0x3303facd24627943a92e9dc87cfbb34b15c49b726eec3ad3487c16be9ab8efe8'",
        "external_knowledge": "optimism_standard_bridge_contract.md",
        "plan": "1. **Define Utility Functions:**\n   - Create temporary functions for string manipulation and number conversion:\n     - `ParseSubStr` to extract a substring from a hexadecimal string.\n     - `HexToDec` to convert a hexadecimal string to a decimal number.\n     - `StripLeadingZeroes` to remove leading zeroes from hexadecimal addresses.\n\n2. **Select Relevant Data:**\n   - Retrieve required information from the blockchain logs:\n     - Transaction hash.\n     - Etherscan link for the transaction.\n     - L1 and L2 token addresses.\n     - Sender and receiver addresses.\n     - Deposited amount.\n\n3. **Apply Formatting and Parsing:**\n   - Use the utility functions to format and parse the retrieved data:\n     - Strip leading zeroes from token and address fields.\n     - Extract and convert deposited amounts from hexadecimal to decimal.\n\n4. **Filter and Finalize Data:**\n   - Filter logs to include only finalized deposits at the specified block:\n     - Ensure logs contain topics.\n     - Match logs with the specific deposit finalized event signature.\n     - Filter by the specified block number.",
        "special_function": null
    },
    {
        "instance_id": "bq416",
        "db": "spider2-public-data.goog_blockchain_arbitrum_one_us\nspider2-public-data.goog_blockchain_avalanche_contract_chain_us\nspider2-public-data.goog_blockchain_cronos_mainnet_us\nspider2-public-data.goog_blockchain_ethereum_goerli_us\nspider2-public-data.goog_blockchain_ethereum_mainnet_us\nspider2-public-data.goog_blockchain_fantom_opera_us\nspider2-public-data.goog_blockchain_optimism_mainnet_us\nspider2-public-data.goog_blockchain_polygon_mainnet_us\nspider2-public-data.goog_blockchain_tron_mainnet_us",
        "question": "Please list the block numbers, source addresses, destination addresses (both in TronLink address format), and transfer amounts for the three largest USDT transactions on the TRON blockchain. The code numbers for USDT contract and transfer event are \"0xa614f803b6fd780986a42c78ec9c7f77e6ded13c\" and \"0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef\" respectively.",
        "SQL": "CREATE TEMP FUNCTION hexToTron(address STRING)\nRETURNS STRING\nLANGUAGE js\nOPTIONS (library=[\"gs://blockchain-etl-bigquery/ethers.js\"])\nAS r\"\"\"\n  function encode58(buffer) {\n    const ALPHABET = '123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz';\n    const digits = [0];\n    for (let i = 0; i < buffer.length; i++) {\n      for (let j = 0; j < digits.length; j++) digits[j] <<= 8;\n      digits[0] += buffer[i];\n      let carry = 0;\n      for (let j = 0; j < digits.length; ++j) {\n        digits[j] += carry;\n        carry = (digits[j] / 58) | 0;\n        digits[j] %= 58;\n      }\n      while (carry) {\n        digits.push(carry % 58);\n        carry = (carry / 58) | 0;\n      }\n    }\n    for (let i = 0; buffer[i] === 0 && i < buffer.length - 1; i++) digits.push(0);\n    return digits.reverse().map((digit) => ALPHABET[digit]).join(\"\");\n  }\n  function sha256(msgBytes) {\n    const msgHex = ethers.utils.hexlify(msgBytes);\n    const hashHex = ethers.utils.sha256(msgHex);\n    return ethers.utils.arrayify(hashHex);\n  }\n  addressBytes = ethers.utils.arrayify('0x' + address.replace(/^0x/, '41'))\n  checkSum = sha256(sha256(addressBytes)).slice(0, 4);\n  return encode58(new Uint8Array([...addressBytes, ...checkSum]));\n\"\"\";\n\nWITH transfers AS (\n  SELECT\n    b.block_number,\n    hexToTron(CONCAT('0x', SUBSTR(l.topics[1], 27))) AS from_address,\n    hexToTron(CONCAT('0x', SUBSTR(l.topics[2], 27))) AS to_address,\n    CAST(l.data AS INT64) / 1000000 AS amount\n  FROM\n    `spider2-public-data.goog_blockchain_tron_mainnet_us.blocks` b\n    JOIN\n    `spider2-public-data.goog_blockchain_tron_mainnet_us.logs` l\n    ON b.block_hash = l.block_hash\n  WHERE\n    l.address = '0xa614f803b6fd780986a42c78ec9c7f77e6ded13c' -- USDT contract\n    AND ARRAY_LENGTH(l.topics) = 3\n    AND l.topics[0] = '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef' -- Transfer events\n)\nSELECT * FROM transfers ORDER BY amount DESC LIMIT 3;",
        "external_knowledge": "blockchain_data_transformations.md",
        "plan": "1. **Define a Conversion Function**:\n   - Create a custom function that converts hexadecimal addresses to TronLink format using JavaScript. This function handles the encoding and checksum calculation needed for the conversion.\n\n2. **Extract Relevant Transaction Data**:\n   - Create a temporary table to store transaction data.\n   - Select the block number, source address, destination address, and transfer amount from relevant blockchain data sources.\n   - Convert the hexadecimal addresses to TronLink format using the custom function.\n   - Calculate the transfer amount by converting the raw data format into a human-readable format (dividing by 1,000,000 to get the correct unit).\n\n3. **Filter for Specific Contract and Event**:\n   - Include only the transactions associated with the specified USDT contract address.\n   - Ensure the transactions are of the 'Transfer' type by matching the event signature.\n   - Ensure the log entries have exactly three topics (indicating valid transfer events).\n\n4. **Sort and Limit Results**:\n   - Sort the transaction records by transfer amount in descending order to identify the largest transactions.\n   - Limit the output to the top three transactions, providing the largest transfer amounts.\n\n5. **Output the Results**:\n   - Display the block numbers, source addresses, destination addresses, and transfer amounts for the three largest transactions.",
        "special_function": null
    },
    {
        "instance_id": "bq226",
        "db": "spider2-public-data.goog_blockchain_arbitrum_one_us\nspider2-public-data.goog_blockchain_avalanche_contract_chain_us\nspider2-public-data.goog_blockchain_cronos_mainnet_us\nspider2-public-data.goog_blockchain_ethereum_goerli_us\nspider2-public-data.goog_blockchain_ethereum_mainnet_us\nspider2-public-data.goog_blockchain_fantom_opera_us\nspider2-public-data.goog_blockchain_optimism_mainnet_us\nspider2-public-data.goog_blockchain_polygon_mainnet_us\nspider2-public-data.goog_blockchain_tron_mainnet_us",
        "question": "Can you find me the complete url of the most frequently used sender's address on the Cronos blockchain since January 1, 2023, where transactions were made to non-null addresses and in blocks larger than 4096 bytes?",
        "SQL": "SELECT\n    CONCAT(\"https://cronoscan.com/address/\", t.from_address) AS cronoscan_link,\nFROM\n    `spider2-public-data.goog_blockchain_cronos_mainnet_us.transactions` AS t\nINNER JOIN\n    `spider2-public-data.goog_blockchain_cronos_mainnet_us.blocks` AS b\nON\n    b.block_hash = t.block_hash\nWHERE\n    t.to_address IS NOT NULL\nAND\n    b.size > 4096\nAND\n    b.block_timestamp > TIMESTAMP(\"2023-01-01 00:00:00\")\nAND\n    t.block_timestamp > TIMESTAMP(\"2023-01-01 00:00:00\")\nGROUP BY\n    t.from_address\nORDER BY\n    COUNT(*) \nDESC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "1. Start by joining two datasets based on a common identifier which associates transaction records with corresponding block records.\n2. Include only records where the transaction destination address is specified.\n3. Consider only blocks that are larger than 4096 bytes.\n4. Restrict the data to records timestamped after \u20182023-01-01 00:00:00\u2019 for both transactions and blocks.\n5. For each transaction record, construct a URL using a predefined format concatenated with the source address from the transaction.\n6. Group the data by the source address of the transactions.\n7. Count the number of transactions for each unique source address and sort these counts in descending order to find the most frequent source address.\n8. Return the link to the source address with the highest count of transactions.\n",
        "special_function": [
            "string-functions/CONCAT",
            "timestamp-functions/TIMESTAMP"
        ]
    },
    {
        "instance_id": "bq016",
        "db": "spider2-public-data.deps_dev_v1",
        "question": "Considering only the highest release versions of NPM packages, which one and its version has the most dependent packages?",
        "SQL": "DECLARE\n    Sys STRING DEFAULT 'NPM';\n\nWITH HighestReleases AS (\n    SELECT\n        Name,\n        Version,\n    FROM (\n        SELECT\n            Name,\n            Version,\n            ROW_NUMBER() OVER (\n                PARTITION BY Name\n                ORDER BY VersionInfo.Ordinal DESC\n            ) AS RowNumber\n        FROM\n            `spider2-public-data.deps_dev_v1.PackageVersions`\n        WHERE\n            System = Sys\n            AND VersionInfo.IsRelease\n    )\n    WHERE RowNumber = 1\n)\n\nSELECT\n    D.Dependency.Name,\n    D.Dependency.Version\nFROM\n    `spider2-public-data.deps_dev_v1.Dependencies` AS D\nJOIN\n    HighestReleases AS H\nUSING\n    (Name, Version)\nWHERE\n    D.System = Sys\nGROUP BY\n    D.Dependency.Name,\n    D.Dependency.Version\nORDER BY\n    COUNT(*) DESC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "1. Firstly, we declare a system variable to denote the PYPI system.\n2. Identify the highest released versions for each package.\n   - Use a subquery to:\n     - Partition data by `Name`.\n     - Order the partitions by `VersionInfo.Ordinal` in descending order to rank versions.\n     - Assign row numbers to each version within their respective partitions.\n   - Filter to include only the first row (highest version) for each package where:\n     - The `System` is 'PYPI' (using the variable `Sys`).\n     - The `VersionInfo.IsRelease` is true (indicating it is a release version).\n3. Join the table Dependencies with the table defined in Step 2.\n4. Aggregate and order dependencies by name and version.\n5. Restrict the output to only one record, which is the dependency with the highest count of dependent packages.",
        "special_function": [
            "json-functions/STRING",
            "numbering-functions/ROW_NUMBER",
            "timestamp-functions/STRING"
        ]
    },
    {
        "instance_id": "bq062",
        "db": "spider2-public-data.deps_dev_v1",
        "question": "What is the most frequently used license by packages in each system?",
        "SQL": "WITH Counts AS (\n    SELECT\n        System,\n        License,\n        COUNT(DISTINCT Name) AS NPackages\n    FROM\n        `spider2-public-data.deps_dev_v1.PackageVersions`\n    CROSS JOIN\n        UNNEST(Licenses) AS License\n    GROUP BY\n        System,\n        License\n),\nRanked AS (\n    SELECT\n        System,\n        License,\n        NPackages,\n        ROW_NUMBER() OVER (PARTITION BY System ORDER BY NPackages DESC) AS LicenseRank\n    FROM Counts\n)\nSELECT\n    System,\n    License\nFROM\n    Ranked\nWHERE\n    LicenseRank <= 1\nORDER BY\n    System,\n    LicenseRank;",
        "external_knowledge": null,
        "plan": "1. Generate a Common Table Expression (CTE) that aggregates the number of distinct packages using each license within each system.\n- Use `CROSS JOIN UNNEST` to handle the array of licenses, effectively normalizing the data for easier counting.\n- Group the results by `System` and `License`.\n2. Generate another CTE that ranks the licenses within each system based on the number of distinct packages using them.\n- Use the `ROW_NUMBER()` window function partitioned by `System` and ordered by `NPackages` in descending order to assign a rank to each license within each system.\n3. Select and output only top ranked licenses for each system.",
        "special_function": [
            "numbering-functions/ROW_NUMBER",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq063",
        "db": "spider2-public-data.deps_dev_v1",
        "question": "What is the github URL of the latest released package from the NPM system that have the highest number of dependencies? Exlcude those package whose names contain character '@' or the URL label is not 'SOURCE_REPO'.",
        "SQL": "DECLARE\n    Sys STRING DEFAULT 'NPM';\n\nWITH HighestReleases AS (\n    SELECT\n        Name,\n        Version,\n    FROM (\n        SELECT\n            Name,\n            Version,\n            ROW_NUMBER() OVER (\n                PARTITION BY Name\n                ORDER BY VersionInfo.Ordinal DESC\n            ) AS RowNumber\n        FROM\n            `spider2-public-data.deps_dev_v1.PackageVersions`\n        WHERE\n            System = Sys\n            AND VersionInfo.IsRelease)\n    WHERE RowNumber = 1\n),\n\nTopDependencies AS (\n    SELECT\n        D.Name,\n        D.Version,\n        COUNT(*) AS NDependencies\n    FROM\n        `spider2-public-data.deps_dev_v1.Dependencies` AS D\n    JOIN\n        HighestReleases AS H\n    ON\n        D.Name = H.Name AND D.Version = H.Version\n    WHERE\n        D.System = Sys\n    GROUP BY\n        Name,\n        Version\n    ORDER BY\n        NDependencies DESC\n    LIMIT 100\n)\n\nSELECT\n    lnk.URL\nFROM \n    `spider2-public-data.deps_dev_v1.PackageVersions` AS P,\n    unnest(Links) as lnk\nJOIN \n    TopDependencies AS T\nON\n    T.Name = P.Name AND T.Version = P.Version\nWHERE\n    P.System = Sys\n    AND P.Name NOT LIKE '%@%'\n    AND lnk.Label = 'SOURCE_REPO'\n    AND lower(lnk.URL) LIKE '%github.com%'\nORDER BY T.NDependencies DESC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "1. Initialize a variable `Sys` with the value 'NPM' to filter data related to the NPM system.\n2. Create a Common Table Expression (CTE) to identify the latest released version of each package in the NPM system.\n- Select `Name` and `Version` from the `PackageVersions` table.\n- Use `ROW_NUMBER` to assign a unique sequential integer to rows within a partition of the dataset, partitioned by `Name` and ordered by `VersionInfo.Ordinal` in descending order.\n- Filter to include only rows where `VersionInfo.IsRelease` is true.\n- Retrieve only the rows where `RowNumber` equals 1, ensuring only the latest released version is selected for each package.\n3. Create another CTE to find packages with the highest number of dependencies.\n- Select `Name`, `Version`, and the count of dependencies from the `Dependencies` table after joining it with the table created in Step 2.\n- Order the results by the count of dependencies (`NDependencies`) in descending order.\n4. Select the `URL` from the `PackageVersions` table, unnesting the `Links` array to access individual URLs.\n- Filter to include only rows where:\n   - `System` equals `Sys`.\n   - `Name` does not contain the character '@'.\n   - `Label` is 'SOURCE_REPO'.\n   - `lnk.URL` contains 'github.com'.\n5. Limit the result to the top 1, which gives the GitHub URL of the package with the highest number of dependencies among the latest releases.",
        "special_function": [
            "json-functions/STRING",
            "numbering-functions/ROW_NUMBER",
            "string-functions/LOWER",
            "timestamp-functions/STRING",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq028",
        "db": "spider2-public-data.deps_dev_v1",
        "question": "Considering only the latest release versions of NPM package, which packages are the top 8 most popular based on the Github star number, as well as their versions?",
        "SQL": "DECLARE\n    Sys STRING DEFAULT 'NPM';\n\nWITH HighestReleases AS (\n    SELECT\n        Name,\n        Version\n    FROM (\n        SELECT\n            Name,\n            Version,\n            ROW_NUMBER() OVER (\n                PARTITION BY Name\n                ORDER BY VersionInfo.Ordinal DESC\n            ) AS RowNumber\n        FROM\n            `spider2-public-data.deps_dev_v1.PackageVersions`\n        WHERE\n            System = Sys\n            AND VersionInfo.IsRelease\n    )\n    WHERE RowNumber = 1\n),\nPVP AS (\n    SELECT\n        Name, Version, ProjectType, ProjectName\n    FROM\n        `spider2-public-data.deps_dev_v1.PackageVersionToProject`\n    JOIN\n        HighestReleases AS HR\n    USING (Name, Version)\n    WHERE\n        System = Sys\n        AND ProjectType = 'GITHUB'\n)\nSELECT PVP.Name, PVP.Version\nFROM\n    PVP\nJOIN\n    `spider2-public-data.deps_dev_v1.Projects` AS P\nON\n    PVP.ProjectType = P.Type AND PVP.ProjectName = P.Name\nORDER BY P.StarsCount DESC LIMIT 8;",
        "external_knowledge": null,
        "plan": "1. Declare system variable: define a variable `Sys` to filter records for the NPM system.\n2. Identify latest release versions:\n- Create a common table expression (CTE) to determine the latest release version for each package.\n- Use `ROW_NUMBER()` window function partitioned by `Name` and ordered by `VersionInfo.Ordinal` in descending order to rank the versions.\n- Filter to keep only the top-ranked version (`RowNumber = 1`) which represents the latest release.\n3. Join latest versions with projects to extract the project name and type and constrain the project type to GITHUB.\n4. Select package details and rank by the number of github stars count:\n5. Limit the final result to the top 3 packages based on the number of GitHub stars and return the package names and versions.",
        "special_function": [
            "json-functions/STRING",
            "numbering-functions/ROW_NUMBER",
            "timestamp-functions/STRING"
        ]
    },
    {
        "instance_id": "bq022",
        "db": "bigquery-public-data.chicago_taxi_trips\nbigquery-public-data.chicago_crime",
        "question": "Given the taxi trip data in Chicago, partition the trips that last no more than 1 hour into 6 quantiles based on trip duration. Please provide the minimum/maximum trip duration (rounded-off to integer minutes), total trips, and average fare for each quantile.",
        "SQL": "SELECT\n  ROUND(MIN(trip_seconds) / 60, 0) AS min_minutes,\n  ROUND(MAX(trip_seconds) / 60, 0) AS max_minutes,\n  COUNT(*) AS total_trips,\n  AVG(fare) AS average_fare\nFROM (\n  SELECT\n    trip_seconds,\n    NTILE(6) OVER (ORDER BY trip_seconds) AS quantile,\n    fare\n  FROM\n    `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n  WHERE\n    trip_seconds BETWEEN 0 AND 3600\n)\nGROUP BY\n  quantile\nORDER BY\n  min_minutes, max_minutes;",
        "external_knowledge": null,
        "plan": "1. **Filter Data:** Start by selecting records where the trip duration is between 0 and 3600 seconds (inclusive), ensuring only trips that last up to one hour are considered.\n\n2. **Partition Data:** Use a window function to assign each trip to one of six quantiles based on trip duration. This partitions the data into evenly distributed groups.\n\n3. **Select Columns:** For each trip, select the trip duration, the assigned quantile, and the fare amount.\n\n4. **Aggregate by Quantile:** Group the data by the quantile value to perform calculations on each quantile group.\n\n5. **Calculate Metrics:**\n   - **Minimum Duration:** Compute the minimum trip duration in seconds for each quantile and convert it to minutes, rounding to the nearest whole number.\n   - **Maximum Duration:** Compute the maximum trip duration in seconds for each quantile and convert it to minutes, rounding to the nearest whole number.\n   - **Total Trips:** Count the number of trips in each quantile.\n   - **Average Fare:** Calculate the average fare for trips in each quantile.",
        "special_function": [
            "mathematical-functions/ROUND",
            "numbering-functions/NTILE",
            "string-functions/FORMAT"
        ]
    },
    {
        "instance_id": "bq362",
        "db": "bigquery-public-data.chicago_taxi_trips\nbigquery-public-data.chicago_crime",
        "question": "Which three companies had the largest increase in trip numbers between two consecutive months in 2018?",
        "SQL": "select company from\n            (select *,\n            row_number() over(partition by company order by month_o_month_calc desc) as rownum\n            from\n            (select *,\n            num_trips - lag(num_trips) over(partition by company order by month) as month_o_month_calc\n                from\n                (SELECT \n                company,\n                format_date(\"%Y-%m\", date_sub((cast(trip_start_timestamp as date)), interval 1 month)) as prev_month,\n                format_date(\"%Y-%m\", cast(trip_start_timestamp as date)) AS month,\n                count(1) AS num_trips\n                from `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n                where extract(YEAR from trip_start_timestamp) = 2018\n                group by company, month, prev_month\n                order by company,month)\n            order by company, month_o_month_calc desc)\n            ) \n        where rownum = 1\n        order by month_o_month_calc desc, company \n        limit 3",
        "external_knowledge": null,
        "plan": "1. **Filter Data for the Year 2018:**\n   - Select records where the year of the trip start date is 2018.\n\n2. **Aggregate Trip Counts:**\n   - For each company, compute the number of trips in each month of 2018.\n   - Format the trip start dates to extract the year and month for grouping.\n   - Also, compute the previous month for each trip start date.\n\n3. **Calculate Month-over-Month Change:**\n   - Compute the difference in trip counts between each month and its previous month for each company using a window function that considers the previous row's trip count.\n\n4. **Rank Changes Within Companies:**\n   - Rank the month-over-month changes within each company in descending order to identify the largest change per company.\n\n5. **Select Largest Change per Company:**\n   - Filter to keep only the largest month-over-month change for each company by selecting the top-ranked row.\n\n6. **Order and Limit Results:**\n   - Order the results by the magnitude of the month-over-month change in descending order and then by company name.\n   - Limit the results to the top three companies with the largest month-over-month increases.\n\nThis step-by-step plan ensures that the query identifies and returns the top three companies with the largest increases in trip numbers between any two consecutive months in 2018.",
        "special_function": [
            "ROW_NUMBER",
            "PARTITION BY",
            "LAG",
            "FORMAT_DATE",
            "CAST",
            "DATE_SUB",
            "EXTRACT"
        ]
    },
    {
        "instance_id": "bq363",
        "db": "bigquery-public-data.chicago_taxi_trips\nbigquery-public-data.chicago_crime",
        "question": "For taxi trips with a duration rounded to the nearest minute, and between 1 and 50 minutes, if the trip durations are divided into 10 quantiles, what are the total number of trips and the average fare for each quantile?",
        "SQL": "SELECT\n  FORMAT('%02.0fm to %02.0fm', min_minutes, max_minutes) AS minutes_range,\n  SUM(trips) AS total_trips,\n  FORMAT('%3.2f', SUM(total_fare) / SUM(trips)) AS average_fare\nFROM (\n  SELECT\n    MIN(duration_in_minutes) OVER (quantiles) AS min_minutes,\n    MAX(duration_in_minutes) OVER (quantiles) AS max_minutes,\n    SUM(trips) AS trips,\n    SUM(total_fare) AS total_fare\n  FROM (\n    SELECT\n      ROUND(trip_seconds / 60) AS duration_in_minutes,\n      NTILE(10) OVER (ORDER BY trip_seconds / 60) AS quantile,\n      COUNT(1) AS trips,\n      SUM(fare) AS total_fare\n    FROM\n      `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n    WHERE\n      ROUND(trip_seconds / 60) BETWEEN 1 AND 50\n    GROUP BY\n      trip_seconds,\n      duration_in_minutes )\n  GROUP BY\n    duration_in_minutes,\n    quantile\n  WINDOW quantiles AS (PARTITION BY quantile)\n  )\nGROUP BY\n  minutes_range\nORDER BY\n  Minutes_range",
        "external_knowledge": null,
        "plan": "1. **Filter and Group Data by Duration:**\n   - Filter trips to include only those with durations rounded to the nearest minute that fall between 1 and 50 minutes.\n   - Round the trip durations to the nearest minute.\n   - Group the trips by these rounded durations.\n\n2. **Calculate Quantiles:**\n   - Divide the filtered trips into 10 quantiles based on their rounded durations.\n   - Assign each trip to a quantile.\n\n3. **Aggregate Data within Quantiles:**\n   - For each quantile, calculate the total number of trips and the total fare collected.\n   - Determine the minimum and maximum trip duration within each quantile.\n\n4. **Summarize Quantile Information:**\n   - For each quantile, calculate the minimum and maximum trip duration.\n   - Sum the total trips and total fare for each quantile.\n\n5. **Format Output:**\n   - Format the duration range for each quantile in a readable format (e.g., \"01m to 05m\").\n   - Calculate the average fare per trip for each quantile.\n   - Format the average fare to two decimal places.\n\n6. **Group and Order Results:**\n   - Group the results by the formatted duration range.\n   - Order the results by the duration range to ensure they are presented in ascending order.\n\nThis structured approach ensures that the query accurately computes the total number of trips and the average fare for each quantile of trip durations, providing a clear and organized output.",
        "special_function": [
            "FORMAT",
            "ROUND",
            "NTILE",
            "PARTITION BY"
        ]
    },
    {
        "instance_id": "bq076",
        "db": "bigquery-public-data.chicago_taxi_trips\nbigquery-public-data.chicago_crime",
        "question": "Which month generally has the greatest number of motor vehicle thefts in 2016?",
        "SQL": "SELECT\n  incidents AS highest_monthly_thefts\nFROM (\n  SELECT\n    year,\n    EXTRACT(MONTH FROM date) AS month,\n    COUNT(1) AS incidents,\n    RANK() OVER (PARTITION BY year ORDER BY COUNT(1) DESC) AS ranking\n  FROM\n    `bigquery-public-data.chicago_crime.crime`\n  WHERE\n    primary_type = 'MOTOR VEHICLE THEFT'\n    AND year = 2016\n  GROUP BY\n    year,\n    month\n)\nWHERE\n  ranking = 1\nORDER BY\n  year DESC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "Which month generally has the greatest number of motor vehicle thefts?\nThe following query summarizes the number of MOTOR VEHICLE THEFT incidents for each year and month, and ranks the month\u2019s total from 1 to 12. Then, the outer SELECT clause limits the final result set to the first overall ranking for each year. According to the data, in 3 of the past 10 years, December had the highest number of car thefts",
        "special_function": [
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "numbering-functions/RANK",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT"
        ]
    },
    {
        "instance_id": "bq077",
        "db": "bigquery-public-data.chicago_taxi_trips\nbigquery-public-data.chicago_crime",
        "question": "For each year from 2010 to 2016, what is the highest number of motor thefts in one month?",
        "SQL": "SELECT\n  year,\n  incidents\nFROM (\n  SELECT\n    year,\n    EXTRACT(MONTH\n    FROM\n      date) AS month,\n    COUNT(1) AS incidents,\n    RANK() OVER (PARTITION BY year ORDER BY COUNT(1) DESC) AS ranking\n  FROM\n    `bigquery-public-data.chicago_crime.crime`\n  WHERE\n    primary_type = 'MOTOR VEHICLE THEFT'\n    AND year BETWEEN 2010 AND 2016\n  GROUP BY\n    year,\n    month )\nWHERE\n  ranking = 1\nORDER BY\n  year ASC",
        "external_knowledge": null,
        "plan": "1. Filters the crime dataset for motor vehicle theft incidents between 2010 and 2016.\n2. Extracts the month from the date of each incident and groups the data by year and month.\n3. Counts the number of incidents for each year-month combination and ranks the months within each year based on the number of incidents.\n4. Selects the highest number of incidents for each year by filtering for ranking = 1 and orders the final results by year in ascending order.",
        "special_function": [
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "numbering-functions/RANK",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT"
        ]
    },
    {
        "instance_id": "bq350",
        "db": "open-targets-prod.platform",
        "question": "For the detailed molecule data, Please display the drug id, drug type and withdrawal status for approved drugs with a black box warning and known drug type among 'Keytruda', 'Vioxx', 'Premarin', and 'Humira'",
        "SQL": "DECLARE\n  my_drug_list ARRAY<STRING>;\nSET\n  my_drug_list = [\n  'Keytruda',\n  'Vioxx',\n  'Humira',\n  'Premarin' ];\n\nSELECT\n  id AS drug_id,\n  tradeNameList.element AS drug_trade_name,\n  drugType AS drug_type,\n  hasBeenWithdrawn AS drug_withdrawn\nFROM\n  `open-targets-prod.platform.molecule`,\n  UNNEST (tradeNames.list) AS tradeNameList\nWHERE\n  tradeNameList.element IN UNNEST(my_drug_list)\n  AND isApproved = TRUE\n  AND blackBoxWarning = TRUE\n  AND drugType != 'Unknown';",
        "external_knowledge": null,
        "plan": "1. **Initialize a List of Drug Names**: Start by declaring a list that contains specific drug names of interest ('Keytruda', 'Vioxx', 'Humira', and 'Premarin').\n\n2. **Select Relevant Columns**: Prepare to select the drug ID, trade name, drug type, and withdrawal status from the dataset.\n\n3. **Unnest Trade Names**: Expand the nested list of trade names associated with each drug entry to allow for individual trade names to be examined.\n\n4. **Filter by Trade Name**: Ensure that only those drugs whose trade name matches one of the names in the initialized list are included.\n\n5. **Filter by Approval Status**: Further restrict the results to only include drugs that have been approved.\n\n6. **Filter by Black Box Warning**: Additionally, include only those drugs that have a black box warning.\n\n7. **Exclude Unknown Drug Types**: Finally, exclude any drugs where the drug type is listed as 'Unknown'.\n\nBy following these steps, the query will return the desired information about the specific drugs of interest that meet all the given criteria.",
        "special_function": [
            "other-functions/SET",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq379",
        "db": "open-targets-prod.platform",
        "question": "Which target approved symbol has the overall association score closest to the mean score for psoriasis?",
        "SQL": "WITH AvgScore AS (\n  SELECT\n    AVG(associations.score) AS avg_score\n  FROM\n    `open-targets-prod.platform.associationByOverallDirect` AS associations\n  JOIN\n    `open-targets-prod.platform.diseases` AS diseases\n  ON\n    associations.diseaseId = diseases.id\n  WHERE\n    diseases.name = 'psoriasis'\n)\nSELECT\n  targets.approvedSymbol AS target_approved_symbol\nFROM\n  `open-targets-prod.platform.associationByOverallDirect` AS associations\nJOIN\n  `open-targets-prod.platform.diseases` AS diseases\nON\n  associations.diseaseId = diseases.id\nJOIN\n  `open-targets-prod.platform.targets` AS targets\nON\n  associations.targetId = targets.id\nCROSS JOIN\n  AvgScore\nWHERE\n  diseases.name = 'psoriasis'\nORDER BY\n  ABS(associations.score - AvgScore.avg_score) ASC\nLIMIT 1",
        "external_knowledge": null,
        "plan": "1. **Calculate the Mean Score:**\n   - Compute the average association score for the specified condition from the association data.\n   \n2. **Retrieve Target Symbols:**\n   - Identify all targets associated with the specified condition by joining the association data with the condition data.\n\n3. **Match Targets to Scores:**\n   - Join the target data with the association data to get the approved symbols of the targets.\n\n4. **Compare with Mean Score:**\n   - For each target associated with the specified condition, calculate the absolute difference between its association score and the previously computed mean score.\n\n5. **Find Closest Match:**\n   - Order the targets by the smallest absolute difference to the mean score.\n\n6. **Select Closest Target:**\n   - Limit the result to the single target whose score is closest to the mean score.",
        "special_function": [
            "mathematical-functions/ABS"
        ]
    },
    {
        "instance_id": "bq078",
        "db": "bigquery-public-data.open_targets_platform",
        "question": "Retrieve the approved symbol of target genes with the highest overall score that are associated with the disease 'EFO_0000676' from the data source 'IMPC'.",
        "SQL": "SELECT\n  T1.targetId AS target_id,\n  T1.datasourceId,\n  targets.approvedSymbol AS approved_symbol,\n  overall_associations.score AS overall_score\nFROM\n  `bigquery-public-data.open_targets_platform.associationByDatasourceDirect` as T1\nJOIN\n  `bigquery-public-data.open_targets_platform.targets` AS targets\nON\n  targetId = targets.id\nJOIN\n  `bigquery-public-data.open_targets_platform.associationByOverallDirect` AS overall_associations\nON\n  T1.targetId = overall_associations.targetId\nWHERE\n  overall_associations.diseaseId = 'EFO_0000676' AND datasourceId = 'impc'\nORDER BY\n  overall_associations.score DESC\nLIMIT\n  1;",
        "external_knowledge": null,
        "plan": "1. **Select Relevant Columns**:\n   - Choose the identifier for the target gene, the identifier for the data source, the approved symbol for the target gene, and the overall score for the association.\n\n2. **Specify Data Sources**:\n   - Use data from a specific dataset that contains associations between diseases and target genes, a dataset containing details about target genes, and another dataset with overall association scores.\n\n3. **Join Datasets**:\n   - Perform an inner join between the first dataset and the target gene details dataset to link target gene identifiers with their approved symbols.\n   - Perform another inner join between the first dataset and the overall association scores dataset to link target gene identifiers with their overall scores.\n\n4. **Apply Filters**:\n   - Filter the records to include only those associated with a specific disease.\n   - Further filter the records to include only those from a specified data source.\n\n5. **Sort and Limit Results**:\n   - Sort the filtered records in descending order based on the overall score to prioritize higher scores.\n   - Limit the results to retrieve only the top record, which corresponds to the target gene with the highest overall score for the specified disease and data source.",
        "special_function": null
    },
    {
        "instance_id": "bq095",
        "db": "open-targets-prod.platform",
        "question": "Generate a list of drugs from the table containing molecular details that have completed clinical trials for pancreatic endocrine carcinoma, disease ID EFO_0007416. Please include each drug's name, the target approved symbol, and links to the relevant clinical trials.",
        "SQL": "SELECT\n  targets.approvedSymbol AS target_symbol,\n  drugs.name AS drug_name,\n  source_urls.element.url AS clinical_trial_reference_url,\nFROM\n  `open-targets-prod.platform.evidence` AS evidence,\n  UNNEST(evidence.urls.list) AS source_urls\nJOIN\n  `open-targets-prod.platform.targets` AS targets\nON\n  evidence.targetId=targets.id\nJOIN\n  `open-targets-prod.platform.molecule` AS drugs\nON\n  evidence.drugId=drugs.id\nWHERE\n  datasourceId=\"chembl\"\n  AND diseaseId=\"EFO_0007416\"\n  AND evidence.clinicalStatus = \"Completed\"",
        "external_knowledge": null,
        "plan": "What drugs have an investigational or approved indication for pancreatic endocrine carcinoma?\n\nWith our evidence dataset, we can generate a list of drugs that have been in clinical studies for pancreatic endocrine carcinoma and obtain links to the supporting clinical trial or package insert record",
        "special_function": null
    },
    {
        "instance_id": "bq109",
        "db": "open-targets-genetics.genetics",
        "question": "Find the average, variance, max-min difference, and the QTL source(right study) of the maximum log2(h4/h3) for data where right gene id is \"ENSG00000169174\", h4 > 0.8, h3 < 0.02, reported trait includes \"lesterol levels\", right biological feature is \"IPSC\", and the variant is '1_55029009_C_T'.",
        "SQL": "WITH coloc_stats AS (\n  SELECT\n    coloc.coloc_log2_h4_h3,\n    coloc.right_study AS qtl_source\n  FROM\n    `open-targets-genetics.genetics.variant_disease_coloc` AS coloc\n  JOIN\n    `open-targets-genetics.genetics.studies` AS studies\n  ON\n    coloc.left_study = studies.study_id\n  WHERE\n    coloc.right_gene_id = \"ENSG00000169174\"\n    AND coloc.coloc_h4 > 0.8\n    AND coloc.coloc_h3 < 0.02\n    AND studies.trait_reported LIKE \"%lesterol levels%\"\n    AND coloc.right_bio_feature = 'IPSC'\n    AND CONCAT(coloc.left_chrom, '_', coloc.left_pos, '_', coloc.left_ref, '_', coloc.left_alt) = '1_55029009_C_T'\n),\nmax_value AS (\n  SELECT\n    MAX(coloc_log2_h4_h3) AS max_log2_h4_h3\n  FROM\n    coloc_stats\n)\n\nSELECT\n  AVG(coloc_log2_h4_h3) AS average,\n  VAR_SAMP(coloc_log2_h4_h3) AS variance,\n  MAX(coloc_log2_h4_h3) - MIN(coloc_log2_h4_h3) AS max_min_difference,\n  (SELECT qtl_source FROM coloc_stats WHERE coloc_log2_h4_h3 = (SELECT max_log2_h4_h3 FROM max_value)) AS qtl_source_of_max\nFROM\n  coloc_stats;",
        "external_knowledge": null,
        "plan": "For a given gene, what studies have evidence of colocalisation with molecular QTLs?\n\nWith our variant_disease_coloc and studies, you can find associated GWAS studies with evidence of colocalisation and an H4 greater than 0.8",
        "special_function": [
            "statistical-aggregate-functions/VAR_SAMP",
            "statistical-aggregate-functions/VARIANCE",
            "string-functions/CONCAT"
        ]
    },
    {
        "instance_id": "bq325",
        "db": "bigquery-public-data.open_targets_genetics",
        "question": "Can you tell me which genes have the strongest links to traits or conditions in each study? I need the names of the top 10 genes that stand out because they have the lowest p-values in their studies",
        "SQL": "WITH ranked_genes AS (\n    SELECT \n        locus2gene.study_id, \n        genes.gene_name, \n        lead_variants.pval,\n        ROW_NUMBER() OVER (\n            PARTITION BY locus2gene.study_id, genes.gene_name \n            ORDER BY lead_variants.pval\n        ) AS rn\n    FROM \n        `bigquery-public-data.open_targets_genetics.locus2gene` AS locus2gene\n    INNER JOIN \n        `bigquery-public-data.open_targets_genetics.studies` AS study_metadata\n        ON locus2gene.study_id = study_metadata.study_id\n    INNER JOIN \n        `bigquery-public-data.open_targets_genetics.genes` AS genes\n        ON locus2gene.gene_id = genes.gene_id\n    INNER JOIN \n        `bigquery-public-data.open_targets_genetics.variant_disease` AS lead_variants\n        ON locus2gene.pos = lead_variants.lead_pos\n        AND locus2gene.chrom = lead_variants.lead_chrom\n        AND locus2gene.study_id = lead_variants.study_id\n)\nSELECT \n    gene_name\nFROM \n    ranked_genes\nWHERE \n    rn = 1\nORDER BY \n    pval ASC\nLIMIT 10;",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. **Create a Common Table Expression (CTE)**: Define a temporary result set to hold intermediate results for easier querying and readability.\n\n2. **Select Required Columns**: Within the CTE, select the necessary columns including study identifiers, gene names, and p-values.\n\n3. **Join Tables**: Perform the necessary joins between the tables to combine information about gene associations, study metadata, gene details, and variant details. Ensure the joins are based on appropriate matching columns.\n\n4. **Rank Genes**: Use the window function to assign a unique row number to each gene within each study, ordered by p-value in ascending order. This ranking helps identify the gene with the lowest p-value within each study.\n\n5. **Filter Top Genes per Study**: In the main query, filter the results to include only the top-ranked gene (i.e., the gene with the lowest p-value) for each study.\n\n6. **Order and Limit Results**: Sort the filtered genes by their p-values in ascending order to identify the genes with the lowest p-values across all studies.\n\n7. **Return Top Results**: Limit the final output to the top 10 genes with the lowest p-values, fulfilling the requirement to find the top 10 genes with the lowest p-values across studies.",
        "special_function": [
            "numbering-functions/ROW_NUMBER"
        ]
    },
    {
        "instance_id": "bq090",
        "db": "bigquery-public-data.cymbal_investments",
        "question": "How much higher the average intrinsic value is for trades using the feeling-lucky strategy compared to those using the momentum strategy under long-side trades?",
        "SQL": "WITH MomentumTrades AS (\n  SELECT\n    StrikePrice - LastPx AS priceDifference\n  FROM\n    `bigquery-public-data.cymbal_investments.trade_capture_report`\n  WHERE\n    SUBSTR(TargetCompID, 0, 4) = 'MOMO'\n    AND (SELECT Side FROM UNNEST(Sides)) = 'LONG'\n),\n\nFeelingLuckyTrades AS (\n  SELECT\n    StrikePrice - LastPx AS priceDifference\n  FROM\n    `bigquery-public-data.cymbal_investments.trade_capture_report`\n  WHERE\n    SUBSTR(TargetCompID, 0, 4) = 'LUCK'\n    AND (SELECT Side FROM UNNEST(Sides)) = 'LONG'\n)\n\nSELECT\n  AVG(FeelingLuckyTrades.priceDifference) - AVG(MomentumTrades.priceDifference) AS averageDifference \nFROM\n  MomentumTrades,\n  FeelingLuckyTrades",
        "external_knowledge": null,
        "plan": "Flatten the nested FIX protocol  data within the trade_capture_report table and reformats the name of the algorithmic trading bots for readability.",
        "special_function": null
    },
    {
        "instance_id": "bq442",
        "db": "bigquery-public-data.cymbal_investments",
        "question": "Please collect the information of the top 6 trade report with the highest closing prices. Refer to the document for all the information I want.",
        "SQL": "SELECT\n  OrderID AS tradeID,\n  MaturityDate AS tradeTimestamp,\n  (\n    CASE SUBSTR(TargetCompID, 0, 4)\n      WHEN 'MOMO' THEN 'Momentum'\n      WHEN 'LUCK' THEN 'Feeling Lucky'\n      WHEN 'PRED' THEN 'Prediction'\n  END\n    ) AS algorithm,\n  Symbol AS symbol,\n  LastPx AS openPrice,\n  StrikePrice AS closePrice,\n  (\n  SELECT\n    Side\n  FROM\n    UNNEST(Sides)\n  ) AS tradeDirection,\n  (CASE (\n    SELECT\n      Side\n    FROM\n      UNNEST(Sides))\n      WHEN 'SHORT' THEN -1\n      WHEN 'LONG' THEN 1\n  END\n    ) AS tradeMultiplier\nFROM\n  `bigquery-public-data.cymbal_investments.trade_capture_report`cv\nORDER BY closePrice DESC\nLIMIT 6",
        "external_knowledge": "Trade_Capture_Report_Data_List.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq079",
        "db": "bigquery-public-data.usfs_fia",
        "question": "Given the latest evaluations of timberland and forestland plots, which state within each category has the highest total acreage? Please provide the state code, the evaluation group, the state name, and the total acres for the top state in each category.",
        "SQL": "WITH timberland_acres AS (\n    Select  \n    pt.plot_sequence_number as plot_sequence_number,\n    p.evaluation_type evaluation_type,\n    p.evaluation_group as evaluation_group,\n    p.evaluation_description as evaluation_description,\n    pt.plot_state_code_name as state_name,\n    p.inventory_year as inventory_year,\n    p.state_code as state_code, \n    CASE\n    WHEN c.proportion_basis = 'MACR' and p.adjustment_factor_for_the_macroplot > 0\n    THEN\n        (p.expansion_factor * c.condition_proportion_unadjusted * p.adjustment_factor_for_the_macroplot) \n    ELSE 0\n    END as macroplot_acres,\n    CASE\n    WHEN c.proportion_basis = 'SUBP' and p.adjustment_factor_for_the_subplot > 0\n    THEN\n        (p.expansion_factor * c.condition_proportion_unadjusted * p.adjustment_factor_for_the_subplot) \n    ELSE 0\n    END as subplot_acres\n    FROM \n    bigquery-public-data.usfs_fia.condition c\n    JOIN \n    bigquery-public-data.usfs_fia.plot_tree pt\n        ON pt.plot_sequence_number = c.plot_sequence_number\n    JOIN \n    bigquery-public-data.usfs_fia.population  p\n        ON p.plot_sequence_number = pt.plot_sequence_number\n    WHERE \n    p.evaluation_type = 'EXPCURR'\n    AND c.condition_status_code = 1\n    AND c.reserved_status_code = 0\n    AND c.site_productivity_class_code IN (1,2,3,4,5,6)\n    GROUP BY \n    plot_sequence_number,\n    evaluation_type,\n    evaluation_group,\n    evaluation_description,\n    macroplot_acres,\n    subplot_acres,\n    inventory_year,\n    state_code,\n    state_name\n),\n\nTop1_timberland AS (\nSELECT\n    state_code,\n    evaluation_group,\n    state_name,\n    SUM(macroplot_acres) + SUM(subplot_acres) AS total_acres,\nFROM (\n    SELECT\n        state_code,\n        evaluation_group,\n        state_name,\n        macroplot_acres,\n        subplot_acres,\n        MAX(evaluation_group) OVER (PARTITION BY state_code) AS latest                   \n    FROM timberland_acres\n)\nWHERE evaluation_group = latest\nGROUP BY state_code, state_name, evaluation_group\nORDER BY total_acres\nLIMIT 1\n),\n\nforestland_acres AS (\nSelect  \n pt.plot_sequence_number as plot_sequence_number,\n p.evaluation_type evaluation_type,\n p.evaluation_group as evaluation_group,\n p.evaluation_description as evaluation_description,\n pt.plot_state_code_name as state_name,\n p.inventory_year as inventory_year,\n p.state_code as state_code, \n CASE\n  WHEN c.proportion_basis = 'MACR' and p.adjustment_factor_for_the_macroplot > 0\n  THEN\n    (p.expansion_factor * c.condition_proportion_unadjusted * p.adjustment_factor_for_the_macroplot) \n  ELSE 0\n END as macroplot_acres,\n CASE\n  WHEN c.proportion_basis = 'SUBP' and p.adjustment_factor_for_the_subplot > 0\n  THEN\n    (p.expansion_factor * c.condition_proportion_unadjusted * p.adjustment_factor_for_the_subplot) \n  ELSE 0\n END as subplot_acres\nFROM \n  bigquery-public-data.usfs_fia.condition  c\nJOIN \n  bigquery-public-data.usfs_fia.plot_tree  pt\n        ON pt.plot_sequence_number = c.plot_sequence_number\nJOIN \n  bigquery-public-data.usfs_fia.population  p\n      ON p.plot_sequence_number = pt.plot_sequence_number\nWHERE \n  p.evaluation_type = 'EXPCURR'\n  AND c.condition_status_code = 1\nGROUP BY \n plot_sequence_number,\n evaluation_type,\n evaluation_group,\n evaluation_description,\n macroplot_acres,\n subplot_acres,\n inventory_year,\n state_code,\n state_name\n),\n\nTop1_forestland AS (\nSELECT\nstate_code,\nevaluation_group,\nstate_name,\nsum(macroplot_acres) + sum(subplot_acres) as total_acres\nFROM (SELECT\n        state_code,\n        evaluation_group,\n        state_name,\n        macroplot_acres,\n        subplot_acres,\n        MAX(evaluation_group) OVER (PARTITION By state_code) as latest                   \n        FROM forestland_acres )\nWHERE evaluation_group = latest\nGROUP by state_code, state_name, evaluation_group\norder by total_acres\nLIMIT 1\n)\n\nSELECT * FROM Top1_timberland\nUNION ALL\nSELECT * FROM Top1_forestland",
        "external_knowledge": null,
        "plan": "What is the approximate amount of timberland acres by state?",
        "special_function": null
    },
    {
        "instance_id": "bq024",
        "db": "bigquery-public-data.usfs_fia",
        "question": "For the year 2012, which top 10 evaluation groups have the largest subplot acres when considering only the condition with the largest subplot acres within each group? Please include the evaluation group, evaluation type, condition status code, evaluation description, state code, macroplot acres, and subplot acres.",
        "SQL": "WITH ranked_evaluations AS (\n  SELECT   \n    p.plot_sequence_number AS plot_sequence_number, \n    p.evaluation_group AS evaluation_group, \n    p.evaluation_type AS evaluation_type,\n    c.condition_status_code AS condition_status_code, \n    p.evaluation_description AS evaluation_description,\n    p.state_code AS state_code,  \n    -- Calculate area - this replaces the \"decode\" logic in example from Oracle \n    CASE \n      WHEN c.proportion_basis = 'MACR' AND p.adjustment_factor_for_the_macroplot > 0 \n      THEN \n        (p.expansion_factor * c.condition_proportion_unadjusted * p.adjustment_factor_for_the_macroplot)  \n      ELSE 0 \n    END AS macroplot_acres, \n    CASE \n      WHEN c.proportion_basis = 'SUBP' AND p.adjustment_factor_for_the_subplot > 0 \n      THEN \n        (p.expansion_factor * c.condition_proportion_unadjusted * p.adjustment_factor_for_the_subplot)  \n      ELSE 0 \n    END AS subplot_acres,\n    ROW_NUMBER() OVER (PARTITION BY p.evaluation_group ORDER BY \n      CASE \n        WHEN c.proportion_basis = 'SUBP' AND p.adjustment_factor_for_the_subplot > 0 \n        THEN (p.expansion_factor * c.condition_proportion_unadjusted * p.adjustment_factor_for_the_subplot)\n        ELSE 0 \n      END DESC) AS rank\n  FROM  \n    `bigquery-public-data.usfs_fia.condition` c \n  JOIN  \n    `bigquery-public-data.usfs_fia.plot_tree` pt ON pt.plot_sequence_number = c.plot_sequence_number \n  JOIN  \n    `bigquery-public-data.usfs_fia.population` p ON p.plot_sequence_number = pt.plot_sequence_number \n  WHERE p.inventory_year = 2012\n)\nSELECT \n  plot_sequence_number, \n  evaluation_group, \n  evaluation_type,\n  condition_status_code, \n  evaluation_description,\n  state_code,  \n  macroplot_acres, \n  subplot_acres\nFROM \n  ranked_evaluations\nWHERE \n  rank = 1\nORDER BY \n  subplot_acres DESC\nLIMIT 10;",
        "external_knowledge": null,
        "plan": "Extract and calculate the adjusted areas of macroplots and subplots for forest survey plots in the United States.\nInclude data such as plot sequence numbers, evaluation types, groupings, descriptions, state names, inventory years, state_code. \nEnsure to compute areas only for plots currently evaluated ('EXPCURR') and in active condition (condition status code = 1).",
        "special_function": null
    },
    {
        "instance_id": "bq220",
        "db": "bigquery-public-data.usfs_fia",
        "question": "Which states had the largest average subplot size and the largest average macroplot size respectively for each of the years 2015, 2016, and 2017, based on accessible forest land and current evaluations (EXPCURR)? Display the type of plot (subplot or macroplot), the year, the state, and the corresponding average size for each type and each specific year.",
        "SQL": "WITH temp AS (\n  SELECT\n    pt.plot_state_code_name AS state_name,\n    p.inventory_year AS inventory_year,\n    p.state_code AS state_code, \n    -- Calculate area - this replaces the \"decode\" logic in example from Oracle\n    CASE\n      WHEN c.proportion_basis = 'MACR' AND p.adjustment_factor_for_the_macroplot > 0 THEN\n        (p.expansion_factor * c.condition_proportion_unadjusted * p.adjustment_factor_for_the_macroplot)\n      ELSE 0\n    END AS macroplot_acres,\n    CASE\n      WHEN c.proportion_basis = 'SUBP' AND p.adjustment_factor_for_the_subplot > 0 THEN\n        (p.expansion_factor * c.condition_proportion_unadjusted * p.adjustment_factor_for_the_subplot)\n      ELSE 0\n    END AS subplot_acres\n  FROM \n    `bigquery-public-data.usfs_fia.condition` c\n  JOIN \n    `bigquery-public-data.usfs_fia.plot_tree` pt\n      ON pt.plot_sequence_number = c.plot_sequence_number\n  JOIN \n    `bigquery-public-data.usfs_fia.population` p\n      ON p.plot_sequence_number = pt.plot_sequence_number\n  WHERE \n    p.evaluation_type = 'EXPCURR'\n    AND c.condition_status_code = 1\n    AND p.inventory_year in (2015, 2016, 2017)\n),\n\nAverageAreas AS (\n  SELECT \n    state_name,\n    inventory_year,\n    AVG(subplot_acres) AS avg_subplot_acres,\n    AVG(macroplot_acres) AS avg_macroplot_acres\n  FROM \n    temp\n  GROUP BY \n    state_name, inventory_year\n),\n\nMaxSubplot AS (\n  SELECT \n    inventory_year,\n    MAX(avg_subplot_acres) AS max_subplot_acres\n  FROM \n    AverageAreas\n  GROUP BY \n    inventory_year\n),\n\nMaxMacroplot AS (\n  SELECT \n    inventory_year,\n    MAX(avg_macroplot_acres) AS max_macroplot_acres\n  FROM \n    AverageAreas\n  GROUP BY \n    inventory_year\n)\n\nSELECT \n  'SUBPLOT' AS type,\n  a.inventory_year,\n  a.state_name,\n  a.avg_subplot_acres AS value\nFROM \n  AverageAreas a\nJOIN \n  MaxSubplot ms ON a.inventory_year = ms.inventory_year \n                 AND a.avg_subplot_acres = ms.max_subplot_acres\n\nUNION ALL\n\nSELECT \n  'MACROPLOT' AS type,\n  a.inventory_year,\n  a.state_name,\n  a.avg_macroplot_acres AS value\nFROM \n  AverageAreas a\nJOIN \n  MaxMacroplot mm ON a.inventory_year = mm.inventory_year \n                  AND a.avg_macroplot_acres = mm.max_macroplot_acres\n\nORDER BY \n  type,\n  inventory_year;",
        "external_knowledge": "subplot_macroplot_size.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq096",
        "db": "bigquery-public-data.gbif",
        "question": "Which year had the first day after January with more than 10 sightings of Sterna paradisaea north of 40 degrees latitude?",
        "SQL": "WITH tenplus AS (\n  SELECT \n    year, \n    EXTRACT(DAYOFYEAR FROM DATE(eventdate)) AS dayofyear, \n    COUNT(*) AS count\n  FROM \n    bigquery-public-data.gbif.occurrences\n  WHERE \n    eventdate IS NOT NULL \n    AND species = 'Sterna paradisaea' \n    AND decimallatitude > 40.0 \n    AND month > 1\n  GROUP BY \n    year, \n    eventdate\n  HAVING \n    COUNT(*) > 10\n)\n\nSELECT \n  year AS year\nFROM \n  tenplus\nGROUP BY \n  year\nORDER BY \n  MIN(dayofyear)\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "Is the Arctic tern (Sterna paradisaea) migrating north earlier in the year?\nWe first retrieve all days of the year on which there are more than 10 observations of the species, then find the earliest day in each year.",
        "special_function": null
    },
    {
        "instance_id": "bq276",
        "db": "bigquery-public-data.geo_international_ports\nbigquery-public-data.geo_us_boundaries\nbigquery-public-data.noaa_hurricanes",
        "question": "Can you provide me with the details of all ports affected by tropical storms in region number 6585, including the port name, storm names, and average storm categories? Please consider only named storms in the North Atlantic basin with wind speeds of at least 35 knots and at least minimal tropical storm strength on the SSHS scale. Additionally, ensure that each port is located within a U.S. state boundary.",
        "SQL": "DECLARE ne INT64 DEFAULT 45;\nDECLARE se INT64 DEFAULT 135;\nDECLARE sw INT64 DEFAULT 225;\nDECLARE nw INT64 DEFAULT 315;\n\nWITH convert_miles AS (\n    SELECT\n      sid AS storm_id,\n      season,\n      latitude,\n      longitude,\n      `bigquery-public-data.persistent_udfs.nautical_miles_conversion`(usa_r34_ne) AS usa_r34_ne,\n      `bigquery-public-data.persistent_udfs.nautical_miles_conversion`(usa_r34_se) AS usa_r34_se,\n      `bigquery-public-data.persistent_udfs.nautical_miles_conversion`(usa_r34_sw) AS usa_r34_sw,\n      `bigquery-public-data.persistent_udfs.nautical_miles_conversion`(usa_r34_nw) AS usa_r34_nw\n    FROM\n      `bigquery-public-data.noaa_hurricanes.hurricanes`\n    WHERE\n      basin = \"NA\"\n      AND usa_wind >= 35\n      AND name != \"NOT_NAMED\"\n      AND usa_sshs >= -1),\n\nts_wind_polygon AS (\n    SELECT\n      storm_id,\n      season,\n      ST_MakePolygon(\n        ST_MakeLine([\n          `bigquery-public-data.persistent_udfs.azimuth_to_geog_point`(latitude, longitude, ne, usa_r34_ne),\n          `bigquery-public-data.persistent_udfs.azimuth_to_geog_point`(latitude, longitude, se, usa_r34_se),\n          `bigquery-public-data.persistent_udfs.azimuth_to_geog_point`(latitude, longitude, sw, usa_r34_sw),\n          `bigquery-public-data.persistent_udfs.azimuth_to_geog_point`(latitude, longitude, nw, usa_r34_nw)]\n        )\n      ) AS tropical_storm_geom\n  FROM\n      convert_miles)\n\nSELECT\n  index_number,\n  port_name,\n  state_name,\n  STRING_AGG(DISTINCT(h.season)) AS storm_years,\n  COUNT(DISTINCT(storm_id)) AS count_storms,\n  STRING_AGG(DISTINCT(name)) AS storm_name,\n  AVG(usa_sshs) AS avg_storm_cat,\n  AVG(usa_wind) AS avg_wind_speed,\n  ST_AsText(port_geom) AS port_geom,\n  ST_AsText(tropical_storm_geom) AS tropical_storm_geom\nFROM\n  ts_wind_polygon t,\n  `bigquery-public-data.geo_international_ports.world_port_index` ,\n  `bigquery-public-data.geo_us_boundaries.states` \nJOIN\n  `bigquery-public-data.noaa_hurricanes.hurricanes` h ON h.sid = t.storm_id\nWHERE\n  port_name IS NOT NULL\n  AND region_number = '6585'\n  AND ST_WITHIN(port_geom, state_geom)\n  AND ST_WITHIN(port_geom, tropical_storm_geom)\nGROUP BY\n  index_number,\n  port_geom,\n  port_name,\n  state_name,\n  tropical_storm_geom;",
        "temporal": "Yes",
        "external_knowledge": "persistent_udfs_routines.md",
        "plan": "1. Filter records from a hurricane dataset based on specific criteria (e.g., basin, wind speed, storm name, and storm category).\n2. For each selected record, convert certain radius measurements from kilometers to nautical miles.\n3. Compute geographical points in four compass directions from the storm's center using the converted nautical mile radii.\n4. Construct a polygon representing the boundary of a tropical storm based on these points.\n5. Ensure the port is within both the state's geographical boundary and the tropical storm's polygon.\n6. Filters the ports based on specific attributes (e.g., region number = 6585).\n7. Aggregate data by grouping on unique identifiers and geographical attributes of the ports, summarizing attributes like storm seasons, storm count, storm names, and average storm characteristics.\n8. Sort the results by average wind speed in descending order.\n9. Output detailed information about the port that has experienced the most severe average wind speeds during storms, including the port's geographical data, details of the storms affecting it, and the storm polygon data.\n",
        "special_function": [
            "aggregate-functions/STRING_AGG",
            "geography-functions/ST_ASTEXT",
            "geography-functions/ST_MAKELINE",
            "geography-functions/ST_MAKEPOLYGON",
            "geography-functions/ST_WITHIN",
            "other-functions/DECLARE"
        ]
    },
    {
        "instance_id": "bq277",
        "db": "bigquery-public-data.geo_international_ports\nbigquery-public-data.geo_us_boundaries\nbigquery-public-data.noaa_hurricanes",
        "question": "Can you provide the name of the port that is most frequently within the geographical area of named tropical storms in the region of the code \u20186585\u2019 with winds of at least 35 knots in the North Atlantic basin, which is also located within a specific region and intersects with interstate roads?",
        "SQL": "DECLARE ne INT64 DEFAULT 45;\nDECLARE se INT64 DEFAULT 135;\nDECLARE sw INT64 DEFAULT 225;\nDECLARE nw INT64 DEFAULT 315;\n\nWITH convert_miles AS (\n    SELECT\n      sid AS storm_id,\n      season,\n      latitude,\n      longitude,\n      `bigquery-public-data.persistent_udfs.nautical_miles_conversion`(usa_r34_ne) AS usa_r34_ne,\n      `bigquery-public-data.persistent_udfs.nautical_miles_conversion`(usa_r34_se) AS usa_r34_se,\n      `bigquery-public-data.persistent_udfs.nautical_miles_conversion`(usa_r34_sw) AS usa_r34_sw,\n      `bigquery-public-data.persistent_udfs.nautical_miles_conversion`(usa_r34_nw) AS usa_r34_nw\n    FROM\n      `bigquery-public-data.noaa_hurricanes.hurricanes`\n    WHERE\n      basin = \"NA\"\n      AND usa_wind >= 35\n      AND name != \"NOT_NAMED\"),\n\n  ts_wind_polygon AS (\n    SELECT\n      storm_id,\n      season,\n      ST_MakePolygon(\n        ST_MakeLine([\n          `bigquery-public-data.persistent_udfs.azimuth_to_geog_point`(latitude, longitude, ne, usa_r34_ne),\n          `bigquery-public-data.persistent_udfs.azimuth_to_geog_point`(latitude, longitude, se, usa_r34_se), \n          `bigquery-public-data.persistent_udfs.azimuth_to_geog_point`(latitude, longitude, sw, usa_r34_sw),\n          `bigquery-public-data.persistent_udfs.azimuth_to_geog_point`(latitude, longitude, nw, usa_r34_nw)]\n        )\n      ) AS tropical_storm_geom\n    FROM\n      convert_miles)\n\nSELECT\n  port_name\nFROM\n  ts_wind_polygon t,\n  `bigquery-public-data.geo_international_ports.world_port_index` p,\n  `bigquery-public-data.geo_us_boundaries.states` s\nJOIN\n  `bigquery-public-data.noaa_hurricanes.hurricanes` h ON h.sid = t.storm_id\nWHERE\n  port_name IS NOT NULL\n  AND region_number = '6585'\n  AND ST_WITHIN(port_geom, state_geom)\n  AND ST_WITHIN(port_geom, tropical_storm_geom)\nGROUP BY\n  port_name,\n  s.state_name\nORDER BY\n  COUNT(DISTINCT(storm_id)) DESC\nLIMIT 1;",
        "external_knowledge": "persistent_udfs_routines.md",
        "plan": "1. Define constants representing the compass angles for northeast, southeast, southwest, and northwest.\n2. Convert the radii of the storm's influence in each direction from a given unit to nautical miles.\n3. Generate polygons representing the geographical area affected by each tropical storm.\n4. Apply conditions to filter ports based on their non-null names, specific region numbers, and route types.\n5. Ensure the ports are within both the state and tropical storm polygons.\n6. Output the port which is most frequently affected by tropical storms according to the count of distinct storms affecting each port.\n",
        "special_function": [
            "ST_MAKEPOLYGON",
            "ST_MAKELINE",
            "ST_WITHIN"
        ]
    },
    {
        "instance_id": "bq278",
        "db": "bigquery-public-data.sunroof_solar",
        "question": "Can you provide a detailed comparison of the solar potential for each state, distinguishing between postal code and census tract levels? Include the number of buildings available for solar installations, the percentage covered by Project Sunroof, the percentage suitable for solar, total potential panel count, total kilowatt capacity, energy generation potential, carbon dioxide offset, and the gap in potential installations.",
        "SQL": "WITH PostalCodeData AS (\n    SELECT\n        state_name,\n        SUM(count_qualified) AS Buildings_Avail_for_Solar,\n        (AVG(percent_covered)) AS Percent_GMap_Covered_by_Proj_Sunroof,\n        (AVG(percent_qualified)) AS Percent_Covered_that_Suitable_for_Solar,\n        SUM(number_of_panels_total) AS Total_Panels_Potential,\n        SUM(kw_total) AS Kw_total_all_panel_250w,\n        SUM(yearly_sunlight_kwh_total) AS Total_Energy_Potential,\n        SUM(carbon_offset_metric_tons) AS Carbon_Offset,\n        SUM(existing_installs_count) AS Current_Buildings_with_Solar_Panel,\n        SUM(count_qualified * (percent_covered/100) * (percent_qualified/100)) - SUM(existing_installs_count) AS Building_Gap\n    FROM\n        `bigquery-public-data.sunroof_solar.solar_potential_by_postal_code`\n    GROUP BY\n        state_name\n),\nCensusTractData AS (\n    SELECT\n        state_name,\n        SUM(count_qualified) AS Buildings_Avail_for_Solar,\n        (AVG(percent_covered)) AS Percent_GMap_Covered_by_Proj_Sunroof,\n        (AVG(percent_qualified)) AS Percent_Covered_that_Suitable_for_Solar,\n        SUM(number_of_panels_total) AS Total_Panels_Potential,\n        SUM(kw_total) AS Kw_total_all_panel_250w,\n        SUM(yearly_sunlight_kwh_total) AS Total_Energy_Potential,\n        SUM(carbon_offset_metric_tons) AS Carbon_Offset,\n        SUM(existing_installs_count) AS Current_Buildings_with_Solar_Panel,\n        SUM(count_qualified * (percent_covered/100) * (percent_qualified/100)) - SUM(existing_installs_count) AS Building_Gap\n    FROM\n        `bigquery-public-data.sunroof_solar.solar_potential_by_censustract`\n    GROUP BY\n        state_name\n)\nSELECT\n    p.state_name AS State,\n    'Postal Code Level' AS Data_Level,\n    p.Buildings_Avail_for_Solar,\n    p.Percent_GMap_Covered_by_Proj_Sunroof,\n    p.Percent_Covered_that_Suitable_for_Solar,\n    p.Total_Panels_Potential,\n    p.Kw_total_all_panel_250w,\n    p.Total_Energy_Potential,\n    p.Carbon_Offset,\n    p.Current_Buildings_with_Solar_Panel,\n    p.Building_Gap\nFROM\n    PostalCodeData p\nUNION ALL\nSELECT\n    c.state_name AS State,\n    'Census Tract Level' AS Data_Level,\n    c.Buildings_Avail_for_Solar,\n    c.Percent_GMap_Covered_by_Proj_Sunroof,\n    c.Percent_Covered_that_Suitable_for_Solar,\n    c.Total_Panels_Potential,\n    c.Kw_total_all_panel_250w,\n    c.Total_Energy_Potential,\n    c.Carbon_Offset,\n    c.Current_Buildings_with_Solar_Panel,\n    c.Building_Gap\nFROM\n    CensusTractData c\nORDER BY\n    State, Data_Level;",
        "external_knowledge": null,
        "plan": "1. Compute the average percentage of buildings from the mapping database that are covered by a specific solar project.\n2. Compute the total potential yearly energy generation from solar installations, measured in kilowatt-hours.\n3. Estimate the potential reduction in carbon dioxide emissions due to solar installations.\n4. Calculate the total number of buildings suitable for solar installations as recorded in a mapping database by summing up the counts of qualified buildings across each state.\n5. Sort the query results by the number of buildings in Google Map suitable for solar in descending order.\n6. Return the related information for the state which has the largest number of qualified buildings.\n",
        "special_function": [
            "conversion-functions/CAST",
            "mathematical-functions/ROUND"
        ]
    },
    {
        "instance_id": "bq102",
        "db": "bigquery-public-data.gnomAD",
        "question": "Identify which start positions are associated with missense variants in the BRCA1 gene on chromosome 17, where the reference base is 'C' and the alternate base is 'T'. Using data from the gnomAD v2.1.1 version.",
        "SQL": "WITH gene_region AS (\n  SELECT \n    MIN(start_position) AS start_pos, \n    MAX(end_position) AS end_pos\n  FROM `bigquery-public-data.gnomAD.v2_1_1_genomes__chr17` AS main_table\n  WHERE EXISTS (\n    SELECT 1 \n    FROM UNNEST(main_table.alternate_bases) AS alternate_bases\n    WHERE EXISTS (\n      SELECT 1 \n      FROM UNNEST(alternate_bases.vep) AS vep\n      WHERE vep.SYMBOL = 'BRCA1'\n    )\n  )\n)\n\n\nSELECT \n  DISTINCT start_position\nFROM `bigquery-public-data.gnomAD.v2_1_1_genomes__chr17` AS main_table,\n     UNNEST(main_table.alternate_bases) AS alternate_bases,\n     UNNEST(alternate_bases.vep) AS vep,\n     gene_region\nWHERE main_table.start_position >= gene_region.start_pos\n  AND main_table.start_position <= gene_region.end_pos\n  AND REGEXP_CONTAINS(vep.Consequence, r\"missense_variant\")\n  AND reference_bases = \"C\"\n  AND alternate_bases.alt = \"T\"",
        "external_knowledge": null,
        "plan": "1. **Define Gene Region**:\n   - Create a temporary dataset to identify the start and end positions of the gene of interest on the specified chromosome.\n   - Filter the data to include only records associated with the gene using an internal subquery to check for the gene symbol within nested data structures.\n   - Extract the minimum start position and maximum end position to determine the gene's region boundaries.\n\n2. **Filter Data by Gene Region**:\n   - Use the defined gene region boundaries to filter the main dataset.\n   - Ensure that only records within the start and end positions of the gene are selected.\n\n3. **Unnest Nested Data**:\n   - Expand nested arrays in the dataset to access individual elements such as alternate bases and variant annotations.\n   - This allows for detailed inspection of each variant associated with the gene.\n\n4. **Filter for Specific Variant Type**:\n   - Identify and select records where the variant is categorized as a missense variant.\n   - Use a regular expression match to ensure that the specific type of variant is captured.\n\n5. **Filter by Reference and Alternate Bases**:\n   - Further refine the selection by ensuring that the reference base is 'C' and the alternate base is 'T'.\n   - This additional filtering narrows down the variants to those of specific interest.\n\n6. **Select Unique Start Positions**:\n   - Retrieve and return the distinct start positions of the variants that meet all the specified criteria.\n   - Ensure that the result set contains only unique positions to avoid duplicates.",
        "special_function": [
            "string-functions/REGEXP_CONTAINS",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq445",
        "db": "bigquery-public-data.gnomAD",
        "question": "Find the start and end positions of the BRCA1 gene, and retrieve the first missense variants based on their protein positions within this region. The variants must have a consequence type of \"missense_variant\". Using data from the gnomAD v2.1.1 version.",
        "SQL": "SELECT vep.Protein_position AS Protein_pos\nFROM bigquery-public-data.gnomAD.v2_1_1_genomes__chr17 AS main_table,\n     main_table.alternate_bases AS alternate_bases,\n     alternate_bases.vep AS vep\nWHERE start_position >= (\n         SELECT MIN(start_position)\n         FROM bigquery-public-data.gnomAD.v2_1_1_genomes__chr17\n         WHERE EXISTS (\n             SELECT 1\n             FROM UNNEST(alternate_bases) AS ab\n             WHERE EXISTS (SELECT 1 FROM ab.vep WHERE SYMBOL = 'BRCA1')\n         )\n     )\n  AND start_position <= (\n         SELECT MAX(end_position)\n         FROM bigquery-public-data.gnomAD.v2_1_1_genomes__chr17\n         WHERE EXISTS (\n             SELECT 1\n             FROM UNNEST(alternate_bases) AS ab\n             WHERE EXISTS (SELECT 1 FROM ab.vep WHERE SYMBOL = 'BRCA1')\n         )\n     )\n  AND REGEXP_CONTAINS(vep.Consequence, r\"missense_variant\")\nORDER BY vep.Protein_position ASC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "Finding variants in a particular gene using v2.1.1 exomes dataset\nUse this query to find all missense variants in BRCA1 gene using the v2.1.1 exomes dataset in BigQuery.",
        "special_function": null
    },
    {
        "instance_id": "bq103",
        "db": "bigquery-public-data.gnomAD",
        "question": "Generate summary statistics on genetic variants in the region between positions 55039447 and 55064852 on chromosome 1. This includes the number of variants, the total allele count, the total number of alleles, and distinct gene symbols (using Variant Effect Predictor, VEP, for gene annotation). Additionally, compute the density of mutations by dividing the length of the region by the number of variants.  Using data from the gnomAD v3 version.",
        "SQL": "WITH summary_stats AS (\n  SELECT\n    COUNT(1) AS num_variants,\n    SUM((SELECT alt.AC FROM UNNEST(alternate_bases) AS alt)) AS sum_AC,\n    SUM(AN) AS sum_AN,\n    -- Also include some information from Variant Effect Predictor (VEP).\n    STRING_AGG(DISTINCT (SELECT annot.symbol FROM UNNEST(alternate_bases) AS alt,\n                                               UNNEST(vep) AS annot LIMIT 1), ', ') AS genes\n  FROM bigquery-public-data.gnomAD.v3_genomes__chr1 AS main_table\n  WHERE start_position >= 55039447 AND start_position <= 55064852\n)\nSELECT\n  ROUND((55064852 - 55039447) / num_variants, 3) AS burden_of_mutation,\n  *\nFROM summary_stats;",
        "external_knowledge": null,
        "plan": "Finding genes with INDEL variants using v2.1.1 genomes dataset\nUse this query to find the 1000 genes with the most INDEL variants on chromosome 17 using the v2.1.1 genomes dataset in BigQuery.",
        "special_function": null
    },
    {
        "instance_id": "bq104",
        "db": "bigquery-public-data.google_trends",
        "question": "Identify which DMA had the highest search scores for the terms that were top rising one year ago",
        "SQL": "WITH LatestWeek AS (\n    SELECT\n        DATE_SUB(MAX(week), INTERVAL 52 WEEK) AS last_year_week\n    FROM\n        `bigquery-public-data.google_trends.top_rising_terms`\n),\nLatestRefreshDate AS (\n    SELECT\n        MAX(refresh_date) AS latest_refresh_date\n    FROM\n        `bigquery-public-data.google_trends.top_rising_terms`\n),\nAggregatedData AS (\n    SELECT\n        term,\n        week,\n        ARRAY_AGG(STRUCT(IF(score IS NULL, NULL, dma_name) AS dma_name, rank, score) ORDER BY score DESC LIMIT 1) AS x\n    FROM\n        `bigquery-public-data.google_trends.top_rising_terms`\n    WHERE\n        week = (SELECT last_year_week FROM LatestWeek)\n        AND refresh_date = (SELECT latest_refresh_date FROM LatestRefreshDate)\n    GROUP BY\n        term, week\n)\n\nSELECT\n    term\nFROM\n    AggregatedData\nORDER BY\n    (SELECT rank FROM UNNEST(x))\nLIMIT 1",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Determine the last year's equivalent week by subtracting 52 weeks from the most recent week in the dataset.\n2. Find the most recent data refresh date.\n3. Aggregate the highest score for each term within the identified week and refresh date, focusing on the top entry for each term.\n4. Display the term, its corresponding week, and the top scoring DMA details.",
        "special_function": null
    },
    {
        "instance_id": "bq411",
        "db": "bigquery-public-data.google_trends",
        "question": "Please identify the top two Google Trends search terms for each weekday over the first two weeks in Sept. 2024, and list them by date from most recent to oldest.",
        "SQL": "WITH cte AS (\n    SELECT\n        refresh_date,\n        term,\n        rank\n    FROM\n        bigquery-public-data.google_trends.top_terms\n    WHERE\n        rank IN (1, 2, 3)\n        AND refresh_date BETWEEN '2024-09-01' AND '2024-09-14'\n        AND EXTRACT(DAYOFWEEK FROM refresh_date) BETWEEN 2 AND 6\n)\nSELECT\n    refresh_date AS Day,\n    MAX(CASE WHEN rank = 1 THEN term END) AS top1_term,\n    MAX(CASE WHEN rank = 2 THEN term END) AS top2_term,\n    MAX(CASE WHEN rank = 3 THEN term END) AS top3_term\nFROM\n    cte\nGROUP BY\n    refresh_date\nORDER BY\n    refresh_date DESC;",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq105",
        "db": "bigquery-public-data.nhtsa_traffic_fatalities\nbigquery-public-data.census_bureau_usa\nbigquery-public-data.utility_us",
        "question": "How many traffic accidents per 100,000 people, specifically due to driver distraction, were recorded in each state in the years 2015 and 2016? Identify the top five states each year with the highest rates. Exclude accidents where the distraction status of the driver was recorded as 'Not Distracted,' 'Unknown if Distracted,' or 'Not Reported.' Use state population data from the 2010 census for calculating the rate.",
        "SQL": "SELECT * FROM\n(\nSELECT\n  '2015' AS year,\n  COUNT(a.consecutive_number) AS total,\n  a.state_name AS state,\n  c.state_pop AS population,\n  (COUNT(a.consecutive_number) / c.state_pop * 100000) AS rate_per_100000\nFROM\n  `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015` a\nJOIN\n  `bigquery-public-data.nhtsa_traffic_fatalities.distract_2015` b\nON\n  a.consecutive_number = b.consecutive_number\nJOIN (\n  SELECT\n    SUM(d.population) AS state_pop,\n    e.state_name AS state\n  FROM\n    `bigquery-public-data.census_bureau_usa.population_by_zip_2010` d\n  JOIN\n    `bigquery-public-data.utility_us.zipcode_area` e\n  ON\n    d.zipcode = e.zipcode\n  GROUP BY\n    state ) c\nON\n  c.state = a.state_name\nWHERE\n  b.driver_distracted_by_name NOT IN ('Not Distracted', 'Unknown if Distracted', 'Not Reported')\nGROUP BY\n  state,\n  population,\n  c.state_pop\nORDER BY\n  rate_per_100000 DESC\nLIMIT 5\n)\nUNION ALL\n(\nSELECT\n  '2016' AS year,\n  COUNT(a.consecutive_number) AS total,\n  a.state_name AS state,\n  c.state_pop AS population,\n  (COUNT(a.consecutive_number) / c.state_pop * 100000) AS rate_per_100000\nFROM\n  `bigquery-public-data.nhtsa_traffic_fatalities.accident_2016` a\nJOIN\n  `bigquery-public-data.nhtsa_traffic_fatalities.distract_2016` b\nON\n  a.consecutive_number = b.consecutive_number\nJOIN (\n  SELECT\n    SUM(d.population) AS state_pop,\n    e.state_name AS state\n  FROM\n    `bigquery-public-data.census_bureau_usa.population_by_zip_2010` d\n  JOIN\n    `bigquery-public-data.utility_us.zipcode_area` e\n  ON\n    d.zipcode = e.zipcode\n  GROUP BY\n    state ) c\nON\n  c.state = a.state_name\nWHERE\n  b.driver_distracted_by_name NOT IN ('Not Distracted', 'Unknown if Distracted', 'Not Reported')\nGROUP BY\n  state,\n  population,\n  c.state_pop\nORDER BY\n  rate_per_100000 DESC\nLIMIT 5\n)",
        "external_knowledge": null,
        "plan": "1. Combine the accident and distraction data for 2015 where the driver was known to be distracted.\n2. Integrate population data to calculate accident rates per state.\n3. Compute the accident rate per 100,000 population for each state.\n4. Order the results by the rate of distraction-caused accidents to identify states with the highest rates.",
        "special_function": null
    },
    {
        "instance_id": "bq108",
        "db": "bigquery-public-data.nhtsa_traffic_fatalities",
        "question": "Calculate the percentage of traffic accidents in 2015 from January to August that involved multiple people and had multiple instances of severe injuries (injury severity 4).",
        "SQL": "SELECT\n  SUM(label)/COUNT(label) AS percentage_of_1,\nFROM\n(\n  SELECT\n    CASE\n      WHEN COUNTIF(injury_severity = 4) > 1\n        then 1\n        else 0\n    END AS label\n  FROM \n    `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015` a,\n    `bigquery-public-data.nhtsa_traffic_fatalities.person_2015` b,\n    `bigquery-public-data.nhtsa_traffic_fatalities.vehicle_2015` c \n  WHERE\n    a.month_of_crash > 0 \n    AND a.month_of_crash < 9\n    AND a.consecutive_number = b.consecutive_number\n    AND b.consecutive_number = c.consecutive_number\n    AND a.consecutive_number IN\n    (\n      SELECT \n        accident.consecutive_number\n      FROM \n        `bigquery-public-data.nhtsa_traffic_fatalities.person_2015` person, `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015` accident\n      WHERE\n        person.consecutive_number = accident.consecutive_number\n      GROUP BY \n        accident.consecutive_number\n      HAVING COUNT(DISTINCT person.person_number) > 1\n    )\n  GROUP BY\n    a.consecutive_number\n)",
        "external_knowledge": null,
        "plan": "1. **Join the Necessary Tables**:\n    - Join three tables: accidents, persons, and vehicles based on a common identifier.\n\n2. **Filter by Date Range**:\n    - Ensure that the accidents occurred between January and August.\n\n3. **Identify Multi-Person Accidents**:\n    - Identify accidents involving more than one person by grouping the data by the accident identifier and counting the distinct persons involved.\n\n4. **Label Accidents with Multiple Severe Injuries**:\n    - For each accident, determine if it has more than one person with a severe injury (injury severity level 4).\n    - Assign a label of 1 if there are multiple severe injuries, otherwise assign a label of 0.\n\n5. **Calculate the Percentage**:\n    - Calculate the percentage of accidents with the label 1 by summing the labels and dividing by the total number of accidents.\n    \n6. **Return the Result**:\n    - Output the calculated percentage as the result.",
        "special_function": [
            "aggregate-functions/COUNTIF",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq067",
        "db": "bigquery-public-data.nhtsa_traffic_fatalities",
        "question": "I want to build a ML model which can predict whether there will be more than one fatality in a crash invloving 2 or more people. Construct a labelled (0 or 1) dataset for me, and the predictors include the state, vehicle type, the number of drunk drivers, day of the week, hour of the day and another two engineered features, whether the accident happened in the work zone and the average absolute difference between travel speed and speed limit. Please use numeric value for each predictor and categorize the speed difference into levels 0 to 4 based on 20MPH increments (lower bound inclusive while upper exclusive).",
        "SQL": "SELECT\n  CASE\n    WHEN COUNTIF(injury_severity = 4) > 1 -- 4 = Fatal injury\n      then 1\n      else 0\n  END AS label,\n  a.state_number AS feature1,\n  c.body_type AS feature2,\n  a.number_of_drunk_drivers AS feature3,\n  a.day_of_week AS feature4,\n  a.hour_of_crash AS feature5,\n  CASE\n    WHEN a.work_zone = \"None\" THEN 0\n    ELSE 1\n  END AS feature6,\n  CASE WHEN avg_diff_speed >= 0 AND avg_diff_speed < 20 THEN 0\n      WHEN avg_diff_speed >= 20 AND avg_diff_speed < 40 THEN 1\n      WHEN avg_diff_speed >= 40 AND avg_diff_speed < 60 THEN 2\n      WHEN avg_diff_speed >= 60 AND avg_diff_speed < 80 THEN 3\n      ELSE 4\n  END AS feature7\nFROM \n  `bigquery-public-data.nhtsa_traffic_fatalities.accident_2016` a,\n  `bigquery-public-data.nhtsa_traffic_fatalities.person_2016` b,\n  `bigquery-public-data.nhtsa_traffic_fatalities.vehicle_2016` c,\n  (\n    SELECT \n      consecutive_number, \n      AVG(ABS(travel_speed - speed_limit)) avg_diff_speed\n    FROM \n      `bigquery-public-data.nhtsa_traffic_fatalities.vehicle_2016` \n    -- travel_speed bounded because codes: 997 = speed greater than 96, 998 = speed greater than 151, 999 = unknown\n    -- Speed_limit bounded because codes: 98= not reported 99= unknown\n    WHERE \n      travel_speed <= 151 AND speed_limit <= 80\n    GROUP BY \n      consecutive_number\n  ) d\nWHERE\n  a.consecutive_number = b.consecutive_number\n  AND b.consecutive_number = c.consecutive_number\n  AND c.consecutive_number = d.consecutive_number\n  AND a.consecutive_number IN\n  (\n    SELECT \n      accident.consecutive_number\n    FROM \n      `bigquery-public-data.nhtsa_traffic_fatalities.person_2016` person, `bigquery-public-data.nhtsa_traffic_fatalities.accident_2016` accident\n    WHERE\n      person.consecutive_number = accident.consecutive_number\n    GROUP BY \n      accident.consecutive_number\n    HAVING COUNT(DISTINCT person.person_number) <> 1\n  )\nGROUP BY\n  a.consecutive_number, a.state_number, c.body_type, a.number_of_drunk_drivers, a.day_of_week, a.hour_of_crash, a.work_zone, avg_diff_speed",
        "external_knowledge": "nhtsa_traffic_fatalities.md",
        "plan": "1. **Label Creation**:\n   - Determine if a crash has more than one fatality by counting the number of severe injuries (categorized as fatal). Assign a binary label (1 for more than one fatality, 0 otherwise).\n\n2. **Feature Extraction**:\n   - Extract relevant features for each crash: state, vehicle type, number of drunk drivers involved, day of the week, and hour of the crash.\n   - Create a binary feature indicating whether the accident occurred in a work zone.\n   - Calculate the average absolute difference between travel speed and speed limit for each crash and categorize this difference into levels (0 to 4) based on 20 MPH increments.\n\n3. **Data Filtering**:\n   - Ensure the data only includes crashes involving at least two people by filtering out crashes with only one person.\n\n4. **Data Aggregation**:\n   - Join the necessary tables to gather all required features for each crash, ensuring the features and label are aggregated correctly. Group the data by crash identifiers and the extracted features to compile the final dataset for model training.",
        "special_function": [
            "aggregate-functions/COUNTIF",
            "mathematical-functions/ABS",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq396",
        "db": "bigquery-public-data.nhtsa_traffic_fatalities",
        "question": "Which top 3 states had the largest differences in the number of traffic accidents between rainy and clear weather during weekends in 2016? Please also provide the respective differences for each state.",
        "SQL": "WITH weekend_accidents AS (\n    SELECT\n        state_name,\n        CASE\n            WHEN atmospheric_conditions_1_name = 'Rain' THEN 'Rain'\n            WHEN atmospheric_conditions_1_name = 'Clear' THEN 'Clear'\n            ELSE 'Other'\n        END AS Weather_Condition,\n        COUNT(DISTINCT consecutive_number) AS num_accidents\n    FROM\n        `bigquery-public-data.nhtsa_traffic_fatalities.accident_2016`\n    WHERE\n        EXTRACT(DAYOFWEEK FROM timestamp_of_crash) IN (1, 7)  -- 1 = Sunday, 7 = Saturday\n        AND atmospheric_conditions_1_name IN ('Rain', 'Clear')\n    GROUP BY\n        state_name, Weather_Condition\n),\n\nweather_difference AS (\n    SELECT\n        state_name,\n        MAX(CASE WHEN Weather_Condition = 'Rain' THEN num_accidents ELSE 0 END) AS Rain_Accidents,\n        MAX(CASE WHEN Weather_Condition = 'Clear' THEN num_accidents ELSE 0 END) AS Clear_Accidents,\n        ABS(MAX(CASE WHEN Weather_Condition = 'Rain' THEN num_accidents ELSE 0 END) -\n            MAX(CASE WHEN Weather_Condition = 'Clear' THEN num_accidents ELSE 0 END)) AS Difference\n    FROM\n        weekend_accidents\n    GROUP BY\n        state_name\n)\n\nSELECT\n    state_name,\n    Difference\nFROM\n    weather_difference\nORDER BY\n    Difference DESC\nLIMIT 3;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq441",
        "db": "bigquery-public-data.nhtsa_traffic_fatalities",
        "question": "Please help me compile the critical details on traffic accidents in 2015, as listed in the info document.",
        "SQL": "SELECT \na.consecutive_number,\na.county,\na.type_of_intersection,\na.light_condition,\na.atmospheric_conditions_1,\na.hour_of_crash,\na.functional_system,\na.related_factors_crash_level_1 related_factors,\nCASE WHEN a.hour_of_ems_arrival_at_hospital BETWEEN 0 AND 23 AND a.hour_of_ems_arrival_at_hospital - a.hour_of_crash > 0 THEN a.hour_of_ems_arrival_at_hospital - a.hour_of_crash ELSE NULL END delay_to_hospital,\nCASE WHEN a.hour_of_arrival_at_scene BETWEEN 0 AND 23 AND a.hour_of_arrival_at_scene - a.hour_of_crash > 0 THEN a.hour_of_arrival_at_scene - a.hour_of_crash ELSE NULL END delay_to_scene,\np.age,\np.person_type,\np.seating_position,\nCASE p.restraint_system_helmet_use WHEN 0 THEN 0 WHEN 1 THEN 0.33 WHEN 2 THEN 0.67 WHEN 3 THEN 1.0 ELSE 0.5 END restraint,\nCASE WHEN p.injury_severity IN (4) THEN 1 ELSE 0 END survived,\nCASE WHEN p.rollover IN ('', 'No Rollover') THEN 0 ELSE 1 END rollover,\nCASE WHEN p.air_bag_deployed BETWEEN 1 AND 9 THEN 1 ELSE 0 END airbag,\nCASE WHEN p.police_reported_alcohol_involvement LIKE ('%Yes%') THEN 1 ELSE 0 END alcohol,\nCASE WHEN p.police_reported_drug_involvement LIKE ('%Yes%') THEN 1 ELSE 0 END drugs,\np.related_factors_person_level1,\nv.travel_speed,\nCASE WHEN v.speeding_related LIKE ('%Yes%') THEN 1 ELSE 0 END speeding_related,\nv.extent_of_damage,\nv.body_type body_type,\nv.vehicle_removal,\nCASE WHEN v.manner_of_collision > 11 THEN 11 ELSE v.manner_of_collision END manner_of_collision,\nCASE WHEN v.roadway_surface_condition > 11 THEN 8 ELSE v.roadway_surface_condition END roadway_surface_condition,\nCASE WHEN v.first_harmful_event < 90 THEN v.first_harmful_event ELSE 0 END first_harmful_event,\nCASE WHEN v.most_harmful_event < 90 THEN v.most_harmful_event ELSE 0 END most_harmful_event,\nFROM `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015` a\nLEFT OUTER JOIN `bigquery-public-data.nhtsa_traffic_fatalities.vehicle_2015` v\nUSING (consecutive_number)\nLEFT OUTER JOIN `bigquery-public-data.nhtsa_traffic_fatalities.person_2015` p\nUSING (consecutive_number)",
        "external_knowledge": "Traffic_Fatalities_Info_List_2015.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq097",
        "db": "bigquery-public-data.sdoh_bea_cainc30\nbigquery-public-data.sdoh_cdc_wonder_natality\nbigquery-public-data.sdoh_cms_dual_eligible_enrollment\nbigquery-public-data.sdoh_hrsa_shortage_areas\nbigquery-public-data.sdoh_hud_housing\nbigquery-public-data.sdoh_hud_pit_homelessness\nbigquery-public-data.sdoh_snap_enrollment\nbigquery-public-data.census_bureau_acs",
        "question": "What is the increasing amount of the average earnings per job between the years 2012 and 2017 for each geographic region in Massachusetts (indicated by \"MA\" at the end of GeoName)?",
        "SQL": "WITH bea_2012 AS (\n  SELECT GeoFIPS, GeoName, Earnings_per_job_avg AS earnings_2012\n  FROM `bigquery-public-data.sdoh_bea_cainc30.fips`\n  WHERE Year='2012-01-01' AND ENDS_WITH(GeoName, \"MA\") IS TRUE\n),\n\nbea_2017 AS (\n  SELECT GeoFIPS, GeoName, Earnings_per_job_avg AS earnings_2017\n  FROM `bigquery-public-data.sdoh_bea_cainc30.fips`\n  WHERE Year='2017-01-01' AND ENDS_WITH(GeoName, \"MA\") IS TRUE\n),\n\nearnings_diff AS (\n  SELECT\n    bea_2017.GeoFIPS, bea_2017.GeoName, bea_2017.earnings_2017, bea_2012.earnings_2012, \n    (bea_2017.earnings_2017 - bea_2012.earnings_2012) AS earnings_change\n   FROM bea_2017 \n   JOIN bea_2012\n   ON bea_2017.GeoFIPS = bea_2012.GeoFIPS\n)\n \nSELECT * FROM earnings_diff WHERE earnings_change IS NOT NULL ORDER BY earnings_change DESC",
        "external_knowledge": null,
        "plan": "In what Massachusetts County has the average earnings per job increased the most from 2012 to 2017\nFind out which Massachusetts counties have seen the largest increase in earnings per job over a 5-year period",
        "special_function": null
    },
    {
        "instance_id": "bq120",
        "db": "bigquery-public-data.sdoh_bea_cainc30\nbigquery-public-data.sdoh_cdc_wonder_natality\nbigquery-public-data.sdoh_cms_dual_eligible_enrollment\nbigquery-public-data.sdoh_hrsa_shortage_areas\nbigquery-public-data.sdoh_hud_housing\nbigquery-public-data.sdoh_hud_pit_homelessness\nbigquery-public-data.sdoh_snap_enrollment\nbigquery-public-data.census_bureau_acs",
        "question": "What are the top 10 regions with the highest total SNAP participation, along with their respective ratios of households earning under $20,000 to SNAP households, as of 2017?",
        "SQL": "WITH acs_2017 AS (\n  SELECT geo_id, income_less_10000 AS i10, income_10000_14999 AS i15, income_15000_19999 AS i20\n  FROM `bigquery-public-data.census_bureau_acs.county_2017_5yr`\n ),\n\nsnap_2017_Jan AS (\n  SELECT FIPS, SNAP_All_Participation_Households AS snap_total\n  FROM `bigquery-public-data.sdoh_snap_enrollment.snap_enrollment`\n  WHERE Date = '2017-01-01'\n)\n\nSELECT acs_2017.geo_id, snap_2017_Jan.snap_total,\n(acs_2017.i10 + acs_2017.i15 + acs_2017.i20) As households_under_20,\n(acs_2017.i10 + acs_2017.i15 + acs_2017.i20)/snap_2017_Jan.snap_total As under_20_snap_ratio \nFROM acs_2017\nJOIN snap_2017_Jan\nON  acs_2017.geo_id = snap_2017_Jan.FIPS\nWHERE snap_2017_Jan.snap_total > 0\nORDER BY snap_2017_Jan.snap_total DESC\nLIMIT 10",
        "external_knowledge": null,
        "plan": "What is the ratio of households under $20,000 in income [from ACS] to the number of households using SNAP benefits in 2017?\nDetermine the percentage of households with income under $20,000 per year are using SNAP benefits",
        "special_function": null
    },
    {
        "instance_id": "bq110",
        "db": "bigquery-public-data.sdoh_bea_cainc30\nbigquery-public-data.sdoh_cdc_wonder_natality\nbigquery-public-data.sdoh_cms_dual_eligible_enrollment\nbigquery-public-data.sdoh_hrsa_shortage_areas\nbigquery-public-data.sdoh_hud_housing\nbigquery-public-data.sdoh_hud_pit_homelessness\nbigquery-public-data.sdoh_snap_enrollment\nbigquery-public-data.census_bureau_acs",
        "question": "What has been the change in the number of homeless veterans in each CoC region of New York between 2012 and 2018?",
        "SQL": "WITH homeless_2012 AS (\n  SELECT Homeless_Veterans AS Vet12, CoC_Name  \n  FROM `bigquery-public-data.sdoh_hud_pit_homelessness.hud_pit_by_coc` \n  WHERE SUBSTR(CoC_Number,0,2) = \"NY\" AND Count_Year = 2012\n),\n \nhomeless_2018 AS (\n  SELECT Homeless_Veterans AS Vet18, CoC_Name  \n  FROM `bigquery-public-data.sdoh_hud_pit_homelessness.hud_pit_by_coc` \n  WHERE SUBSTR(CoC_Number,0,2) = \"NY\" AND Count_Year = 2018\n),\n \nveterans_change AS (\n  SELECT homeless_2012.COC_Name, Vet12, Vet18, Vet18 - Vet12 AS VetChange\n  FROM homeless_2018\n  JOIN homeless_2012\n  ON homeless_2018.CoC_Name = homeless_2012.CoC_Name\n)\n\nSELECT COC_Name, VetChange FROM veterans_change\nORDER BY CoC_Name;",
        "external_knowledge": null,
        "plan": "1. **Filter and Extract Data for 2012**: Create a temporary dataset that selects the number of a specific group of individuals and their associated regions from the main dataset for the year 2012, limited to a specific state.\n\n2. **Filter and Extract Data for 2018**: Create another temporary dataset that selects the same group of individuals and regions from the main dataset for the year 2018, again limited to the same state.\n\n3. **Calculate and Display Change**: Combine the two temporary datasets by matching regions, and compute the change in the number of individuals between 2012 and 2018. Finally, display the region, the counts from both years, and the computed change.",
        "special_function": null
    },
    {
        "instance_id": "bq395",
        "db": "bigquery-public-data.sdoh_bea_cainc30\nbigquery-public-data.sdoh_cdc_wonder_natality\nbigquery-public-data.sdoh_cms_dual_eligible_enrollment\nbigquery-public-data.sdoh_hrsa_shortage_areas\nbigquery-public-data.sdoh_hud_housing\nbigquery-public-data.sdoh_hud_pit_homelessness\nbigquery-public-data.sdoh_snap_enrollment\nbigquery-public-data.census_bureau_acs",
        "question": "Which 5 states' percentage change in unsheltered homeless individuals from 2015 to 2018 were top 5 closest to the national average? Please provide the state abbreviation.",
        "SQL": "WITH homeless_2015 AS (\n  SELECT Unsheltered_Homeless AS U15, SUBSTR(CoC_Number, 0, 2) as State_Abbr\n  FROM `bigquery-public-data.sdoh_hud_pit_homelessness.hud_pit_by_coc`\n  WHERE Count_Year = 2015\n),\n \nhomeless_2018 AS (\n  SELECT Unsheltered_Homeless AS U18, SUBSTR(CoC_Number, 0, 2) as State_Abbr\n  FROM `bigquery-public-data.sdoh_hud_pit_homelessness.hud_pit_by_coc`\n  WHERE Count_Year = 2018\n),\n\nunsheltered_change AS (\n  SELECT homeless_2018.State_Abbr, \n         SUM(U15) AS Unsheltered_2015, \n         SUM(U18) AS Unsheltered_2018, \n         (SUM(U18) - SUM(U15)) / SUM(U15) * 100 AS Percent_Change\n  FROM homeless_2018\n  JOIN homeless_2015\n  ON homeless_2018.State_Abbr = homeless_2015.State_Abbr\n  GROUP BY State_Abbr\n),\n\naverage_change AS (\n  SELECT AVG(Percent_Change) AS Avg_Change\n  FROM unsheltered_change\n),\n\nclosest_to_avg AS (\n  SELECT State_Abbr\n  FROM unsheltered_change, average_change\n  ORDER BY ABS(Percent_Change - Avg_Change)\n  LIMIT 5\n)\n\nSELECT State_Abbr FROM closest_to_avg;",
        "external_knowledge": null,
        "plan": "What states have seen the largest increase in unsheltered homlessness since 2015?\nThis query returns the list of states that have the largest increases in unsheltered homeless population since 2015",
        "special_function": null
    },
    {
        "instance_id": "bq352",
        "db": "bigquery-public-data.sdoh_bea_cainc30\nbigquery-public-data.sdoh_cdc_wonder_natality\nbigquery-public-data.sdoh_cms_dual_eligible_enrollment\nbigquery-public-data.sdoh_hrsa_shortage_areas\nbigquery-public-data.sdoh_hud_housing\nbigquery-public-data.sdoh_hud_pit_homelessness\nbigquery-public-data.sdoh_snap_enrollment\nbigquery-public-data.census_bureau_acs",
        "question": "Please list the average number of prenatal weeks in 2018 for counties in Wisconsin where more than 5% of the employed population had commutes of 45-59 minutes in 2017.",
        "SQL": "WITH natality_2018 AS (\n  SELECT County_of_Residence_FIPS AS FIPS, Ave_Number_of_Prenatal_Wks AS Vist_Ave, County_of_Residence\n  FROM `bigquery-public-data.sdoh_cdc_wonder_natality.county_natality` \n  WHERE SUBSTR(County_of_Residence_FIPS, 0, 2) = \"55\" AND Year = '2018-01-01'\n),\n\nacs_2017 AS (\n  SELECT geo_id, commute_45_59_mins, employed_pop\n  FROM `bigquery-public-data.census_bureau_acs.county_2017_5yr`\n),\n\ncorr_tbl AS (\n  SELECT\n    n.County_of_Residence,\n    ROUND((a.commute_45_59_mins / a.employed_pop) * 100, 2) AS percent_high_travel,\n    n.Vist_Ave\n  FROM acs_2017 a\n  JOIN natality_2018 n\n  ON a.geo_id = n.FIPS\n)\n\nSELECT County_of_Residence, Vist_Ave\nFROM corr_tbl\nWHERE percent_high_travel > 5",
        "external_knowledge": null,
        "plan": "Is there an association between high commute times [from ACS] and average number of prenatal visits in Wisconsin by County?\nThis query examines the potential correlation between the average number of prenatal visits and length of commutes",
        "special_function": [
            "mathematical-functions/ROUND",
            "string-functions/SUBSTR"
        ]
    },
    {
        "instance_id": "bq074",
        "db": "bigquery-public-data.sdoh_bea_cainc30\nbigquery-public-data.sdoh_cdc_wonder_natality\nbigquery-public-data.sdoh_cms_dual_eligible_enrollment\nbigquery-public-data.sdoh_hrsa_shortage_areas\nbigquery-public-data.sdoh_hud_housing\nbigquery-public-data.sdoh_hud_pit_homelessness\nbigquery-public-data.sdoh_snap_enrollment\nbigquery-public-data.census_bureau_acs",
        "question": "Count the number of counties that experienced an increase in unemployment from 2015 to 2018, using 5-year ACS data, and a decrease in dual-eligible enrollee counts between December 1, 2015, and December 1, 2018.",
        "SQL": "WITH acs_2018 AS (\n  SELECT geo_id, unemployed_pop AS unemployed_2018  \n  FROM `bigquery-public-data.census_bureau_acs.county_2018_5yr` \n),\n \nacs_2015 AS (\n  SELECT geo_id, unemployed_pop AS unemployed_2015  \n  FROM `bigquery-public-data.census_bureau_acs.county_2015_5yr` \n),\n \nunemployed_change AS (\n  SELECT\n    u18.unemployed_2018, u18.geo_id, u15.unemployed_2015,\n    (u18.unemployed_2018 - u15.unemployed_2015) AS u_change\n  FROM acs_2018 u18\n  JOIN acs_2015 u15\n  ON u18.geo_id = u15.geo_id\n),\n \nduals_Jan_2018 AS (\n  SELECT Public_Total AS duals_2018, County_Name, FIPS \n  FROM `bigquery-public-data.sdoh_cms_dual_eligible_enrollment.dual_eligible_enrollment_by_county_and_program` \n  WHERE Date = '2018-12-01'\n),\n\nduals_Jan_2015 AS (\n  SELECT Public_Total AS duals_2015, County_Name, FIPS\n  FROM `bigquery-public-data.sdoh_cms_dual_eligible_enrollment.dual_eligible_enrollment_by_county_and_program` \n  WHERE Date = '2015-12-01'\n),\n\nduals_change AS (\n  SELECT\n    d18.FIPS, d18.County_Name, d18.duals_2018, d15.duals_2015,\n    (d18.duals_2018 - d15.duals_2015) AS total_duals_diff\n  FROM duals_Jan_2018 d18\n  JOIN duals_Jan_2015 d15\n  ON d18.FIPS = d15.FIPS\n),\n \ncorr_tbl AS (\n  SELECT unemployed_change.geo_id, duals_change.County_Name, unemployed_change.u_change, duals_change.total_duals_diff\n  FROM unemployed_change\n  JOIN duals_change\n  ON unemployed_change.geo_id = duals_change.FIPS\n)\n\n\nSELECT COUNT(*)\nFROM corr_tbl\nWHERE\nu_change >0\nAND\ncorr_tbl.total_duals_diff < 0",
        "external_knowledge": null,
        "plan": "1. Prepare datasets for 2015 and 2018 to capture unemployment figures and dual-eligible enrollee counts by county.\n2. Calculate the changes in unemployment and dual-eligible enrollee counts for each county between 2015 and 2018.\n3. Combine the datasets on county identifiers to align the changes in both metrics.\n4. Count the counties where unemployment increased and dual-eligible enrollee counts decreased during the specified period.",
        "special_function": [
            "date-functions/DATE"
        ]
    },
    {
        "instance_id": "bq066",
        "db": "bigquery-public-data.sdoh_bea_cainc30\nbigquery-public-data.sdoh_cdc_wonder_natality\nbigquery-public-data.sdoh_cms_dual_eligible_enrollment\nbigquery-public-data.sdoh_hrsa_shortage_areas\nbigquery-public-data.sdoh_hud_housing\nbigquery-public-data.sdoh_hud_pit_homelessness\nbigquery-public-data.sdoh_snap_enrollment\nbigquery-public-data.census_bureau_acs",
        "question": "Could you assess the relationship between the poverty rates from the previous year's census data and the percentage of births without maternal morbidity for the years 2016 to 2018? Use only data for births where no maternal morbidity was reported and for each year, use the 5-year census data from the year before to compute the Pearson correlation coefficient",
        "SQL": "WITH poverty_and_natality AS (\n  SELECT\n    EXTRACT(YEAR FROM n.Year) AS data_year,\n    p.geo_id AS county_fips,\n    (p.poverty / p.pop_determined_poverty_status) * 100 AS poverty_rate,\n    SUM(n.Births) AS total_births,\n    SUM(CASE WHEN n.Maternal_Morbidity_YN = 0 THEN n.Births ELSE 0 END) AS births_without_morbidity\n  FROM\n    `bigquery-public-data.census_bureau_acs.county_2015_5yr` p\n  JOIN\n    `bigquery-public-data.sdoh_cdc_wonder_natality.county_natality_by_maternal_morbidity` n\n  ON p.geo_id = n.County_of_Residence_FIPS\n  WHERE\n    p.pop_determined_poverty_status > 0 AND\n    EXTRACT(YEAR FROM n.Year) = 2016\n  GROUP BY\n    p.geo_id, p.poverty, p.pop_determined_poverty_status, EXTRACT(YEAR FROM n.Year)\n  UNION ALL\n  SELECT\n    EXTRACT(YEAR FROM n.Year) AS data_year,\n    p.geo_id AS county_fips,\n    (p.poverty / p.pop_determined_poverty_status) * 100 AS poverty_rate,\n    SUM(n.Births) AS total_births,\n    SUM(CASE WHEN n.Maternal_Morbidity_YN = 0 THEN n.Births ELSE 0 END) AS births_without_morbidity\n  FROM\n    `bigquery-public-data.census_bureau_acs.county_2016_5yr` p\n  JOIN\n    `bigquery-public-data.sdoh_cdc_wonder_natality.county_natality_by_maternal_morbidity` n\n  ON p.geo_id = n.County_of_Residence_FIPS\n  WHERE\n    p.pop_determined_poverty_status > 0 AND\n    EXTRACT(YEAR FROM n.Year) = 2017\n  GROUP BY\n    p.geo_id, p.poverty, p.pop_determined_poverty_status, EXTRACT(YEAR FROM n.Year)\n  UNION ALL\n  SELECT\n    EXTRACT(YEAR FROM n.Year) AS data_year,\n    p.geo_id AS county_fips,\n    (p.poverty / p.pop_determined_poverty_status) * 100 AS poverty_rate,\n    SUM(n.Births) AS total_births,\n    SUM(CASE WHEN n.Maternal_Morbidity_YN = 0 THEN n.Births ELSE 0 END) AS births_without_morbidity\n  FROM\n    `bigquery-public-data.census_bureau_acs.county_2017_5yr` p\n  JOIN\n    `bigquery-public-data.sdoh_cdc_wonder_natality.county_natality_by_maternal_morbidity` n\n  ON p.geo_id = n.County_of_Residence_FIPS\n  WHERE\n    p.pop_determined_poverty_status > 0 AND\n    EXTRACT(YEAR FROM n.Year) = 2018\n  GROUP BY\n    p.geo_id, p.poverty, p.pop_determined_poverty_status, EXTRACT(YEAR FROM n.Year)\n)\n\nSELECT\n  data_year,\n  CORR(poverty_rate, (births_without_morbidity / total_births) * 100) AS correlation_coefficient\nFROM\n  poverty_and_natality\nGROUP BY\n  data_year",
        "external_knowledge": null,
        "plan": "1.Extract morbidity and total birth counts for each county for 2018 from separate datasets.\n2.Calculate the percentage of maternal morbidity per county based on the birth data.\n3.Retrieve poverty rates from the 2017 ACS data, ensuring to match the data year closely to the morbidity data year.\n4.Merge the morbidity data with poverty data on county FIPS codes.\n5.Calculate the Pearson correlation coefficient between poverty rates and maternal morbidity percentages to assess their relationship.",
        "special_function": [
            "mathematical-functions/ROUND",
            "statistical-aggregate-functions/CORR"
        ]
    },
    {
        "instance_id": "bq114",
        "db": "bigquery-public-data.openaq\nbigquery-public-data.epa_historical_air_quality",
        "question": "What are the top three cities where the difference between the PM2.5 measurements in 1990 from the EPA and in 2020 from OpenAQ is the greatest, given that the locations are matched with latitude and longitude rounded to two decimal places?",
        "SQL": "SELECT\n  aq.city,\n  epa.arithmetic_mean,\n  aq.value,\n  aq.timestamp,\n  (epa.arithmetic_mean - aq.value)\nFROM\n  `bigquery-public-data.openaq.global_air_quality` AS aq\nJOIN\n  `bigquery-public-data.epa_historical_air_quality.air_quality_annual_summary` AS epa\nON\n  ROUND(aq.latitude, 2) = ROUND(epa.latitude, 2)\n  AND ROUND(aq.longitude, 2) = ROUND(epa.longitude, 2)\nWHERE\n  epa.units_of_measure = \"Micrograms/cubic meter (LC)\"\n  AND epa.parameter_name = \"Acceptable PM2.5 AQI & Speciation Mass\"\n  AND epa.year = 1990\n  AND aq.pollutant = \"pm25\"\n  AND EXTRACT(YEAR FROM aq.timestamp) = 2020\nORDER BY\n  (epa.arithmetic_mean - aq.value) DESC\nLIMIT 3",
        "external_knowledge": null,
        "plan": "Which city in the US has improved its air quality (PM2.5) the most since 1990?\nThis query joins the EPA Historical Air Quality dataset with OpenAQ to retrieve the cities in the US with the most improved PM2.5 concentrations since 1990.",
        "special_function": [
            "mathematical-functions/ROUND"
        ]
    },
    {
        "instance_id": "bq116",
        "db": "bigquery-public-data.sec_quarterly_financials",
        "question": "What was the highest annual revenue in billions of dollars reported by a U.S. state in 2016, across the main revenue categories and covering all four quarters?",
        "SQL": "SELECT\n  SUM(QuickSummary.revenue) / 1e9 AS revenue_per_state_in_billions\nFROM (\n  SELECT\n    submission_number,\n    MAX(value) AS revenue\n  FROM `bigquery-public-data.sec_quarterly_financials.quick_summary`\n  WHERE\n    measure_tag IN ('Revenues', 'SalesRevenueNet',\n                    'SalesRevenueGoodsNet')\n    AND fiscal_year = 2016\n    AND fiscal_period_focus = 'FY'\n    AND number_of_quarters = 4\n  GROUP BY\n    submission_number) QuickSummary\n INNER JOIN (\n   SELECT\n     submission_number,\n     MAX(stprba) AS state\n   FROM\n     `bigquery-public-data.sec_quarterly_financials.submission`\n   WHERE\n     stprba IS NOT NULL\n     AND stprba != ''\n     AND countryba = 'US'\n   GROUP BY\n     submission_number) Submission\n ON\n   QuickSummary.submission_number = Submission.submission_number\nGROUP BY\n  state\nORDER BY\n  revenue_per_state_in_billions DESC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "Total company revenue per State in 2016:\nOther information about companies can be found in the SEC dataset as well. For example the State or Province of the companies mailing address can be found if the company is located in either the US or Canada. Using this information it is possible to compute the total revenues that companies in each state managed to achieve in a given year. For the year 2016 this example query will rank states by total corporate revenues.",
        "special_function": null
    },
    {
        "instance_id": "bq015",
        "db": "bigquery-public-data.stackoverflow\nfh-bigquery.hackernews",
        "question": "Rank the top 10 most discussed tags on Stack Overflow questions that were mentioned on Hacker News since 2014.",
        "SQL": "SELECT \n    tag, \n    SUM(c) AS c \nFROM (\n    SELECT \n        CONCAT('stackoverflow.com/questions/', CAST(b.id AS STRING)) AS url,\n        title, \n        c, \n        answer_count, \n        favorite_count, \n        view_count, \n        score, \n        SPLIT(tags, '|') AS tags \n    FROM \n        `bigquery-public-data.stackoverflow.posts_questions` AS a \n    JOIN (\n        SELECT \n            CAST(REGEXP_EXTRACT(text, r'stackoverflow.com/questions/([0-9]+)/') AS INT64) AS id,\n            COUNT(*) AS c \n        FROM \n            `fh-bigquery.hackernews.comments` \n        WHERE \n            text LIKE '%stackoverflow.com/questions/%' \n            AND EXTRACT(YEAR FROM time_ts) >= 2014 \n        GROUP BY \n            1 \n        ORDER BY \n            2 DESC\n    ) AS b ON a.id = b.id\n), UNNEST(tags) AS tag \nGROUP BY \n    1 \nORDER BY \n    2 DESC \nLIMIT 10",
        "external_knowledge": null,
        "plan": "1.Pull details from Stack Overflow questions that match with Hacker News comments based on question IDs, focusing on discussions from 2014 onwards. \n2.Break down and count how often each tag associated with these questions gets mentioned. \n3.Then, list the top 10 tags that came up most frequently.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "json-functions/STRING",
            "string-functions/CONCAT",
            "string-functions/REGEXP_EXTRACT",
            "string-functions/SPLIT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/STRING",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq041",
        "db": "bigquery-public-data.stackoverflow",
        "question": "What are the monthly statistics for new StackOverflow users created in 2021, including the percentage of new users who asked questions and the percentage of those who asked questions and then answered questions within their first 30 days?",
        "SQL": "DECLARE yr, conversion_window INT64;\nSET (yr, conversion_window) = (2021, 30);\n\nWITH users AS (\n  SELECT *\n  FROM `bigquery-public-data.stackoverflow.users`\n  WHERE EXTRACT(YEAR FROM creation_date) = yr\n),\n\nusers_questions AS (\n  SELECT \n    u.display_name, \n    u.id AS user_id, \n    u.creation_date AS signup, \n    COUNT(q.id) AS questions, \n    MIN(q.creation_date) AS first_question\n  FROM users u\n  LEFT JOIN `bigquery-public-data.stackoverflow.posts_questions` q \n    ON q.owner_user_id = u.id \n    AND DATE_DIFF(q.creation_date, u.creation_date, DAY) <= conversion_window\n  GROUP BY \n    u.display_name, \n    u.id, \n    u.creation_date\n),\n\nusers_questions_answers AS (\n  SELECT \n    display_name, \n    user_id, \n    signup, \n    questions, \n    first_question, \n    COUNT(a.id) AS answers_after_question\n  FROM users_questions uq\n  LEFT JOIN `bigquery-public-data.stackoverflow.posts_answers` a \n    ON a.owner_user_id = uq.user_id \n    AND a.creation_date > uq.first_question\n    AND DATE_DIFF(a.creation_date, uq.first_question, DAY) <= conversion_window\n  GROUP BY \n    display_name, \n    user_id, \n    signup, \n    questions, \n    first_question\n)\n\nSELECT \n  EXTRACT(MONTH FROM signup) AS month,\n  COUNT(user_id) AS new_users,\n  COUNT(DISTINCT CASE WHEN questions > 0 THEN user_id ELSE NULL END) AS asked,\n  ROUND(COUNT(DISTINCT CASE WHEN questions > 0 THEN user_id ELSE NULL END) / COUNT(user_id) * 100, 2) AS pct_asked,\n  COUNT(DISTINCT CASE WHEN answers_after_question > 0 THEN user_id ELSE NULL END) AS then_answered,\n  ROUND(COUNT(DISTINCT CASE WHEN answers_after_question > 0 THEN user_id ELSE NULL END) / COUNT(user_id) * 100, 2) AS pct_then_answered\nFROM users_questions_answers\nGROUP BY \n  EXTRACT(MONTH FROM signup)\nORDER BY \n  month ASC;",
        "external_knowledge": null,
        "plan": "1. **Initialize Variables**: Define the year of interest and the period (in days) for tracking user activity after sign-up.\n\n2. **Filter Users by Year**: Retrieve all users who registered in the specified year.\n\n3. **Join and Aggregate User Questions**:\n   - Join the filtered users with their questions posted within the specified period after sign-up.\n   - Count the number of questions each user asked within this period.\n   - Identify the date of their first question within this period.\n\n4. **Join and Aggregate User Answers**:\n   - Further join the result with answers posted by these users.\n   - Count the number of answers each user provided after their first question within the specified period.\n\n5. **Calculate Monthly Statistics**:\n   - Extract the month from each user\u2019s sign-up date.\n   - Count the total number of new users each month.\n   - Count the number of users who asked at least one question within the specified period.\n   - Calculate the percentage of users who asked questions.\n   - Count the number of users who asked questions and then answered within the specified period.\n   - Calculate the percentage of users who asked questions and then answered.\n\n6. **Aggregate and Order Results**:\n   - Group the statistics by month.\n   - Order the results chronologically by month.",
        "special_function": [
            "date-functions/DATE_DIFF",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "mathematical-functions/ROUND",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "conditional-functions/CASE",
            "other-functions/SET"
        ]
    },
    {
        "instance_id": "bq121",
        "db": "bigquery-public-data.stackoverflow",
        "question": "How do the average reputation and number of badges vary among Stack Overflow users based on the number of complete years they have been members, considering only those who joined on or before October 1, 2021?",
        "SQL": "SELECT User_Tenure,\n       COUNT(1) AS Num_Users,\n       AVG(reputation) AS Avg_Reputation,\n       AVG(num_badges) AS Avg_Num_Badges\nFROM (\n  SELECT users.id AS user,\n         DATE_DIFF(DATE('2021-10-01'), DATE(ANY_VALUE(users.creation_date)), YEAR) AS user_tenure,\n         ANY_VALUE(users.reputation) AS reputation,\n         SUM(IF(badges.user_id IS NULL, 0, 1)) AS num_badges\n  FROM `bigquery-public-data.stackoverflow.users` users\n  LEFT JOIN `bigquery-public-data.stackoverflow.badges` badges\n  ON users.id = badges.user_id\n  WHERE DATE(users.creation_date) <= DATE('2021-10-01')\n  GROUP BY user\n)\nGROUP BY User_Tenure\nORDER BY User_Tenure",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "What is the reputation and badge count of users across different tenures on StackOverflow?\nThis query breaks down Stack Overflow users into different cohorts by the number of years they\u2019ve been on the platform, and computes the average reputation and number of badges for each cohort. It\u2019s not surprising that users who have been on StackOverflow longer would have higher reputation and number of badges on average. In addition, it\u2019s interesting to see that users typically only begin to have multiple badges after two years on StackOverflow.",
        "special_function": [
            "aggregate-functions/ANY_VALUE",
            "mathematical-functions/ROUND",
            "timestamp-functions/CURRENT_TIMESTAMP",
            "timestamp-functions/TIMESTAMP_DIFF",
            "conditional-functions/IF"
        ]
    },
    {
        "instance_id": "bq123",
        "db": "bigquery-public-data.stackoverflow",
        "question": "Which day of the week has the third highest percentage of questions answered within an hour? Please tell me the day along with the percentage.",
        "SQL": "WITH first_answers AS (\n  SELECT\n    parent_id AS question_id,\n    MIN(creation_date) AS first_answer_date\n  FROM\n    `bigquery-public-data.stackoverflow.posts_answers`\n  GROUP BY\n    parent_id\n)\n\nSELECT\n  FORMAT_DATE('%A', DATE(q.creation_date)) AS question_day,\n  SUM(CASE WHEN f.first_answer_date IS NOT NULL AND TIMESTAMP_DIFF(f.first_answer_date, q.creation_date, MINUTE) <= 60 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS percent_questions\nFROM\n  `bigquery-public-data.stackoverflow.posts_questions` q\nLEFT JOIN\n  first_answers f\nON\n  q.id = f.question_id\nGROUP BY\n  question_day\nORDER BY\n  percent_questions DESC\nLIMIT 1 OFFSET 2",
        "external_knowledge": null,
        "plan": "Which day of the week has most questions answered within an hour?\nIn this query, we find the best day of the week to ask questions to get an answer very quickly. The query returns day of the week as integers from 1 to 7 (1 = Sunday, 2 = Monday, etc), and the number of questions and answers on each day. We also query how many of these questions received an answer within 1 hour of submission, and the corresponding percentage. The volume of questions and answers is the highest in the middle of the week (Tue, Wed, and Thur), but questions are answered within 1 hour is higher on Saturdays and Sundays",
        "special_function": [
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "mathematical-functions/ROUND",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/TIMESTAMP_ADD"
        ]
    },
    {
        "instance_id": "bq280",
        "db": "bigquery-public-data.stackoverflow",
        "question": "Please provide the display name of the user who has answered the most questions on Stack Overflow, considering only users with a reputation greater than 10.",
        "SQL": "WITH UserAnswers AS (\n  SELECT\n    owner_user_id AS answer_owner_id,\n    COUNT(id) AS answer_count\n  FROM bigquery-public-data.stackoverflow.posts_answers\n  WHERE owner_user_id IS NOT NULL\n  GROUP BY owner_user_id\n),\nDetailedUsers AS (\n  SELECT\n    id AS user_id,\n    display_name AS user_display_name,\n    reputation\n  FROM bigquery-public-data.stackoverflow.users\n  WHERE display_name IS NOT NULL AND reputation > 10\n),\nRankedUsers AS (\n  SELECT\n    u.user_display_name,\n    u.reputation,\n    a.answer_count,\n    ROW_NUMBER() OVER (ORDER BY a.answer_count DESC) AS rank\n  FROM DetailedUsers u\n  JOIN UserAnswers a ON u.user_id = a.answer_owner_id\n)\nSELECT\n  user_display_name,\nFROM RankedUsers\nWHERE rank = 1;",
        "external_knowledge": null,
        "plan": "1. Calculate the total number of answers each user has posted and filter out entries where the user ID is not available. \n2. Group the results by user ID to ensure each user is represented once with their total answer count.\n3. Select users who have both a visible display name and a reputation greater than 10.\n4. Combine the user-specific data (display names and reputation) with their corresponding answer counts into a single dataset.\n5. Implement a ranking system within the combined dataset based on the number of answers each user has posted. \n6. Order the users in descending order of answer count.\n7. Extract and return the display name of the top-ranked user.\n",
        "special_function": [
            "numbering-functions/RANK",
            "numbering-functions/ROW_NUMBER"
        ]
    },
    {
        "instance_id": "bq300",
        "db": "bigquery-public-data.stackoverflow",
        "question": "What is the highest number of answers received for a single Python 2 specific question on Stack Overflow, excluding any discussions that involve Python 3?",
        "SQL": "WITH\n  python2_questions AS (\n    SELECT\n      q.id AS question_id,\n      q.title,\n      q.body AS question_body,\n      q.tags\n    FROM\n      `bigquery-public-data.stackoverflow.posts_questions` q\n    WHERE\n      (LOWER(q.tags) LIKE '%python-2%'\n      OR LOWER(q.tags) LIKE '%python-2.x%'\n      OR (\n        LOWER(q.title) LIKE '%python 2%'\n        OR LOWER(q.body) LIKE '%python 2%'\n        OR LOWER(q.title) LIKE '%python2%'\n        OR LOWER(q.body) LIKE '%python2%'\n      ))\n      AND (\n        LOWER(q.title) NOT LIKE '%python 3%'\n        AND LOWER(q.body) NOT LIKE '%python 3%'\n        AND LOWER(q.title) NOT LIKE '%python3%'\n        AND LOWER(q.body) NOT LIKE '%python3%'\n      )\n  )\n\nSELECT\n  COUNT(*) AS count_number\nFROM\n  python2_questions q\nLEFT JOIN\n  `bigquery-public-data.stackoverflow.posts_answers` a\nON\n  q.question_id = a.parent_id\nGROUP BY q.question_id\nORDER BY count_number DESC\nLIMIT 1",
        "external_knowledge": null,
        "plan": "1. **Filter Questions Related to Python 2:**\n   - Create a temporary dataset of questions that are specific to Python 2 by checking if they contain specific tags or keywords related to Python 2.\n   - Ensure the selected questions do not mention Python 3 by excluding any questions with keywords or tags related to Python 3.\n\n2. **Join with Answers Table:**\n   - Merge the filtered dataset of Python 2 specific questions with the answers dataset, linking questions with their corresponding answers using a common identifier.\n\n3. **Count the Number of Answers:**\n   - For each question in the merged dataset, count the number of associated answers.\n\n4. **Sort and Limit Results:**\n   - Sort the counted results in descending order to identify the question with the highest number of answers.\n   - Limit the final output to just one result, which will be the highest count of answers received for a single Python 2 specific question.\n\n5. **Return the Result:**\n   - Output the highest count of answers as the final result.",
        "special_function": [
            "string-functions/LOWER"
        ]
    },
    {
        "instance_id": "bq301",
        "db": "bigquery-public-data.stackoverflow",
        "question": "Retrieve details of accepted answers related to JavaScript security topics such as XSS, cross-site scripting, exploits, and cybersecurity, for questions posted in January 2016 on Stack Overflow. For each accepted answer, include the answer's ID, the answerer's reputation, score, and comment count, along with the associated question's tags, score, answer count, the asker's reputation, view count, and comment count.",
        "SQL": "SELECT\n    answer.id AS a_id,\n    (SELECT users.reputation FROM `bigquery-public-data.stackoverflow.users` users\n        WHERE users.id = answer.owner_user_id) AS a_user_reputation,\n    answer.score AS a_score,\n    answer.comment_count AS answer_comment_count,\n    questions.tags as q_tags,\n    questions.score AS q_score,  \n    questions.answer_count AS answer_count, \n    (SELECT users.reputation FROM `bigquery-public-data.stackoverflow.users` users\n        WHERE users.id = questions.owner_user_id) AS q_user_reputation,\n    questions.view_count AS q_view_count,\n    questions.comment_count AS q_comment_count\nFROM\n   `bigquery-public-data.stackoverflow.posts_answers` AS answer \nLEFT JOIN\n   `bigquery-public-data.stackoverflow.posts_questions` AS questions\n      ON answer.parent_id = questions.id\nWHERE\n    answer.id = questions.accepted_answer_id\n    AND \n    (\n        questions.tags LIKE '%javascript%' AND\n        (questions.tags LIKE '%xss%' OR\n        questions.tags LIKE '%cross-site%' OR\n        questions.tags LIKE '%exploit%' OR\n        questions.tags LIKE '%cybersecurity%')\n    )\n    AND DATE(questions.creation_date) BETWEEN '2016-01-01' AND '2016-01-31'\n    AND DATE(answer.creation_date) BETWEEN '2016-01-01' AND '2016-01-31'",
        "external_knowledge": null,
        "plan": "1. **Select Relevant Answer Details**: Extract key details of each answer, including the answer ID, the score of the answer, and the number of comments on the answer.\n\n2. **Fetch Answerer's Reputation**: For each answer, retrieve the reputation of the user who posted the answer by matching the user ID of the answer owner.\n\n3. **Select Associated Question Details**: For each answer, fetch the details of the associated question, including the tags, the score of the question, the number of answers to the question, the view count, and the number of comments on the question.\n\n4. **Fetch Asker's Reputation**: For each question, retrieve the reputation of the user who posted the question by matching the user ID of the question owner.\n\n5. **Join Answers with Questions**: Link each answer to its associated question using the question's ID and the parent ID of the answer.\n\n6. **Filter for Accepted Answers**: Ensure that only answers which have been accepted as the best answer for their respective questions are included.\n\n7. **Filter by Tags**: Ensure that the associated questions have tags related to JavaScript security topics, specifically those containing keywords like JavaScript, XSS, cross-site scripting, exploits, or cybersecurity.\n\n8. **Filter by Date**: Ensure that the questions were posted within January 2016 and that the corresponding answers were also posted within January 2016.\n\n9. **Compile Results**: Combine all the retrieved information into a single result set, including the answer's ID, answerer's reputation, answer's score, answer's comment count, question's tags, question's score, question's answer count, asker's reputation, question's view count, and question's comment count.",
        "special_function": [
            "date-functions/DATE"
        ]
    },
    {
        "instance_id": "bq302",
        "db": "bigquery-public-data.stackoverflow",
        "question": "What is the monthly proportion of Stack Overflow questions tagged with 'python' in the year 2022?",
        "SQL": "WITH\n-- Get recent data\nRecentData AS (\n    SELECT\n        FORMAT_TIMESTAMP('%Y%m', creation_date) AS month_index,\n        tags\n    FROM\n        `bigquery-public-data.stackoverflow.posts_questions`\n    WHERE\n        EXTRACT(YEAR FROM DATE(creation_date)) = 2022\n),\n\n-- Monthly number of questions posted\nMonthlyQuestions AS (\n    SELECT\n        month_index,\n        COUNT(*) AS num_questions\n    FROM\n        RecentData\n    GROUP BY\n        month_index\n),\n\n-- Monthly number of questions posted with specific tags\nTaggedQuestions AS (\n    SELECT\n        month_index,\n        tag,\n        COUNT(*) AS num_tags\n    FROM\n        RecentData,\n        UNNEST(SPLIT(tags, '|')) AS tag\n    WHERE\n        tag IN ('python')\n    GROUP BY\n        month_index, tag\n)\n\nSELECT\n    a.month_index,\n    a.num_tags / b.num_questions AS proportion\nFROM\n    TaggedQuestions a\nLEFT JOIN\n    MonthlyQuestions b ON a.month_index = b.month_index\nORDER BY\n    a.month_index, proportion DESC;",
        "external_knowledge": null,
        "plan": "1. **Filter Data by Year**: Extract data for the specific year of interest, filtering out all records that do not match the specified year.\n\n2. **Format Creation Date**: Transform the creation date of each record into a month-based index to facilitate monthly aggregation.\n\n3. **Aggregate Monthly Questions**: Calculate the total number of questions posted each month by grouping the data based on the month index and counting the records.\n\n4. **Extract and Count Specific Tags**: Split the tags associated with each question into individual tags, filter for the specific tag of interest, and count the occurrences of this tag for each month.\n\n5. **Calculate Proportion**: Combine the monthly counts of all questions and the counts of questions with the specific tag. Compute the proportion of questions with the specific tag by dividing the count of tagged questions by the total number of questions for each month.\n\n6. **Sort Results**: Order the final results by the month index to present the data chronologically.",
        "special_function": [
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "string-functions/SPLIT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/FORMAT_TIMESTAMP",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq303",
        "db": "bigquery-public-data.stackoverflow",
        "question": "What are the user IDs and tags for comments, answers, and questions posted by users with IDs between 16712208 and 18712208 on Stack Overflow during July to December 2019?",
        "SQL": "SELECT u_id, tags\nFROM (\n    -- select comments with tags from the post\n    SELECT cm.u_id, cm.creation_date, cm.text, pq.tags, \"comment\" as type\n    FROM (\n            SELECT a.parent_id as q_id, c.user_id as u_id, c.creation_date as creation_date, c.text as text\n            FROM `bigquery-public-data.stackoverflow.comments` as c\n            INNER JOIN `bigquery-public-data.stackoverflow.posts_answers` as a ON (a.id = c.post_id)\n            WHERE c.user_id BETWEEN 16712208 AND 18712208\n              AND DATE(c.creation_date) BETWEEN '2019-07-01' AND '2019-12-31'\n            \n            UNION ALL \n            \n            SELECT q.id as q_id, c.user_id as u_id, c.creation_date as creation_date, c.text as text\n            FROM `bigquery-public-data.stackoverflow.comments` as c\n            INNER JOIN `bigquery-public-data.stackoverflow.posts_questions` as q ON (q.id = c.post_id)\n            WHERE c.user_id BETWEEN 16712208 AND 18712208\n              AND DATE(c.creation_date) BETWEEN '2019-07-01' AND '2019-12-31'\n        ) as cm\n    INNER JOIN `bigquery-public-data.stackoverflow.posts_questions` as pq ON (pq.id = cm.q_id)\n        \n    UNION ALL\n    -- select answers with tags related to the post\n    SELECT pa.owner_user_id as u_id, pa.creation_date as creation_date, pa.body as text, pq.tags as tags, \"answer\" as type\n    FROM `bigquery-public-data.stackoverflow.posts_answers` as pa\n    LEFT OUTER JOIN `bigquery-public-data.stackoverflow.posts_questions` as pq ON pq.id = pa.parent_id\n    WHERE pa.owner_user_id BETWEEN 16712208 AND 18712208\n      AND DATE(pa.creation_date) BETWEEN '2019-07-01' AND '2019-12-31'\n    \n    UNION ALL\n    -- select posts\n    SELECT pq.owner_user_id as u_id, pq.creation_date as creation_date, pq.body as text, pq.tags as tags, \"question\" as type\n    FROM `bigquery-public-data.stackoverflow.posts_questions` as pq\n    WHERE pq.owner_user_id BETWEEN 16712208 AND 18712208\n      AND DATE(pq.creation_date) BETWEEN '2019-07-01' AND '2019-12-31'\n)\nORDER BY u_id, creation_date;",
        "external_knowledge": null,
        "plan": "1. **Filter Comments on Answers**:\n   - Retrieve comments made by users within the specified ID range.\n   - Ensure the comments are posted within the given date range.\n   - Identify the original question associated with each comment by joining with answers.\n\n2. **Filter Comments on Questions**:\n   - Retrieve comments made by users within the specified ID range.\n   - Ensure the comments are posted within the given date range.\n   - Identify the original question associated with each comment by joining with questions.\n\n3. **Combine Comment Results**:\n   - Union the results of comments on answers and questions.\n   - For each combined comment result, include the user ID, creation date, comment text, tags from the associated question, and label it as a comment.\n\n4. **Filter Answers**:\n   - Retrieve answers made by users within the specified ID range.\n   - Ensure the answers are posted within the given date range.\n   - Join with questions to get the tags associated with each answer.\n   - Include the user ID, creation date, answer text, tags from the associated question, and label it as an answer.\n\n5. **Filter Questions**:\n   - Retrieve questions made by users within the specified ID range.\n   - Ensure the questions are posted within the given date range.\n   - Include the user ID, creation date, question text, tags, and label it as a question.\n\n6. **Combine All Results**:\n   - Union the results of comments, answers, and questions.\n   - Select the relevant fields: user ID and tags.\n\n7. **Order Results**:\n   - Sort the combined results by user ID and creation date to maintain chronological order.",
        "special_function": [
            "date-functions/DATE",
            "string-functions/LEFT"
        ]
    },
    {
        "instance_id": "bq304",
        "db": "bigquery-public-data.stackoverflow",
        "question": "What are the top 50 most viewed 'how' questions for each of the following Android-related tags on StackOverflow: 'android-layout', 'android-activity', 'android-intent', 'android-edittext', 'android-fragments', 'android-recyclerview', 'listview', 'android-actionbar', 'google-maps', and 'android-asynctask'? Ensure that each tag has at least 50 questions and exclude any questions containing terms typically associated with troubleshooting, such as 'fail', 'problem', 'error', 'wrong', 'fix', 'bug', 'issue', 'solve', or 'trouble'.",
        "SQL": "WITH\ntags_to_use AS (\n    SELECT tag, idx\n    FROM UNNEST([\n        'android-layout', \n        'android-activity', \n        'android-intent', \n        'android-edittext', \n        'android-fragments', \n        'android-recyclerview', \n        'listview', \n        'android-actionbar', \n        'google-maps', \n        'android-asynctask'\n    ]) AS tag WITH OFFSET idx\n),\nandroid_how_to_questions AS (\n    SELECT\n        PQ.*\n    FROM\n        bigquery-public-data.stackoverflow.posts_questions PQ\n    WHERE\n        EXISTS (\n            SELECT 1\n            FROM UNNEST(SPLIT(PQ.tags, '|')) tag\n            WHERE tag IN (SELECT tag FROM tags_to_use)\n        )\n        AND (LOWER(PQ.title) LIKE '%how%' OR LOWER(PQ.body) LIKE '%how%')\n        AND NOT (LOWER(PQ.title) LIKE '%fail%' OR LOWER(PQ.title) LIKE '%problem%' OR LOWER(PQ.title) LIKE '%error%'\n                 OR LOWER(PQ.title) LIKE '%wrong%' OR LOWER(PQ.title) LIKE '%fix%' OR LOWER(PQ.title) LIKE '%bug%'\n                 OR LOWER(PQ.title) LIKE '%issue%' OR LOWER(PQ.title) LIKE '%solve%' OR LOWER(PQ.title) LIKE '%trouble%')\n        AND NOT (LOWER(PQ.body) LIKE '%fail%' OR LOWER(PQ.body) LIKE '%problem%' OR LOWER(PQ.body) LIKE '%error%'\n                 OR LOWER(PQ.body) LIKE '%wrong%' OR LOWER(PQ.body) LIKE '%fix%' OR LOWER(PQ.body) LIKE '%bug%'\n                 OR LOWER(PQ.body) LIKE '%issue%' OR LOWER(PQ.body) LIKE '%solve%' OR LOWER(PQ.body) LIKE '%trouble%')\n),\nquestions_with_tag_rankings AS (\n    SELECT\n        T.id AS tag_id,\n        TTU.idx AS tag_offset,\n        T.tag_name,\n        T.wiki_post_id AS tag_wiki_post_id,\n        Q.id AS question_id,\n        Q.title,\n        Q.tags,\n        Q.view_count,\n        RANK() OVER (PARTITION BY T.id ORDER BY Q.view_count DESC) AS question_view_count_rank,\n        COUNT(*) OVER (PARTITION BY T.id) AS total_valid_questions\n    FROM\n        bigquery-public-data.stackoverflow.tags T\n    INNER JOIN\n        tags_to_use TTU ON T.tag_name = TTU.tag\n    INNER JOIN\n        android_how_to_questions Q ON T.tag_name IN UNNEST(SPLIT(Q.tags, '|'))\n)\nSELECT\n    question_id\nFROM\n    questions_with_tag_rankings\nWHERE\n    question_view_count_rank <= 50 AND total_valid_questions >= 50\nORDER BY\n    tag_offset ASC, question_view_count_rank ASC;",
        "external_knowledge": null,
        "plan": "1. **Define Relevant Tags**:\n   - Create a list of specific Android-related tags that are of interest.\n   \n2. **Filter Questions by Tags and Content**:\n   - Select questions that include any of the specified tags.\n   - Ensure these questions contain 'how' in the title or body.\n   - Exclude questions that contain troubleshooting terms like 'fail', 'problem', 'error', 'wrong', 'fix', 'bug', 'issue', 'solve', or 'trouble' in the title or body.\n\n3. **Rank Questions by Tag**:\n   - For each tag, rank the filtered questions based on their view count.\n   - Ensure each tag has at least 50 valid questions.\n\n4. **Select Top Questions**:\n   - For each tag, select the top 50 questions based on the view count ranking.\n\n5. **Order the Results**:\n   - Order the final list of question IDs first by the tag's position in the original list and then by the view count ranking within each tag.",
        "special_function": [
            "numbering-functions/RANK",
            "string-functions/LOWER",
            "string-functions/SPLIT",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq310",
        "db": "bigquery-public-data.stackoverflow",
        "question": "What is the title of the most viewed \"how\" question related to Android development on StackOverflow, across specified tags such as 'android-layout', 'android-activity', 'android-intent', and others",
        "SQL": "WITH\ntags_to_use AS (\n    SELECT tag, idx\n    FROM UNNEST([\n        'android-layout', \n        'android-activity', \n        'android-intent', \n        'android-edittext', \n        'android-fragments', \n        'android-recyclerview', \n        'listview', \n        'android-actionbar', \n        'google-maps', \n        'android-asynctask'\n    ]) AS tag WITH OFFSET idx\n),\nandroid_how_to_questions AS (\n    SELECT\n        PQ.*\n    FROM\n        `bigquery-public-data.stackoverflow.posts_questions` PQ\n    WHERE\n        EXISTS (\n            SELECT 1\n            FROM UNNEST(SPLIT(PQ.tags, '|')) tag\n            WHERE tag IN (SELECT tag FROM tags_to_use)\n        )\n        AND (LOWER(PQ.title) LIKE '%how%' OR LOWER(PQ.body) LIKE '%how%')\n),\nmost_viewed_question AS (\n    SELECT\n        T.id AS tag_id,\n        T.tag_name,\n        Q.id AS question_id,\n        Q.title,\n        Q.tags,\n        Q.view_count\n    FROM\n        `bigquery-public-data.stackoverflow.tags` T\n    INNER JOIN\n        tags_to_use TTU ON T.tag_name = TTU.tag\n    INNER JOIN\n        android_how_to_questions Q ON T.tag_name IN UNNEST(SPLIT(Q.tags, '|'))\n    ORDER BY Q.view_count DESC\n    LIMIT 1\n)\nSELECT\n    title\nFROM\n    most_viewed_question;",
        "external_knowledge": null,
        "plan": "1. **Create a List of Relevant Tags**:\n    - Define a list of tags related to Android development that are of interest.\n\n2. **Filter Questions by Tags and Content**:\n    - From the database of questions, filter out those that contain any of the specified tags.\n    - Additionally, ensure that these questions include the keyword \"how\" in either their title or body to identify \"how-to\" questions.\n\n3. **Identify the Most Viewed Question**:\n    - Join the relevant tags with the filtered \"how-to\" questions to ensure we are focusing on questions that match our tags of interest.\n    - Sort these questions by their view count in descending order to identify the most viewed one.\n\n4. **Retrieve the Title of the Most Viewed Question**:\n    - Select the title of the question that has the highest view count from the sorted list.\n    - Limit the result to the top-most entry to get only the title of the single most viewed question.\n\nBy following these steps, you can determine the title of the most viewed \"how-to\" question related to Android development within the specified tags.",
        "special_function": [
            "string-functions/LOWER",
            "string-functions/SPLIT",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq305",
        "db": "bigquery-public-data.stackoverflow",
        "question": "Identify the top 10 users by the total view count of their associated questions. Include users who own a question, provide an accepted answer, have an answer with a score above 5, rank in the top 3 for a question, or have an answer with a score over 20% of the total answer score for that question. Use these criteria to determine the questions and answers to include in the view count calculation.",
        "SQL": "select\n       sum(ifnull(p.view_count,0)) as reached\n     , rp.owner_user_id\nfrom `bigquery-public-data.stackoverflow.posts_questions` p\ninner join (\n    \n    (Select id, owner_user_id \n    From `bigquery-public-data.stackoverflow.posts_questions` \n    Where post_type_id = 1 \n    and owner_user_id is not null)\n  \n    Union DISTINCT\n    \n    (Select parent_id, owner_user_id \n     From `bigquery-public-data.stackoverflow.posts_answers`\n     Where post_type_id = 2\n     And id in (select accepted_answer_id from `bigquery-public-data.stackoverflow.posts_questions`)\n     and owner_user_id is not null)\n    \n    Union DISTINCT\n    \n    (Select parent_id, owner_user_id\n    From `bigquery-public-data.stackoverflow.posts_answers`\n   Where post_type_id = 2\n     And score > 5\n     and owner_user_id is not null)\n     \n    Union DISTINCT\n    \n    (Select a.parent_id, a.owner_user_id\n    From `bigquery-public-data.stackoverflow.posts_answers` a\n         Join `bigquery-public-data.stackoverflow.posts_questions` q On a.parent_id = q.id\n   Where a.post_type_id = 2\n     And a.score > 0.20 * (select sum(score) from `bigquery-public-data.stackoverflow.posts_answers` where parent_id=q.id)\n     And a.score > 0\n     and a.owner_user_id is not null)\n     \n    Union DISTINCT\n    \n    (Select x.parent_id, x.owner_user_id \n    From (Select a.parent_id , a.owner_user_id, Rank() Over(Partition By a.parent_id, a.owner_user_id Order By ta.score Desc) AnswerRank\n            From `bigquery-public-data.stackoverflow.posts_answers` a\n                 Join `bigquery-public-data.stackoverflow.posts_answers` ta On ta.parent_id = a.parent_id\n           Where a.post_type_id = 2\n             And a.score > 0\n             and a.owner_user_id is not null\n          ) x\n          Where AnswerRank <= 3)\n    \n   )\nrp on rp.id = p.id\ngroup by rp.owner_user_id\norder by sum(ifnull(p.view_count,0)) desc\nLIMIT 10",
        "external_knowledge": null,
        "plan": "1. **Initialize the View Count Calculation**:\n   - Begin with an initial setup to sum the view counts for questions.\n\n2. **Identify Relevant Users**:\n   - Establish a list of users who are relevant for the calculation based on various criteria.\n\n3. **Criteria for User Inclusion**:\n   - **Question Owners**: Include users who own a question.\n   - **Accepted Answer Providers**: Include users whose answers have been accepted.\n   - **High Score Answer Providers**: Include users whose answers have a score above a certain threshold.\n   - **Top Ranked Answer Providers**: Include users whose answers rank within the top 3 for a question.\n   - **Significant Score Contribution Providers**: Include users whose answers contribute significantly (over 20%) to the total score of answers for a question.\n\n4. **Combine All Relevant Users**:\n   - Use a union operation to merge all identified users from the different criteria into a single list, ensuring each user is only included once.\n\n5. **Calculate View Counts for Questions**:\n   - Join the list of relevant users with the questions they are associated with to sum up the view counts for these questions.\n\n6. **Group and Sum View Counts by User**:\n   - Group the summed view counts by user to get the total view count each user has reached through their questions.\n\n7. **Order and Limit the Results**:\n   - Sort the users by their total view counts in descending order to identify the top users.\n   - Limit the result to the top 10 users.\n\nBy following these steps, the query accurately identifies the top users based on the total view count of their associated questions, using the specified criteria.",
        "special_function": [
            "numbering-functions/RANK",
            "conditional-functions/IFNULL"
        ]
    },
    {
        "instance_id": "bq306",
        "db": "bigquery-public-data.stackoverflow",
        "question": "Identify the top 10 tags for user 1908967 by calculating a reputation score based on upvotes and accepted answers before June 7, 2018. The score is calculated as 10 times the upvotes plus 15 times the accepted answers.",
        "SQL": "SELECT\n  votes.tag AS vote_tag,\n  (votes.count * 10 + accepts.count * 15) AS reputation,\n  votes.count AS vote_count,\n  accepts.count AS accept_count\nFROM (\n  SELECT\n    tag,\n    COUNT(tag) AS count\n  FROM (\n    SELECT SPLIT(q.tags, '|') AS tag\n    FROM `bigquery-public-data.stackoverflow.votes` v\n      LEFT JOIN `bigquery-public-data.stackoverflow.posts_answers` a\n        ON v.post_id = a.id\n      LEFT JOIN `bigquery-public-data.stackoverflow.posts_questions` q\n        ON a.parent_id = q.id\n    WHERE a.owner_user_id = 1908967\n      AND v.vote_type_id = 2\n      AND DATE(v.creation_date) < DATE('2018-06-07')\n  ), UNNEST(tag) AS tag\n  GROUP BY tag\n  ORDER BY count DESC\n) AS votes\nFULL OUTER JOIN (\n  SELECT\n    tag,\n    COUNT(tag) AS count\n  FROM (\n    SELECT SPLIT(q.tags, '|') AS tag\n    FROM `bigquery-public-data.stackoverflow.votes` v\n      LEFT JOIN `bigquery-public-data.stackoverflow.posts_answers` a\n        ON v.post_id = a.id\n      LEFT JOIN `bigquery-public-data.stackoverflow.posts_questions` q\n        ON a.parent_id = q.id\n    WHERE a.owner_user_id = 1908967\n      AND v.vote_type_id = 1\n      AND DATE(v.creation_date) < DATE('2018-06-07')\n  ), UNNEST(tag) AS tag\n  GROUP BY tag\n  ORDER BY count DESC\n) AS accepts\nON votes.tag = accepts.tag\nORDER BY reputation DESC\nLIMIT 10",
        "external_knowledge": null,
        "plan": "1. **Initialize Subqueries for Votes and Accepted Answers:**\n   - **Subquery for Votes:**\n     1. Retrieve records from the votes and answers tables.\n     2. Join the votes with answers and questions to gather tags associated with each vote.\n     3. Filter results to include only upvotes (vote_type_id = 2) made before June 7, 2018, for the specified user.\n     4. Split the tags string into individual tags.\n     5. Count occurrences of each tag.\n   - **Subquery for Accepted Answers:**\n     1. Similar to the votes subquery but filter for accepted answers (vote_type_id = 1).\n     2. Count occurrences of each tag for accepted answers.\n\n2. **Combine Subquery Results:**\n   - Perform a full outer join on the results of the votes and accepted answers subqueries based on tags to ensure all tags from both subqueries are included.\n\n3. **Calculate Reputation Score:**\n   - For each tag, compute the reputation score as 10 times the vote count plus 15 times the accepted answer count.\n\n4. **Select and Sort Results:**\n   - Select the tag, calculated reputation score, vote count, and accept count.\n   - Sort the results by the reputation score in descending order.\n\n5. **Limit Results:**\n   - Limit the final output to the top 10 tags based on the calculated reputation score.",
        "special_function": [
            "date-functions/DATE",
            "string-functions/SPLIT",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq307",
        "db": "bigquery-public-data.stackoverflow",
        "question": "Find the top 10 most common first gold badges on Stack Overflow, showing how many users earned each and the average days from account creation to earning the badge.",
        "SQL": "SELECT badge_name AS First_Gold_Badge,\n       COUNT(1) AS Num_Users,\n       ROUND(AVG(tenure_in_days)) AS Avg_Num_Days\nFROM\n(\n  SELECT\n    badges.user_id AS user_id,\n    badges.name AS badge_name,\n    TIMESTAMP_DIFF(badges.date, users.creation_date, DAY) AS tenure_in_days,\n    ROW_NUMBER() OVER (PARTITION BY badges.user_id\n                       ORDER BY badges.date) AS row_number\n  FROM\n    `bigquery-public-data.stackoverflow.badges` badges\n  JOIN\n    `bigquery-public-data.stackoverflow.users` users\n  ON badges.user_id = users.id\n  WHERE badges.class = 1\n)\nWHERE row_number = 1\nGROUP BY First_Gold_Badge\nORDER BY Num_Users DESC\nLIMIT 10;",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. **Initial Badge and User Data Retrieval**:\n   - Retrieve user and badge data by joining the user and badge tables on the user ID.\n   - Filter the badges to include only those of a specific type (e.g., gold badges).\n\n2. **Calculate Tenure**:\n   - For each badge, calculate the number of days between the user's account creation date and the date the badge was earned.\n\n3. **Identify First Badge**:\n   - Assign a row number to each badge per user based on the date the badge was earned, ordering from the earliest to the latest.\n\n4. **Filter to First Badge**:\n   - Filter the dataset to include only the first badge earned by each user, identified by the row number being 1.\n\n5. **Aggregate Data**:\n   - Group the filtered data by the badge name.\n   - For each badge name, count the number of users who earned it as their first badge.\n   - Calculate the average number of days from account creation to earning the badge, rounding to the nearest whole number.\n\n6. **Sort and Limit Results**:\n   - Order the results by the number of users in descending order to find the most common first badges.\n   - Limit the results to the top 10 most common first badges.\n\n7. **Output**:\n   - Display the badge name, the number of users who earned it, and the average number of days from account creation to earning the badge.",
        "special_function": [
            "mathematical-functions/ROUND",
            "numbering-functions/ROW_NUMBER",
            "timestamp-functions/TIMESTAMP_DIFF"
        ]
    },
    {
        "instance_id": "bq308",
        "db": "bigquery-public-data.stackoverflow",
        "question": "Show the number of Stack Overflow questions asked each day of the week in 2021, and find out how many and what percentage of those were answered within one hour.",
        "SQL": "SELECT\n  Day_of_Week,\n  COUNT(1) AS Num_Questions,\n  SUM(answered_in_1h) AS Num_Answered_in_1H,\n  ROUND(100 * SUM(answered_in_1h) / COUNT(1),1) AS Percent_Answered_in_1H\nFROM\n(\n  SELECT\n    q.id AS question_id,\n    EXTRACT(DAYOFWEEK FROM q.creation_date) AS day_of_week,\n    MAX(IF(a.parent_id IS NOT NULL AND\n           (UNIX_SECONDS(a.creation_date)-UNIX_SECONDS(q.creation_date))/(60*60) <= 1, 1, 0)) AS answered_in_1h\n  FROM\n    `bigquery-public-data.stackoverflow.posts_questions` q\n  LEFT JOIN\n    `bigquery-public-data.stackoverflow.posts_answers` a\n  ON q.id = a.parent_id\n  WHERE EXTRACT(YEAR FROM a.creation_date) = 2020\n    AND EXTRACT(YEAR FROM q.creation_date) = 2020\n  GROUP BY question_id, day_of_week\n)\nGROUP BY\n  Day_of_Week\nORDER BY\n  Day_of_Week;",
        "external_knowledge": null,
        "plan": "1. **Extract Day of the Week and Answered Status**:\n    - For each question, determine the day of the week it was asked.\n    - Check if the question has any answers.\n    - If there are answers, determine if any of them were provided within one hour of the question being posted.\n\n2. **Filter by Year**:\n    - Ensure that both the question and its answers were created in the year 2020.\n\n3. **Group Data**:\n    - Group the data by question and day of the week.\n    - For each group, calculate if it has been answered within one hour.\n\n4. **Aggregate by Day of the Week**:\n    - Group the results by the day of the week.\n    - Count the total number of questions for each day of the week.\n    - Sum the number of questions answered within one hour for each day of the week.\n\n5. **Calculate Percentage**:\n    - For each day of the week, compute the percentage of questions that were answered within one hour.\n\n6. **Order Results**:\n    - Sort the results by the day of the week in ascending order.\n\nThis plan ensures that we get a clear picture of the number of questions asked each day of the week in 2020, how many were answered within an hour, and what percentage that represents.",
        "special_function": [
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "mathematical-functions/ROUND",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/UNIX_SECONDS",
            "conditional-functions/IF"
        ]
    },
    {
        "instance_id": "bq309",
        "db": "bigquery-public-data.stackoverflow",
        "question": "Show the top 10 longest Stack Overflow questions where the question has an accepted answer or an answer with a score-to-view ratio above 0.01, including the user's reputation, net votes, and badge count.",
        "SQL": "WITH badge_counts AS (\n  SELECT\n    c.id,\n    COUNT(DISTINCT d.id) AS badge_number\n  FROM\n    `bigquery-public-data.stackoverflow.users` AS c\n  JOIN\n    `bigquery-public-data.stackoverflow.badges` AS d\n  ON\n    c.id = d.user_id\n  GROUP BY\n    c.id\n),\nlabeled_questions AS (\n  SELECT\n    a.id,\n    IF(\n      a.id IN (\n        SELECT DISTINCT b.id\n        FROM\n          `bigquery-public-data.stackoverflow.posts_answers` AS a\n        JOIN\n          `bigquery-public-data.stackoverflow.posts_questions` AS b\n        ON\n          a.parent_id = b.id\n        WHERE\n          b.accepted_answer_id IS NULL\n          AND a.score / b.view_count > 0.01\n      ) OR accepted_answer_id IS NOT NULL,\n      1,\n      0\n    ) AS label,\n    a.owner_user_id,\n    LENGTH(a.body) AS body_length\n  FROM\n    `bigquery-public-data.stackoverflow.posts_questions` AS a\n)\nSELECT\n  lq.id,\n  b.reputation,\n  b.up_votes - b.down_votes AS net_votes,\n  e.badge_number\nFROM\n  labeled_questions AS lq\nJOIN\n  `bigquery-public-data.stackoverflow.users` AS b\nON\n  lq.owner_user_id = b.id\nJOIN\n  badge_counts AS e\nON\n  b.id = e.id\nWHERE\n  lq.label = 1\nORDER BY\n  lq.body_length DESC\nLIMIT\n  10;",
        "external_knowledge": null,
        "plan": "1. **Create Badge Counts**:\n   - Generate a temporary table that calculates the number of unique badges each user has by joining the user data with badge data.\n   - Group the results by user IDs.\n\n2. **Label Questions**:\n   - Generate a temporary table that includes all questions, where each question is labeled based on specific criteria:\n     - Check if the question has an accepted answer.\n     - If not, check if it has any answer with a score-to-view ratio greater than 0.01.\n     - Assign a label of 1 if any of these conditions are met, otherwise assign 0.\n   - Also, include the length of the question's body and the question owner's user ID in this table.\n\n3. **Filter and Join Data**:\n   - Select questions from the labeled questions table where the label is 1 (indicating they meet the criteria).\n   - Join this filtered list with user data to retrieve the user\u2019s reputation and net votes.\n   - Also, join with the badge counts table to retrieve the number of badges each user has.\n\n4. **Sort and Limit Results**:\n   - Order the resulting questions by the length of their bodies in descending order.\n   - Limit the final output to the top 10 longest questions.",
        "special_function": [
            "string-functions/LENGTH",
            "conditional-functions/IF"
        ]
    },
    {
        "instance_id": "bq124",
        "db": "bigquery-public-data.fhir_synthea",
        "question": "Can you identify how many alive patients, currently managing chronic conditions such as diabetes or hypertension, are prescribed seven or more medications?",
        "SQL": "With INFO AS (\nSELECT \n  MR.patientId, \n  P.last_name,\n  ARRAY_TO_STRING(P.first_name, \" \") AS First_name,\n  Condition.Codes, \n  Condition.Conditions,\n  MR.med_count AS COUNT_NUMBER\nFROM\n  (SELECT \n    id, \n    name[safe_offset(0)].family as last_name, \n    name[safe_offset(0)].given as first_name, \n    TIMESTAMP(deceased.dateTime) AS deceased_datetime \n  FROM `bigquery-public-data.fhir_synthea.patient`) AS P\nJOIN\n  (SELECT  subject.patientId as patientId, \n           COUNT(DISTINCT medication.codeableConcept.coding[safe_offset(0)].code) AS med_count\n   FROM    `bigquery-public-data.fhir_synthea.medication_request`\n   WHERE   status = 'active'\n   GROUP BY 1\n   ) AS MR\nON MR.patientId = P.id \nJOIN\n  (SELECT \n  PatientId, \n  STRING_AGG(DISTINCT condition_desc, \", \") AS Conditions, \n  STRING_AGG(DISTINCT condition_code, \", \") AS Codes\n  FROM(\n    SELECT \n      subject.patientId as PatientId, \n              code.coding[safe_offset(0)].code condition_code,\n              code.coding[safe_offset(0)].display condition_desc\n       FROM `bigquery-public-data.fhir_synthea.condition`\n       wHERE \n         code.coding[safe_offset(0)].display = 'Diabetes'\n         OR \n         code.coding[safe_offset(0)].display = 'Hypertension' \n    )\n  GROUP BY PatientId\n  ) AS Condition\nON MR.patientId = Condition.PatientId\nWHERE med_count >= 7 \nAND P.deceased_datetime is NULL /*only alive patients*/\nGROUP BY patientId, last_name, first_name, Condition.Codes, Condition.Conditions, MR.med_count\nORDER BY last_name\n)\n\nSELECT COUNT(*) FROM INFO",
        "external_knowledge": null,
        "plan": "1.Pull up information on patients who are still alive from the healthcare records.\n2.Check how many different medications each patient is currently prescribed by looking at their active prescriptions.\n3.Focus on those who are dealing with chronic issues like diabetes or hypertension.\n4.Specifically, zoom in on patients who are juggling seven or more medications.\n5.Count how many alive patients, currently managing chronic conditions such as diabetes or hypertension, are prescribed seven or more medications",
        "special_function": [
            "aggregate-functions/STRING_AGG",
            "array-functions/ARRAY_TO_STRING",
            "timestamp-functions/TIMESTAMP",
            "other-functions/ARRAY_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "bq391",
        "db": "bigquery-public-data.fhir_synthea",
        "question": "Could you find out which health conditions have the most types of medications per case, for living patients whose last names start with 'A' and have only one unique condition? I'd like to see the top eight conditions and their codes, ranked by the highest number of different meds prescribed to any single patient.",
        "SQL": "WITH PatientConditions AS (\n  SELECT \n    P.id AS patientId,\n    P.name[safe_offset(0)].family AS last_name,\n    P.name[safe_offset(0)].given AS first_name,\n    TIMESTAMP(P.deceased.dateTime) AS deceased_datetime,\n    COUNT(DISTINCT MR.medication_code) AS med_count,\n    STRING_AGG(DISTINCT C.condition_code, \", \") AS Codes,\n    STRING_AGG(DISTINCT C.condition_desc, \", \") AS Conditions,\n    COUNT(DISTINCT C.condition_code) AS condition_count\n  FROM `bigquery-public-data.fhir_synthea.patient` P\n  JOIN (\n    SELECT \n      subject.patientId AS patientId, \n      medication.codeableConcept.coding[safe_offset(0)].code AS medication_code\n    FROM `bigquery-public-data.fhir_synthea.medication_request`\n    WHERE status = 'active'\n  ) MR ON P.id = MR.patientId\n  JOIN (\n    SELECT \n      subject.patientId AS patientId,\n      code.coding[safe_offset(0)].code AS condition_code,\n      code.coding[safe_offset(0)].display AS condition_desc\n    FROM `bigquery-public-data.fhir_synthea.condition`\n  ) C ON P.id = C.patientId\n  WHERE P.deceased.dateTime IS NULL\n  AND P.name[safe_offset(0)].family LIKE 'A%'\n  GROUP BY P.id, P.name[safe_offset(0)].family, P.name[safe_offset(0)].given, P.deceased.dateTime\n)\nSELECT\n  Conditions,\n  Codes\nFROM \n  PatientConditions\nWHERE condition_count = 1\nGROUP BY \n  Conditions, \n  Codes\nORDER BY MAX(med_count) DESC\nLIMIT 8;",
        "external_knowledge": null,
        "plan": "1. **Create a Subquery for Patient Conditions:**\n   - Begin by selecting patient details, including their ID, last name, first name, and whether they are deceased.\n   - Count the number of distinct medications prescribed to each patient.\n   - Aggregate the distinct conditions and their descriptions for each patient.\n   - Count the distinct conditions each patient has.\n\n2. **Filter and Join Data:**\n   - Join the patient data with medication request data, filtering for only active medications.\n   - Join the patient data with condition data.\n   - Ensure only living patients are considered.\n   - Filter patients whose last names start with the specified letter.\n\n3. **Group Data:**\n   - Group the data by patient ID, last name, first name, and deceased status to ensure accurate aggregation of medications and conditions.\n\n4. **Select Relevant Information:**\n   - From the aggregated data, select the conditions and their codes for further processing.\n   - Filter to include only patients with exactly one condition.\n\n5. **Order and Limit Results:**\n   - Order the results by the highest count of medications in descending order.\n   - Limit the result to the top entry to find the patient with the highest medication count.\n\nThis process ensures that we identify living patients with a specific last name pattern, exactly one condition, and the highest count of active medications.",
        "special_function": [
            "aggregate-functions/STRING_AGG",
            "timestamp-functions/TIMESTAMP",
            "other-functions/ARRAY_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "bq126",
        "db": "bigquery-public-data.the_met",
        "question": "What are the titles, artist names, mediums, and original image URLs of objects with 'Photograph' in their names from the 'Photographs' department, created not by an unknown artist, with an object end date of 1839 or earlier?",
        "SQL": "SELECT\n  o.artist_display_name,\n  o.title,\n  o.object_end_date,\n  o.medium,\n  i.original_image_url\nFROM (\n  SELECT\n    object_id,\n    title,\n    artist_display_name,\n    object_end_date,\n    medium\n  FROM\n    `bigquery-public-data.the_met.objects`\n  WHERE\n    department = \"Photographs\"\n    AND object_name LIKE \"%Photograph%\"\n    AND artist_display_name != \"Unknown\"\n    AND object_end_date <= 1839\n) o\nINNER JOIN (\n  SELECT\n    original_image_url,\n    object_id\n  FROM\n    `bigquery-public-data.the_met.images`\n) i\nON\n  o.object_id = i.object_id\nORDER BY\n  o.object_end_date\n;",
        "external_knowledge": null,
        "plan": "What are the earliest photographs in the collection?\nThe Met\u2019s Department of Photographs has some of the earliest photographic works. This query retrieves artwork from the \u201cPhotograph\u201d department and joins the object and image tables to return the results, presented with the oldest items first. In this case, we see the British photography pioneer William Henry Fox Talbot\u2019s botanical images.",
        "special_function": null
    },
    {
        "instance_id": "bq366",
        "db": "bigquery-public-data.the_met",
        "question": "What are the top three most frequently associated labels with artworks from each historical period in The Met's collection, only considering labels linked to 50 or more artworks? Provide me with the period, label, and the associated count.",
        "SQL": "SELECT period, description, c FROM (\n  SELECT \na.period, \nb.description, \ncount(*) c, \nrow_number() over (partition by period order by count(*) desc) seqnum \n  FROM `bigquery-public-data.the_met.objects` a\n  JOIN (\n    SELECT \n        label.description as description, \n        object_id \n    FROM `bigquery-public-data.the_met.vision_api_data`, UNNEST(labelAnnotations) label\n  ) b\n  ON a.object_id = b.object_id\n  WHERE a.period is not null\n  group by 1,2\n)\nWHERE seqnum <= 3\nAND c >= 500 # only include labels that have 50 or more pieces associated with it\nORDER BY period, c desc;",
        "external_knowledge": null,
        "plan": "",
        "special_function": [
            "ROW_NUMBER",
            "PARTITION BY",
            "UNNEST"
        ]
    },
    {
        "instance_id": "bq414",
        "db": "bigquery-public-data.the_met",
        "question": "Retrieve the object id, title, and the formatted metadata date (as a string in 'YYYY-MM-DD' format) for objects in the \"The Libraries\" department where the cropConfidence is greater than 0.5, the object's title contains the word \"book\".",
        "SQL": "SELECT \n  a.object_id,\n  a.title,\n  FORMAT_TIMESTAMP('%Y-%m-%d', a.metadata_date) AS formatted_metadata_date\nFROM `bigquery-public-data.the_met.objects` a\nJOIN (\n  SELECT object_id,\n         cropHints.confidence AS cropConfidence\n  FROM `bigquery-public-data.the_met.vision_api_data`, \n       UNNEST(cropHintsAnnotation.cropHints) cropHints\n) b\nON a.object_id = b.object_id\nWHERE a.department = \"The Libraries\"\nAND b.cropConfidence > 0.5\nAND a.title LIKE \"%book%\"",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq200",
        "db": "bigquery-public-data.baseball",
        "question": "Show the full name of the fastest pitcher on each team with their maximum valid pitch speed, using both regular and post-season data",
        "SQL": "SELECT \n    _pitcher,\n    _speed\nFROM (\n    -- #rank of the player by his speed (ignoring ties)\n    SELECT _team\n        ,_pitcher\n        ,_speed\n        ,row_number() OVER (\n            PARTITION BY _team ORDER BY _speed DESC\n            ) _rnk\n    FROM (\n        -- #finalizing the table with player-team link\n        SELECT _team\n            ,_pitcher\n            ,max(_speed) _speed\n        FROM (\n            -- #glueing tables with players and teams\n            SELECT _pitcher\n                ,coalesce(_home_team, _away_team) _team\n                ,_speed\n            FROM (\n                -- #setting up the table with players\n                SELECT venueId\n                    ,pitcherId\n                    ,pitcherFirstName || ' ' || pitcherLastName AS _pitcher\n                    ,max(pitchSpeed) _speed\n                FROM (\n                    -- #full table comprising all seasons\n                    SELECT venueId\n                        ,pitcherId\n                        ,pitcherFirstName\n                        ,pitcherLastName\n                        ,pitchSpeed\n                    FROM `bigquery-public-data.baseball.games_wide`\n                    ) _all\n                WHERE pitchSpeed != 0\n                GROUP BY venueId\n                    ,pitcherId\n                    ,pitcherFirstName || ' ' || pitcherLastName\n                ) _grp\n            -- #joining table with player-team links\n            LEFT JOIN (\n                SELECT pitcherId\n                    ,venueId\n                    -- we need to check here if the pitcher is in the list of the homing team\n                    ,CASE \n                        WHEN pitcherId IN unnest(\n                                                [homeFielder1\t\t\n                                                ,homeFielder2\t\t\t\n                                                ,homeFielder3\t\t\t\n                                                ,homeFielder4\t\t\t\n                                                ,homeFielder5\t\t\t\n                                                ,homeFielder6\t\t\t\n                                                ,homeFielder7\t\t\t\n                                                ,homeFielder8\t\t\t\n                                                ,homeFielder9\t\t\t\n                                                ,homeFielder10\t\t\t\n                                                ,homeFielder11\t\t\t\n                                                ,homeFielder12\t\t\t\n                                                ,homeBatter1\t\t\t\n                                                ,homeBatter2\t\t\t\n                                                ,homeBatter3\t\t\t\n                                                ,homeBatter4\t\t\t\n                                                ,homeBatter5\t\t\t\n                                                ,homeBatter6\t\t\t\n                                                ,homeBatter7\t\t\t\n                                                ,homeBatter8\t\t\t\n                                                ,homeBatter9]\n                            )\n                            THEN homeTeamName\n                        ELSE NULL\n                        END _home_team\n                    -- we need to check here if the pitcher is in the list of the guest team\n                    ,CASE \n                        WHEN pitcherId IN unnest(\n                                                [awayFielder1\t\t\n                                                ,awayFielder2\t\t\n                                                ,awayFielder3\t\t\n                                                ,awayFielder4\t\t\n                                                ,awayFielder5\t\t\n                                                ,awayFielder6\t\t\n                                                ,awayFielder7\t\t\n                                                ,awayFielder8\t\t\n                                                ,awayFielder9\t\t\n                                                ,awayFielder10\t\t\n                                                ,awayFielder11\t\t\n                                                ,awayFielder12\t\t\n                                                ,awayBatter1\t\t\n                                                ,awayBatter2\t\t\n                                                ,awayBatter3\t\t\n                                                ,awayBatter4\t\t\n                                                ,awayBatter5\t\t\n                                                ,awayBatter6\t\t\n                                                ,awayBatter7\t\t\n                                                ,awayBatter8\t\t\n                                                ,awayBatter9]\n                            )\n                            THEN awayTeamName\n                        ELSE NULL\n                        END _away_team\n                FROM (\n                    SELECT *\n                    FROM `bigquery-public-data.baseball.games_wide`\n                    \n                    UNION ALL\n                    \n                    SELECT *\n                    FROM `bigquery-public-data.baseball.games_post_wide`\n                    ) t\n                ) _team ON _team.pitcherId = _grp.pitcherId\n                AND _team.venueId = _grp.venueId\n            ) _total\n        GROUP BY _team\n            ,_pitcher\n        ) _filter\n    WHERE 1 = 1\n        AND _team IS NOT NULL\n        AND _pitcher IS NOT NULL\n        AND _pitcher != ''\n        AND _speed != 0\n    ) _rank\nWHERE 1 = 1\n    AND _rnk = 1",
        "external_knowledge": null,
        "plan": "1. Select the necessary fields from the baseball games dataset, such as pitcher ID, team, and pitch speed.\n2. Rank pitchers within each team based on their maximum pitch speed in descending order.\n3. Filter out the pitchers ranked first within their team, which identifies the fastest pitcher per team.\n4. Display the team, pitcher, and their fastest recorded pitch speed.",
        "special_function": [
            "numbering-functions/ROW_NUMBER",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE",
            "conditional-functions/COALESCE",
            "conditional-functions/IF",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq458",
        "db": "spider2-public-data.word_vectors_us",
        "question": "Please help me calculate normalized document vectors for each article by tokenizing the body text into words, obtaining word vectors, and weighting these vectors by the 0.4th root of word frequency. Then, aggregate these vectors to form an article vector, and normalize them to unit length. Finally, retrieve the ID, date, title, and the computed article vector for each entry.",
        "SQL": "WITH all_articles AS \n(\n  SELECT  \n    id,\n    date,\n    title, \n    `spider2-public-data.word_vectors_us.tokenise_no_stop`(body) article\n  FROM `spider2-public-data.word_vectors_us.nature`\n),\n# Unnest so we have one word per row\nmy_word_lists AS \n(\n  SELECT  \n    id, \n    word \n  FROM all_articles, \n  UNNEST(article) AS word \n),\n\n# Fetch the vectors for each word and weight by word frequency\nlookup_vectors AS (\n  SELECT  \n        id,\n        word, \n        wv_component / POW(frequency, 0.4) AS tf_normalised_wv_component, \n        wv_component_index  \n  FROM my_word_lists \n  JOIN `spider2-public-data.word_vectors_us.glove_vectors` \n    USING (word)\n  JOIN `spider2-public-data.word_vectors_us.word_frequencies` \n    USING (word)\n  , UNNEST(vector) wv_component WITH OFFSET wv_component_index),\n\n# Aggregate to create a document vector\naggregate_vectors_over_article AS (\n  SELECT   id,\n           wv_component_index, \n           SUM(tf_normalised_wv_component) aggregated_wv_component \n  FROM lookup_vectors \n  GROUP BY id, wv_component_index),\n\n# Normalise vectors to unit length, so need to work out the length (magnitude)\nvector_lengths AS (\n  SELECT id, \n         SQRT(SUM(aggregated_wv_component * aggregated_wv_component)) AS magnitude \n  FROM aggregate_vectors_over_article\n  GROUP BY id),\n\n# Divide by the magnitude to normalise and put it back in to an array\nnormalised_aggregate_vectors AS (\n    SELECT id,  \n           ARRAY_AGG(aggregated_wv_component / magnitude ORDER BY wv_component_index) AS article_vector\n    FROM aggregate_vectors_over_article \n    JOIN vector_lengths\n      USING (id)\n    GROUP BY id)\n\nSELECT  id, \n        date, \n        title, \n        article_vector \nFROM normalised_aggregate_vectors\nJOIN all_articles\nUSING (id)",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq459",
        "db": "spider2-public-data.word_vectors_us",
        "question": "I would like to process articles by tokenizing and normalizing word vectors to compute cosine similarity scores and weighting these vectors by the 0.4th root of word frequency, in order to identify the top 10 most relevant articles related to the phrase \"Epigenetics and cerebral organoids: promising directions in autism spectrum disorders\". Please show me the id, date, title and cosine similarity scores of them.",
        "SQL": "WITH all_articles AS (\n  SELECT  \n    id,\n    date,\n    title, \n    `spider2-public-data.word_vectors_us.tokenise_no_stop`(body) article\n  FROM `spider2-public-data.word_vectors_us.nature`\n),\n\nmy_word_lists AS (\n  SELECT  \n    id, \n    word \n  FROM all_articles, UNNEST(article) AS word \n),\n\nlookup_vectors AS (\n  SELECT  \n        id,\n        word, \n        wv_component / POW(frequency, 0.4) AS tf_normalised_wv_component, \n        wv_component_index  \n  FROM my_word_lists \n  JOIN `spider2-public-data.word_vectors_us.glove_vectors` \n    USING (word)\n  JOIN `spider2-public-data.word_vectors_us.word_frequencies` \n    USING (word)\n  , UNNEST(vector) wv_component WITH OFFSET wv_component_index),\n\naggregate_vectors_over_article AS (\n  SELECT   id,\n           wv_component_index, \n           SUM(tf_normalised_wv_component) aggregated_wv_component \n  FROM lookup_vectors \n  GROUP BY id, wv_component_index),\n\nvector_lengths AS (\n  SELECT id, \n         SQRT(SUM(aggregated_wv_component * aggregated_wv_component)) AS magnitude \n  FROM aggregate_vectors_over_article\n  GROUP BY id),\n\nnormalised_aggregate_vectors AS (\n    SELECT id,  \n           ARRAY_AGG(aggregated_wv_component / magnitude ORDER BY wv_component_index) AS article_vector\n    FROM aggregate_vectors_over_article \n    JOIN vector_lengths\n      USING (id)\n    GROUP BY id),\nnature_vectors AS (\nSELECT  id, \n        date, \n        title, \n        article_vector \nFROM normalised_aggregate_vectors\nJOIN all_articles\nUSING (id)\n),\n\n\nall_tokens AS (\n  SELECT `spider2-public-data.word_vectors_us.tokenise_no_stop` \n  (\"Epigenetics and cerebral organoids: promising directions in autism spectrum disorders\") AS tokens\n),\n\ntokens_no_stops AS (\n          SELECT  word \n          FROM all_tokens, UNNEST(tokens) AS word \n  ),\n\nlookup_vectors_1 AS (SELECT  \n                          word, \n                          vector_component / POW(frequency, 0.4) AS weighted_vector_component, \n                          vector_component_index  \nFROM tokens_no_stops \nJOIN `spider2-public-data.word_vectors_us.glove_vectors` \nUSING (word)\nJOIN `spider2-public-data.word_vectors_us.word_frequencies`\nUSING (word)\n, \nUNNEST(vector) vector_component WITH OFFSET vector_component_index),\n\n  aggregate_vector AS (SELECT vector_component_index, SUM(weighted_vector_component) AS agg_vector_component \n                    FROM lookup_vectors_1 \n                    GROUP BY vector_component_index),\n\n\nvector_length AS (SELECT SQRT(SUM(agg_vector_component * agg_vector_component)) AS magnitude \nFROM aggregate_vector),\n\n\nnorm_vector AS (\n  SELECT ARRAY_AGG(agg_vector_component / (SELECT magnitude FROM vector_length) ORDER BY vector_component_index) AS vector\n  FROM aggregate_vector)\n\nSELECT  id,\n        date,\n        title,\n        (SELECT SUM(doc_vector_component * search_vector_component) \n         FROM \n            UNNEST(dv.article_vector) doc_vector_component WITH OFFSET dv_component_index\n         JOIN\n            UNNEST(s.vector) search_vector_component WITH OFFSET sv_component_index\n         ON dv_component_index = sv_component_index) AS cosine_similarity\nFROM nature_vectors dv\nCROSS JOIN norm_vector s\nORDER BY cosine_similarity DESC\nLIMIT 10;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq460",
        "db": "spider2-public-data.word_vectors_us",
        "question": "Please process articles by creating normalized word vectors,  and weighting these vectors by the 0.4th root of word frequency, and computes cosine similarity scores to identify the top 10 articles most similar to the article with ID \"8a78ef2d-d5f7-4d2d-9b47-5adb25cbd373\". I want the id, date, title and cosine similarity scores of them.",
        "SQL": "WITH all_articles AS (\n    SELECT  \n        id,\n        date,\n        title, \n        `spider2-public-data.word_vectors_us.tokenise_no_stop`(body) article\n    FROM `spider2-public-data.word_vectors_us.nature`), \n\nmy_word_lists AS (\n    SELECT  \n        id, \n        word \n    FROM all_articles, UNNEST(article) AS word \n   ),\n\nlookup_vectors AS (\n  SELECT  \n    id,\n    word, \n    wv_component / POW(frequency, 0.4) AS tf_normalised_wv_component, \n    wv_component_index  \n  FROM my_word_lists \n  JOIN `spider2-public-data.word_vectors_us.glove_vectors` \n    USING (word)\n  JOIN `spider2-public-data.word_vectors_us.word_frequencies` \n    USING (word)\n  , UNNEST(vector) wv_component WITH OFFSET wv_component_index),\n\naggregate_vectors_over_article AS (\n  SELECT  \n    id,\n    wv_component_index, \n    SUM(tf_normalised_wv_component) aggregated_wv_component \n  FROM lookup_vectors \n  GROUP BY id, wv_component_index),\n\nvector_lengths AS (\n  SELECT \n    id, \n    SQRT(SUM(aggregated_wv_component * aggregated_wv_component)) AS magnitude \n  FROM aggregate_vectors_over_article\n  GROUP BY id),\n\nnormalised_aggregate_vectors AS (\n    SELECT \n        id,  \n        ARRAY_AGG(aggregated_wv_component / magnitude ORDER BY wv_component_index) AS article_vector\n    FROM aggregate_vectors_over_article \n    JOIN vector_lengths\n      USING (id)\n    GROUP BY id),\nnature_vectors AS (\nSELECT  id, \n        date, \n        title, \n        article_vector \nFROM normalised_aggregate_vectors\nJOIN all_articles\nUSING (id)\n),\n  search_vector AS (\nSELECT article_vector AS vector\n  FROM nature_vectors\n  where id = \"8a78ef2d-d5f7-4d2d-9b47-5adb25cbd373\" )\n\nSELECT id,\n        date,\n        title,\n  \n(SELECT SUM(doc_vector_component * search_vector_component) \nFROM \n  UNNEST(dv.article_vector) doc_vector_component WITH OFFSET dv_component_index\nJOIN\n  UNNEST(s.vector) search_vector_component WITH OFFSET sv_component_index\nON dv_component_index = sv_component_index) AS cosine_similarity\nFROM nature_vectors dv\nCROSS JOIN search_vector s \nORDER BY cosine_similarity DESC\nLIMIT 10;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq204",
        "db": "bigquery-public-data.eclipse_megamovie",
        "question": "Find the user with the highest total clicks across all records from all available photo collections.",
        "SQL": "SELECT user\nFROM (\nSelect user\n      From `bigquery-public-data.eclipse_megamovie.photos_v_0_1`\n      UNION ALL\n      Select user\n      From`bigquery-public-data.eclipse_megamovie.photos_v_0_2`\n      UNION ALL\n      Select user\n      From`bigquery-public-data.eclipse_megamovie.photos_v_0_3`\n) \nGROUP BY user \nHAVING COUNT (user)=( \nSELECT MAX(mycount) \nFROM ( \nSELECT user, COUNT(user) mycount \nFROM (\nSelect user\n      From `bigquery-public-data.eclipse_megamovie.photos_v_0_1`\n      UNION ALL\n      Select user\n      From`bigquery-public-data.eclipse_megamovie.photos_v_0_2`\n      UNION ALL\n      Select user\n      From`bigquery-public-data.eclipse_megamovie.photos_v_0_3`\n)\nGROUP BY user))\nORDER BY COUNT(user) \nLIMIT 1",
        "external_knowledge": null,
        "plan": "1. **Union All Data**:\n   - Combine all records from three different collections into one dataset using `UNION ALL`. This step ensures that every record from each collection is included in the analysis.\n\n2. **Aggregate User Clicks**:\n   - Group the combined data by the user and count the total number of clicks (or records) associated with each user. This provides a count of how many times each user appears across all collections.\n\n3. **Find Maximum Clicks**:\n   - From the aggregated user data, determine the maximum count of clicks any single user has. This identifies the highest total clicks achieved by any user across all collections.\n\n4. **Identify Top User(s)**:\n   - Select the user(s) whose total click count matches the maximum click count found in the previous step. This ensures that only the user(s) with the highest total clicks are considered.\n\n5. **Order and Limit Results**:\n   - Order the final results by the click count to maintain consistency and limit the output to the top user. This step ensures that only the user with the highest click count is returned.",
        "special_function": null
    },
    {
        "instance_id": "bq389",
        "db": "bigquery-public-data.epa_historical_air_quality",
        "question": "Please calculate the monthly average levels of PM10, PM2.5 FRM, PM2.5 non-FRM, volatile organic emissions, SO2 (scaled by a factor of 10), and Lead (scaled by a factor of 100) air pollutants in California for the year 2020.",
        "SQL": "SELECT\n  pm10.month AS month,\n  pm10.avg AS pm10,\n  pm25_frm.avg AS pm25_frm,\n  pm25_nonfrm.avg AS pm25_nonfrm,\n  co.avg AS co,\n  so2.avg AS so2,\n  lead.avg AS lead\nFROM\n  (SELECT AVG(arithmetic_mean) AS avg, \n          EXTRACT(YEAR FROM date_local) AS year, \n          EXTRACT(MONTH FROM date_local) AS month\n   FROM `bigquery-public-data.epa_historical_air_quality.pm10_daily_summary`\n   WHERE state_name = 'California' AND EXTRACT(YEAR FROM date_local) = 2020\n   GROUP BY year, month) AS pm10\nJOIN\n  (SELECT AVG(arithmetic_mean) AS avg, \n          EXTRACT(YEAR FROM date_local) AS year, \n          EXTRACT(MONTH FROM date_local) AS month\n   FROM `bigquery-public-data.epa_historical_air_quality.pm25_frm_daily_summary`\n   WHERE state_name = 'California' AND EXTRACT(YEAR FROM date_local) = 2020\n   GROUP BY year, month) AS pm25_frm\nON pm10.year = pm25_frm.year AND pm10.month = pm25_frm.month\nJOIN\n  (SELECT AVG(arithmetic_mean) AS avg, \n          EXTRACT(YEAR FROM date_local) AS year, \n          EXTRACT(MONTH FROM date_local) AS month\n   FROM `bigquery-public-data.epa_historical_air_quality.pm25_nonfrm_daily_summary`\n   WHERE state_name = 'California' AND EXTRACT(YEAR FROM date_local) = 2020\n   GROUP BY year, month) AS pm25_nonfrm\nON pm10.year = pm25_nonfrm.year AND pm10.month = pm25_nonfrm.month\nJOIN\n  (SELECT AVG(arithmetic_mean) * 100 AS avg, \n          EXTRACT(YEAR FROM date_local) AS year, \n          EXTRACT(MONTH FROM date_local) AS month\n   FROM `bigquery-public-data.epa_historical_air_quality.lead_daily_summary`\n   WHERE state_name = 'California' AND EXTRACT(YEAR FROM date_local) = 2020\n   GROUP BY year, month) AS lead\nON pm10.year = lead.year AND pm10.month = lead.month\nJOIN\n  (SELECT AVG(arithmetic_mean) AS avg, \n          EXTRACT(YEAR FROM date_local) AS year, \n          EXTRACT(MONTH FROM date_local) AS month\n   FROM `bigquery-public-data.epa_historical_air_quality.voc_daily_summary`\n   WHERE state_name = 'California' AND EXTRACT(YEAR FROM date_local) = 2020\n   GROUP BY year, month) AS co\nON pm10.year = co.year AND pm10.month = co.month\nJOIN\n  (SELECT AVG(arithmetic_mean) * 10 AS avg, \n          EXTRACT(YEAR FROM date_local) AS year, \n          EXTRACT(MONTH FROM date_local) AS month\n   FROM `bigquery-public-data.epa_historical_air_quality.so2_daily_summary`\n   WHERE state_name = 'California' AND EXTRACT(YEAR FROM date_local) = 2020\n   GROUP BY year, month) AS so2\nON pm10.year = so2.year AND pm10.month = so2.month\nORDER BY\n  month;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq345",
        "db": "spider2-public-data.idc_v17",
        "question": "How large are the DICOM image files with SEG or RTSTRUCT modalities and the SOP Class UID \"1.2.840.10008.5.1.4.1.1.66.4\", when grouped by collection, study, and series IDs, if they have no references to other series, images, or sources? Can you also provide a viewer URL formatted as \"https://viewer.imaging.datacommons.cancer.gov/viewer/\" followed by the study ID, and list these sizes in kilobytes, sorted from largest to smallest?",
        "SQL": "WITH seg_rtstruct AS (\n  SELECT\n    collection_id,\n    StudyInstanceUID,\n    SeriesInstanceUID,\n    CONCAT(\"https://viewer.imaging.datacommons.cancer.gov/viewer/\", StudyInstanceUID) AS viewer_url,\n    instance_size\n  FROM\n    `spider2-public-data.idc_v17.dicom_all`\n  WHERE\n    Modality IN (\"SEG\", \"RTSTRUCT\")\n    AND SOPClassUID = \"1.2.840.10008.5.1.4.1.1.66.4\"\n    AND ARRAY_LENGTH(ReferencedSeriesSequence) = 0\n    AND ARRAY_LENGTH(ReferencedImageSequence) = 0\n    AND ARRAY_LENGTH(SourceImageSequence) = 0\n)\n\n\nSELECT\n  seg_rtstruct.collection_id,\n  seg_rtstruct.SeriesInstanceUID,\n  seg_rtstruct.StudyInstanceUID,\n  seg_rtstruct.viewer_url,\n  SUM(seg_rtstruct.instance_size) / 1024 AS collection_size_KB\nFROM\n  seg_rtstruct\nGROUP BY\n  seg_rtstruct.collection_id,\n  seg_rtstruct.SeriesInstanceUID,\n  seg_rtstruct.StudyInstanceUID,\n  seg_rtstruct.viewer_url\nORDER BY\n  collection_size_KB DESC;",
        "external_knowledge": null,
        "plan": "1. **Define a Common Table Expression (CTE)**:\n   - Create a temporary result set to simplify the main query.\n   - Select relevant columns and compute lengths of specific sequences for each series.\n\n2. **Filter for Specific Modality and SOP Class UID**:\n   - Ensure only records with the specified modality and SOP Class UID are included in the CTE.\n\n3. **Check Sequence Lengths**:\n   - Calculate the length of the referenced series, referenced images, and source images sequences for each series.\n\n4. **Main Query**:\n   - Count the number of distinct collections from the CTE where:\n     - The length of referenced series sequence is zero.\n     - The length of referenced image sequence is zero.\n     - The length of source image sequence is zero.\n\n5. **Return Result**:\n   - Output the count of distinct collections that meet the criteria of having no referenced series, referenced images, or source images.",
        "special_function": null
    },
    {
        "instance_id": "bq346",
        "db": "spider2-public-data.idc_v17",
        "question": "Which five segmentation categories appear most frequently in publicly accessible DICOM SEG data, where the modality is \"SEG\" and the SOPClassUID is \"1.2.840.10008.5.1.4.1.1.66.4\"?",
        "SQL": "WITH\n  sampled_sops AS (\n    SELECT\n      collection_id,\n      SeriesDescription,\n      SeriesInstanceUID,\n      SOPInstanceUID AS seg_SOPInstanceUID,\n      COALESCE(\n        ReferencedSeriesSequence[SAFE_OFFSET(0)].ReferencedInstanceSequence[SAFE_OFFSET(0)].ReferencedSOPInstanceUID,\n        ReferencedImageSequence[SAFE_OFFSET(0)].ReferencedSOPInstanceUID,\n        SourceImageSequence[SAFE_OFFSET(0)].ReferencedSOPInstanceUID\n      ) AS referenced_sop\n    FROM\n      `spider2-public-data.idc_v17.dicom_all`\n    WHERE\n      Modality = \"SEG\"\n      AND SOPClassUID = \"1.2.840.10008.5.1.4.1.1.66.4\"\n      AND Access = \"Public\"\n  ),\n  segmentations_data AS (\n    SELECT\n      dicom_all.collection_id,\n      dicom_all.PatientID,\n      dicom_all.SOPInstanceUID,\n      segmentations.SegmentedPropertyCategory.CodeMeaning AS segmentation_category,\n      segmentations.SegmentedPropertyType.CodeMeaning AS segmentation_type\n    FROM\n      sampled_sops\n    JOIN\n      `spider2-public-data.idc_v17.dicom_all` AS dicom_all\n    ON\n      sampled_sops.referenced_sop = dicom_all.SOPInstanceUID\n    JOIN\n      `spider2-public-data.idc_v17.segmentations` AS segmentations\n    ON\n      segmentations.SOPInstanceUID = sampled_sops.seg_SOPInstanceUID\n  )\nSELECT\n  segmentation_category,\n  COUNT(*) AS count_\nFROM\n  segmentations_data\nGROUP BY\n  segmentation_category\nORDER BY\n  count_ DESC\nLIMIT 5;",
        "external_knowledge": null,
        "plan": "1. **Filtering Data**:\n   - Start by filtering the data from the main dataset to include only the records where the modality matches \"SEG\" and a specific SOP Class UID.\n   - Ensure that the data is publicly accessible.\n\n2. **Extracting Relevant Information**:\n   - For each filtered record, extract relevant identifiers, including the collection ID, series description, series instance UID, SOP instance UID, and referenced SOP instance UID.\n   - Use the first available referenced SOP instance UID from a set of possible sequences to link to other data.\n\n3. **Joining Additional Information**:\n   - From the filtered records, join with the main dataset again using the referenced SOP instance UID to get additional details like patient ID.\n   - Also, join with another dataset to get specific segmentation details related to the SOP instance UID.\n\n4. **Aggregating Data**:\n   - Group the data by segmentation categories.\n   - Count the number of occurrences for each segmentation category.\n\n5. **Sorting and Limiting Results**:\n   - Order the grouped data by the count of occurrences in descending order.\n   - Limit the results to the top five most frequently occurring segmentation categories.\n\nBy following these steps, the query identifies the top five segmentation categories that appear most frequently in the specified DICOM SEG data, ensuring the data is publicly accessible and matches the given modality and SOP Class UID criteria.",
        "special_function": null
    },
    {
        "instance_id": "bq347",
        "db": "spider2-public-data.idc_v17",
        "question": "Which modality has the highest count of SOP instances, including MR series with SeriesInstanceUID = \"1.3.6.1.4.1.14519.5.2.1.3671.4754.105976129314091491952445656147\" and all associated segmentation data, along with the total count of instances?",
        "SQL": "WITH union_mr_seg AS (\n  SELECT\n    dicom_all_mr.SOPInstanceUID,\n    '' AS segPropertyTypeCodeMeaning, \n    '' AS segPropertyCategoryCodeMeaning\n  FROM\n    `spider2-public-data.idc_v17.dicom_all` AS dicom_all_mr\n  WHERE\n    dicom_all_mr.SeriesInstanceUID IN UNNEST([\"1.3.6.1.4.1.14519.5.2.1.3671.4754.105976129314091491952445656147\"])\n    \n  UNION ALL\n\n  SELECT\n    dicom_all_seg.SOPInstanceUID,\n    segmentations.SegmentedPropertyType.CodeMeaning AS segPropertyTypeCodeMeaning,\n    segmentations.SegmentedPropertyCategory.CodeMeaning AS segPropertyCategoryCodeMeaning\n  FROM\n    `spider2-public-data.idc_v17.dicom_all` AS dicom_all_seg\n  JOIN\n    `spider2-public-data.idc_v17.segmentations` AS segmentations\n  ON\n    dicom_all_seg.SOPInstanceUID = segmentations.SOPInstanceUID\n)\n\nSELECT\n  dc_all.Modality,\n  COUNT(*) AS count_\nFROM \n  `spider2-public-data.idc_v17.dicom_all` AS dc_all\nINNER JOIN\n  union_mr_seg\nON \n  dc_all.SOPInstanceUID = union_mr_seg.SOPInstanceUID\nGROUP BY\n  dc_all.Modality\nORDER BY\n  count_ DESC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "1. **Create a Combined Data Set**:\n   - Form a temporary data set that combines specific data from two sources.\n   - The first part includes instances that match a particular identifier.\n   - The second part includes instances from a different source that are linked to the first set based on a common attribute.\n\n2. **Select Relevant Attributes**:\n   - For the first source, select instances and provide placeholder values for additional attributes.\n   - For the second source, select instances and also include specific attributes related to those instances.\n\n3. **Union the Data Sets**:\n   - Combine the results from the two selections into a single data set.\n\n4. **Join with Main Data**:\n   - Join the combined data set with the main data source using a common identifier to link relevant entries.\n\n5. **Group and Count**:\n   - Group the joined data by a specific attribute.\n   - Count the number of instances in each group.\n\n6. **Order and Limit Results**:\n   - Order the grouped results by the count in descending order.\n   - Limit the output to the top result to identify the group with the highest count.\n\n7. **Output**:\n   - Return the attribute of the group and the total count of instances for the top group.",
        "special_function": null
    },
    {
        "instance_id": "bq390",
        "db": "spider2-public-data.idc_v17",
        "question": "Please provide the study instance UIDs for studies that include both T2-weighted axial magnetic resonance imaging and anatomical structure segmentations of the peripheral zone, in prostate repeatability collection.",
        "SQL": "WITH\n-- Studies that have MR volumes\nmr_studies AS (\n  SELECT\n    dicom_all_mr.StudyInstanceUID\n  FROM\n    spider2-public-data.idc_v17.dicom_all AS dicom_all_mr\n  WHERE\n    Modality = 'MR'\n    AND collection_id = 'qin_prostate_repeatability'\n    AND REGEXP_CONTAINS(SeriesDescription, r\"T2 Weighted Axial\")\n),\n\nseg_studies AS (\n  SELECT\n    dicom_all_seg.StudyInstanceUID\n  FROM\n    spider2-public-data.idc_v17.dicom_all AS dicom_all_seg\n  JOIN\n    spider2-public-data.idc_v17.segmentations AS segmentations\n  ON\n    dicom_all_seg.SOPInstanceUID = segmentations.SOPInstanceUID\n  WHERE\n    collection_id = 'qin_prostate_repeatability'\n    AND REGEXP_CONTAINS(segmentations.SegmentedPropertyType.CodeMeaning, r\"Peripheral zone\")\n    AND segmentations.SegmentedPropertyCategory.CodeMeaning = 'Anatomical Structure'\n)\n\nSELECT DISTINCT\n  mr_studies.StudyInstanceUID\nFROM\n  mr_studies\nJOIN\n  seg_studies\nON\n  mr_studies.StudyInstanceUID = seg_studies.StudyInstanceUID",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq421",
        "db": "spider2-public-data.idc_v17",
        "question": "Can you list all unique pairs of embedding medium and staining substance code meanings, along with the number of occurrences for each pair, based on distinct embedding medium and staining substance codes from the 'SM' modality in the DICOM dataset's un-nested specimen preparation sequences, ensuring that the codes are from the SCT coding scheme?",
        "SQL": "WITH\n  SpecimenPreparationSequence_unnested AS (\n  SELECT\n    SOPInstanceUID,\n    concept_name_code_sequence.CodeMeaning AS cnc_cm,\n    concept_name_code_sequence.CodingSchemeDesignator AS cnc_csd,\n    concept_name_code_sequence.CodeValue AS cnc_val,\n    concept_code_sequence.CodeMeaning AS ccs_cm,\n    concept_code_sequence.CodingSchemeDesignator AS ccs_csd,\n    concept_code_sequence.CodeValue AS ccs_val\n  FROM\n    `spider2-public-data.idc_v17.dicom_all`,\n    UNNEST(SpecimenDescriptionSequence[SAFE_OFFSET(0)].SpecimenPreparationSequence) AS preparation_unnest_step1,\n    UNNEST(preparation_unnest_step1.SpecimenPreparationStepContentItemSequence) AS preparation_unnest_step2,\n    UNNEST(preparation_unnest_step2.ConceptNameCodeSequence) AS concept_name_code_sequence,\n    UNNEST(preparation_unnest_step2.ConceptCodeSequence) AS concept_code_sequence ),\n  \n  slide_embedding AS (\n  SELECT\n    SOPInstanceUID,\n    ARRAY_AGG(DISTINCT(CONCAT(ccs_cm, \":\", ccs_csd, \":\", ccs_val))) AS embeddingMedium_code_str\n  FROM\n    SpecimenPreparationSequence_unnested\n  WHERE\n    (cnc_csd = 'SCT' AND cnc_val = '430863003') -- CodeMeaning is 'Embedding medium'\n  GROUP BY\n    SOPInstanceUID ),\n    \n  slide_staining AS (\n  SELECT\n    SOPInstanceUID,\n    ARRAY_AGG(DISTINCT(CONCAT(ccs_cm, \":\", ccs_csd, \":\", ccs_val))) AS staining_usingSubstance_code_str\n  FROM\n    SpecimenPreparationSequence_unnested\n  WHERE\n    (cnc_csd = 'SCT' AND cnc_val = '424361007') -- CodeMeaning is 'Using substance'\n  GROUP BY\n    SOPInstanceUID )\n  \n-- Step 1: Find the most frequent embedding medium\nSELECT\n  embeddingMedium_CodeMeaning,\n  staining_usingSubstance_CodeMeaning,\n  COUNT(*)\nFROM (\n  SELECT\n    d.SOPInstanceUID,\n    d.instance_size,\n    -- Get embedding medium code meaning\n    ARRAY(\n      SELECT IF (code IS NULL, NULL, SPLIT(code, ':')[SAFE_OFFSET(0)])\n      FROM UNNEST(e.embeddingMedium_code_str) AS code\n    ) AS embeddingMedium_CodeMeaning,\n    -- Get staining substance code meaning\n    ARRAY(\n      SELECT IF (code IS NULL, NULL, SPLIT(code, ':')[SAFE_OFFSET(0)])\n      FROM UNNEST(s.staining_usingSubstance_code_str) AS code\n    ) AS staining_usingSubstance_CodeMeaning\n  FROM\n    `spider2-public-data.idc_v17.dicom_all` AS d\n  LEFT JOIN\n    slide_embedding AS e ON d.SOPInstanceUID = e.SOPInstanceUID\n  LEFT JOIN\n    slide_staining AS s ON d.SOPInstanceUID = s.SOPInstanceUID\n  WHERE\n    d.Modality = \"SM\"\n) AS embedding_data\n-- Flatten both arrays for embedding medium and staining substance\nCROSS JOIN\n  UNNEST(embeddingMedium_CodeMeaning) AS embeddingMedium_CodeMeaning\nCROSS JOIN\n  UNNEST(staining_usingSubstance_CodeMeaning) AS staining_usingSubstance_CodeMeaning\nGROUP BY\n  embeddingMedium_CodeMeaning, staining_usingSubstance_CodeMeaning",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq422",
        "db": "spider2-public-data.idc_v17",
        "question": "What are the average series sizes in MiB for the top 3 patients with the highest slice interval difference tolerance and the top 3 patients with the highest maximum exposure difference, considering only CT images from the 'nlst' collection?",
        "SQL": "WITH\n  nonLocalizerRawData AS (\n    SELECT\n      SeriesInstanceUID,\n      StudyInstanceUID,\n      PatientID,\n      SAFE_CAST(Exposure AS FLOAT64) AS Exposure,\n      SAFE_CAST(ipp AS FLOAT64) AS zImagePosition,\n      LEAD(SAFE_CAST(ipp AS FLOAT64)) OVER (PARTITION BY SeriesInstanceUID ORDER BY SAFE_CAST(ipp AS FLOAT64)) - SAFE_CAST(ipp AS FLOAT64) AS slice_interval,\n      instance_size AS instanceSize  -- Ensure instanceSize is included here\n    FROM\n      `spider2-public-data.idc_v17.dicom_all` bid\n    LEFT JOIN\n      UNNEST(bid.ImagePositionPatient) ipp WITH OFFSET AS axes\n    WHERE\n      collection_id = 'nlst' \n      AND Modality = 'CT' \n  ),\n  geometryChecks AS (\n    SELECT\n      SeriesInstanceUID,\n      StudyInstanceUID,\n      PatientID,\n      ARRAY_AGG(DISTINCT slice_interval IGNORE NULLS) AS sliceIntervalDifferences,\n      ARRAY_AGG(DISTINCT Exposure IGNORE NULLS) AS distinctExposures,\n      SUM(instanceSize) / 1024 / 1024 AS seriesSizeInMB\n    FROM\n      nonLocalizerRawData\n    GROUP BY\n      SeriesInstanceUID, \n      StudyInstanceUID,\n      PatientID\n  ),\n  patientMetrics AS (\n    SELECT\n      PatientID,\n      MAX(sid) AS maxSliceIntervalDifference,\n      MIN(sid) AS minSliceIntervalDifference,\n      MAX(sid) - MIN(sid) AS sliceIntervalDifferenceTolerance,\n      MAX(de) AS maxExposure,\n      MIN(de) AS minExposure,\n      MAX(de) - MIN(de) AS maxExposureDifference,\n      seriesSizeInMB\n    FROM\n      geometryChecks\n    LEFT JOIN\n      UNNEST(sliceIntervalDifferences) AS sid\n    LEFT JOIN\n      UNNEST(distinctExposures) AS de\n    WHERE\n      sid IS NOT NULL\n      AND de IS NOT NULL\n    GROUP BY\n      PatientID,\n      seriesSizeInMB\n  ),\n  top3BySliceInterval AS (\n    SELECT\n      PatientID,\n      seriesSizeInMB\n    FROM\n      patientMetrics\n    ORDER BY\n      sliceIntervalDifferenceTolerance DESC\n    LIMIT 3\n  ),\n  top3ByMaxExposure AS (\n    SELECT\n      PatientID,\n      seriesSizeInMB\n    FROM\n      patientMetrics\n    ORDER BY\n      maxExposureDifference DESC\n    LIMIT 3\n  )\nSELECT\n  'Top 3 by Slice Interval' AS MetricGroup,\n  AVG(seriesSizeInMB) AS AverageSeriesSizeInMB\nFROM\n  top3BySliceInterval\nUNION ALL\nSELECT\n  'Top 3 by Max Exposure' AS MetricGroup,\n  AVG(seriesSizeInMB) AS AverageSeriesSizeInMB\nFROM\n  top3ByMaxExposure;",
        "external_knowledge": null,
        "plan": "1. **Extract Initial Data**:\n   - Retrieve relevant data, including unique identifiers, exposure levels, image positions, and instance sizes.\n   - Ensure all values are correctly formatted and calculate the slice intervals for each series.\n\n2. **Aggregate Data**:\n   - Group the data by unique identifiers to aggregate slice interval differences and distinct exposures.\n   - Compute the total series size in MiB for each series.\n\n3. **Compute Patient Metrics**:\n   - For each patient, calculate the maximum and minimum slice interval differences, then derive the tolerance by subtracting the minimum from the maximum.\n   - Similarly, calculate the maximum and minimum exposure differences, then derive the maximum exposure difference.\n   - Include the total series size in MiB for further analysis.\n\n4. **Identify Top Patients**:\n   - Determine the top 3 patients with the highest slice interval difference tolerance.\n   - Determine the top 3 patients with the highest maximum exposure difference.\n\n5. **Calculate Averages**:\n   - For the top 3 patients identified by slice interval difference tolerance, compute the average series size in MiB.\n   - For the top 3 patients identified by maximum exposure difference, compute the average series size in MiB.\n\n6. **Combine Results**:\n   - Present the results in a unified format, distinguishing between the two metric groups (slice interval and exposure), and report the average series sizes for each group.",
        "special_function": null
    },
    {
        "instance_id": "bq069",
        "db": "spider2-public-data.idc_v17",
        "question": "Could you help me generate a CT Image Series report which excludes the NLST study and those do not conform to some geometrical checks, also filter out those series that require additional decompression steps before passing to dcm2niix for conversion to NIFTI format.",
        "SQL": "WITH\n  -- Create a common table expression (CTE) named localizerAndJpegCompressedSeries\n  localizerAndJpegCompressedSeries AS (\n  Select SeriesInstanceUID  from \n  `spider2-public-data.idc_v17.dicom_all` bid, bid.ImageType image_type \n   where \n   image_type='LOCALIZER' OR  \n   TransferSyntaxUID  IN ( '1.2.840.10008.1.2.4.70','1.2.840.10008.1.2.4.51')\n   --these are the classes that require additional processing before converting DICOM files to NIfTI using dcm2niix\n  ),\n  -- Create a common table expression (CTE) named nonLocalizerRawData \n  nonLocalizerRawData AS (\n    SELECT\n      SeriesInstanceUID,\n      StudyInstanceUID,\n      PatientID,\n      SOPInstanceUID,\n      SliceThickness,\n      ImageType,\n      TransferSyntaxUID,\n      SeriesNumber,\n      aws_bucket,\n      crdc_series_uuid,\n\n      -- Cast Exposure column as FLOAT64 data type\n      SAFE_CAST(Exposure AS FLOAT64) Exposure,\n      -- Cast unnested Image Patient Position column as FLOAT64 data type and rename it as zImagePosition as we filter for the z-axis\n      SAFE_CAST(ipp AS FLOAT64) AS zImagePosition,\n      -- first and second coordinates \n      CONCAT(ipp2, '/', ipp3) AS xyImagePosition,\n      -- Calculate the difference between the current and next zImagePosition for each SeriesInstanceUID for slice_interval\n      LEAD(SAFE_CAST(ipp AS FLOAT64)) OVER (PARTITION BY SeriesInstanceUID ORDER BY SAFE_CAST(ipp AS FLOAT64)) - SAFE_CAST(ipp AS FLOAT64) AS slice_interval,\n      -- Convert ImageOrientationPatient array to a string separated by \"/\"\n      ARRAY_TO_STRING(ImageOrientationPatient, '/') AS iop,\n      (\n        -- Extract the first three elements of ImageOrientationPatient array and convert them to FLOAT64 data type\n        SELECT ARRAY_AGG(SAFE_CAST(part AS FLOAT64))\n        FROM UNNEST(ImageOrientationPatient) part WITH OFFSET index\n        WHERE index BETWEEN 0 AND 2\n      ) AS x_vector,\n      (\n        -- Extract the last three elements of ImageOrientationPatient array and convert them to FLOAT64 data type\n        SELECT ARRAY_AGG(SAFE_CAST(part AS FLOAT64))\n        FROM UNNEST(ImageOrientationPatient) part WITH OFFSET index\n        WHERE index BETWEEN 3 AND 5\n      ) AS y_vector,\n      -- Convert PixelSpacing array to a string separated by \"/\"\n      ARRAY_TO_STRING(PixelSpacing, '/') AS pixelSpacing,\n      -- Store the number of rows and columns in the pixel matrix\n      `Rows` AS pixelRows,\n      `Columns` AS pixelColumns,\n      -- Store the size of the SOP Instance in bytes\n      instance_size AS instanceSize,\n    FROM\n      `spider2-public-data.idc_v17.dicom_all` bid\n    LEFT JOIN\n      UNNEST(bid.ImagePositionPatient) ipp WITH OFFSET AS axes\n    LEFT JOIN\n      UNNEST(bid.ImagePositionPatient) ipp2 WITH OFFSET AS axis1\n    LEFT JOIN\n      UNNEST(bid.ImagePositionPatient) ipp3 WITH OFFSET AS axis2\n    WHERE\n      -- Filter for CT images in the NLST collection that are not localizers \n      -- and removing the transfer syntax ids that require additional processing (decompression before passing to dcm2niix)\n      -- extract x, y, z coordinates from ImagePositionPatient, respectively\n      collection_id != 'nlst' AND Modality = 'CT' AND axes = 2 AND axis1 = 0 AND axis2 = 1 \n      AND SeriesInstanceUID not in (Select SeriesInstanceuID from  localizerAndJpegCompressedSeries)\n  )\n,\ncrossProduct AS (\n  SELECT\n    SOPInstanceUID,\n    SeriesInstanceUID,\n    -- Calculate the cross product of x_vector and y_vector for each row in nonLocalizerRawData\n    (SELECT AS STRUCT\n      (x_vector[OFFSET(1)]*y_vector[OFFSET(2)] - x_vector[OFFSET(2)]*y_vector[OFFSET(1)]) AS x,\n      (x_vector[OFFSET(2)]*y_vector[OFFSET(0)] - x_vector[OFFSET(0)]*y_vector[OFFSET(2)]) AS y,\n      (x_vector[OFFSET(0)]*y_vector[OFFSET(1)] - x_vector[OFFSET(1)]*y_vector[OFFSET(0)]) AS z\n    ) AS xyCrossProduct\n  FROM nonLocalizerRawData\n),\ndotProduct AS (\n  SELECT\n    SOPInstanceUID,\n    SeriesInstanceUID,\n    xyCrossProduct,\n    -- Calculate the dot product of xyCrossProduct and [0, 0, 1] for each row in crossProduct\n    (\n      SELECT SUM(element1 * element2)\n      FROM UNNEST([xyCrossProduct.x, xyCrossProduct.y, xyCrossProduct.z]) element1 WITH OFFSET pos\n      JOIN UNNEST([0, 0, 1]) element2 WITH OFFSET pos\n      USING(pos)\n    ) AS xyDotProduct\n  FROM crossProduct\n)\n,\ngeometryChecks AS (\n  SELECT\n    SeriesInstanceUID,\n    seriesNumber,\n    aws_bucket,\n    crdc_series_uuid,\n    StudyInstanceUID,\n    PatientID,\n    -- Aggregate distinct slice_interval values into an array \n    ARRAY_AGG(DISTINCT(slice_interval) ignore nulls) AS sliceIntervalDifferences,\n    -- Aggregate distinct Exposure values into an array \n    ARRAY_AGG(DISTINCT(Exposure) ignore nulls) AS distinctExposures,\n    -- Count the number of distinct Image Orientation Patient values \n    COUNT(DISTINCT iop) iopCount,\n    -- Count the number of distinct pixelSpacing values \n    COUNT(DISTINCT pixelSpacing) pixelSpacingCount,\n    -- Count the number of distinct zImagePosition values \n    COUNT(Distinct zImagePosition) positionCount,\n    -- Count the number of distinct xyImagePosition values     \n    COUNT(Distinct xyImagePosition) xyPositionCount,\n     -- Count the number of distinct SOPInstanceUIDs \n     COUNT(Distinct SOPInstanceUID) sopInstanceCount,\n     -- Count the number of distinct SliceThickness values \n     COUNT(Distinct SliceThickness) sliceThicknessCount,\n     -- Count the number of distinct Exposure values \n     COUNT(Distinct Exposure) exposureCount,\n     -- Count the number of distinct pixel row values \n     COUNT(Distinct pixelRows) pixelRowCount,\n     -- Count the number of distinct pixel column values      \n     COUNT(Distinct pixelColumns) pixelColumnCount,\n     --Determining maximum dotProduct..ideally this should be zero\n     max(xyDotProduct) dotProduct,\n     -- Calculate sum of instanceSize divided by 1024*1024 to get te size in MB\n     sum(instanceSize) / 1024 / 1024 seriesSizeInMiB,\n  FROM\n      nonLocalizerRawData\n  JOIN dotProduct using (SeriesInstanceUID, SOPInstanceUID)\n  GROUP BY\n      SeriesInstanceUID, \n      seriesNumber,\n      aws_bucket,\n      crdc_series_uuid,\n      StudyInstanceUID,\n      PatientID\n  HAVING\n    iopCount = 1 --we expect only one image orientation in a series\n    AND pixelSpacingCount = 1  --we expect identical pixel spacing in a series\n    AND sopInstanceCount = positionCount --we expect position counts are same as sopInstances count. this would also allow us to filter 4D series\n    AND xyPositionCount = 1 --we expect first two values are same across the series\n    AND pixelColumnCount = 1 --we expect consistent pixel Columns across the series\n    AND pixelRowCount = 1 --we expect consistent pixel Rows across the series\n    AND abs(dotProduct) between 0.99 and 1.01 --we expect the dot product of x and y vectors to be ideally one, however we are allowing for minor deviations (0.01)\n)\n\nSELECT\n  SeriesInstanceUID,\n  seriesNumber,\n  StudyInstanceUID,\n  PatientID,\n  dotProduct,\n  sopInstanceCount,\n  sliceThicknessCount,\n  max(sid) as maxSliceIntervalDifference,\n  min(sid) as minSliceIntervalDifference,\n  max(sid) - min (sid) as sliceIntervalifferenceTolerance,\n  exposureCount,\n  max(de) as maxExposure,\n  min(de) as minExposure,\n  max(de) - min (de) as maxExposureDifference,\n  seriesSizeInMiB\nFROM\n  geometryChecks\nLEFT JOIN\n  UNNEST(sliceIntervalDifferences) sid\nLEFT JOIN\n  UNNEST(distinctExposures) de\nGROUP BY\n  SeriesInstanceUID,\n  seriesNumber,\n  StudyInstanceUID,\n  PatientID,\n  dotProduct,\n  sopInstanceCount,\n  sliceThicknessCount,\n  exposureCount,\n  seriesSizeInMiB\nORDER BY\n    sliceIntervalifferenceTolerance desc,\n    maxExposureDifference desc,\n    SeriesInstanceUID desc",
        "external_knowledge": "nonNlstCohort.md",
        "plan": "1. **Data Filtering and Exclusion**:\n   - Create a list of image series to exclude based on specific criteria such as study collection, modality, and compression formats.\n   - Filter out any series containing localizer images and those using specified compression formats.\n\n2. **Data Extraction and Transformation**:\n   - Extract relevant attributes including identifiers, image positions, orientations, pixel spacing, exposure values, and other necessary metadata.\n   - Perform necessary type casting and calculations, such as the difference in image positions (slice intervals) and the conversion of orientation and spacing arrays to strings for comparison.\n\n3. **Vector Calculations**:\n   - Compute the cross product of the image orientation vectors for each image series.\n   - Calculate the dot product between the cross product and a reference vector to ensure geometric alignment.\n\n4. **Geometric and Consistency Checks**:\n   - Aggregate and count distinct values for various attributes to enforce consistency within each image series.\n   - Apply conditions to ensure all instances in a series have consistent attributes such as orientation, pixel spacing, image positions, and dimensions.\n   - Ensure the dot product falls within a specified range to maintain geometric alignment.\n\n5. **Final Selection and Reporting**:\n   - Aggregate and compute summary statistics for each series, including the number of instances, slice thickness values, exposure values, and total size.\n   - Filter out series that do not meet the minimum number of instances and other specified tolerances.\n   - Sort the final results by specific criteria to generate the desired report.",
        "special_function": [
            "aggregate-functions/ARRAY_AGG",
            "array-functions/ARRAY",
            "array-functions/ARRAY_TO_STRING",
            "conversion-functions/CAST",
            "conversion-functions/SAFE_CAST",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "json-functions/STRING",
            "mathematical-functions/ABS",
            "navigation-functions/LEAD",
            "string-functions/CONCAT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/STRING",
            "other-functions/UNNEST",
            "other-functions/ARRAY_SUBSCRIPT",
            "other-functions/STRUCT_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "bq219",
        "db": "bigquery-public-data.iowa_liquor_sales",
        "question": "Which two liquor categories, each contributing an average of at least 1% to monthly sales volume over 24 months, have the lowest Pearson correlation coefficient in their sales percentages?",
        "SQL": "WITH\nMonthlyTotals AS\n(\n  SELECT\n    FORMAT_DATE('%Y-%m', date) AS month,\n    SUM(volume_sold_gallons) AS total_monthly_volume\n\n  FROM\n    `bigquery-public-data.iowa_liquor_sales.sales`\n\n  WHERE\n    # Start w/ date given by query parameter\n    date >= \"2022-01-01\" AND\n    # Remove current month so as to avoid partial data\n    FORMAT_DATE('%Y-%m', date) < FORMAT_DATE('%Y-%m', CURRENT_DATE())\n    \n  GROUP BY\n    month\n),\n\nMonthCategory AS\n(\n  SELECT\n    FORMAT_DATE('%Y-%m', date) AS month,\n    category,\n    category_name,\n\n    SUM(volume_sold_gallons) AS category_monthly_volume,\n\n    SAFE_DIVIDE(\n      SUM(volume_sold_gallons),\n      total_monthly_volume\n      ) * 100 AS category_pct_of_month_volume\n\n  FROM\n    `bigquery-public-data.iowa_liquor_sales.sales` Sales\n    \n  LEFT JOIN\n    MonthlyTotals ON \n      FORMAT_DATE('%Y-%m', Sales.date) = MonthlyTotals.month\n\n  WHERE\n    # Start w/ date given by query parameter\n    date >= \"2022-01-01\" AND    \n    # Remove current month so as to avoid partial data\n    FORMAT_DATE('%Y-%m', date) < FORMAT_DATE('%Y-%m', CURRENT_DATE())\n\n  GROUP BY\n    month, category, category_name, total_monthly_volume\n),\n\nmiddle_info AS (\nSELECT\n  Category1.category AS category1,\n  Category1.category_name AS category_name1,\n\n  Category2.category AS category2,\n  Category2.category_name AS category_name2,\n\n  COUNT(DISTINCT Category1.month) AS num_months,\n\n  CORR(\n    Category1.category_pct_of_month_volume,\n    Category2.category_pct_of_month_volume\n    ) AS category_corr_across_months,\n\n  AVG(Category1.category_pct_of_month_volume) AS\n    category1_avg_pct_of_month_volume,\n  AVG(Category2.category_pct_of_month_volume) AS\n    category2_avg_pct_of_month_volume\n\nFROM\n  MonthCategory Category1\n\nINNER JOIN\n  MonthCategory Category2 ON\n  (\n    Category1.month = Category2.month\n  )\n\nGROUP BY\n  category1, category_name1, category2, category_name2\n\nHAVING\n  num_months >= 24 AND\n  category1_avg_pct_of_month_volume >= 1 AND\n  category2_avg_pct_of_month_volume >= 1\n  \n)\n\nSELECT category_name1, category_name2\nFROM middle_info\nORDER BY category_corr_across_months\nLIMIT 1",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. **Calculate Monthly Totals:**\n   - For each month, sum the total sales volume.\n   - Format the date to the year-month format for consistency.\n   - Ensure only complete months within the specified date range are considered.\n\n2. **Compute Monthly Category Sales:**\n   - For each month and category, calculate the total sales volume.\n   - Compute the percentage contribution of each category to the monthly total sales volume.\n   - Ensure the date range matches the one used in the first step.\n\n3. **Calculate Correlation and Averages:**\n   - Pair each category with every other category for each month.\n   - Calculate the Pearson correlation coefficient of the monthly sales percentages between each pair of categories.\n   - Compute the average monthly sales percentage for each category in each pair.\n   - Ensure that the data spans at least 24 months.\n\n4. **Filter and Select the Categories:**\n   - Only consider category pairs where both categories contribute an average of at least 1% to the monthly sales volume.\n   - Select the pair of categories with the lowest Pearson correlation coefficient.\n   - Return the names of these two categories.",
        "special_function": null
    },
    {
        "instance_id": "bq199",
        "db": "bigquery-public-data.iowa_liquor_sales",
        "question": "Identify the top 10 liquor categories in Iowa by average price per liter in 2021, and provide their average prices per liter for 2019, 2020, and 2021.",
        "SQL": "WITH price_2020 AS (\n  SELECT \n    category_name AS category, \n    AVG(state_bottle_retail / (bottle_volume_ml / 1000)) AS avg_price_liter_2020\n  FROM \n    `bigquery-public-data.iowa_liquor_sales.sales`\n  WHERE \n    bottle_volume_ml > 0 \n    AND EXTRACT(YEAR FROM date) = 2020\n  GROUP BY \n    category\n),\nprice_2019 AS (\n  SELECT \n    category_name AS category, \n    AVG(state_bottle_retail / (bottle_volume_ml / 1000)) AS avg_price_liter_2019\n  FROM \n    `bigquery-public-data.iowa_liquor_sales.sales`\n  WHERE \n    bottle_volume_ml > 0 \n    AND EXTRACT(YEAR FROM date) = 2019\n  GROUP BY \n    category\n),\nprice_2021 AS (\n  SELECT \n    category_name AS category, \n    AVG(state_bottle_retail / (bottle_volume_ml / 1000)) AS avg_price_liter_2021\n  FROM \n    `bigquery-public-data.iowa_liquor_sales.sales`\n  WHERE \n    bottle_volume_ml > 0 \n    AND EXTRACT(YEAR FROM date) = 2021\n  GROUP BY \n    category\n)\nSELECT \n  price_2021.category, \n  price_2019.avg_price_liter_2019, \n  price_2020.avg_price_liter_2020, \n  price_2021.avg_price_liter_2021\nFROM \n  price_2021\nLEFT JOIN \n  price_2019 ON price_2021.category = price_2019.category\nLEFT JOIN \n  price_2020 ON price_2021.category = price_2020.category\nORDER BY \n  price_2021.avg_price_liter_2021 DESC\nLIMIT \n  10;",
        "external_knowledge": null,
        "plan": "1. **Define Subqueries for Each Year**:\n   - Create separate subqueries to calculate the average price per liter for each year (2019, 2020, and 2021).\n   - In each subquery, filter the data to include only records with a valid bottle volume.\n   - Extract the year from the date and filter for the specific year.\n   - Group the data by liquor category and calculate the average price per liter.\n\n2. **Join Subqueries on Category**:\n   - Use the results of the subqueries to join them based on the liquor category.\n   - Perform left joins to ensure that all categories from 2021 are included, even if they don't have corresponding data for 2019 or 2020.\n\n3. **Select and Order Results**:\n   - Select the liquor category and the calculated average prices per liter for 2019, 2020, and 2021 from the joined results.\n   - Order the final results by the average price per liter for 2021 in descending order to identify the top categories.\n\n4. **Limit the Number of Results**:\n   - Limit the output to the top 10 categories based on the highest average price per liter in 2021.\n\nBy following these steps, the query will identify the top 10 liquor categories in Iowa by average price per liter in 2021 and provide their average prices per liter for 2019, 2020, and 2021.",
        "special_function": null
    },
    {
        "instance_id": "bq218",
        "db": "bigquery-public-data.iowa_liquor_sales",
        "question": "What are the top 5 items with the highest year-over-year growth percentage in total sales revenue for the year 2023?",
        "SQL": "WITH AnnualSales AS (\n  SELECT\n    item_description,\n    EXTRACT(YEAR FROM date) AS year,\n    SUM(sale_dollars) AS total_sales_revenue,\n    COUNT(DISTINCT invoice_and_item_number) AS unique_purchases\n  FROM\n    `bigquery-public-data.iowa_liquor_sales.sales`\n  WHERE\n    EXTRACT(YEAR FROM date) IN (2022, 2023)\n    AND item_description IS NOT NULL\n    AND sale_dollars IS NOT NULL\n  GROUP BY\n    item_description, year\n),\nYoYGrowth AS (\n  SELECT\n    curr.item_description,\n    curr.year,\n    curr.total_sales_revenue,\n    curr.unique_purchases,\n    LAG(curr.total_sales_revenue) OVER(PARTITION BY curr.item_description ORDER BY curr.year) AS prev_year_sales_revenue,\n    (curr.total_sales_revenue - LAG(curr.total_sales_revenue) OVER(PARTITION BY curr.item_description ORDER BY curr.year)) / LAG(curr.total_sales_revenue) OVER(PARTITION BY curr.item_description ORDER BY curr.year) * 100 AS yoy_growth_percentage\n  FROM\n    AnnualSales curr\n),\ntotal_info AS (\nSELECT\n  item_description,\n  year,\n  total_sales_revenue,\n  unique_purchases,\n  prev_year_sales_revenue,\n  yoy_growth_percentage\nFROM\n  YoYGrowth\nWHERE\n  year = 2023\n  AND prev_year_sales_revenue IS NOT NULL -- Exclude rows where there's no previous year data to calculate YoY growth\nORDER BY\n  year, total_sales_revenue \nDESC\n)\n\nSELECT item_description\nFROM total_info\norder by yoy_growth_percentage\nDESC\nLIMIT 5",
        "external_knowledge": null,
        "plan": "1. **Filter and Aggregate Data**:\n   - Create a temporary dataset that includes item descriptions, year, total sales revenue, and the count of unique purchases.\n   - Filter the records to include only the years 2022 and 2023.\n   - Ensure that item descriptions and sales values are not null.\n   - Group the data by item description and year to calculate the total sales revenue and unique purchases for each item per year.\n\n2. **Calculate Year-over-Year Growth**:\n   - From the aggregated data, calculate the sales revenue of the previous year for each item.\n   - Compute the year-over-year growth percentage by comparing the current year's total sales revenue with the previous year's total sales revenue.\n\n3. **Filter for the Desired Year**:\n   - Select records where the year is 2023 and ensure there is previous year sales data available for calculating growth.\n   - Order the records by the year and total sales revenue in descending order.\n\n4. **Select Top Items**:\n   - From the filtered data, select only the item descriptions.\n   - Order the items by their year-over-year growth percentage in descending order.\n   - Limit the results to the top 5 items with the highest year-over-year growth percentage.",
        "special_function": null
    },
    {
        "instance_id": "bq049",
        "db": "bigquery-public-data.census_bureau_usa\nbigquery-public-data.iowa_liquor_sales",
        "question": "Display the monthly per capita Bourbon Whiskey sales in 2022 for the zip code with the third-highest total sales in Dubuque County, considering only the population aged 21 and over.",
        "SQL": "WITH DUBUQUE_LIQUOR_CTE AS (\nSELECT\n  CASE\n      WHEN UPPER(category_name) LIKE 'BUTTERSCOTCH SCHNAPPS' THEN 'All Other' --Edge case is not a scotch\n      WHEN UPPER(category_name) LIKE '%WHISKIES' \n            AND UPPER(category_name) NOT LIKE '%RYE%'\n            AND UPPER(category_name) NOT LIKE '%BOURBON%'\n            AND UPPER(category_name) NOT LIKE '%SCOTCH%'     THEN 'Other Whiskey'\n      WHEN UPPER(category_name) LIKE '%RYE%'                 THEN 'Rye Whiskey'\n      WHEN UPPER(category_name) LIKE '%BOURBON%'             THEN 'Bourbon Whiskey'\n      WHEN UPPER(category_name) LIKE '%SCOTCH%'              THEN 'Scotch Whiskey'\n      ELSE 'All Other'\n  END                              AS category_group,\n  EXTRACT(MONTH FROM date)         AS month,    -- At the time of this query, there is only data until month 6.\n  LEFT(CAST(zip_code AS string),5) AS zip_code, -- Casting to string necessary because zip_code has a mix of int & str types.\n  ROUND(SUM(sale_dollars), 2)      AS sale_dollars_sum,\n\nFROM \n  bigquery-public-data.iowa_liquor_sales.sales\n\nWHERE\n  UPPER(county)               = 'DUBUQUE'\n  AND EXTRACT(YEAR FROM date) = 2022\n\nGROUP BY\n  category_group,\n  month,\n  zip_code\n  \nORDER BY \n  category_group,\n  month,\n  zip_code\n),\n\nDUBUQUE_POPULATION_CTE AS (\nSELECT\n  zipcode,\n  SUM(population) AS population_sum\nFROM bigquery-public-data.census_bureau_usa.population_by_zip_2010\nWHERE \n  minimum_age >= 21\nGROUP BY \n  zipcode\n),\nMONTH_INFO AS (\nSELECT \n  l.month,\n  l.zip_code,\n  l.sale_dollars_sum,\n  ROUND(sale_dollars_sum/p.population_sum, 2) AS dollars_per_capita\nFROM \n  DUBUQUE_LIQUOR_CTE AS l\n  LEFT JOIN \n  DUBUQUE_POPULATION_CTE AS p\n  ON l.zip_code = p.zipcode\nWHERE\n  category_group = 'Bourbon Whiskey'\nGROUP BY \n  category_group,\n  zip_code,\n  month,\n  sale_dollars_sum,\n  zipcode,\n  population_sum\nORDER BY\n  zip_code,\n  month\n),\nzip_code_sales AS (\n    SELECT\n        zip_code,\n        SUM(sale_dollars_sum) AS total_sale_dollars_sum\n    FROM MONTH_INFO\n    GROUP BY zip_code\n),\nranked_zip_codes AS (\n    SELECT\n        zip_code,\n        total_sale_dollars_sum,\n        ROW_NUMBER() OVER (ORDER BY total_sale_dollars_sum DESC) AS rank\n    FROM zip_code_sales\n)\nSELECT\n    t.month,\n    t.zip_code,\n    t.dollars_per_capita\nFROM MONTH_INFO t\nJOIN ranked_zip_codes r\nON t.zip_code = r.zip_code\nWHERE r.rank = 3\nORDER BY t.month;",
        "external_knowledge": null,
        "plan": "1. Filter 2022 liquor sales data for Bourbon Whiskey in Dubuque County and group by zip code and month.\n2. Sum the sales dollars for each zip code and month\n3. Obtain the adult population per zip code from the census data.\n4. Calculate the monthly per capita sales by dividing the total sales by the population in each zip code\n5. Display the per capita sales for each month and zip code.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "json-functions/STRING",
            "mathematical-functions/ROUND",
            "numbering-functions/RANK",
            "numbering-functions/ROW_NUMBER",
            "string-functions/LEFT",
            "string-functions/UPPER",
            "time-functions/EXTRACT",
            "time-functions/TIME",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/STRING",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq360",
        "db": "bigquery-public-data.nppes",
        "question": "Which of the top 10 most common healthcare provider specializations in Mountain View, CA, has a specialist count closest to the average of these ten specializations?",
        "SQL": "WITH specialist_counts AS (\n  SELECT\n    healthcare_provider_taxonomy_1_specialization,\n    COUNT(DISTINCT npi) AS number_specialist\n  FROM\n    `bigquery-public-data.nppes.npi_optimized`\n  WHERE\n    provider_business_practice_location_address_city_name = \"MOUNTAIN VIEW\"\n    AND provider_business_practice_location_address_state_name = \"CA\"\n    AND healthcare_provider_taxonomy_1_specialization > \"\"\n  GROUP BY\n    healthcare_provider_taxonomy_1_specialization\n),\ntop_10_specialists AS (\n  SELECT\n    healthcare_provider_taxonomy_1_specialization,\n    number_specialist\n  FROM\n    specialist_counts\n  ORDER BY\n    number_specialist DESC\n  LIMIT 10\n),\naverage_value AS (\n  SELECT\n    AVG(number_specialist) AS average_specialist\n  FROM\n    top_10_specialists\n),\nclosest_to_average AS (\n  SELECT\n    healthcare_provider_taxonomy_1_specialization,\n    number_specialist,\n    ABS(number_specialist - (SELECT average_specialist FROM average_value)) AS difference\n  FROM\n    top_10_specialists\n)\nSELECT\n  healthcare_provider_taxonomy_1_specialization\nFROM\n  closest_to_average\nORDER BY\n  difference\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "1. **Count Specialists by Specialization**:\n   - Identify all unique specializations within the specified city and state.\n   - Count the number of specialists for each specialization.\n\n2. **Select Top 10 Specializations**:\n   - From the previous results, order the specializations by the number of specialists in descending order.\n   - Select the top 10 specializations based on the highest counts.\n\n3. **Calculate Average Number of Specialists**:\n   - Compute the average number of specialists across the top 10 specializations identified in the previous step.\n\n4. **Determine Differences from Average**:\n   - For each of the top 10 specializations, calculate the absolute difference between its number of specialists and the average number of specialists.\n\n5. **Identify Closest Specialization**:\n   - From the results of the previous step, order the specializations by the calculated difference.\n   - Select the specialization with the smallest difference, i.e., the one closest to the average.",
        "special_function": [
            "mathematical-functions/ABS"
        ]
    },
    {
        "instance_id": "bq286",
        "db": "bigquery-public-data.usa_names",
        "question": "Can you tell me the name of the most popular female baby in Wyoming for the year 2021, based on the proportion of female babies given that name compared to the total number of female babies given the same name across all states?",
        "SQL": "SELECT\n  a.name AS name\nFROM\n  `bigquery-public-data.usa_names.usa_1910_current` a\nJOIN (\n  SELECT\n    name,\n    gender,\n    year,\n    SUM(number) AS total_number\n  FROM\n    `bigquery-public-data.usa_names.usa_1910_current`\n  GROUP BY\n    name,\n    gender,\n    year) b\nON\n  a.name = b.name\n  AND a.gender = b.gender\n  AND a.year = b.year\nWHERE \n    a.gender = 'F' AND\n    a.state = 'WY' AND\n    a.year = 2021\nORDER BY (a.number / b.total_number) DESC\nLIMIT 1",
        "external_knowledge": null,
        "plan": "1. Calculate the total count of each name grouped by name, gender, and year.\n2. Join the above result with the main dataset containing records of names based on matching names, genders, and years.\n3. Filter the joined data to focus on records where the gender is female ('F') and the state is specific ('WY'), and the year is a particular year (2021).\n4. For the filtered results, calculate the ratio of the count of each name to its total count across all states.\n5. Sort the results in descending order based on the calculated ratio.\n6. Return the name with the highest proportion.\n",
        "special_function": null
    },
    {
        "instance_id": "bq044",
        "db": "spider2-public-data.TCGA_versioned",
        "question": "For bladder cancer patients who have mutations in the CDKN2A (cyclin-dependent kinase inhibitor 2A) gene, using clinical data from the Genomic Data Commons Release 39, what types of mutations are they, what is their gender, vital status, and days to death - and for four downstream genes (MDM2 (MDM2 proto-oncogene), TP53 (tumor protein p53), CDKN1A (cyclin-dependent kinase inhibitor 1A), and CCNE1 (Cyclin E1)), what are the gene expression levels for each patient?",
        "SQL": "SELECT\n  genex.case_barcode AS case_barcode,\n  genex.HGNC_gene_symbol AS HGNC_gene_symbol,\n  clinical_info.Variant_Type AS Variant_Type,\n  clinical_info.demo__gender AS gender,\n  clinical_info.demo__vital_status AS vital_status,\n  clinical_info.demo__days_to_death AS days_to_death,\n  genex.normalized_count AS normalized_count\nFROM ( /* This will get the clinical information for the cases*/\n  SELECT\n    case_list.Variant_Type AS Variant_Type,\n    case_list.case_barcode AS case_barcode,\n    clinical.demo__gender,\n    clinical.demo__vital_status,\n    clinical.demo__days_to_death\n  FROM\n    /* this will get the unique list of cases having the CDKN2A gene mutation in bladder cancer BLCA cases*/\n    (SELECT\n      mutation.case_barcode,\n      mutation.Variant_Type\n    FROM\n      `spider2-public-data.TCGA_versioned.somatic_mutation_hg19_DCC_2017_02` AS mutation\n    WHERE\n      mutation.Hugo_Symbol = 'CDKN2A'\n      AND project_short_name = 'TCGA-BLCA'\n    GROUP BY\n      mutation.case_barcode,\n      mutation.Variant_Type\n    ORDER BY\n      mutation.case_barcode\n      ) AS case_list /* end case_list */\n  INNER JOIN\n    `spider2-public-data.TCGA_versioned.clinical_gdc_r39` AS clinical\n  ON\n    case_list.case_barcode = clinical.submitter_id /* end clinical annotation */ ) AS clinical_info\nINNER JOIN\n  `spider2-public-data.TCGA_versioned.RNAseq_hg19_gdc_2017_02` AS genex\nON\n  genex.case_barcode = clinical_info.case_barcode\nWHERE\n  genex.HGNC_gene_symbol IN ('MDM2', 'TP53', 'CDKN1A', 'CCNE1')\nORDER BY\n  case_barcode,\n  HGNC_gene_symbol",
        "external_knowledge": "TCGA_Study_Abbreviations.md",
        "plan": "1. Find the patients with bladder cancer who have mutations in the CDKN2A gene, and display the patient ID and the type of mutation. We now have the list of patients who have a mutation in the CDKN2A gene and the type of mutation.\n2. Bring in the patient data from the ISB-CGC TCGA Clinical table so that we can see each patient\u2019s gender, vital status and days to death.\n3. Show the gene expression levels for the four genes of interest: 'MDM2', 'TP53', 'CDKN1A', 'CCNE1'.",
        "special_function": null
    },
    {
        "instance_id": "bq043",
        "db": "spider2-public-data.TCGA_versioned",
        "question": "What are the RNA expression levels of the genes MDM2, TP53, CDKN1A, and CCNE1, along with associated clinical information, in bladder cancer patients with CDKN2A mutations in the 'TCGA-BLCA' project?  Use clinical data from the Genomic Data Commons Release 39, data about somatic mutations derived from the hg19 human genome reference in Feb 2017.",
        "SQL": "SELECT\n  genex.case_barcode AS case_barcode,\n  genex.sample_barcode AS sample_barcode,\n  genex.aliquot_barcode AS aliquot_barcode,\n  genex.HGNC_gene_symbol AS HGNC_gene_symbol,\n  clinical_info.Variant_Type AS Variant_Type,\n  genex.gene_id AS gene_id,\n  genex.normalized_count AS normalized_count,\n  genex.project_short_name AS project_short_name,\n  clinical_info.demo__gender AS gender,\n  clinical_info.demo__vital_status AS vital_status,\n  clinical_info.demo__days_to_death AS days_to_death\nFROM ( \n  SELECT\n    case_list.Variant_Type AS Variant_Type,\n    case_list.case_barcode AS case_barcode,\n    clinical.demo__gender,\n    clinical.demo__vital_status,\n    clinical.demo__days_to_death\n  FROM\n    (SELECT\n      mutation.case_barcode,\n      mutation.Variant_Type\n    FROM\n      `spider2-public-data.TCGA_versioned.somatic_mutation_hg19_DCC_2017_02` AS mutation\n    WHERE\n      mutation.Hugo_Symbol = 'CDKN2A'\n      AND project_short_name = 'TCGA-BLCA'\n    GROUP BY\n      mutation.case_barcode,\n      mutation.Variant_Type\n    ORDER BY\n      mutation.case_barcode\n      ) AS case_list /* end case_list */\n  INNER JOIN\n    `spider2-public-data.TCGA_versioned.clinical_gdc_r39` AS clinical\n  ON\n    case_list.case_barcode = clinical.submitter_id /* end clinical annotation */ ) AS clinical_info\nINNER JOIN\n  `spider2-public-data.TCGA_versioned.RNAseq_hg19_gdc_2017_02` AS genex\nON\n  genex.case_barcode = clinical_info.case_barcode\nWHERE\n  genex.HGNC_gene_symbol IN ('MDM2', 'TP53', 'CDKN1A','CCNE1')\nORDER BY\n  case_barcode,\n  HGNC_gene_symbol",
        "external_knowledge": null,
        "plan": "Bring in the patient data from the ISB-CGC TCGA Clinical table so that we can see each patient\u2019s gender, vital status and days to death.",
        "special_function": null
    },
    {
        "instance_id": "bq143",
        "db": "isb-cgc-bq.CPTAC\nisb-cgc-bq.PDC_metadata",
        "question": "Use CPTAC proteomics and RNAseq data for Clear Cell Renal Cell Carcinoma to select 'Primary Tumor' and 'Solid Tissue Normal' samples. Join the datasets on sample submitter IDs and gene symbols. Calculate the correlation between protein abundance (log2 ratio) and gene expression levels (log-transformed+1 FPKM) for each gene and sample type. Filter out correlations with an absolute value greater than 0.5, and compute the average correlation for each sample type.",
        "SQL": "WITH \nquant AS (\n    SELECT \n        meta.sample_submitter_id, \n        meta.sample_type, \n        quant.case_id, \n        quant.aliquot_id, \n        quant.gene_symbol, \n        CAST(quant.protein_abundance_log2ratio AS FLOAT64) AS protein_abundance_log2ratio \n    FROM \n        `isb-cgc-bq.CPTAC.quant_proteome_CPTAC_CCRCC_discovery_study_pdc_current` AS quant\n    JOIN \n        `isb-cgc-bq.PDC_metadata.aliquot_to_case_mapping_current` AS meta\n        ON quant.case_id = meta.case_id\n        AND quant.aliquot_id = meta.aliquot_id\n        AND meta.sample_type IN ('Primary Tumor', 'Solid Tissue Normal')\n),\ngexp AS (\n    SELECT DISTINCT \n        meta.sample_submitter_id, \n        meta.sample_type, \n        rnaseq.gene_name, \n        LOG(rnaseq.fpkm_unstranded + 1) AS HTSeq__FPKM   -- Confirm the correct column name here\n    FROM \n        `isb-cgc-bq.CPTAC.RNAseq_hg38_gdc_current` AS rnaseq\n    JOIN \n        `isb-cgc-bq.PDC_metadata.aliquot_to_case_mapping_current` AS meta\n        ON meta.sample_submitter_id = rnaseq.sample_barcode\n),\ncorrelation AS (\n    SELECT \n        quant.gene_symbol, \n        gexp.sample_type, \n        COUNT(*) AS n, \n        CORR(protein_abundance_log2ratio, HTSeq__FPKM) AS corr  -- Confirm the correct column name here\n    FROM \n        quant \n    JOIN \n        gexp \n        ON quant.sample_submitter_id = gexp.sample_submitter_id\n        AND gexp.gene_name = quant.gene_symbol\n        AND gexp.sample_type = quant.sample_type\n    GROUP BY \n        quant.gene_symbol, gexp.sample_type\n),\npval AS (\n    SELECT  \n        gene_symbol, \n        sample_type, \n        n, \n        corr\n    FROM \n        correlation\n    WHERE \n        ABS(corr) <= 0.5\n)\nSELECT sample_type, AVG(corr)\nFROM pval\nGROUP BY sample_type;",
        "external_knowledge": null,
        "plan": "Create a SQL query that retrieves and merges protein and gene expression data for Clear Cell Renal Cell Carcinoma (CCRCC) from specified databases, filters the samples by type, calculates Pearson correlation coefficients for each gene across tumor and normal samples, and computes p-values for these correlations using a specific function. The results should include gene symbols, sample types, correlation values, and p-values.",
        "special_function": null
    },
    {
        "instance_id": "bq147",
        "db": "spider2-public-data.TCGA_versioned",
        "question": "Can you find which TCGA breast cancer cases include both normal and other types of tissue samples, focusing on protein-coding genes?",
        "SQL": "WITH rna as (\n    SELECT\n       case_barcode,\n       sample_barcode,\n       aliquot_barcode,\n       Ensembl_gene_id_v,\n       gene_name,\n       unstranded,\n       fpkm_uq_unstranded,\n       sample_type_name,\n    FROM `spider2-public-data.TCGA_versioned.RNAseq_hg38_gdc_r35`\n    WHERE gene_type = 'protein_coding'\n    AND project_short_name = 'TCGA-BRCA'\n),\ncases as (\n  SELECT case_barcode, agg FROM\n      (SELECT\n        case_barcode,\n        array_agg(distinct sample_type_name) agg\n      FROM rna\n      GROUP BY case_barcode)\n  WHERE array_length(agg) > 1\n  AND (\"Solid Tissue Normal\" in UNNEST(agg))\n  )\nSELECT * FROM cases",
        "external_knowledge": null,
        "plan": "1. **Filter the Data**:\n   - Select the relevant records from the dataset where the gene type is 'protein_coding' and the project name matches the specified breast cancer project.\n   \n2. **Create a Temporary Dataset**:\n   - Create a temporary dataset from the filtered data that includes necessary columns such as case identifiers, sample identifiers, gene identifiers, gene names, expression values, and sample types.\n\n3. **Aggregate Sample Types per Case**:\n   - Group the data by case identifiers.\n   - For each group, create an array that contains all distinct sample types associated with each case.\n\n4. **Filter Cases with Multiple Sample Types**:\n   - From the aggregated data, select cases where the array of sample types contains more than one distinct type.\n   - Ensure that one of the sample types in the array is specifically \"Solid Tissue Normal\".\n\n5. **Return the Result**:\n   - Select and return the case identifiers and their associated sample types that meet the criteria of having multiple sample types including \"Solid Tissue Normal\".\n\nBy following these steps, you will identify the cases that include both normal and other types of tissue samples in the context of protein-coding genes.",
        "special_function": null
    },
    {
        "instance_id": "bq148",
        "db": "spider2-public-data.TCGA_versioned",
        "question": "Could you list the top five protein-coding genes with the highest expression variability in 'Solid Tissue Normal' samples from multi-tissue TCGA-BRCA cases?",
        "SQL": "WITH rna as (\n    SELECT\n       case_barcode,\n       sample_barcode,\n       aliquot_barcode,\n       Ensembl_gene_id_v,\n       gene_name,\n       unstranded,\n       fpkm_uq_unstranded,\n       sample_type_name,\n    FROM `spider2-public-data.TCGA_versioned.RNAseq_hg38_gdc_r35`\n    WHERE gene_type = 'protein_coding'\n    AND project_short_name = 'TCGA-BRCA'\n),\ncases as (\n  SELECT case_barcode, agg FROM\n      (SELECT\n        case_barcode,\n        array_agg(distinct sample_type_name) agg\n      FROM rna\n      GROUP BY case_barcode)\n  WHERE \"Solid Tissue Normal\" in UNNEST(agg)\n  ),\nmean_expr as (\n  SELECT * FROM (\n    SELECT\n      rna.Ensembl_gene_id_v,\n      VARIANCE(rna.fpkm_uq_unstranded) var_fpkm\n    FROM rna\n    JOIN cases ON rna.case_barcode = cases.case_barcode\n    WHERE rna.sample_type_name = 'Solid Tissue Normal'\n    GROUP BY rna.Ensembl_gene_id_v)\n  ORDER BY var_fpkm DESC\n  )\n\nSELECT * FROM mean_expr LIMIT 5",
        "external_knowledge": null,
        "plan": "1. **Filter RNA Data**:\n   - Select RNA expression data focusing on protein-coding genes from a specific project.\n   - Ensure that the data includes necessary identifiers and expression values.\n\n2. **Identify Relevant Cases**:\n   - Aggregate the sample types for each case to identify which cases include 'Solid Tissue Normal' samples.\n   - Filter to retain only those cases that have 'Solid Tissue Normal' samples.\n\n3. **Calculate Expression Variability**:\n   - Join the filtered RNA data with the identified cases to focus on the 'Solid Tissue Normal' samples.\n   - Calculate the variability (variance) of expression levels for each gene across these samples.\n\n4. **Sort and Limit Results**:\n   - Order the genes by their expression variability in descending order.\n   - Select the top five genes with the highest variability.\n\nBy following these steps, you will obtain a list of the top five protein-coding genes with the highest expression variability in 'Solid Tissue Normal' samples from multi-tissue cases.",
        "special_function": null
    },
    {
        "instance_id": "bq175",
        "db": "spider2-public-data.TCGA_versioned\nmitelman-db.prod",
        "question": "Identify cytoband names on chromosome 1 in the TCGA-KIRC segment allelic dataset where the frequency of amplifications, gains, and heterozygous deletions each rank within the top 11. Calculate these rankings based on the maximum copy number observed across various genomic studies of kidney cancer, reflecting the severity of genetic alterations.",
        "SQL": "WITH copy AS (\n  SELECT case_barcode, chromosome, start_pos, end_pos, MAX(copy_number) as copy_number\n  FROM `spider2-public-data.TCGA_versioned.copy_number_segment_allelic_hg38_gdc_r23`\n  WHERE project_short_name = 'TCGA-KIRC'\n  GROUP BY case_barcode, chromosome, start_pos, end_pos\n),\ntotal_cases AS (\n  SELECT COUNT(DISTINCT case_barcode) as total\n  FROM copy \n),\ncytob AS (\n  SELECT chromosome, cytoband_name, hg38_start, hg38_stop\n  FROM `mitelman-db.prod.CytoBands_hg38`\n),\njoined AS (\n  SELECT cytob.chromosome, cytoband_name, case_barcode, copy_number\n  FROM copy\n  LEFT JOIN cytob ON cytob.chromosome = copy.chromosome\n  WHERE (cytob.hg38_start >= copy.start_pos AND copy.end_pos >= cytob.hg38_start)\n    OR (copy.start_pos >= cytob.hg38_start AND copy.start_pos <= cytob.hg38_stop)\n),\ncbands AS(\n  SELECT chromosome, cytoband_name, case_barcode, MAX(copy_number) as copy_number\n  FROM joined\n  GROUP BY chromosome, cytoband_name, case_barcode\n),\naberrations AS (\n  SELECT\n    cytoband_name,\n    SUM(IF(copy_number > 3, 1, 0)) AS total_amp,\n    SUM(IF(copy_number = 3, 1, 0)) AS total_gain,\n    SUM(IF(copy_number = 0, 1, 0)) AS total_homodel,\n    SUM(IF(copy_number = 1, 1, 0)) AS total_heterodel,\n    SUM(IF(copy_number = 2, 1, 0)) AS total_normal\n  FROM cbands\n  WHERE chromosome='chr1'\n  GROUP BY cytoband_name\n),\nresults AS (\n  SELECT \n    cytoband_name,\n    total_cases.total,\n    100 * total_amp / total as freq_amp, \n    100 * total_gain / total as freq_gain,\n    100 * total_heterodel / total as freq_heterodel,\n    RANK() OVER (ORDER BY 100 * total_amp / total DESC) AS rank_amp,\n    RANK() OVER (ORDER BY 100 * total_gain / total DESC) AS rank_gain,\n    RANK() OVER (ORDER BY 100 * total_heterodel / total DESC) AS rank_heterodel\n  FROM aberrations, total_cases\n)\nSELECT cytoband_name\nFROM results\nWHERE rank_amp <= 11 AND rank_gain <= 11  AND rank_heterodel <= 11",
        "external_knowledge": "Comprehensive_Guide_to_Copy_Number_Variations_in_Cancer_Genomics.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq176",
        "db": "spider2-public-data.TCGA_versioned\nmitelman-db.prod",
        "question": "Identify the case barcodes from the TCGA-LAML study with the highest weighted average copy number in cytoband 15q11 on chromosome 15, using segment data and cytoband overlaps from TCGA's genomic and Mitelman databases.",
        "SQL": "WITH copy AS (\n  SELECT case_barcode,        #sample_barcode,        aliquot_barcode, \n    chromosome,        start_pos,        end_pos,        MAX(copy_number) as copy_number\n  FROM `spider2-public-data.TCGA_versioned.copy_number_segment_allelic_hg38_gdc_r23` \n  WHERE  project_short_name = 'TCGA-LAML'\n  GROUP BY case_barcode, chromosome,        start_pos,        end_pos\n),\ntotal_cases AS (\n  SELECT COUNT( DISTINCT case_barcode) as total\n  FROM copy \n),\ncytob AS (\n  SELECT chromosome, cytoband_name, hg38_start, hg38_stop,\n  FROM mitelman-db.prod.CytoBands_hg38\n),\njoined AS (\n  SELECT cytob.chromosome, cytoband_name, hg38_start, hg38_stop,\n    case_barcode,\n    ( ABS(hg38_stop - hg38_start) + ABS(end_pos - start_pos) \n      - ABS(hg38_stop - end_pos) - ABS(hg38_start - start_pos) )/2.0  AS overlap ,\n    copy_number  \n  FROM copy\n  LEFT JOIN cytob\n  ON cytob.chromosome = copy.chromosome \n  WHERE \n    ( cytob.hg38_start >= copy.start_pos AND copy.end_pos >= cytob.hg38_start )\n    OR ( copy.start_pos >= cytob.hg38_start  AND  copy.start_pos <= cytob.hg38_stop )\n),\nINFO AS (\nSELECT chromosome, cytoband_name, hg38_start, hg38_stop, case_barcode,\n    ROUND( SUM(overlap*copy_number) / SUM(overlap) ) as copy_number\nFROM joined\nGROUP BY \n   chromosome, cytoband_name, hg38_start, hg38_stop, case_barcode\n)\n\nSELECT case_barcode\nFROM INFO\nWHERE\nchromosome = 'chr15' AND cytoband_name = '15q11'\nORDER BY copy_number\nDESC\nLIMIT 1",
        "external_knowledge": null,
        "plan": "1. **Extract Relevant Data:**\n   - Retrieve data related to case identifiers, chromosome, start and end positions, and the maximum copy number from a specific genomic database.\n   - Filter the data to include only entries from the specified study.\n   - Group the data by case identifier, chromosome, start, and end positions to get the maximum copy number for each segment.\n\n2. **Count Unique Cases:**\n   - Count the distinct number of cases from the previously filtered data.\n\n3. **Get Cytoband Information:**\n   - Retrieve cytoband information including chromosome, cytoband name, start, and end positions from a cytoband database.\n\n4. **Join Segment and Cytoband Data:**\n   - Perform a join operation between the segment data and the cytoband data based on the chromosome.\n   - Calculate the overlap between segment positions and cytoband positions.\n   - Retain relevant data including chromosome, cytoband name, start and end positions, case identifier, overlap length, and copy number.\n\n5. **Compute Weighted Average Copy Number:**\n   - For each combination of chromosome, cytoband, and case identifier, compute the weighted average copy number.\n   - This is done by summing the product of overlap length and copy number, then dividing by the total overlap length.\n\n6. **Identify Case with Highest Copy Number:**\n   - Filter the results to include only entries for a specific chromosome and cytoband.\n   - Sort the results by the computed copy number in descending order.\n   - Limit the output to the top result to identify the case with the highest weighted average copy number in the specified cytoband.",
        "special_function": null
    },
    {
        "instance_id": "bq170",
        "db": "spider2-public-data.TCGA_versioned\nmitelman-db.prod",
        "question": "For breast cancer cases (TCGA-BRCA) from REL 23 of the active GDC archive, identify and categorize copy number variations (CNVs) across different cytobands on all chromosomes. CNVs include amplifications, gains, homozygous deletions, heterozygous deletions, and normal diploid states. For each cytoband, tell me its name and start/end position, and calculate the frequency of each CNV type as a percentage of the total number of cases (rounded to 2 decimals).",
        "SQL": "WITH copy AS (\n    SELECT\n        case_barcode,\n        chromosome,\n        start_pos,\n        end_pos,\n        MAX(copy_number) as copy_number\n    FROM `spider2-public-data.TCGA_versioned.copy_number_segment_allelic_hg38_gdc_r23` \n    WHERE  project_short_name = 'TCGA-BRCA'\n    GROUP BY case_barcode, chromosome, start_pos, end_pos\n),\ntotal_cases AS (\n    SELECT COUNT(DISTINCT case_barcode) as total\n    FROM copy \n),\ncytob AS (\n    SELECT chromosome, cytoband_name, hg38_start, hg38_stop,\n    FROM mitelman-db.prod.CytoBands_hg38\n),\njoined AS (\n    SELECT\n        cytob.chromosome,\n        cytoband_name,\n        hg38_start,\n        hg38_stop,\n        case_barcode,\n        (ABS(hg38_stop - hg38_start) + ABS(end_pos - start_pos) \n        - ABS(hg38_stop - end_pos) - ABS(hg38_start - start_pos)) / 2.0  AS overlap,\n        copy_number\n    FROM copy\n    LEFT JOIN cytob\n    ON cytob.chromosome = copy.chromosome \n    WHERE \n        ( cytob.hg38_start >= copy.start_pos AND copy.end_pos >= cytob.hg38_start )\n        OR ( copy.start_pos >= cytob.hg38_start  AND  copy.start_pos <= cytob.hg38_stop )\n),\ncbands AS(\n    SELECT chromosome, cytoband_name, hg38_start, hg38_stop, case_barcode,\n        ROUND( SUM(overlap * copy_number) / SUM(overlap) ) as copy_number\n    FROM joined\n    GROUP BY \n    chromosome, cytoband_name, hg38_start, hg38_stop, case_barcode\n),\naberrations AS (\n  SELECT\n    chromosome,\n    cytoband_name,\n    hg38_start,\n    hg38_stop,\n    SUM( IF( copy_number = 0, 1, 0) ) AS total_homodel,\n    SUM( IF( copy_number = 1, 1, 0) ) AS total_heterodel,\n    SUM( IF( copy_number = 2, 1, 0) )  AS total_normal,\n    SUM( IF( copy_number = 3 ,1, 0) ) AS total_gain,\n    SUM( IF (copy_number > 3 , 1 , 0) ) AS total_amp\n\n  FROM cbands\n  GROUP BY chromosome, cytoband_name, hg38_start, hg38_stop\n),\ntcga AS (\n    SELECT\n        chromosome, cytoband_name, hg38_start, hg38_stop,\n        total,  \n        ROUND(100 * total_homodel/ total, 2) as freq_homodel, \n        ROUND(100 * total_heterodel / total, 2)as freq_heterodel, \n        ROUND(100 * total_normal / total, 2) as freq_normal,\n        ROUND(100 * total_gain / total, 2) as freq_gain,\n        ROUND(100 * total_amp / total, 2)as freq_amp\n    FROM aberrations, total_cases\n    ORDER BY chromosome, hg38_start, hg38_stop\n)\nSELECT cytoband_name, hg38_start, hg38_stop, freq_homodel, freq_heterodel, freq_normal, freq_gain, freq_amp\nFROM tcga;",
        "external_knowledge": "copy_number_variations.md",
        "plan": "1. **Filter Data**:\n   - Select relevant records from the dataset for specific cancer cases.\n   - Aggregate the data to identify maximum copy number variations for each case and genomic region.\n\n2. **Count Total Cases**:\n   - Calculate the total number of unique cases to use later for percentage calculations.\n\n3. **Retrieve Cytoband Information**:\n   - Extract cytoband details including chromosome, name, and start/end positions.\n\n4. **Join Datasets**:\n   - Merge the filtered cancer case data with cytoband information based on overlapping genomic regions.\n\n5. **Calculate Overlaps and Aggregate Copy Numbers**:\n   - For each case and cytoband, calculate the extent of overlap between the case's genomic region and the cytoband.\n   - Aggregate the copy number data weighted by the overlap to determine the predominant copy number variation for each case within each cytoband.\n\n6. **Categorize Copy Number Variations**:\n   - Group the data by cytoband and categorize the copy number variations into different types: homozygous deletions, heterozygous deletions, normal diploid states, gains, and amplifications.\n   - Count the occurrences of each type of variation for every cytoband.\n\n7. **Calculate Frequencies**:\n   - Using the total number of cases, calculate the frequency of each type of copy number variation for each cytoband as a percentage.\n   - Round the frequencies to two decimal places.\n\n8. **Final Output**:\n   - Select the cytoband name, start/end positions, and the calculated frequencies of each type of copy number variation.\n   - Order the results by chromosome and cytoband positions for clarity and structure.",
        "special_function": null
    },
    {
        "instance_id": "bq150",
        "db": "spider2-public-data.TCGA_hg19_data_v0",
        "question": "Assess whether different genetic variants affect the log10-transformed TP53 expression levels in TCGA-BRCA samples using sequencing and mutation data. Provide the total number of samples, the number of mutation types, the mean square between groups, the mean square within groups, and the F-statistic.",
        "SQL": "WITH\ncohortExpr AS (\n  SELECT\n    sample_barcode,\n    LOG10(normalized_count) AS expr\n  FROM\n    `spider2-public-data.TCGA_hg19_data_v0.RNAseq_Gene_Expression_UNC_RSEM`\n  WHERE\n    project_short_name = 'TCGA-BRCA'\n    AND HGNC_gene_symbol = 'TP53'\n    AND normalized_count IS NOT NULL\n    AND normalized_count > 0\n),\ncohortVar AS (\n  SELECT\n    Variant_Type,\n    sample_barcode_tumor AS sample_barcode\n  FROM\n    `spider2-public-data.TCGA_hg19_data_v0.Somatic_Mutation_MC3`\n  WHERE\n    SYMBOL = 'TP53'\n),\ncohort AS (\n  SELECT\n    e.sample_barcode AS sample_barcode,\n    v.Variant_Type AS group_name,\n    e.expr\n  FROM\n    cohortExpr e\n  JOIN\n    cohortVar v\n  ON\n    e.sample_barcode = v.sample_barcode\n),\ngrandMeanTable AS (\n  SELECT\n    AVG(expr) AS grand_mean\n  FROM\n    cohort\n),\ngroupMeansTable AS (\n  SELECT\n    AVG(expr) AS group_mean,\n    group_name,\n    COUNT(sample_barcode) AS n\n  FROM\n    cohort\n  GROUP BY\n    group_name\n),\nssBetween AS (\n  SELECT\n    g.group_name,\n    g.group_mean,\n    gm.grand_mean,\n    g.n,\n    g.n * POW(g.group_mean - gm.grand_mean, 2) AS n_diff_sq\n  FROM\n    groupMeansTable g\n  CROSS JOIN\n    grandMeanTable gm\n),\nssWithin AS (\n  SELECT\n    c.group_name AS group_name,\n    c.expr,\n    b.group_mean,\n    b.n AS n,\n    POW(c.expr - b.group_mean, 2) AS s2\n  FROM\n    cohort c\n  JOIN\n    ssBetween b\n  ON\n    c.group_name = b.group_name\n),\nnumerator AS (\n  SELECT\n    SUM(n_diff_sq) / (COUNT(group_name) - 1) AS mean_sq_between\n  FROM\n    ssBetween\n),\ndenominator AS (\n  SELECT\n    COUNT(DISTINCT(group_name)) AS k,\n    COUNT(group_name) AS n,\n    SUM(s2) / (COUNT(group_name) - COUNT(DISTINCT(group_name))) AS mean_sq_within\n  FROM\n    ssWithin\n)\n\n  SELECT\n    n,\n    k,\n    mean_sq_between,\n    mean_sq_within,\n    mean_sq_between / mean_sq_within AS F\n  FROM\n    numerator,\n    denominator",
        "external_knowledge": "TCGA_F_Score.md",
        "plan": "1. **Filter Gene Expression Data**:\n   - Select samples from a specific project.\n   - Focus on a particular gene.\n   - Apply log10 transformation to the expression levels, ensuring only positive, non-null values are considered.\n\n2. **Filter Mutation Data**:\n   - Select samples and their variant types for the same gene from mutation data.\n\n3. **Combine Expression and Mutation Data**:\n   - Join the filtered expression data with the filtered mutation data based on sample identifiers to create a combined dataset.\n\n4. **Calculate Grand Mean**:\n   - Compute the overall average expression level across all samples in the combined dataset.\n\n5. **Calculate Group Means**:\n   - Compute the average expression level and count of samples for each mutation type group.\n\n6. **Calculate Between-Group Sum of Squares**:\n   - For each group, compute the squared difference between the group mean and the grand mean, weighted by the number of samples in the group.\n\n7. **Calculate Within-Group Sum of Squares**:\n   - For each sample, compute the squared difference between the sample's expression level and the mean expression level of its group.\n\n8. **Calculate Mean Square Between Groups**:\n   - Sum the between-group sum of squares and divide by the number of groups minus one.\n\n9. **Calculate Mean Square Within Groups**:\n   - Sum the within-group sum of squares and divide by the total number of samples minus the number of groups.\n\n10. **Compute F-statistic**:\n    - Calculate the F-statistic by dividing the mean square between groups by the mean square within groups.\n\n11. **Select F-statistic**:\n    - Retrieve the computed F-statistic as the final result.",
        "special_function": null
    },
    {
        "instance_id": "bq152",
        "db": "spider2-public-data.TCGA_hg38_data_v0\nisb-cgc.TCGA_bioclin_v0\nisb-cgc.QotM",
        "question": "Identify the biological pathway that shows the most significant change in gene expression between TCGA-UCEC samples with non-synonymous mutations in the PARP1 gene and those without such mutations. Analyze differences using t-statistics for each gene on log10(1 + expression data). Aggregate these t-statistics across pathways to determine the most affected pathway. Please provide the pathway name and its corresponding score",
        "SQL": "WITH s1 AS (\n  SELECT\n    sample_barcode_tumor AS sample_barcode\n  FROM\n    `spider2-public-data.TCGA_hg38_data_v0.Somatic_Mutation_DR10`\n  WHERE\n    project_short_name = 'TCGA-UCEC'\n  GROUP BY\n    1\n),\n\nsampleGroup AS (\n  SELECT\n    sample_barcode\n  FROM\n    `spider2-public-data.TCGA_hg38_data_v0.RNAseq_Gene_Expression`\n  WHERE\n    project_short_name = 'TCGA-UCEC'\n    AND sample_barcode IN (SELECT sample_barcode FROM s1)\n  GROUP BY\n    1\n),\n\ngrp1 AS (\n  SELECT\n    sample_barcode_tumor AS sample_barcode\n  FROM\n    `spider2-public-data.TCGA_hg38_data_v0.Somatic_Mutation_DR10`\n  WHERE\n    Hugo_Symbol = 'PARP1'\n    AND One_Consequence <> 'synonymous_variant'\n    AND sample_barcode_tumor IN (SELECT sample_barcode FROM sampleGroup)\n  GROUP BY\n    sample_barcode\n),\n\ngrp2 AS (\n  SELECT\n    sample_barcode\n  FROM\n    sampleGroup\n  WHERE\n    sample_barcode NOT IN (SELECT sample_barcode FROM grp1)\n),\n\nsummaryGrp1 AS (\n  SELECT\n    gene_name AS symbol,\n    AVG(LOG10(HTSeq__FPKM_UQ + 1)) AS genemean,\n    VAR_SAMP(LOG10(HTSeq__FPKM_UQ + 1)) AS genevar,\n    COUNT(sample_barcode) AS genen\n  FROM\n    `spider2-public-data.TCGA_hg38_data_v0.RNAseq_Gene_Expression`\n  WHERE\n    sample_barcode IN (SELECT sample_barcode FROM grp1)\n    AND gene_name IN (SELECT Symbol FROM `isb-cgc.QotM.WikiPathways_20170425_Annotated`)\n  GROUP BY\n    gene_name\n),\n\nsummaryGrp2 AS (\n  SELECT\n    gene_name AS symbol,\n    AVG(LOG10(HTSeq__FPKM_UQ + 1)) AS genemean,\n    VAR_SAMP(LOG10(HTSeq__FPKM_UQ + 1)) AS genevar,\n    COUNT(sample_barcode) AS genen\n  FROM\n    `spider2-public-data.TCGA_hg38_data_v0.RNAseq_Gene_Expression`\n  WHERE\n    sample_barcode IN (SELECT sample_barcode FROM grp2)\n    AND gene_name IN (SELECT Symbol FROM `isb-cgc.QotM.WikiPathways_20170425_Annotated`)\n  GROUP BY\n    gene_name\n),\n\ntStatsPerGene AS (\n  SELECT\n    grp1.symbol AS symbol,\n    grp1.genen AS grp1_n,\n    grp2.genen AS grp2_n,\n    grp1.genemean AS grp1_mean,\n    grp2.genemean AS grp2_mean,\n    grp1.genemean - grp2.genemean AS meandiff,\n    IF (\n      (grp1.genevar > 0 AND grp2.genevar > 0 AND grp1.genen > 0 AND grp2.genen > 0),\n      (grp1.genemean - grp2.genemean) / SQRT((POW(grp1.genevar, 2) / grp1.genen) + (POW(grp2.genevar, 2) / grp2.genen)),\n      0.0\n    ) AS tstat\n  FROM\n    summaryGrp1 AS grp1\n  JOIN\n    summaryGrp2 AS grp2 ON grp1.symbol = grp2.symbol\n  GROUP BY\n    grp1.symbol, grp1.genemean, grp2.genemean, grp1.genevar, grp2.genevar, grp1.genen, grp2.genen\n),\n\ngeneSetTable AS (\n  SELECT\n    gs.pathway,\n    gs.wikiID,\n    gs.Symbol,\n    st.grp1_n,\n    st.grp2_n,\n    st.grp1_mean,\n    st.grp2_mean,\n    st.meandiff,\n    st.tstat\n  FROM\n    `isb-cgc.QotM.WikiPathways_20170425_Annotated` AS gs\n  JOIN\n    tStatsPerGene AS st ON st.symbol = gs.symbol\n  GROUP BY\n    gs.pathway, gs.wikiID, gs.Symbol, st.grp1_n, st.grp2_n, st.grp1_mean, st.grp2_mean, st.meandiff, st.tstat\n),\n\ngeneSetScores AS (\n  SELECT\n    pathway,\n    wikiID,\n    COUNT(symbol) AS n_genes,\n    AVG(ABS(meandiff)) AS avgAbsDiff,\n    (SQRT(COUNT(symbol)) / COUNT(symbol)) * SUM(tstat) AS score\n  FROM\n    geneSetTable\n  GROUP BY\n    pathway, wikiID\n)\n\nSELECT\n  pathway, score\nFROM\n  geneSetScores\nORDER BY\n  score DESC\nLIMIT 1",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq155",
        "db": "spider2-public-data.TCGA_hg38_data_v0\nisb-cgc.TCGA_bioclin_v0\nisb-cgc.QotM",
        "question": "Help me calculate the t-statistic based on the Pearson correlation coefficient between all possible pairs of gene `SNORA31` in the RNAseq data (Log10 transformation) and unique identifiers in the microRNA data available in TCGA. The cohort for this analysis consists of BRCA patients that are 80 years old or younger at the time of diagnosis and Stage I,II,IIA as pathological state. And only consider samples of size more than 25 and with absolute Pearson correlation at least 0.3, and less than 1.0.",
        "SQL": "WITH cohort AS(\n    SELECT case_barcode FROM `isb-cgc.TCGA_bioclin_v0.Clinical`\n    WHERE project_short_name = \"TCGA-BRCA\" AND age_at_diagnosis <= 80\n        AND pathologic_stage in ('Stage I','Stage II','Stage IIA')\n),\ntable1 AS (\n    SELECT\n        symbol,\n        data AS rnkdata,\n        ParticipantBarcode\n    FROM (\n        SELECT\n            gene_name AS symbol, \n            AVG( LOG10( HTSeq__Counts + 1 ) )  AS data,\n            case_barcode AS ParticipantBarcode\n        FROM `spider2-public-data.TCGA_hg38_data_v0.RNAseq_Gene_Expression`\n        WHERE case_barcode IN ( SELECT case_barcode FROM cohort )     # cohort \n                AND gene_name  =  'SNORA31'   # labels \n                AND HTSeq__Counts IS NOT NULL  \n        GROUP BY\n            ParticipantBarcode, symbol\n    )\n),\ntable2 AS (\n    SELECT\n        symbol,\n        data AS rnkdata,\n        ParticipantBarcode\n    FROM (\n    SELECT\n        mirna_id AS symbol, \n        AVG( reads_per_million_miRNA_mapped ) AS data,\n        case_barcode AS ParticipantBarcode\n    FROM `spider2-public-data.TCGA_hg38_data_v0.miRNAseq_Expression`\n    WHERE case_barcode IN ( SELECT case_barcode FROM cohort )     # cohort \n            AND mirna_id  IS NOT NULL   # labels \n            AND reads_per_million_miRNA_mapped IS NOT NULL  \n    GROUP BY\n        ParticipantBarcode, symbol\n    )\n),\nsumm_table AS (\n    SELECT \n        n1.symbol as symbol1,\n        n2.symbol as symbol2,\n        COUNT(n1.ParticipantBarcode) as n,\n        CORR(n1.rnkdata , n2.rnkdata) as correlation    \n    FROM\n    table1 AS n1\n    INNER JOIN\n    table2 AS n2\n    ON\n    n1.ParticipantBarcode = n2.ParticipantBarcode\n    \n    GROUP BY\n        symbol1, symbol2\n)\n\nSELECT symbol1, symbol2, \n        ABS(correlation) * SQRT( (n - 2) / (1 - correlation * correlation)) AS t\nFROM summ_table\nWHERE n > 25 AND ABS(correlation) >= 0.3 AND ABS(correlation) < 1.0",
        "external_knowledge": null,
        "plan": "1. **Define Cohort**: Create a subset of patients based on specific criteria (diagnosed with a particular type of cancer, age at diagnosis, and pathological stage).\n\n2. **Extract Gene Data**: From the gene expression dataset, select data for a specific gene of interest and aggregate the data for each patient in the cohort.\n\n3. **Extract microRNA Data**: From the microRNA expression dataset, select data for all microRNA identifiers and aggregate the data for each patient in the cohort.\n\n4. **Calculate Correlations**: Join the gene and microRNA datasets on patient identifiers and compute the Pearson correlation coefficient for each gene-microRNA pair.\n\n5. **Filter and Compute t-statistic**: Filter the results to include only pairs with sample sizes above a threshold and correlations within a specified range. Then, calculate the t-statistic for each filtered pair.",
        "special_function": null
    },
    {
        "instance_id": "bq141",
        "db": "spider2-public-data.TCGA_hg38_data_v0\nisb-cgc.TCGA_bioclin_v0\nisb-cgc.QotM",
        "question": "Using the TCGA-KIRP dataset, predict the clinical stage of patients based on the expression levels of genes 'MT-CO3', 'MT-CO1', and 'MT-CO2'. Select patients with non-null clinical stages (clinical_stage not null) where disease_code is 'KIRP'. Retrieve their gene expression data for the specified genes. Randomly split these patients into a training set (90%) and a test set (10%) based on their case_barcode. For each clinical stage in the training set, calculate the average expression of each gene. For each patient in the test set, compute the Euclidean distance between their gene expressions and the stage-specific averages from the training set. Assign each test patient to the clinical stage with the minimum distance (i.e., the closest stage average) as their predicted stage. Output the case_barcode along with the predicted clinical stage.",
        "SQL": "WITH\nclin AS (\n    SELECT\n        case_barcode,\n        clinical_stage    \n    FROM\n        `isb-cgc.TCGA_bioclin_v0.Clinical`\n    WHERE\n        disease_code = 'KIRP'\n        AND clinical_stage <> 'NULL'\n),\n\nexpr AS (\n    SELECT\n        case_barcode,\n        IF(MOD(ABS(FARM_FINGERPRINT(case_barcode)), 10) > 1, 'TRAIN', 'TEST') AS class,\n        gene_name,\n        HTSeq__FPKM_UQ\n    FROM\n        `spider2-public-data.TCGA_hg38_data_v0.RNAseq_Gene_Expression`\n    WHERE\n        project_short_name = 'TCGA-KIRP'\n        AND gene_name IN ('MT-CO3', 'MT-CO1', 'MT-CO2')\n        AND case_barcode IN (SELECT case_barcode FROM clin)\n    GROUP BY\n        case_barcode, gene_name, HTSeq__FPKM_UQ\n),\n\nmean AS (\n    SELECT\n        expr.class,\n        clin.clinical_stage,\n        AVG(CASE WHEN gene_name = 'MT-CO3' THEN HTSeq__FPKM_UQ END) AS gene1,\n        AVG(CASE WHEN gene_name = 'MT-CO1' THEN HTSeq__FPKM_UQ END) AS gene2,\n        AVG(CASE WHEN gene_name = 'MT-CO2' THEN HTSeq__FPKM_UQ END) AS gene3\n    FROM\n        expr\n    JOIN\n        clin ON expr.case_barcode = clin.case_barcode\n    WHERE\n        expr.class = 'TRAIN'\n    GROUP BY\n        expr.class, clin.clinical_stage\n),\n\ntest AS (\n    SELECT\n        case_barcode,\n        class,\n        MAX(CASE WHEN gene_name = 'MT-CO3' THEN HTSeq__FPKM_UQ END) AS gene1,\n        MAX(CASE WHEN gene_name = 'MT-CO1' THEN HTSeq__FPKM_UQ END) AS gene2,\n        MAX(CASE WHEN gene_name = 'MT-CO2' THEN HTSeq__FPKM_UQ END) AS gene3\n    FROM\n        expr\n    WHERE\n        class = 'TEST'\n    GROUP BY\n        case_barcode, class\n),\n\ndistance AS (\n    SELECT\n        case_barcode,\n        gene1,\n        gene2,\n        SQRT(POWER((SELECT gene1 FROM mean WHERE clinical_stage = 'Stage I') - gene1, 2) + \n             POWER((SELECT gene2 FROM mean WHERE clinical_stage = 'Stage I') - gene2, 2) + \n             POWER((SELECT gene3 FROM mean WHERE clinical_stage = 'Stage I') - gene3, 2)) AS stage1,\n        SQRT(POWER((SELECT gene1 FROM mean WHERE clinical_stage = 'Stage II') - gene1, 2) + \n             POWER((SELECT gene2 FROM mean WHERE clinical_stage = 'Stage II') - gene2, 2) + \n             POWER((SELECT gene3 FROM mean WHERE clinical_stage = 'Stage II') - gene3, 2)) AS stage2,\n        SQRT(POWER((SELECT gene1 FROM mean WHERE clinical_stage = 'Stage III') - gene1, 2) + \n             POWER((SELECT gene2 FROM mean WHERE clinical_stage = 'Stage III') - gene2, 2) + \n             POWER((SELECT gene3 FROM mean WHERE clinical_stage = 'Stage III') - gene3, 2)) AS stage3,\n        SQRT(POWER((SELECT gene1 FROM mean WHERE clinical_stage = 'Stage IV') - gene1, 2) + \n             POWER((SELECT gene2 FROM mean WHERE clinical_stage = 'Stage IV') - gene2, 2) + \n             POWER((SELECT gene3 FROM mean WHERE clinical_stage = 'Stage IV') - gene3, 2)) AS stage4\n    FROM\n        test\n),\n\npred AS (\n    SELECT\n        case_barcode,\n        CASE \n            WHEN stage1 < stage2 AND stage1 < stage3 AND stage1 < stage4 THEN 'Stage I'\n            WHEN stage2 < stage1 AND stage2 < stage3 AND stage2 < stage4 THEN 'Stage II'\n            WHEN stage3 < stage1 AND stage3 < stage2 AND stage3 < stage4 THEN 'Stage III'\n            ELSE 'Stage IV'\n        END AS prediction\n    FROM\n        distance\n)\n\nSELECT\n    a.case_barcode,\n    a.prediction,\nFROM\n    pred AS a",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq046",
        "db": "isb-cgc.TCGA_bioclin_v0\nisb-cgc.GDC_metadata",
        "question": "Find case barcodes and their corresponding GDC file URLs for female patients aged 30 or younger diagnosed with breast cancer, whose clinical history includes problematic prior treatments for other cancers or redacted annotations. Only consider relevant clinical and annotation data from TCGA with GDC archive release 14.",
        "SQL": "WITH select_on_annotations AS (\n    SELECT\n        case_barcode,\n        category AS categoryName,\n        classification AS classificationName\n    FROM\n        `isb-cgc.TCGA_bioclin_v0.Annotations`\n    WHERE\n        entity_type = \"Patient\"\n        AND ( category = \"History of unacceptable prior treatment related to a prior/other malignancy\"\n            OR classification = \"Redaction\" )\n    GROUP BY\n        case_barcode,\n        categoryName,\n        classificationName\n),\nselect_on_clinical AS (\n    SELECT\n        case_barcode,\n        vital_status,\n        days_to_last_known_alive,\n        ethnicity,\n        histological_type,\n        menopause_status,\n        race\n    FROM\n        `isb-cgc.TCGA_bioclin_v0.Clinical`\n    WHERE\n        ( disease_code = \"BRCA\"\n        AND age_at_diagnosis <= 30\n        AND gender = \"FEMALE\" )\n),\n-- Combine the cohort with the metadata tables to create a list of GDC urls\ncohort AS (\n    SELECT\n        case_barcode\n    FROM (\n        SELECT\n            a.categoryName,\n            a.classificationName,\n            c.case_barcode\n        FROM\n            select_on_annotations AS a\n        FULL JOIN\n            select_on_clinical AS c\n        ON\n            a.case_barcode = c.case_barcode\n        WHERE\n            a.case_barcode IS NOT NULL\n            OR c.case_barcode IS NOT NULL\n        ORDER BY\n            a.classificationName,\n            a.categoryName,\n            c.case_barcode )\n    WHERE\n        categoryName IS NULL\n        AND classificationName IS NULL\n        AND case_barcode IS NOT NULL\n    ORDER BY\n        case_barcode\n),\ngdc AS (\n    SELECT a.case_barcode, b.case_gdc_id\n    FROM cohort AS a\n    INNER JOIN `isb-cgc.GDC_metadata.rel14_caseData` AS b -- GDC archive release 14\n    ON a.case_barcode = b.case_barcode\n),\ncurr AS (\n    SELECT c.case_barcode, c.case_gdc_id, d.file_gdc_id\n    FROM gdc as c\n    INNER JOIN `isb-cgc.GDC_metadata.rel14_fileData_current` AS d -- GDC archive release 14\n    ON c.case_gdc_id = d.case_gdc_id\n),\nurl AS ( \n    SELECT e.case_barcode, e.case_gdc_id, e.file_gdc_id, f.file_gdc_url\n    FROM curr AS e\n    INNER JOIN `isb-cgc.GDC_metadata.rel14_GDCfileID_to_GCSurl_NEW` AS f -- GDC archive release 14\n    ON e.file_gdc_id = f.file_gdc_id\n)\nSELECT case_barcode, file_gdc_url FROM url ORDER BY case_barcode;",
        "external_knowledge": null,
        "plan": "1. Filter Annotations Data:\n   - Extract relevant case identifiers and associated categories or classifications from the annotations data, focusing on patient records that either have a history of problematic prior treatments for other cancers or contain redacted information.\n\n2. Filter Clinical Data:\n   - Extract relevant case identifiers and necessary clinical details from the clinical data, focusing on female patients aged 30 or younger diagnosed with breast cancer.\n\n3. Combine Filtered Data:\n   - Merge the filtered annotations data with the filtered clinical data to create a cohort of case identifiers. Ensure that only cases present in either the annotations or clinical data are included.\n\n4. Link to GDC Archive:\n   - Match the combined cohort of case identifiers to the corresponding records in the GDC archive to retrieve GDC-specific case identifiers.\n\n5. Retrieve File URLs:\n   - Use the GDC-specific case identifiers to fetch the corresponding file URLs from the GDC archive, and finally, output the case identifiers along with their associated file URLs.",
        "special_function": null
    },
    {
        "instance_id": "bq153",
        "db": "spider2-public-data.pancancer_atlas_filtered",
        "question": "Calculate the average log10(normalized_count + 1) expression level of the IGF2 gene for each histology type among LGG patients. Include only patients with valid IGF2 expression data and histology types not enclosed in square brackets. Match gene expression and clinical data using ParticipantBarcode.",
        "SQL": "WITH\ntable1 AS (\nSELECT  symbol, data, ParticipantBarcode\nFROM ( \n   SELECT \n         Symbol AS symbol, AVG( LOG10( normalized_count + 1 )) AS data, ParticipantBarcode\n   FROM  `spider2-public-data.pancancer_atlas_filtered.EBpp_AdjustPANCAN_IlluminaHiSeq_RNASeqV2_genExp_filtered` \n   WHERE Study = 'LGG' AND Symbol ='IGF2' AND normalized_count IS NOT NULL\n   GROUP BY \n         ParticipantBarcode, symbol\n   )\n)\n,table2 AS (\nSELECT\n   symbol,\n   avgdata AS data,\n   ParticipantBarcode\nFROM (\n   SELECT\n      'icd_o_3_histology' AS symbol, \n      icd_o_3_histology AS avgdata,\n      bcr_patient_barcode AS ParticipantBarcode\n   FROM `spider2-public-data.pancancer_atlas_filtered.clinical_PANCAN_patient_with_followup_filtered`\n   WHERE acronym = 'LGG' AND icd_o_3_histology IS NOT NULL  \n         AND NOT REGEXP_CONTAINS(icd_o_3_histology,r\"^(\\[.*\\]$)\")     \n   )\n)\n,table_data AS (\nSELECT \n   n1.data as data1,\n   n2.data as data2,\n   n1.ParticipantBarcode\nFROM\n   table1 AS n1\nINNER JOIN\n   table2 AS n2\nON\n   n1.ParticipantBarcode = n2.ParticipantBarcode\n) \n\nSELECT \n    data2 AS Histology_Type, \n    AVG(data1) AS Average_Log_Expression\nFROM \n    table_data\nGROUP BY \n    data2;",
        "external_knowledge": null,
        "plan": "1. **Filter and Transform Data for Gene Expression:**\n   - Select records related to a specific study and gene.\n   - Ensure the data is not null.\n   - Calculate the average log-transformed expression level for each participant.\n\n2. **Filter and Select Histology Information:**\n   - Select records related to the same study.\n   - Ensure the histology information is valid and not null.\n   - Extract the required histology information for each participant.\n\n3. **Combine Datasets by Participant:**\n   - Perform an inner join on the two sets of filtered data using participant identifiers to combine gene expression data with histology data.\n\n4. **Compute Average Expression by Histology Type:**\n   - Group the combined data by histology type.\n   - Calculate the average log-transformed expression level for each histology type.",
        "special_function": null
    },
    {
        "instance_id": "bq154",
        "db": "spider2-public-data.pancancer_atlas_filtered",
        "question": "What is the Kruskal-Wallis score (H-score) among groups of LGG patients, where the IGF2 gene expression is calculated by first applying a log10 transformation to the normalized counts, then averaging them, and the groups are based on ICD-O-3 histology codes?",
        "SQL": "WITH\ntable1 AS (\nSELECT  symbol, data, ParticipantBarcode\nFROM ( \n   SELECT \n         Symbol AS symbol, AVG( LOG10( normalized_count + 1 )) AS data, ParticipantBarcode\n   FROM  `spider2-public-data.pancancer_atlas_filtered.EBpp_AdjustPANCAN_IlluminaHiSeq_RNASeqV2_genExp_filtered` \n   WHERE Study = 'LGG' AND Symbol ='IGF2' AND normalized_count IS NOT NULL\n   GROUP BY \n         ParticipantBarcode, symbol\n   )\n)\n,table2 AS (\nSELECT\n   symbol,\n   avgdata AS data,\n   ParticipantBarcode\nFROM (\n   SELECT\n      'icd_o_3_histology' AS symbol, \n      icd_o_3_histology AS avgdata,\n      bcr_patient_barcode AS ParticipantBarcode\n   FROM `spider2-public-data.pancancer_atlas_filtered.clinical_PANCAN_patient_with_followup_filtered`\n   WHERE acronym = 'LGG' AND icd_o_3_histology IS NOT NULL  \n         AND NOT REGEXP_CONTAINS(icd_o_3_histology,r\"^(\\[.*\\]$)\")     \n   )\n)\n,table_data AS (\nSELECT \n   n1.data as data1,\n   n2.data as data2,\n   n1.ParticipantBarcode\nFROM\n   table1 AS n1\nINNER JOIN\n   table2 AS n2\nON\n   n1.ParticipantBarcode = n2.ParticipantBarcode\n),\nsumm_table  AS (\nSELECT \n   COUNT( ParticipantBarcode) AS ni,\n   SUM( rnkdata ) AS Si,\n   SUM( rnkdata * rnkdata ) AS Qi,\n   data2\nFROM (    \n   SELECT \n      (RANK() OVER (ORDER BY data1 ASC)) + (COUNT(*) OVER ( PARTITION BY CAST(data1 as STRING)) - 1)/2.0 AS rnkdata,\n      data2, ParticipantBarcode\n   FROM\n      table_data \n   WHERE data2 IN ( SELECT data2 from table_data GROUP BY data2 HAVING count(data2)>1 )   \n)\nGROUP BY\n   data2\n)\n\nSELECT \n    Ngroups,\n    N as Nsamples,        \n    (N-1)*( sumSi2overni - (sumSi *sumSi)/N ) / (  sumQi  - (sumSi *sumSi)/N )    AS  Hscore \nFROM (\n  SELECT \n      SUM( ni ) As N, \n      SUM( Si ) AS sumSi,\n      SUM( Qi ) AS sumQi,\n      SUM( Si * Si  / ni ) AS sumSi2overni,\n      COUNT ( data2 ) AS Ngroups    \n  FROM  summ_table\n  )\nWHERE \n   Ngroups > 1\nORDER BY Hscore DESC",
        "external_knowledge": "Regulome_Explorer_Kruskal-Wallis_test_for_numerical_and_categorical_data.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq156",
        "db": "spider2-public-data.pancancer_atlas_filtered",
        "question": "Compute the t-score (rounded to 2 decimals) to compare the difference in mean expression levels of gene DRG2 between two groups (TP53 mutated vs. non-mutated) in the Lower Grade Glioma study. Note that, categorical groups with number of samples smaller than 10 or zero variance should be ignored. Refer to `t_score.md` about how to compute t-score.",
        "SQL": "WITH barcodes AS (\n   SELECT Tumor_SampleBarcode AS SampleBarcode   \n   FROM `spider2-public-data.pancancer_atlas_filtered.MC3_MAF_V5_one_per_tumor_sample`\n   WHERE Study = 'LGG'       \n),\ntable1 AS (\n    SELECT Symbol, data, ParticipantBarcode\n    FROM ( \n    SELECT \n        Symbol AS symbol,\n        AVG( LOG10( normalized_count + 1 )) AS data,\n        ParticipantBarcode\n    FROM  `spider2-public-data.pancancer_atlas_filtered.EBpp_AdjustPANCAN_IlluminaHiSeq_RNASeqV2_genExp_filtered` \n    WHERE Study = 'LGG' AND Symbol ='DRG2' AND normalized_count IS NOT NULL\n        AND SampleBarcode  IN (SELECT * FROM barcodes)\n    GROUP BY \n        ParticipantBarcode, symbol\n    )\n),\ntable2 AS (\n    SELECT   \n        ParticipantBarcode \n    FROM (\n        SELECT\n            ParticipantBarcode AS ParticipantBarcode\n        FROM `spider2-public-data.pancancer_atlas_filtered.MC3_MAF_V5_one_per_tumor_sample`\n        WHERE Study = 'LGG' AND Hugo_Symbol = 'TP53'\n                AND FILTER = 'PASS'  \n        GROUP BY ParticipantBarcode\n    ) \n),\nsumm_table AS (\n    SELECT \n        Symbol,\n        COUNT( n1.ParticipantBarcode) as Ny,\n        SUM( n1.data )  as Sy,\n        SUM( n1.data * n1.data ) as Qy\n    FROM\n        table1 AS n1\n        INNER JOIN\n        table2 AS n2\n        ON\n        n1.ParticipantBarcode = n2.ParticipantBarcode\n    GROUP BY Symbol\n)\nSELECT\n    ROUND(ABS(avg_y - avg_n)/ SQRT( var_y /Ny + var_n/Nn), 2) as tscore\nFROM (\n    SELECT Ny, \n        Sy / Ny as avg_y,\n        ( Qy - Sy*Sy/Ny )/(Ny - 1) as var_y, \n        Nt - Ny as Nn,\n        (St - Sy)/(Nt - Ny) as avg_n,\n        (Qt - Qy - (St-Sy)*(St-Sy)/(Nt - Ny) )/(Nt - Ny -1 ) as var_n\n    FROM  summ_table as n1\n    LEFT JOIN (\n        SELECT \n            Symbol,\n            COUNT( ParticipantBarcode ) as Nt,\n            SUM( data ) as St,\n            SUM( data*data ) as Qt\n        FROM table1 GROUP BY Symbol\n    ) as n2\n    ON n1.Symbol = n2.Symbol      \n)\nWHERE\n   Ny > 10 AND Nn > 10 AND var_y > 0 and var_n > 0",
        "external_knowledge": "t_score.md",
        "plan": "1. **Extract Barcodes of LGG Samples**: Fetch the barcodes (sample identifiers) for Lower Grade Glioma (LGG) samples.\n\n2. **Obtain Expression Data for DRG2**: For each patient, retrieve the average log-transformed expression of the gene DRG2 from the RNA-seq table. Only include samples from the LGG study.\n\n3. **Identify TP53-Mutated Patients**: Select patients who have a TP53 mutation with a \"PASS\" filter from the mutation table.\n\n4. **Summarize Expression Data for TP53 Mutated Group**: For the TP53-mutated group, compute the number of patients (Ny), the sum of their expression levels (Sy), and the sum of squared expression levels (Qy).\n\n5. **Summarize Expression Data for the Entire Population (Including Non-Mutated)**: Compute similar statistics for the entire population of LGG samples, which includes both TP53-mutated and non-mutated patients.\n\n6. **Calculate Group-Level Statistics and Compute t-Score**: Calculate the average expression (avg_y, avg_n) and variance (var_y, var_n) for both the mutated group (TP53) and the non-mutated group. Then, compute the t-score based on the difference in means. Remember to apply the filtering criteria, where both groups have at least 10 samples and positive variance.",
        "special_function": [
            "mathematical-functions/ABS",
            "mathematical-functions/LOG10",
            "mathematical-functions/ROUND",
            "mathematical-functions/SQRT"
        ]
    },
    {
        "instance_id": "bq157",
        "db": "spider2-public-data.pancancer_atlas_filtered",
        "question": "Please refer to the T test for numerics instruction to help me compute the T score to show the statistical difference in the expression of the DRG2 gene between LGG patients with and without TP53 mutation, based on the logarithm of normalized gene counts, averaged across patients.",
        "SQL": "WITH\nbarcodes AS (\n   SELECT Tumor_SampleBarcode AS SampleBarcode   \n   FROM `spider2-public-data.pancancer_atlas_filtered.MC3_MAF_V5_one_per_tumor_sample`\n   WHERE Study = 'LGG'       \n)\n,table1 AS (\nSELECT Symbol, data, ParticipantBarcode\nFROM ( \n   SELECT \n         Symbol AS symbol, AVG( LOG10( normalized_count + 1 )) AS data, ParticipantBarcode\n   FROM  `spider2-public-data.pancancer_atlas_filtered.EBpp_AdjustPANCAN_IlluminaHiSeq_RNASeqV2_genExp_filtered` \n   WHERE Study = 'LGG' AND Symbol ='DRG2' AND normalized_count IS NOT NULL\n         AND SampleBarcode  IN (SELECT * FROM barcodes)\n         \n   GROUP BY \n         ParticipantBarcode, symbol\n   )\n)\n,table2 AS (\nSELECT   \n   ParticipantBarcode \nFROM\n   (\n   SELECT\n      ParticipantBarcode AS ParticipantBarcode\n   FROM `spider2-public-data.pancancer_atlas_filtered.MC3_MAF_V5_one_per_tumor_sample`\n   WHERE Study = 'LGG' AND Hugo_Symbol = 'TP53'\n         AND FILTER = 'PASS'  \n   GROUP BY ParticipantBarcode\n   ) \n)\n,summ_table AS (\nSELECT \n   Symbol,\n   COUNT( n1.ParticipantBarcode) as Ny,\n   SUM( n1.data )  as Sy,\n   SUM( n1.data * n1.data ) as Qy\n   \nFROM\n   table1 AS n1\nINNER JOIN\n   table2 AS n2\nON\n   n1.ParticipantBarcode = n2.ParticipantBarcode\nGROUP BY Symbol\n)\n\nSELECT \n    Ny, Nn,\n    avg_y, avg_n,\n    ABS(avg_y - avg_n)/ SQRT( var_y /Ny + var_n/Nn )  as tscore\nFROM (\nSELECT Ny, \n       Sy / Ny as avg_y,\n       ( Qy - Sy*Sy/Ny )/(Ny - 1) as var_y, \n       Nt - Ny as Nn,\n       (St - Sy)/(Nt - Ny) as avg_n,\n       (Qt - Qy - (St-Sy)*(St-Sy)/(Nt - Ny) )/(Nt - Ny -1 ) as var_n\nFROM  summ_table as n1\nLEFT JOIN ( SELECT Symbol, COUNT( ParticipantBarcode ) as Nt, SUM( data ) as St, SUM( data*data ) as Qt\n            FROM table1 GROUP BY Symbol\n           ) as n2\nON n1.Symbol = n2.Symbol      \n)\nWHERE\n   Ny > 10 AND Nn > 10 AND var_y > 0 and var_n > 0",
        "external_knowledge": "Regulome_Explorer_T_test_for_numerical_and_binary_data.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq158",
        "db": "spider2-public-data.pancancer_atlas_filtered",
        "question": "Which top five histological types of breast cancer (BRCA) in the PanCancer Atlas exhibit the highest percentage of CDH1 gene mutations?",
        "SQL": "WITH\ntable1 AS (\n    SELECT\n        histological_type AS data1,\n        bcr_patient_barcode AS ParticipantBarcode\n    FROM spider2-public-data.pancancer_atlas_filtered.clinical_PANCAN_patient_with_followup_filtered\n    WHERE acronym = 'BRCA' AND histological_type IS NOT NULL      \n),\ntable2 AS (\n    SELECT\n        Hugo_Symbol AS symbol, \n        ParticipantBarcode\n    FROM spider2-public-data.pancancer_atlas_filtered.MC3_MAF_V5_one_per_tumor_sample\n    WHERE Study = 'BRCA' AND Hugo_Symbol = 'CDH1'\n          AND FILTER = 'PASS'  \n    GROUP BY\n        ParticipantBarcode, symbol\n),\nsumm_table AS (\n    SELECT \n        n1.data1,\n        IF(n2.ParticipantBarcode IS NULL, 'NO', 'YES') AS data2,\n        COUNT(*) AS Nij\n    FROM\n        table1 AS n1\n    LEFT JOIN\n        table2 AS n2 ON n1.ParticipantBarcode = n2.ParticipantBarcode\n    GROUP BY\n        n1.data1, data2\n),\npercentages AS (\n    SELECT\n        data1,\n        SUM(CASE WHEN data2 = 'YES' THEN Nij ELSE 0 END) AS mutation_count,\n        SUM(Nij) AS total,\n        SUM(CASE WHEN data2 = 'YES' THEN Nij ELSE 0 END) / SUM(Nij) AS mutation_percentage\n    FROM summ_table\n    GROUP BY data1\n)\nSELECT data1 AS Histological_Type\nFROM percentages\nORDER BY mutation_percentage DESC\nLIMIT 5",
        "external_knowledge": null,
        "plan": "1. **Filter and Select Relevant Data**:\n   - Extract a list of participants with their respective cancer types from a clinical dataset, focusing specifically on a certain type of cancer. Ensure that the cancer type information is not missing.\n\n2. **Identify Gene Mutations**:\n   - From a mutation dataset, select participants who have mutations in a specific gene. Filter out any records that do not pass quality control checks and group by participant and gene symbol to eliminate duplicate entries.\n\n3. **Combine Datasets**:\n   - Perform a left join between the clinical data (step 1) and the mutation data (step 2) based on participant identifiers. This will associate each participant's cancer type with whether they have the specific gene mutation or not.\n\n4. **Summarize Mutation Data**:\n   - For each cancer type, count the number of participants with and without the specific gene mutation. Summarize this data to get the total counts.\n\n5. **Calculate Mutation Percentages**:\n   - For each cancer type, calculate the percentage of participants that have the specific gene mutation. This involves dividing the count of participants with the mutation by the total number of participants for each cancer type.\n\n6. **Identify the Highest Mutation Percentage**:\n   - Sort the cancer types by their mutation percentage in descending order and select the top entry, which corresponds to the cancer type with the highest percentage of participants having the specific gene mutation.",
        "special_function": [
            "conditional-functions/CASE",
            "conditional-functions/IF"
        ]
    },
    {
        "instance_id": "bq159",
        "db": "spider2-public-data.pancancer_atlas_filtered",
        "question": "Calculate the chi-square value to assess the association between histological types and the presence of CDH1 gene mutations in BRCA patients using data from the PanCancer Atlas. Focus on patients with known histological types and consider only reliable mutation entries.  Exclude any histological types or mutation statuses with marginal totals less than or equal to 10. Match clinical and mutation data using ParticipantBarcode",
        "SQL": "WITH\ntable1 AS (\nSELECT\n   symbol,\n   avgdata AS data,\n   ParticipantBarcode\nFROM (\n   SELECT\n      'histological_type' AS symbol, \n      histological_type AS avgdata,\n      bcr_patient_barcode AS ParticipantBarcode\n   FROM `spider2-public-data.pancancer_atlas_filtered.clinical_PANCAN_patient_with_followup_filtered`\n   WHERE acronym = 'BRCA'   AND histological_type IS NOT NULL      \n   )\n)\n,table2 AS (\nSELECT\n   symbol,\n   ParticipantBarcode\nFROM (\n   SELECT\n      Hugo_Symbol AS symbol, \n      ParticipantBarcode AS ParticipantBarcode\n   FROM `spider2-public-data.pancancer_atlas_filtered.MC3_MAF_V5_one_per_tumor_sample`\n   WHERE Study = 'BRCA' AND Hugo_Symbol = 'CDH1'\n         AND FILTER = 'PASS'  \n   GROUP BY\n      ParticipantBarcode, symbol\n   )\n)\n,summ_table AS (\nSELECT \n   n1.data as data1,\n   IF( n2.ParticipantBarcode is null, 'NO', 'YES') as data2,\n   COUNT(*) as Nij\nFROM\n   table1 AS n1\nLEFT JOIN\n   table2 AS n2\nON\n   n1.ParticipantBarcode = n2.ParticipantBarcode\nGROUP BY\n  data1, data2\n) \n,expected_table AS (\nSELECT data1, data2\nFROM (     \n    SELECT data1, SUM(Nij) as Ni   \n    FROM summ_table\n    GROUP BY data1 ) \nCROSS JOIN ( \n    SELECT data2, SUM(Nij) as Nj\n    FROM summ_table\n    GROUP BY data2 )\n    \nWHERE Ni > 10 AND Nj > 10\n)\n,contingency_table AS (\nSELECT\n   T1.data1,\n   T1.data2,\n   IF( Nij IS NULL, 0, Nij) as Nij,\n   (SUM(Nij) OVER (PARTITION BY T1.data1))*(SUM(Nij) OVER (PARTITION BY T1.data2))/ SUM(Nij) OVER () AS  E_nij\n    \nFROM\n   expected_table AS T1\nLEFT JOIN\n   summ_table AS T2\nON \n  T1.data1 = T2.data1 AND T1.data2 = T2.data2\n)\n\n   SELECT\n     SUM( (Nij - E_nij)*(Nij - E_nij) / E_nij ) as Chi2    \n   FROM contingency_table",
        "external_knowledge": null,
        "plan": "1. **Define the first dataset**:\n    - Extract records related to a specific condition from a clinical data source.\n    - Select relevant identifiers and attributes for these records.\n\n2. **Define the second dataset**:\n    - Extract records related to gene mutations from a genetic data source.\n    - Filter these records to include only those related to a specific gene and study.\n    - Group the records by relevant identifiers.\n\n3. **Create a summary table**:\n    - Perform a left join between the first and second datasets on a common identifier.\n    - Categorize the presence or absence of gene mutations.\n    - Count the occurrences for each combination of attributes.\n\n4. **Calculate expected counts**:\n    - Sum the counts from the summary table for each category of the first attribute.\n    - Sum the counts for each category of the second attribute.\n    - Cross join these sums to generate all possible combinations of categories.\n    - Filter combinations where both sums are greater than a specified threshold.\n\n5. **Build the contingency table**:\n    - Join the expected counts with the summary table to align actual and expected counts.\n    - Compute expected counts for each combination of categories.\n    - Replace null values with zeroes where necessary.\n\n6. **Compute the chi-square statistic**:\n    - Calculate the chi-square value using the formula involving actual and expected counts.\n    - Sum the chi-square contributions for all combinations to get the final statistic.\n\n7. **Return the result**:\n    - Output the computed chi-square value.",
        "special_function": [
            "conditional-functions/IF"
        ]
    },
    {
        "instance_id": "bq161",
        "db": "isb-cgc-bq.pancancer_atlas",
        "question": "Calculate the net difference between the number of pancreatic adenocarcinoma (PAAD) patients in TCGA's dataset who are confirmed to have mutations in both KRAS and TP53 genes, and those without mutations in either gene. Utilize patient clinical and follow-up data alongside genomic mutation details from TCGA\u2019s cancer genomics database, focusing specifically on PAAD studies where the mutations have passed quality filters.",
        "SQL": "WITH\nbarcodes AS (\n   SELECT bcr_patient_barcode AS ParticipantBarcode\n   FROM `isb-cgc-bq.pancancer_atlas.Filtered_clinical_PANCAN_patient_with_followup`\n   WHERE acronym = 'PAAD'\n)\n,table1 AS (\nSELECT\n   t1.ParticipantBarcode,\n   IF( t2.ParticipantBarcode is null, 'NO', 'YES') as data\nFROM\n   barcodes AS t1\nLEFT JOIN\n   (\n   SELECT\n      ParticipantBarcode AS ParticipantBarcode\n   FROM `isb-cgc-bq.pancancer_atlas.Filtered_MC3_MAF_V5_one_per_tumor_sample`\n   WHERE Study = 'PAAD' AND Hugo_Symbol = 'KRAS'\n         AND FILTER = 'PASS'\n   GROUP BY ParticipantBarcode\n   ) AS t2\nON t1.ParticipantBarcode = t2.ParticipantBarcode\n)\n,table2 AS (\nSELECT\n   t1.ParticipantBarcode,\n   IF( t2.ParticipantBarcode is null, 'NO', 'YES') as data\nFROM\n   barcodes AS t1\nLEFT JOIN\n   (\n   SELECT\n      ParticipantBarcode AS ParticipantBarcode\n   FROM `isb-cgc-bq.pancancer_atlas.Filtered_MC3_MAF_V5_one_per_tumor_sample`\n   WHERE Study = 'PAAD' AND Hugo_Symbol = 'TP53'\n         AND FILTER = 'PASS'\n   GROUP BY ParticipantBarcode\n   ) AS t2\nON t1.ParticipantBarcode = t2.ParticipantBarcode\n),\n\nINFO AS (\nSELECT\n   n1.data as data1,\n   n2.data as data2,\n   COUNT(*) as Nij\nFROM\n   table1 AS n1\nINNER JOIN\n   table2 AS n2\nON\n   n1.ParticipantBarcode = n2.ParticipantBarcode\nGROUP BY\n  data1, data2\n)\n\nSELECT \n(SELECT Nij FROM INFO WHERE data1=\"YES\" AND data2=\"YES\")\n-\n(SELECT Nij FROM INFO WHERE data1=\"NO\" AND data2=\"NO\")",
        "external_knowledge": null,
        "plan": "1. **Identify Target Patients**:\n   - Select all patients diagnosed with the specified cancer type from the clinical and follow-up data.\n\n2. **Check for First Gene Mutation**:\n   - For each patient, determine if they have a mutation in the first specified gene that passes quality filters.\n\n3. **Record Mutation Status for First Gene**:\n   - Create a list where each patient is marked as either having or not having a mutation in the first gene.\n\n4. **Check for Second Gene Mutation**:\n   - For each patient, determine if they have a mutation in the second specified gene that passes quality filters.\n\n5. **Record Mutation Status for Second Gene**:\n   - Create a list where each patient is marked as either having or not having a mutation in the second gene.\n\n6. **Combine Mutation Data**:\n   - Merge the two lists to compile a combined mutation status for each patient, indicating whether they have mutations in both genes, one gene, or neither.\n\n7. **Count Patients by Mutation Status**:\n   - Count the number of patients in each of the four possible mutation status categories: both genes mutated, only the first gene mutated, only the second gene mutated, and neither gene mutated.\n\n8. **Calculate Net Difference**:\n   - Calculate the net difference by subtracting the number of patients without mutations in either gene from the number of patients with mutations in both genes.",
        "special_function": null
    },
    {
        "instance_id": "bq151",
        "db": "isb-cgc-bq.pancancer_atlas",
        "question": "Using TCGA dataset, calculate the chi-squared statistic to evaluate the association between KRAS and TP53 gene mutations in patients diagnosed with pancreatic adenocarcinoma (PAAD). Incorporate clinical follow-up data and high-quality mutation annotations to accurately determine the frequency of patients with co-occurring KRAS and TP53 mutations compared to those with each mutation occurring independently. Ensure that patient records are meticulously matched based on unique identifiers to maintain data integrity. This analysis aims to identify and quantify potential correlations between KRAS and TP53 genetic alterations within the PAAD patient population.",
        "SQL": "WITH\nbarcodes AS (\n   SELECT bcr_patient_barcode AS ParticipantBarcode\n   FROM isb-cgc-bq.pancancer_atlas.Filtered_clinical_PANCAN_patient_with_followup\n   WHERE acronym = 'PAAD'\n),\ntable1 AS (\nSELECT\n   t1.ParticipantBarcode,\n   IF(t2.ParticipantBarcode IS NULL, 'NO', 'YES') AS data\nFROM\n   barcodes AS t1\nLEFT JOIN\n   (\n   SELECT\n      ParticipantBarcode AS ParticipantBarcode\n   FROM isb-cgc-bq.pancancer_atlas.Filtered_MC3_MAF_V5_one_per_tumor_sample\n   WHERE Study = 'PAAD' AND Hugo_Symbol = 'KRAS'\n         AND FILTER = 'PASS'\n   GROUP BY ParticipantBarcode\n   ) AS t2\nON t1.ParticipantBarcode = t2.ParticipantBarcode\n),\ntable2 AS (\nSELECT\n   t1.ParticipantBarcode,\n   IF(t2.ParticipantBarcode IS NULL, 'NO', 'YES') AS data\nFROM\n   barcodes AS t1\nLEFT JOIN\n   (\n   SELECT\n      ParticipantBarcode AS ParticipantBarcode\n   FROM isb-cgc-bq.pancancer_atlas.Filtered_MC3_MAF_V5_one_per_tumor_sample\n   WHERE Study = 'PAAD' AND Hugo_Symbol = 'TP53'\n         AND FILTER = 'PASS'\n   GROUP BY ParticipantBarcode\n   ) AS t2\nON t1.ParticipantBarcode = t2.ParticipantBarcode\n),\nsumm_table AS (\nSELECT\n   n1.data AS data1,\n   n2.data AS data2,\n   COUNT(*) AS Nij\nFROM\n   table1 AS n1\nINNER JOIN\n   table2 AS n2\nON\n   n1.ParticipantBarcode = n2.ParticipantBarcode\nGROUP BY\n  data1, data2\n),\ncontingency_table AS (\nSELECT\n  MAX(IF((data1 = 'YES') AND (data2 = 'YES'), Nij, 0)) AS a,\n  MAX(IF((data1 = 'YES') AND (data2 = 'NO'), Nij, 0)) AS b,\n  MAX(IF((data1 = 'NO') AND (data2 = 'YES'), Nij, 0)) AS c,\n  MAX(IF((data1 = 'NO') AND (data2 = 'NO'), Nij, 0)) AS d,\n  (MAX(IF((data1 = 'YES') AND (data2 = 'YES'), Nij, 0)) + MAX(IF((data1 = 'YES') AND (data2 = 'NO'), Nij, 0))) AS row1_total,\n  (MAX(IF((data1 = 'NO') AND (data2 = 'YES'), Nij, 0)) + MAX(IF((data1 = 'NO') AND (data2 = 'NO'), Nij, 0))) AS row2_total,\n  (MAX(IF((data1 = 'YES') AND (data2 = 'YES'), Nij, 0)) + MAX(IF((data1 = 'NO') AND (data2 = 'YES'), Nij, 0))) AS col1_total,\n  (MAX(IF((data1 = 'YES') AND (data2 = 'NO'), Nij, 0)) + MAX(IF((data1 = 'NO') AND (data2 = 'NO'), Nij, 0))) AS col2_total,\n  SUM(Nij) AS grand_total\nFROM summ_table\n)\nSELECT\n  POWER((a - (row1_total * col1_total) / grand_total), 2) / ((row1_total * col1_total) / grand_total) +\n  POWER((b - (row1_total * col2_total) / grand_total), 2) / ((row1_total * col2_total) / grand_total) +\n  POWER((c - (row2_total * col1_total) / grand_total), 2) / ((row2_total * col1_total) / grand_total) +\n  POWER((d - (row2_total * col2_total) / grand_total), 2) / ((row2_total * col2_total) / grand_total) AS chi_square_statistic\nFROM contingency_table\nWHERE a IS NOT NULL AND b IS NOT NULL AND c IS NOT NULL AND d IS NOT NULL;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq162",
        "db": "isb-cgc-bq.HTAN_versioned",
        "question": "According to the 5th revision (r5) of the HTAN data, list the different imaging assay types and their respective data levels (Level1, Level2, Level3, Level4) available at the HTAN WUSTL center. Exclude any records where the 'Component' is NULL or contains 'Auxiliary' or 'OtherAssay'. For each assay, provide the available data levels.",
        "SQL": "WITH prov AS (\n    SELECT DISTINCT HTAN_Data_File_ID, entityId, Component, HTAN_Center\n    FROM isb-cgc-bq.HTAN_versioned.id_provenance_r5\n    WHERE Component IS NOT NULL\n    AND Component NOT LIKE '%Auxiliary%'\n    AND Component NOT LIKE 'OtherAssay'\n  ),\n  img AS(SELECT * EXCEPT(HTAN_Data_File_ID) FROM (\n    SELECT HTAN_Data_File_ID,Imaging_Assay_Type,entityId\n    FROM isb-cgc-bq.HTAN_versioned.imaging_level2_metadata_r5\n    WHERE Component IS NOT NULL\n    UNION ALL\n    SELECT il3s.HTAN_Data_File_ID,il2.Imaging_Assay_Type,il3s.entityId\n    FROM isb-cgc-bq.HTAN_versioned.imaging_level2_metadata_r5 il2\n    JOIN (SELECT * FROM isb-cgc-bq.HTAN_versioned.id_provenance_r5\n      WHERE Component = 'ImagingLevel3Segmentation') il3s\n    ON il2.HTAN_Data_File_ID = il3s.HTAN_Parent_Data_File_ID\n    UNION ALL\n    SELECT il4.HTAN_Data_File_ID,il2.Imaging_Assay_Type,il4.entityId\n    FROM isb-cgc-bq.HTAN_versioned.imaging_level2_metadata_r5 il2\n    JOIN (SELECT * FROM isb-cgc-bq.HTAN_versioned.id_provenance_r5\n      WHERE Component = 'ImagingLevel3Segmentation') il3s\n    ON il2.HTAN_Data_File_ID = il3s.HTAN_Parent_Data_File_ID\n    JOIN (SELECT * FROM isb-cgc-bq.HTAN_versioned.id_provenance_r5\n      WHERE Component = 'ImagingLevel4') il4\n    ON il3s.HTAN_Data_File_ID = il4.HTAN_Parent_Data_File_ID\n  )\n  ),\n  files AS (\n    SELECT HTAN_Center,array_to_string([SPLIT(Component, 'Level')[OFFSET(0)], Imaging_Assay_Type], ' - ') AS Assay,\n    REGEXP_EXTRACT(Component, r'Level\\d') AS Level\n    FROM prov\n    LEFT JOIN img USING(entityId)\n    GROUP BY HTAN_Center, Assay, Level\n    ORDER BY HTAN_Center, Assay, Level\n  )\n  SELECT Assay, STRING_AGG(DISTINCT Level ORDER BY Level) AS Levels\n  FROM files\n  WHERE HTAN_Center = \"HTAN WUSTL\"\n  GROUP BY Assay",
        "external_knowledge": null,
        "plan": "1. **Filter Relevant Data**:\n   - Create a temporary dataset that includes distinct records of certain entities and components, ensuring the components are not null and exclude specific types.\n\n2. **Combine Imaging Assay Data**:\n   - Create another temporary dataset that combines imaging assay data from different levels:\n     - Include data from a specific level where the component is not null.\n     - Merge with another level's data based on a matching file ID.\n     - Further merge with an additional level's data based on another matching file ID.\n\n3. **Prepare Assay Data**:\n   - In a new temporary dataset, select relevant fields from the previously combined dataset:\n     - Construct a descriptive assay field by combining component and assay type information.\n     - Extract the level information from the component.\n     - Ensure the data is grouped and ordered by specific attributes.\n\n4. **Aggregate Data Levels**:\n   - In the final step, select the constructed assay field and aggregate the distinct levels associated with each assay:\n     - Filter the data for a specific center.\n     - Group the data by the assay field.\n     - Concatenate the distinct levels into a single string, ordered by level.\n\n5. **Return Results**:\n   - Output the assay types along with their respective aggregated data levels for the specified center.",
        "special_function": null
    },
    {
        "instance_id": "bq163",
        "db": "spider2-public-data.HTAN",
        "question": "Identify the top 20 genes with the largest expression disparities between male and female 74-year-old epithelial cells in cluster 41 of MSK-SCLC patients, comparing average X_values by sex.",
        "SQL": "SELECT\n  A.feature_name,\n  A.avg_counts_clust41,\n  B.avg_counts_clust41,\n  A.avg_counts_clust41 - B.avg_counts_clust41 as mean_diff\nFROM (\n  SELECT\n    feature_name,\n    AVG(X_value) AS avg_counts_clust41\n  FROM\n    `spider2-public-data.HTAN.scRNAseq_MSK_SCLC_combined_samples_current`\n  WHERE development_stage = '74-year-old human stage' AND Cell_Type = 'epithelial cell' AND clusters = '41' AND sex = 'female'\n  GROUP BY\n    1) AS A\nINNER JOIN (\n  SELECT\n    feature_name,\n    AVG(X_value) AS avg_counts_clust41\n  FROM\n    `spider2-public-data.HTAN.scRNAseq_MSK_SCLC_combined_samples_current`\n  WHERE development_stage = '74-year-old human stage' AND Cell_Type = 'epithelial cell' AND clusters = '41' AND sex = 'male'\n  GROUP BY\n    1) AS B\nON\n  A.feature_name = B.feature_name\nGROUP BY 1,2,3\nORDER BY mean_diff DESC\nLIMIT 20",
        "external_knowledge": null,
        "plan": "1. **Setup Subqueries for Female Data:**\n   - Create a subquery to calculate the average expression levels for each gene in the specified cell type and cluster for female patients.\n   - Filter the data to include only samples from the specified age group, cell type, cluster, and sex (female).\n   - Group the results by gene to compute the average expression levels.\n\n2. **Setup Subqueries for Male Data:**\n   - Create a similar subquery to calculate the average expression levels for each gene, but for male patients.\n   - Apply the same filters as the female subquery but change the sex to male.\n   - Group the results by gene to compute the average expression levels.\n\n3. **Join Subqueries:**\n   - Perform an inner join on the results of the female and male subqueries based on the gene identifier to align the expression data for each gene between the two sexes.\n\n4. **Calculate Expression Differences:**\n   - Calculate the difference in average expression levels between female and male for each gene.\n   - Create a new column to store this difference.\n\n5. **Order and Limit Results:**\n   - Order the results by the calculated expression difference in descending order to prioritize genes with the largest differences.\n   - Limit the results to the top 20 genes to identify those with the largest expression differences.\n\n6. **Return Results:**\n   - Select the gene identifier, the average expression levels for both sexes, and the calculated difference for the final output.",
        "special_function": null
    },
    {
        "instance_id": "bq164",
        "db": "spider2-public-data.HTAN",
        "question": "Consolidate metadata from spatial transcriptomics and scRNAseq datasets\u2014including levels 1 through 4 and auxiliary files\u2014for the run ID 'HT264P1-S1H2Fc2U1Z1Bs1-H2Bs2-Test'. Include Filename, HTAN Parent Biospecimen ID, Component, File Format, Entity ID, and Run ID.",
        "SQL": "WITH l1 AS (\n    SELECT Filename,\n        HTAN_Parent_Biospecimen_ID,\n        Component,\n        File_Format,\n        entityId,\n        Run_ID\n    FROM `spider2-public-data.HTAN.10xvisium_spatialtranscriptomics_scRNAseq_level1_metadata_current`\n    WHERE RUN_ID = 'HT264P1-S1H2Fc2U1Z1Bs1-H2Bs2-Test'\n),\nl2 AS (\n    SELECT Filename,\n        HTAN_Parent_Biospecimen_ID,\n        Component,\n        File_Format,\n        entityId,\n        Run_ID\n    FROM `spider2-public-data.HTAN.10xvisium_spatialtranscriptomics_scRNAseq_level2_metadata_current`\n    WHERE RUN_ID = 'HT264P1-S1H2Fc2U1Z1Bs1-H2Bs2-Test'\n),\nl3 AS (\n    SELECT Filename,\n        HTAN_Parent_Biospecimen_ID,\n        Component,\n        File_Format,\n        entityId,\n        Run_ID\n    FROM `spider2-public-data.HTAN.10xvisium_spatialtranscriptomics_scRNAseq_level3_metadata_current`\n    WHERE RUN_ID = 'HT264P1-S1H2Fc2U1Z1Bs1-H2Bs2-Test'\n),\nl4 AS (\n    SELECT Filename,\n        HTAN_Parent_Biospecimen_ID,\n        Component,\n        File_Format,\n        entityId,\n        Run_ID\n    FROM `spider2-public-data.HTAN.10xvisium_spatialtranscriptomics_scRNAseq_level4_metadata_current`\n    WHERE RUN_ID = 'HT264P1-S1H2Fc2U1Z1Bs1-H2Bs2-Test'\n),\naux AS (\n    SELECT Filename,\n        HTAN_Parent_Biospecimen_ID,\n        Component,\n        File_Format,\n        entityId,\n        Run_ID\n    FROM `spider2-public-data.HTAN.10xvisium_spatialtranscriptomics_auxiliaryfiles_metadata_current`\n    WHERE RUN_ID = 'HT264P1-S1H2Fc2U1Z1Bs1-H2Bs2-Test'\n)\nSELECT * FROM l1\nUNION ALL \nSELECT * FROM l2\nUNION ALL \nSELECT * FROM l3\nUNION ALL \nSELECT * FROM l4\nUNION ALL\nSELECT * FROM aux",
        "external_knowledge": null,
        "plan": "1. **Define Initial Subqueries**:\n    - Create subqueries to extract relevant metadata from multiple levels of data, ensuring each subquery selects the same set of columns.\n    - Each subquery filters the data based on a specific run identifier provided in the user instruction.\n\n2. **Filtering by Run Identifier**:\n    - Within each subquery, include a condition to filter the data where the run identifier matches the specified value.\n\n3. **Union of Subqueries**:\n    - Combine the results of all subqueries using the `UNION ALL` operator. This operation ensures that all records from each level of metadata and auxiliary files are included in the final result.\n\n4. **Select Consolidated Results**:\n    - Perform a final selection to retrieve the combined dataset, consolidating metadata into a single result set that includes file names, biospecimen identifiers, data components, file formats, entity IDs, and run identifiers as specified in the instruction.\n\nThis approach ensures that all relevant metadata from different data levels and auxiliary files is collected and consolidated based on the specified run identifier.",
        "special_function": null
    },
    {
        "instance_id": "bq166",
        "db": "spider2-public-data.TCGA_versioned\nmitelman-db.prod",
        "question": "Analyze the largest copy number of chromosomal aberrations including amplifications, gains, homozygous deletions, heterozygous deletions, and normal copy states across cytogenetic bands in TCGA-KIRC kidney cancer samples. Use segment allelic data to identify the maximum copy number aberrations within each chromosomal segment, and report their frequencies, sorted by chromosome and cytoband.",
        "SQL": "WITH copy AS (\n  SELECT case_barcode,        #sample_barcode,        aliquot_barcode, \n    chromosome,        start_pos,        end_pos,        MAX(copy_number) as copy_number\n  FROM `spider2-public-data.TCGA_versioned.copy_number_segment_allelic_hg38_gdc_r23` \n  WHERE  project_short_name = 'TCGA-KIRC'\n  GROUP BY case_barcode, chromosome,        start_pos,        end_pos\n),\ntotal_cases AS (\n  SELECT COUNT( DISTINCT case_barcode) as total\n  FROM copy \n),\ncytob AS (\n  SELECT chromosome, cytoband_name, hg38_start, hg38_stop,\n  FROM `mitelman-db.prod.CytoBands_hg38`\n),\njoined AS (\n  SELECT cytob.chromosome, cytoband_name, hg38_start, hg38_stop,\n    case_barcode,\n    copy_number  \n  FROM copy\n  LEFT JOIN cytob\n  ON cytob.chromosome = copy.chromosome \n  WHERE \n    ( cytob.hg38_start >= copy.start_pos AND copy.end_pos >= cytob.hg38_start )\n    OR ( copy.start_pos >= cytob.hg38_start  AND  copy.start_pos <= cytob.hg38_stop )\n),\ncbands AS(\nSELECT chromosome, cytoband_name, hg38_start, hg38_stop, case_barcode,\n    MAX(copy_number) as copy_number\nFROM joined\nGROUP BY \n   chromosome, cytoband_name, hg38_start, hg38_stop, case_barcode\n),\naberrations AS (\n  SELECT\n    chromosome,\n    cytoband_name,\n    -- Amplifications: more than two copies for diploid > 4\n    SUM( IF (copy_number > 3 , 1 , 0) ) AS total_amp,\n    -- Gains: at most two extra copies\n    SUM( IF( copy_number = 3 ,1, 0) ) AS total_gain,\n    -- Homozygous deletions, or complete deletions\n    SUM( IF( copy_number = 0, 1, 0) ) AS total_homodel,\n    -- Heterozygous deletions, 1 copy lost\n    SUM( IF( copy_number = 1, 1, 0) ) AS total_heterodel,\n    -- Normal for Diploid = 2\n    SUM( IF( copy_number = 2, 1, 0) )  AS total_normal\n\n  FROM cbands\n  GROUP BY chromosome, cytoband_name, cytoband_name\n)\nSELECT chromosome, cytoband_name,\n  total,  \n  100 * total_amp / total as freq_amp, \n  100 * total_gain / total as freq_gain,\n  100 * total_homodel/ total as freq_homodel, \n  100 * total_heterodel / total as freq_heterodel, \n  100 * total_normal / total as freq_normal  \nFROM aberrations, total_cases\nORDER BY chromosome, cytoband_name",
        "external_knowledge": "Comprehensive_Guide_to_Copy_Number_Variations_in_Cancer_Genomics.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq165",
        "db": "mitelman-db.prod",
        "question": "Can you use CytoConverter genomic coordinates to calculate the frequency of chromosomal gains and losses across a cohort of breast cancer (morphology='3111') and adenocarcinoma (topology='0401') samples? Concretely, please include the number and frequency (2 decimals in percentage) of amplifications (gains of more than 1 copy), gains (1 extra copy), losses (1 copy) and homozygous deletions (loss of 2 copies) for each chromosomal band. And sort the result by the ordinal of each chromosome and the starting-ending base-pair position of each band in ascending order.",
        "SQL": "WITH cyto_cases AS (\n    SELECT DISTINCT\n        c.Refno,\n        c.CaseNo,\n        c.InvNo\n    FROM\n        `mitelman-db.prod.CytogenInvValid` c\n    JOIN `mitelman-db.prod.Reference` Reference ON c.Refno = Reference.Refno\n    JOIN `mitelman-db.prod.Cytogen` Cytogen ON Cytogen.RefNo = c.RefNo AND Cytogen.CaseNo = c.CaseNo\n    LEFT JOIN `mitelman-db.prod.Koder` KoderM ON Cytogen.Morph = KoderM.Kod AND KoderM.KodTyp = 'MORPH'\n    LEFT JOIN `mitelman-db.prod.Koder` KoderT ON Cytogen.Topo = KoderT.Kod AND KoderT.KodTyp = 'TOP'\n    WHERE\n        Cytogen.Morph IN ('3111')\n        AND Cytogen.Topo IN ('0401')\n),\nSampleCount AS (\n    SELECT COUNT(*) AS sCount\n    FROM cyto_cases\n),\nCase_CC_Kary_Result AS (\n    SELECT cc_result.*\n    FROM cyto_cases\n    LEFT JOIN `mitelman-db.prod.CytoConverted` AS cc_result\n    ON cc_result.RefNo = cyto_cases.RefNo\n       AND cc_result.CaseNo = cyto_cases.CaseNo\n       AND cc_result.InvNo = cyto_cases.InvNo\n),\nClone_imbal_sums AS (\n    SELECT\n        cytoBands.chromosome,\n        cytoBands.cytoband_name,\n        cytoBands.hg38_start,\n        cytoBands.hg38_stop,\n        Case_CC_Kary_Result.RefNo,\n        Case_CC_Kary_Result.CaseNo,\n        Case_CC_Kary_Result.InvNo,\n        Case_CC_Kary_Result.Clone,\n        SUM(CASE WHEN type = 'Gain' THEN 1 ELSE 0 END) AS totalGain,\n        SUM(CASE WHEN type = 'Loss' THEN 1 ELSE 0 END) AS totalLoss\n    FROM `mitelman-db.prod.CytoBands_hg38` AS cytoBands\n    INNER JOIN Case_CC_Kary_Result ON cytoBands.chromosome = Case_CC_Kary_Result.Chr\n    WHERE cytoBands.hg38_start >= Case_CC_Kary_Result.Start\n          AND cytoBands.hg38_stop <= Case_CC_Kary_Result.End\n    GROUP BY\n        cytoBands.chromosome,\n        cytoBands.cytoband_name,\n        cytoBands.hg38_start,\n        cytoBands.hg38_stop,\n        Case_CC_Kary_Result.RefNo,\n        Case_CC_Kary_Result.CaseNo,\n        Case_CC_Kary_Result.InvNo,\n        Case_CC_Kary_Result.Clone\n),\nAMP_DEL_counts AS (\n    SELECT\n        Clone_imbal_sums.chromosome,\n        Clone_imbal_sums.cytoband_name,\n        Clone_imbal_sums.hg38_start,\n        Clone_imbal_sums.hg38_stop,\n        Clone_imbal_sums.RefNo,\n        Clone_imbal_sums.CaseNo,\n        Clone_imbal_sums.InvNo,\n        Clone_imbal_sums.Clone,\n        CASE WHEN Clone_imbal_sums.totalGain > 1 THEN Clone_imbal_sums.totalGain ELSE 0 END AS amplified,\n        CASE WHEN Clone_imbal_sums.totalLoss > 1 THEN Clone_imbal_sums.totalLoss ELSE 0 END AS hozy_deleted,\n        CASE WHEN Clone_imbal_sums.totalGain > 1 THEN 1 ELSE 0 END AS amp_count,\n        CASE WHEN Clone_imbal_sums.totalLoss > 1 THEN 1 ELSE 0 END AS hozy_del_count\n    FROM Clone_imbal_sums\n),\nSingular_imbal AS (\n    SELECT\n        Clone_imbal_sums.chromosome,\n        Clone_imbal_sums.cytoband_name,\n        Clone_imbal_sums.hg38_start,\n        Clone_imbal_sums.hg38_stop,\n        Clone_imbal_sums.RefNo,\n        Clone_imbal_sums.CaseNo,\n        Clone_imbal_sums.InvNo,\n        Clone_imbal_sums.Clone,\n        Clone_imbal_sums.totalGain - AMP_DEL_counts.amplified AS Singular_gain,\n        Clone_imbal_sums.totalLoss - AMP_DEL_counts.hozy_deleted AS Singular_loss,\n        AMP_DEL_counts.amp_count,\n        AMP_DEL_counts.hozy_del_count\n    FROM Clone_imbal_sums\n    INNER JOIN AMP_DEL_counts ON\n        Clone_imbal_sums.chromosome = AMP_DEL_counts.chromosome AND\n        Clone_imbal_sums.cytoband_name = AMP_DEL_counts.cytoband_name AND\n        Clone_imbal_sums.hg38_start = AMP_DEL_counts.hg38_start AND\n        Clone_imbal_sums.hg38_stop = AMP_DEL_counts.hg38_stop AND\n        Clone_imbal_sums.RefNo = AMP_DEL_counts.RefNo AND\n        Clone_imbal_sums.CaseNo = AMP_DEL_counts.CaseNo AND\n        Clone_imbal_sums.InvNo = AMP_DEL_counts.InvNo AND\n        Clone_imbal_sums.Clone = AMP_DEL_counts.Clone\n),\nSample_dist_count AS (\n    SELECT\n        Singular_imbal.chromosome,\n        Singular_imbal.cytoband_name,\n        Singular_imbal.hg38_start,\n        Singular_imbal.hg38_stop,\n        Singular_imbal.RefNo,\n        Singular_imbal.CaseNo,\n        Singular_imbal.InvNo,\n        CASE WHEN SUM(Singular_imbal.Singular_gain) > 0 THEN 1 ELSE 0 END AS Sample_dist_singular_gain,\n        CASE WHEN SUM(Singular_imbal.Singular_loss) > 0 THEN 1 ELSE 0 END AS Sample_dist_singular_loss,\n        CASE WHEN SUM(Singular_imbal.amp_count) > 0 THEN 1 ELSE 0 END AS Sample_dist_amp,\n        CASE WHEN SUM(Singular_imbal.hozy_del_count) > 0 THEN 1 ELSE 0 END AS Sample_dist_del\n    FROM Singular_imbal\n    GROUP BY\n        Singular_imbal.chromosome,\n        Singular_imbal.cytoband_name,\n        Singular_imbal.hg38_start,\n        Singular_imbal.hg38_stop,\n        Singular_imbal.RefNo,\n        Singular_imbal.CaseNo,\n        Singular_imbal.InvNo\n)\nSELECT\n    Sample_dist_count.chromosome,\n    CASE\n        WHEN SUBSTRING(Sample_dist_count.chromosome, 4) = 'X' THEN 23\n        WHEN SUBSTRING(Sample_dist_count.chromosome, 4) = 'Y' THEN 24\n        ELSE CAST(SUBSTRING(Sample_dist_count.chromosome, 4) AS INT64)\n    END AS chr_ord,\n    Sample_dist_count.cytoband_name,\n    Sample_dist_count.hg38_start,\n    Sample_dist_count.hg38_stop,\n    SampleCount.sCount,\n    SUM(Sample_dist_count.Sample_dist_singular_gain) AS total_gain,\n    SUM(Sample_dist_count.Sample_dist_singular_loss) AS total_loss,\n    SUM(Sample_dist_count.Sample_dist_amp) AS total_amp,\n    SUM(Sample_dist_count.Sample_dist_del) AS total_del,\n    ROUND(SUM(Sample_dist_count.Sample_dist_singular_gain) / SampleCount.sCount * 100, 2) AS gain_freq,\n    ROUND(SUM(Sample_dist_count.Sample_dist_singular_loss) / SampleCount.sCount * 100, 2) AS loss_freq,\n    ROUND(SUM(Sample_dist_count.Sample_dist_amp) / SampleCount.sCount * 100, 2) AS amp_freq,\n    ROUND(SUM(Sample_dist_count.Sample_dist_del) / SampleCount.sCount * 100, 2) AS del_freq\nFROM\n    Sample_dist_count,\n    SampleCount\nGROUP BY\n    Sample_dist_count.chromosome,\n    Sample_dist_count.cytoband_name,\n    Sample_dist_count.hg38_start,\n    Sample_dist_count.hg38_stop,\n    SampleCount.sCount\nORDER BY\n    chr_ord,\n    Sample_dist_count.hg38_start,\n    Sample_dist_count.hg38_stop",
        "external_knowledge": null,
        "plan": "1. **Filter Cases**:\n    - Identify distinct cases from a specific dataset that match the given morphology and topology criteria for breast cancer and adenocarcinoma.\n\n2. **Count Samples**:\n    - Calculate the total number of cases that meet the criteria from step 1.\n\n3. **Join with Genomic Coordinates**:\n    - Retrieve genomic coordinate data for the filtered cases by joining with another dataset based on matching reference numbers, case numbers, and investigation numbers.\n\n4. **Summarize Chromosomal Alterations**:\n    - For each chromosomal band, sum the occurrences of gains and losses by joining the genomic coordinates with chromosomal band data and grouping by relevant attributes.\n\n5. **Categorize Alterations**:\n    - Classify the summarized alterations into amplifications (gains > 1 copy), homozygous deletions (losses > 1 copy), and differentiate between singular gains and losses by subtracting the amplified and deleted counts.\n\n6. **Sample Distribution Count**:\n    - For each chromosomal band, calculate the sample distribution counts for singular gains, singular losses, amplifications, and deletions. This involves grouping by chromosomal attributes and using conditional sums.\n\n7. **Calculate Frequencies**:\n    - Compute the total number and frequency (percentage with 2 decimal precision) of each type of alteration (gains, losses, amplifications, and deletions) for each chromosomal band.\n\n8. **Sort Results**:\n    - Order the final result set by the ordinal value of chromosomes and the starting and ending positions of each chromosomal band in ascending order to ensure correct sequence presentation.\n\nThis plan outlines the logical steps taken to filter, join, summarize, classify, count, calculate, and order the data to provide the requested information on chromosomal gains and losses.",
        "special_function": null
    },
    {
        "instance_id": "bq169",
        "db": "mitelman-db.prod",
        "question": "For cases in which:\n- Chromosome 13 has a loss of genetic material between positions 48,303,751 and 48,481,890,\n- Chromosome 17 has a loss of genetic material between positions 7,668,421 and 7,687,490, and\n- Chromosome 11 has a gain of genetic material between positions 108,223,067 and 108,369,102,\nretrieve distinct case information where all three conditions above are met in one clone. For each case, also return the chromosomal details for each region (chromosome number, start and end positions) and the corresponding karyotype information.",
        "SQL": "WITH\n    rb1 AS (SELECT c.* FROM `mitelman-db.prod.CytoConverted` c\n        WHERE c.ChrOrd = 13\n            AND c.Start <= 48303751\n            AND c.End >= 48481890\n            AND c.Type = 'Loss'),\n    tp53 AS (SELECT c.* FROM `mitelman-db.prod.CytoConverted` c\n        WHERE c.ChrOrd = 17\n            AND c.Start <= 7668421\n            AND c.End >= 7687490\n            AND c.Type = 'Loss'),\n    atm AS (SELECT c.* FROM `mitelman-db.prod.CytoConverted` c\n        WHERE c.ChrOrd = 11\n            AND c.Start <= 108223067\n            AND c.End >= 108369102\n            AND c.Type = 'Gain')\nSELECT DISTINCT r.RefNo,\n                r.CaseNo,\n                r.InvNo,\n                r.ChrOrd,\n                r.Start,\n                r.End,\n                t.ChrOrd,\n                t.Start,\n                t.End,\n                m.ChrOrd,\n                m.Start,\n                m.End,\n                kc.CloneShort\nFROM rb1 r\nJOIN tp53 t\n  ON t.RefNo = r.RefNo\n  AND t.CaseNo = r.CaseNo\n  AND t.InvNo = r.InvNo\n  AND t.Clone = r.Clone\nJOIN atm m\n  ON m.RefNo = r.RefNo\n  AND m.CaseNo = r.CaseNo\n  AND m.InvNo = r.InvNo\n  AND m.Clone = r.Clone\nJOIN `mitelman-db.prod.KaryClone` kc\n  ON r.RefNo = kc.RefNo\n  AND r.CaseNo = kc.CaseNo\n  AND r.InvNo = kc.InvNo\n  AND r.Clone = kc.CloneNo\nORDER BY r.RefNo,\n        r.CaseNo,\n        r.InvNo",
        "external_knowledge": null,
        "plan": "1. **Create Temporary Tables for Each Condition:**\n    - **First Condition (Chromosome 13 Loss):** Create a temporary table containing records where:\n        - Chromosome number is 13.\n        - Genetic material has a loss type.\n        - Start position is less than 48,303,751.\n        - End position is greater than 48,481,890.\n    - **Second Condition (Chromosome 17 Loss):** Create a temporary table containing records where:\n        - Chromosome number is 17.\n        - Genetic material has a loss type.\n        - Start position is less than 7,668,421.\n        - End position is greater than 7,687,490.\n    - **Third Condition (Chromosome 11 Gain):** Create a temporary table containing records where:\n        - Chromosome number is 11.\n        - Genetic material has a gain type.\n        - Start position is less than 108,223,067.\n        - End position is greater than 108,369,102.\n\n2. **Identify Common Cases:**\n    - Perform an intersection of the case identifiers (reference number, case number, investigation number) from the three temporary tables to identify cases that meet all three conditions.\n\n3. **Retrieve Detailed Information for Each Case:**\n    - For each case identified in the intersection, retrieve the corresponding detailed chromosomal information from the temporary tables created in step 1.\n    - Join these details using the case identifiers to ensure all relevant chromosomal information is included.\n\n4. **Include Additional Case Information:**\n    - Join the result with another table to fetch additional karyotype information using the case identifiers.\n\n5. **Return and Order Results:**\n    - Select and return distinct results including case identifiers, chromosomal details for each condition, and the karyotype information.\n    - Order the final result set by the case identifiers for clarity and organization.",
        "special_function": null
    },
    {
        "instance_id": "bq111",
        "db": "mitelman-db.prod",
        "question": "Follow the instruction documentation guide, please help me compute Pearson correlation for each chromosome comparing Mitelman DB frequencies with those computed from TCGA.",
        "SQL": "WITH \ncyto_cases AS (\n SELECT DISTINCT\n c.Refno,\n c.CaseNo,\n c.InvNo,\nFROM\n `mitelman-db.prod.CytogenInvValid` c,\n `mitelman-db.prod.Reference` Reference,\n `mitelman-db.prod.Cytogen` Cytogen\nLEFT JOIN `mitelman-db.prod.Koder` KoderM\nON\n (Cytogen.Morph = KoderM.Kod AND KoderM.KodTyp = 'MORPH')\nLEFT JOIN `mitelman-db.prod.Koder` KoderT\nON\n (Cytogen.Topo = KoderT.Kod AND KoderT.KodTyp = 'TOP')\nWHERE\n Cytogen.RefNo = c.RefNo\n AND Cytogen.CaseNo = c.CaseNo\n AND c.Refno = Reference.Refno\n AND Cytogen.Morph IN ('3111')\n AND Cytogen.Topo IN ('0401')\n),\nSampleCount AS(\n\n SELECT COUNT(*) AS sCount\n FROM cyto_cases\n),\nCase_CC_Kary_Result AS (\n SELECT cc_result.*\n FROM cyto_cases\n LEFT JOIN `mitelman-db.prod.CytoConverted` AS cc_result\n ON cc_result.RefNo = cyto_cases.RefNo\n AND cc_result.caseNo = cyto_cases.caseNo\n AND cc_result.invNo = cyto_cases.invNo\n),\nClone_imbal_sums AS (\n SELECT cytoBands.chromosome,\n cytoBands.cytoband_name,\n cytoBands.hg38_start,\n cytoBands.hg38_stop,\n Case_CC_Kary_Result.RefNo,\n Case_CC_Kary_Result.CaseNo,\n Case_CC_Kary_Result.InvNo,\n Case_CC_Kary_Result.Clone,\n SUM( CASE WHEN type = 'Gain' THEN 1 ELSE 0 END ) AS totalGain,\n SUM( CASE WHEN type = 'Loss' THEN 1 ELSE 0 END ) AS totalLoss\n FROM `mitelman-db.prod.CytoBands_hg38` AS cytoBands\n INNER JOIN Case_CC_Kary_Result\n  ON cytoBands.chromosome = Case_CC_Kary_Result.Chr\n WHERE cytoBands.hg38_start >= Case_CC_Kary_Result.Start\n AND cytoBands.hg38_stop <= Case_CC_Kary_Result.End\n GROUP BY\n cytoBands.chromosome,\n cytoBands.cytoband_name,\n cytoBands.hg38_start,\n cytoBands.hg38_stop,\n Case_CC_Kary_Result.RefNo,\n Case_CC_Kary_Result.CaseNo,\n Case_CC_Kary_Result.InvNo,\n Case_CC_Kary_Result.Clone\n),\nAMP_DEL_counts AS (\n SELECT Clone_imbal_sums.chromosome, Clone_imbal_sums.cytoband_name, Clone_imbal_sums.hg38_start, Clone_imbal_sums.hg38_stop,\n Clone_imbal_sums.RefNo, Clone_imbal_sums.CaseNo, Clone_imbal_sums.InvNo, Clone_imbal_sums.Clone,\n CASE WHEN Clone_imbal_sums.totalGain > 1 THEN Clone_imbal_sums.totalGain ELSE 0 END AS amplified,\n CASE WHEN Clone_imbal_sums.totalLoss > 1 THEN Clone_imbal_sums.totalLoss ELSE 0 END AS hozy_deleted,\n CASE WHEN Clone_imbal_sums.totalGain > 1 THEN 1 ELSE 0 END AS amp_count,\n CASE WHEN Clone_imbal_sums.totalLoss > 1 THEN 1 ELSE 0 END AS hozy_del_count,\n FROM Clone_imbal_sums\n),\nSingular_imbal AS (\n SELECT Clone_imbal_sums.chromosome,\n Clone_imbal_sums.cytoband_name,\n Clone_imbal_sums.hg38_start,\n Clone_imbal_sums.hg38_stop,\n Clone_imbal_sums.RefNo,\n Clone_imbal_sums.CaseNo,\n Clone_imbal_sums.InvNo,\n Clone_imbal_sums.Clone,\n Clone_imbal_sums.totalGain - AMP_DEL_counts.amplified AS Singular_gain,\n Clone_imbal_sums.totalLoss - AMP_DEL_counts.hozy_deleted AS Singular_loss,\n AMP_DEL_counts.amp_count,\n AMP_DEL_counts.hozy_del_count\n FROM Clone_imbal_sums\n INNER JOIN AMP_DEL_counts\n ON Clone_imbal_sums.chromosome = AMP_DEL_counts.chromosome\n AND Clone_imbal_sums.cytoband_name = AMP_DEL_counts.cytoband_name\n AND Clone_imbal_sums.hg38_start= AMP_DEL_counts.hg38_start\n AND Clone_imbal_sums.hg38_stop = AMP_DEL_counts.hg38_stop\n AND Clone_imbal_sums.RefNo= AMP_DEL_counts.RefNo\n AND Clone_imbal_sums.CaseNo = AMP_DEL_counts.CaseNo\n AND Clone_imbal_sums.InvNo = AMP_DEL_counts.InvNo\n AND Clone_imbal_sums.Clone = AMP_DEL_counts.Clone\n),\nSample_dist_count AS (\n SELECT Singular_imbal.chromosome,\n Singular_imbal.cytoband_name,\n Singular_imbal.hg38_start,\n Singular_imbal.hg38_stop,\n Singular_imbal.RefNo,\n Singular_imbal.CaseNo,\n Singular_imbal.InvNo,\n CASE WHEN SUM(Singular_imbal.Singular_gain)> 0 THEN 1 ELSE 0 END AS Sample_dist_singular_gain,\n CASE WHEN SUM(Singular_imbal.Singular_loss)> 0 THEN 1 ELSE 0 END AS Sample_dist_singular_loss,\n CASE WHEN SUM(Singular_imbal.amp_count)>0 THEN 1 ELSE 0 END AS Sample_dist_amp,\n CASE WHEN SUM(Singular_imbal.hozy_del_count)>0 THEN 1 ELSE 0 END AS Sample_dist_del,\n FROM Singular_imbal\n GROUP BY\n Singular_imbal.chromosome,\n Singular_imbal.cytoband_name,\n Singular_imbal.hg38_start,\n Singular_imbal.hg38_stop,\n Singular_imbal.RefNo,\n Singular_imbal.CaseNo,\n Singular_imbal.InvNo\n),\nmitelman AS ( \nSELECT Sample_dist_count.chromosome,\n CASE WHEN SUBSTRING(Sample_dist_count.chromosome, 4) = 'X' THEN 23\n      WHEN SUBSTRING(Sample_dist_count.chromosome, 4) = 'Y' THEN 24\n      ELSE CAST(SUBSTRING(Sample_dist_count.chromosome, 4) AS INT64)\n END AS chr_ord,\n Sample_dist_count.cytoband_name,\n Sample_dist_count.hg38_start,\n Sample_dist_count.hg38_stop,\n SampleCount.sCount,\n SUM(Sample_dist_count.Sample_dist_singular_gain) AS total_gain,\n SUM(Sample_dist_count.Sample_dist_singular_loss) AS total_loss,\n SUM(Sample_dist_count.Sample_dist_amp) AS total_amp,\n SUM(Sample_dist_count.Sample_dist_del) AS total_del,\n ROUND(SUM(Sample_dist_count.Sample_dist_singular_gain)/SampleCount.sCount*100, 2) AS gain_freq,\n ROUND(SUM(Sample_dist_count.Sample_dist_singular_loss)/SampleCount.sCount*100, 2) AS loss_freq,\n ROUND(SUM(Sample_dist_count.Sample_dist_amp)/SampleCount.sCount*100, 2) AS amp_freq,\n ROUND(SUM(Sample_dist_count.Sample_dist_del)/SampleCount.sCount*100, 2) AS del_freq\n FROM Sample_dist_count, SampleCount\n GROUP BY\n Sample_dist_count.chromosome,\n Sample_dist_count.cytoband_name,\n Sample_dist_count.hg38_start,\n Sample_dist_count.hg38_stop,\n SampleCount.sCount\n ORDER BY chr_ord, Sample_dist_count.hg38_start\n)\n,\ncopy AS (\n  SELECT case_barcode,\t#sample_barcode,\taliquot_barcode, \n    chromosome,\tstart_pos,\tend_pos,\tMAX(copy_number) as copy_number\n  FROM `isb-cgc-bq.TCGA_versioned.copy_number_segment_allelic_hg38_gdc_r23` \n  WHERE  project_short_name = 'TCGA-BRCA'\n  GROUP BY case_barcode, chromosome,\tstart_pos,\tend_pos\n),\ntotal_cases AS (\n  SELECT COUNT( DISTINCT case_barcode) as total\n  FROM copy \n),\ncytob AS (\n  SELECT chromosome, cytoband_name, hg38_start, hg38_stop,\n  FROM mitelman-db.prod.CytoBands_hg38\n),\njoined AS (\n  SELECT cytob.chromosome, cytoband_name, hg38_start, hg38_stop,\n    case_barcode,\n    ( ABS(hg38_stop - hg38_start) + ABS(end_pos - start_pos) \n      - ABS(hg38_stop - end_pos) - ABS(hg38_start - start_pos) )/2.0  AS overlap ,\n    copy_number  \n  FROM copy\n  LEFT JOIN cytob\n  ON cytob.chromosome = copy.chromosome \n  WHERE \n    #cytob.hg38_start >= copy.start_pos AND cytob.hg38_start <= copy.end_pos  \n    ( cytob.hg38_start >= copy.start_pos AND copy.end_pos >= cytob.hg38_start )\n    OR ( copy.start_pos >= cytob.hg38_start  AND  copy.start_pos <= cytob.hg38_stop )\n),\ncbands AS(\nSELECT chromosome, cytoband_name, hg38_start, hg38_stop, case_barcode,\n    ROUND( SUM(overlap*copy_number) / SUM(overlap) ) as copy_number\n    #ARRAY_AGG( copy_number ORDER BY overlap DESC )[OFFSET(0)] as copy_number\n    #ANY_VALUE(copy_number) as copy_number\n    #MAX(copy_number) as copy_number\n    #MIN(copy_number) as copy_number\nFROM joined\nGROUP BY \n   chromosome, cytoband_name, hg38_start, hg38_stop, case_barcode\n),\naberrations AS (\n  SELECT\n    chromosome,\n    cytoband_name,\n    hg38_start,\n    hg38_stop,\n    -- Amplifications: more than two copies for diploid > 4\n    SUM( IF (copy_number > 3 , 1 , 0) ) AS total_amp,\n    -- Gains: at most two extra copies\n    SUM( IF( copy_number = 3 ,1, 0) ) AS total_gain,\n    -- Homozygous deletions, or complete deletions\n    SUM( IF( copy_number = 0, 1, 0) ) AS total_homodel,\n    -- Heterozygous deletions, 1 copy lost\n    SUM( IF( copy_number = 1, 1, 0) ) AS total_heterodel,\n    -- Normal for Diploid = 2\n    SUM( IF( copy_number = 2, 1, 0) )  AS total_normal\n\n  FROM cbands\n  GROUP BY chromosome, cytoband_name, hg38_start, hg38_stop\n),\ntcga AS (\nSELECT chromosome, cytoband_name, hg38_start, hg38_stop,\n  total,  \n  100 * total_amp / total as freq_amp, \n  100 * total_gain / total as freq_gain,\n  100 * total_homodel/ total as freq_homodel, \n  100 * total_heterodel / total as freq_heterodel, \n  100 * total_normal / total as freq_normal  \nFROM aberrations, total_cases\nORDER BY chromosome, hg38_start, hg38_stop\n)\n \nSELECT chromosome,\n  corr_amp, `isb-cgc-bq.functions.corr_pvalue_current`(corr_amp, N) AS pvalue_amp,\n  corr_gain, `isb-cgc-bq.functions.corr_pvalue_current`(corr_gain, N) AS pvalue_gain,\n  corr_loss, `isb-cgc-bq.functions.corr_pvalue_current`(corr_loss, N) AS pvalue_loss,\n  corr_del, `isb-cgc-bq.functions.corr_pvalue_current`(corr_del, N) AS pvalue_del,\nFROM (\n  SELECT mitelman.chromosome,\n      COUNT(*) AS N, \n      CORR( mitelman.amp_freq, tcga.freq_amp ) as corr_amp,\n      CORR(mitelman.gain_freq, tcga.freq_gain ) as corr_gain,\n      CORR(mitelman.loss_freq, tcga.freq_heterodel ) as corr_loss,\n      CORR(mitelman.del_freq, tcga.freq_homodel ) as corr_del,\n  FROM mitelman\n  JOIN tcga\n  ON mitelman.chromosome = tcga.chromosome\n    AND mitelman.cytoband_name = tcga.cytoband_name \n  GROUP BY mitelman.chromosome\n)\nWHERE N > 5\nORDER BY chromosome",
        "external_knowledge": "Correlations_between_Mitelman_and_TCGA_datasets.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq451",
        "db": "spider2-public-data.1000_genomes",
        "question": "Extract genotype data for single nucleotide polymorphisms (SNPs) from chromosome X , ensuring that the start positions are not between 59999 and 2699519 nor between 154931042 and 155260559.   Output the sample ID, counts of homozygous reference alleles, homozygous alternate alleles, heterozygous alternate alleles, the total number of callable sites, the total number of SNVs,  the percentage of heterozygous alternate alleles among all SNVs, and the percentage of homozygous alternate alleles among all SNVs.",
        "SQL": "SELECT\n  sample_id,\n  (hom_AA_count + het_RA_count + hom_RR_count) AS all_callable_sites,\n  hom_AA_count,\n  het_RA_count,\n  hom_RR_count,\n  (hom_AA_count + het_RA_count) AS all_snvs,\n  ROUND((het_RA_count / NULLIF(hom_AA_count + het_RA_count, 0)) * 1000) / 1000 AS perct_het_alt_in_snvs,\n  ROUND((hom_AA_count / NULLIF(hom_AA_count + het_RA_count, 0)) * 1000) / 1000 AS perct_hom_alt_in_snvs\nFROM (\n  SELECT\n    sample_id,\n    SUM(IF(0 = first_allele AND 0 = second_allele, 1, 0)) AS hom_RR_count,\n    SUM(IF(first_allele = second_allele AND first_allele > 0, 1, 0)) AS hom_AA_count,\n    SUM(IF((first_allele != second_allele OR second_allele IS NULL OR first_allele IS NULL)\n        AND (first_allele > 0 OR second_allele > 0), 1, 0)) AS het_RA_count\n  FROM (\n    SELECT\n      reference_name,\n      call.call_set_name AS sample_id,\n      call.genotype[SAFE_OFFSET(0)] AS first_allele,\n      call.genotype[SAFE_OFFSET(1)] AS second_allele\n    FROM\n      `spider2-public-data.1000_genomes.variants`,\n      UNNEST(call) AS call\n    WHERE\n      reference_name = 'X'\n      AND vt = 'SNP'\n      AND start NOT BETWEEN 59999 AND 2699519\n      AND start NOT BETWEEN 154931042 AND 155260559\n      AND ARRAY_LENGTH(call.genotype) >= 1\n  )\n  GROUP BY\n    sample_id,\n    reference_name\n) AS g\nJOIN\n  `spider2-public-data.1000_genomes.sample_info` p\nON\n  g.sample_id = p.sample\nORDER BY\n  perct_het_alt_in_snvs DESC,\n  sample_id",
        "external_knowledge": "1000_genomes_alleles_type.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq452",
        "db": "spider2-public-data.1000_genomes",
        "question": "Identify variants on chromosome 12, calculate their chi-squared scores using allele counts in cases and controls, and return the start, end, chi-squared score (after Yates's correction for continuity) of top variants where the chi-squared score is no less than 29.71679, ensuring that each group has expected counts of at least 5 for the chi-squared calculation.",
        "SQL": "SELECT *\nFROM (\n  SELECT\n    `start`,\n    `end`,\n    ROUND(\n      POW(ABS(case_ref_count - (ref_count / allele_count) * case_count) - 0.5, 2) / ((ref_count / allele_count) * case_count) +\n      POW(ABS(control_ref_count - (ref_count / allele_count) * control_count) - 0.5, 2) / ((ref_count / allele_count) * control_count) +\n      POW(ABS(case_alt_count - (alt_count / allele_count) * case_count) - 0.5, 2) / ((alt_count / allele_count) * case_count) +\n      POW(ABS(control_alt_count - (alt_count / allele_count) * control_count) - 0.5, 2) / ((alt_count / allele_count) * control_count),\n      3\n    ) AS chi_squared_score\n  FROM (\n    SELECT\n      reference_name,\n      `start`,\n      `end`,\n      reference_bases,\n      alternate_bases,\n      vt,\n      SUM(ref_count + alt_count) AS allele_count,\n      SUM(ref_count) AS ref_count,\n      SUM(alt_count) AS alt_count,\n      SUM(IF(is_case, CAST(ref_count + alt_count AS INT64), 0)) AS case_count,\n      SUM(IF(NOT is_case, CAST(ref_count + alt_count AS INT64), 0)) AS control_count,\n      SUM(IF(is_case, ref_count, 0)) AS case_ref_count,\n      SUM(IF(is_case, alt_count, 0)) AS case_alt_count,\n      SUM(IF(NOT is_case, ref_count, 0)) AS control_ref_count,\n      SUM(IF(NOT is_case, alt_count, 0)) AS control_alt_count\n    FROM (\n      SELECT\n        v.reference_name,\n        v.`start`,\n        v.`end`,\n        v.reference_bases,\n        v.alternate_bases,\n        v.vt,\n        ('EAS' = p.super_population) AS is_case,\n        IF(call.genotype[SAFE_OFFSET(0)] = 0, 1, 0) AS ref_count,\n        IF(call.genotype[SAFE_OFFSET(0)] = 1, 1, 0) AS alt_count\n      FROM\n        `spider2-public-data.1000_genomes.variants` AS v,\n        UNNEST(v.call) AS call\n      JOIN\n        `spider2-public-data.1000_genomes.sample_info` AS p\n      ON\n        call.call_set_name = p.sample\n      WHERE\n        v.reference_name = '12'\n    )\n    GROUP BY\n      reference_name,\n      `start`,\n      `end`,\n      reference_bases,\n      alternate_bases,\n      vt\n  )\n  WHERE\n    (ref_count / allele_count) * case_count >= 5.0\n    AND (ref_count / allele_count) * control_count >= 5.0\n    AND (alt_count / allele_count) * case_count >= 5.0\n    AND (alt_count / allele_count) * control_count >= 5.0\n)\nWHERE\n  chi_squared_score >= 29.71679\nORDER BY\n  chi_squared_score DESC",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq453",
        "db": "spider2-public-data.1000_genomes",
        "question": "What are the reference names, start positions, end positions, reference bases, alternate bases, variant types, chi-squared scores (calculated using Hardy-Weinberg equilibrium), and the observed and expected counts of homozygous reference, heterozygous, and homozygous alternate genotypes, including their allele frequencies and allele frequencies, for variants on chromosome 17 between positions 41196311 and 41277499?",
        "SQL": "SELECT\n  reference_name,\n  start,\n  `END`,\n  reference_bases,\n  alt,\n  vt,\n  POW(hom_ref_count - expected_hom_ref_count, 2) / expected_hom_ref_count +\n    POW(hom_alt_count - expected_hom_alt_count, 2) / expected_hom_alt_count +\n    POW(het_count - expected_het_count, 2) / expected_het_count AS chi_squared_score,\n  total_count,\n  hom_ref_count,\n  expected_hom_ref_count AS expected_hom_ref_count,\n  het_count,\n  expected_het_count AS expected_het_count,\n  hom_alt_count,\n  expected_hom_alt_count AS expected_hom_alt_count,\n  alt_freq AS alt_freq,\n  alt_freq_from_1KG\nFROM (\n  SELECT\n    reference_name,\n    start,\n    `END`,\n    reference_bases,\n    alt,\n    vt,\n    alt_freq_from_1KG,\n    hom_ref_freq + (0.5 * het_freq) AS hw_ref_freq,\n    1 - (hom_ref_freq + (0.5 * het_freq)) AS alt_freq,\n    POW(hom_ref_freq + (0.5 * het_freq), 2) * total_count AS expected_hom_ref_count,\n    POW(1 - (hom_ref_freq + (0.5 * het_freq)), 2) * total_count AS expected_hom_alt_count,\n    2 * (hom_ref_freq + (0.5 * het_freq)) * (1 - (hom_ref_freq + (0.5 * het_freq))) * total_count AS expected_het_count,\n    total_count,\n    hom_ref_count,\n    het_count,\n    hom_alt_count,\n    hom_ref_freq,\n    het_freq,\n    hom_alt_freq\n  FROM (\n    SELECT\n      reference_name,\n      start,\n      `END`,\n      reference_bases,\n      STRING_AGG(DISTINCT alternate_base) AS alt,\n      vt,\n      af AS alt_freq_from_1KG,\n      COUNTIF(first_allele IN (0, 1) AND second_allele IN (0, 1)) AS total_count,\n      COUNTIF(first_allele = 0 AND second_allele = 0) AS hom_ref_count,\n      COUNTIF((first_allele = 0 AND second_allele = 1) OR (first_allele = 1 AND second_allele = 0)) AS het_count,\n      COUNTIF(first_allele = 1 AND second_allele = 1) AS hom_alt_count,\n      SAFE_DIVIDE(COUNTIF(first_allele = 0 AND second_allele = 0), COUNTIF(first_allele IN (0, 1) AND second_allele IN (0, 1))) AS hom_ref_freq,\n      SAFE_DIVIDE(COUNTIF((first_allele = 0 AND second_allele = 1) OR (first_allele = 1 AND second_allele = 0)), COUNTIF(first_allele IN (0, 1) AND second_allele IN (0, 1))) AS het_freq,\n      SAFE_DIVIDE(COUNTIF(first_allele = 1 AND second_allele = 1), COUNTIF(first_allele IN (0, 1) AND second_allele IN (0, 1))) AS hom_alt_freq\n    FROM (\n      SELECT\n        reference_name,\n        start,\n        `END`,\n        reference_bases,\n        vt,\n        af,\n        call.call_set_name,\n        call.genotype[OFFSET(0)] AS first_allele,\n        call.genotype[OFFSET(1)] AS second_allele,\n        alternate_base\n      FROM\n        `spider2-public-data.1000_genomes.variants`,\n        UNNEST(call) AS call,\n        UNNEST(alternate_bases) AS alternate_base\n      WHERE\n        reference_name = '17'\n        AND start BETWEEN 41196311 AND 41277499\n    )\n    GROUP BY\n      reference_name,\n      start,\n      `END`,\n      reference_bases,\n      vt,\n      af\n  )\n)",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq454",
        "db": "spider2-public-data.1000_genomes",
        "question": "Identify the number of common autosomal variants (with an allele frequency \u2265 0.05) shared by different combinations of super populations, total population size for each super population, variant types, and sample counts. Exclude sex chromosomes (X, Y, MT) from the analysis.",
        "SQL": "WITH\n  population_counts AS (\n  SELECT\n    super_population,\n    COUNT(population) AS super_population_count\n  FROM\n    `spider2-public-data.1000_genomes.sample_info`\n  WHERE\n    In_Phase1_Integrated_Variant_Set = TRUE\n  GROUP BY\n    super_population),\n  --\n  autosome_calls AS (\n  SELECT\n    reference_name,\n    start,\n    `end`,\n    reference_bases,\n    alternate_bases[ORDINAL(1)] AS alt,  -- 1000 Genomes is biallelic.\n    vt,\n    af IS NOT NULL\n    AND af >= 0.05 AS is_common_variant,\n    call.call_set_name,\n    super_population\n  FROM\n    `spider2-public-data.1000_genomes.variants` AS v, v.call AS call\n  JOIN\n    `spider2-public-data.1000_genomes.sample_info` AS p\n  ON\n    call.call_set_name = p.sample\n  WHERE\n    reference_name NOT IN (\"X\", \"Y\", \"MT\")\n    AND EXISTS (SELECT gt FROM UNNEST(call.genotype) gt WHERE gt > 0)),\n  --\n  super_population_autosome_variants AS (\n  SELECT\n    reference_name,\n    start,\n    `end`,\n    reference_bases,\n    alt,\n    vt,\n    super_population,\n    is_common_variant,\n    COUNT(call_set_name) AS num_samples\n  FROM\n    autosome_calls\n  GROUP BY\n    reference_name,\n    start,\n    `end`,\n    reference_bases,\n    alt,\n    vt,\n    super_population,\n    is_common_variant )\n\nSELECT\n  p.super_population AS super_population,\n  super_population_count,\n  is_common_variant,\n  num_samples,\n  -- num_samples / super_population_count AS percent_samples,\n  COUNT(1) AS num_variants_shared_by_this_many_samples\nFROM\n  super_population_autosome_variants AS v\nJOIN population_counts AS p\nON\n  v.super_population = p.super_population\nGROUP BY\n  super_population,\n  super_population_count,\n  is_common_variant,\n  num_samples",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq415",
        "db": "spider2-public-data.human_genome_variants",
        "question": "List the samples in the genome data that rank in the top 10 for the number of homozygous reference genotypes, considering only the primary reference allele, ordered in descending sequence.",
        "SQL": "WITH calls AS (\n  SELECT\n    call.name AS call_set_name,\n    alt_offset + 1 AS alt_num,\n    (SELECT LOGICAL_AND(gt = 0) FROM UNNEST(call.genotype) gt) AS reference_match_call\n  FROM\n    `spider2-public-data.human_genome_variants.1000_genomes_phase_3_variants_20150220` v,\n    UNNEST(v.call) call, v.alternate_bases alt WITH OFFSET alt_offset\n),\n\ncompute_sums AS (\n  SELECT\n    call_set_name,\n    SUM(CAST(alt_num = 1 AND reference_match_call AS INT64)) AS hom_RR_count\n  FROM calls\n  GROUP BY call_set_name\n)\n\nSELECT\n  call_set_name,\n  hom_RR_count\nFROM compute_sums\nORDER BY hom_RR_count DESC\nLIMIT 10",
        "external_knowledge": "Homozygous_Reference_Genotype.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq279",
        "db": "bigquery-public-data.austin_bikeshare\nbigquery-public-data.austin_311\nbigquery-public-data.austin_crime\nbigquery-public-data.austin_incidents\nbigquery-public-data.austin_waste",
        "question": "Can you provide the number of distinct active and closed bike share stations for each year 2013 and 2014?",
        "SQL": "SELECT\n    t.year,\n    CASE \n        WHEN t.year = 2013 THEN (\n                                  SELECT \n                                    COUNT(DISTINCT station_id)\n                                  FROM \n                                    `bigquery-public-data.austin_bikeshare.bikeshare_trips` t\n                                  INNER JOIN \n                                    `bigquery-public-data.austin_bikeshare.bikeshare_stations` s\n                                  ON \n                                    t.start_station_id = s.station_id\n                                  WHERE \n                                    s.status = 'active' AND EXTRACT(YEAR FROM start_time) = 2013\n                                 ) \n        WHEN t.year = 2014 THEN (\n                                  SELECT \n                                    COUNT(DISTINCT station_id)\n                                  FROM \n                                    `bigquery-public-data.austin_bikeshare.bikeshare_trips` t\n                                  INNER JOIN \n                                    `bigquery-public-data.austin_bikeshare.bikeshare_stations` s\n                                  ON \n                                    t.start_station_id = s.station_id\n                                  WHERE \n                                    s.status = 'active' AND EXTRACT(YEAR FROM start_time) = 2014\n                                 )\n    END\n    AS number_status_active,\n    CASE \n        WHEN t.year = 2013 THEN (\n                                  SELECT \n                                   COUNT(DISTINCT station_id)\n                                  FROM \n                                  `bigquery-public-data.austin_bikeshare.bikeshare_trips` t\n                                  INNER JOIN \n                                  `bigquery-public-data.austin_bikeshare.bikeshare_stations` s\n                                  ON \n                                   t.start_station_id = s.station_id\n                                  WHERE \n                                   s.status = 'closed' AND EXTRACT(YEAR FROM start_time) = 2013\n                                 ) \n        WHEN t.year = 2014 THEN (\n                                  SELECT \n                                  COUNT(DISTINCT station_id)\n                                  FROM \n                                    `bigquery-public-data.austin_bikeshare.bikeshare_trips` t\n                                  INNER JOIN \n                                    `bigquery-public-data.austin_bikeshare.bikeshare_stations` s\n                                  ON \n                                    t.start_station_id = s.station_id\n                                  WHERE \n                                    s.status = 'closed' AND EXTRACT(YEAR FROM start_time) = 2014\n                                 )\n    END\n    AS number_status_closed\nFROM\n    (\n      SELECT \n         EXTRACT(YEAR FROM start_time) AS year,\n         start_station_id\n      FROM\n         `bigquery-public-data.austin_bikeshare.bikeshare_trips`\n    ) \n    AS t\nINNER JOIN\n    `bigquery-public-data.austin_bikeshare.bikeshare_stations` s\nON\n    t.start_station_id = s.station_id\nWHERE\n    t.year BETWEEN 2013 AND 2014\nGROUP BY\n    t.year\nORDER BY\n    t.year",
        "external_knowledge": null,
        "plan": "1. Extract the year and start station ID from the trips data for the years 2013 and 2014.\n2. Join the extracted trip data with the station data on the station ID to consolidate trip and station information.\n3. Limit the joined data to only include records from the years 2013 and 2014.\n4. For each year, count the distinct station IDs where the station status is 'active'. This involves filtering the joined data for 'active' stations and then counting distinct station IDs for each year.\n5. Similarly, for each year, count the distinct station IDs where the station status is 'closed'. This involves filtering the joined data for 'closed' stations and then counting distinct station IDs.\n6. Group the results by year to ensure that the counts of active and closed stations are organized by year.\n7. Order the final results by year to provide a chronological view of the data.\n",
        "special_function": [
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq281",
        "db": "bigquery-public-data.austin_bikeshare\nbigquery-public-data.austin_311\nbigquery-public-data.austin_crime\nbigquery-public-data.austin_incidents\nbigquery-public-data.austin_waste",
        "question": "What is the highest number of electric bike rides lasting more than 10 minutes taken by subscribers with 'Student Membership' in a single day, excluding rides starting or ending at 'Mobile Station' or 'Repair Shop'?",
        "SQL": "SELECT\n  COUNT(1) AS num_rides\nFROM\n  `bigquery-public-data.austin_bikeshare.bikeshare_trips` \nWHERE \nstart_station_name \n    NOT IN ('Mobile Station', 'Repair Shop')\nAND\nend_station_name \n    NOT IN ('Mobile Station', 'Repair Shop')\nAND \nsubscriber_type = 'Student Membership'\nAND\nbike_type = 'electric'\nAND\nduration_minutes > 10\nGROUP BY \n    EXTRACT(YEAR from start_time), \n    EXTRACT(MONTH from start_time), \n    EXTRACT(DAY from start_time)\nORDER BY num_rides DESC\nLIMIT 1",
        "external_knowledge": null,
        "plan": "1. Filter the dataset to exclude records where the starting or ending locations are listed as either 'Mobile Station' or 'Repair Shop'.\n2. Narrow down the dataset to include only those records where users have students subscription type and the equipment used is electric.\n3. Consider only records where the duration of usage is greater than ten minutes.\n4. Organize the filtered data by grouping it based on the year, month, and day of the start time.\n5. For each grouped set of data (each day), count the total number of records that match the earlier set conditions.\n6. Order the results by the count of records in descending order to identify the date with the highest number of trips meeting all the specified conditions.\n7. Limit the output to only the top result from the sorted list and show the maximum usage in a single day under the defined conditions.\n",
        "special_function": [
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT"
        ]
    },
    {
        "instance_id": "bq282",
        "db": "bigquery-public-data.austin_bikeshare\nbigquery-public-data.austin_311\nbigquery-public-data.austin_crime\nbigquery-public-data.austin_incidents\nbigquery-public-data.austin_waste",
        "question": "Can you tell me the numeric value of the active council district in Austin which has the highest number of bike trips that start and end within the same district, but not at the same station?",
        "SQL": "SELECT \n  district\nFROM (\n  SELECT\n    S.starting_district AS district,\n    T.start_station_id,\n    T.end_station_id\n  FROM\n    `bigquery-public-data.austin_bikeshare.bikeshare_trips` AS T\n  INNER JOIN (\n    SELECT\n      station_id,\n      council_district AS starting_district\n    FROM\n      `bigquery-public-data.austin_bikeshare.bikeshare_stations`\n    WHERE\n      status = \"active\"\n  ) AS S ON T.start_station_id = S.station_id\n  WHERE\n    S.starting_district IN (\n      SELECT council_district\n      FROM `bigquery-public-data.austin_bikeshare.bikeshare_stations`\n      WHERE\n        status = \"active\" AND\n        station_id = SAFE_CAST(T.end_station_id AS INT64)\n    )\n    AND T.start_station_id != SAFE_CAST(T.end_station_id AS INT64)\n) \nGROUP BY district\nORDER BY COUNT(*) DESC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "1. Filter the stations table to keep records whose status is 'active'.\n2. Perform an inner join between the trips table and the filtered active stations.\n3. Select the district from the stations table for end stations that are active and match the end station ID from the trips table.\n4. Include only those trips where the starting district is present in the list of active end station districts.\n5. Exclude trips where the start station ID is the same as the end station ID.\n6. Group the results by the district and count the number of trips originating from each district.\n7. Order the districts by the descending count of trips to find the district with the highest number of trips.\n8. Limit the output to the district with the maximum number of trips.",
        "special_function": [
            "conversion-functions/SAFE_CAST"
        ]
    },
    {
        "instance_id": "bq006",
        "db": "bigquery-public-data.austin_bikeshare\nbigquery-public-data.austin_311\nbigquery-public-data.austin_crime\nbigquery-public-data.austin_incidents\nbigquery-public-data.austin_waste",
        "question": "What is the date with the second highest Z-score for daily counts of 'PUBLIC INTOXICATION' incidents in Austin for the year 2016? List the date in the format of '2016-xx-xx'.",
        "SQL": "WITH incident_stats AS (\n  SELECT \n    COUNT(descript) AS total_pub_intox\n  FROM \n    `bigquery-public-data.austin_incidents.incidents_2016` \n  WHERE \n    descript = 'PUBLIC INTOXICATION' \n  GROUP BY \n    date\n),\naverage_and_stddev AS (\n  SELECT \n    AVG(total_pub_intox) AS avg, \n    STDDEV(total_pub_intox) AS stddev \n  FROM \n    incident_stats\n),\ndaily_z_scores AS (\n  SELECT \n    date, \n    COUNT(descript) AS total_pub_intox, \n    ROUND((COUNT(descript) - a.avg) / a.stddev, 2) AS z_score\n  FROM \n    `bigquery-public-data.austin_incidents.incidents_2016`,\n    (SELECT avg, stddev FROM average_and_stddev) AS a\n  WHERE \n    descript = 'PUBLIC INTOXICATION'\n  GROUP BY \n    date, avg, stddev\n)\n\nSELECT \n  date\nFROM \n  daily_z_scores\nORDER BY \n  z_score DESC\nLIMIT 1\nOFFSET 1",
        "external_knowledge": null,
        "plan": "Analyze daily occurrences of public intoxication incidents in Austin for the year 2016. \nCalculate the total number of incidents per day and compute the Z-score for each day's incidents to identify days with significantly higher or lower incident counts. \nThe output should include the date, total number of incidents, and the Z-score.",
        "special_function": [
            "date-functions/DATE",
            "mathematical-functions/ROUND",
            "statistical-aggregate-functions/STDDEV"
        ]
    },
    {
        "instance_id": "bq283",
        "db": "bigquery-public-data.austin_bikeshare\nbigquery-public-data.austin_311\nbigquery-public-data.austin_crime\nbigquery-public-data.austin_incidents\nbigquery-public-data.austin_waste",
        "question": "Identify the top 15 active stations based on the number of trip starts. For each of these stations, provide the station ID, the total number of trips that start there, the percentage this represents out of all trips from active stations, and the average duration of trips starting from the station.",
        "SQL": "WITH StationStats AS (\n  SELECT\n    BT.start_station_id,\n    COUNT(BT.trip_id) AS total_trips,\n    RANK() OVER (ORDER BY COUNT(BT.trip_id) DESC) AS station_rank,\n    ((COUNT(BT.trip_id) / SUM(COUNT(BT.trip_id)) OVER ()) * 100) AS percentage_of_total_trips,\n    AVG(BT.duration_minutes) AS avg_duration_minutes\n  FROM\n    `bigquery-public-data.austin_bikeshare.bikeshare_trips` BT\n  JOIN\n    `bigquery-public-data.austin_bikeshare.bikeshare_stations` BS\n    ON BT.start_station_id = BS.station_id\n  WHERE\n    BS.status = 'active'\n  GROUP BY\n    BT.start_station_id\n)\n\nSELECT\n  start_station_id,\n  total_trips,\n  percentage_of_total_trips,\n  avg_duration_minutes\nFROM\n  StationStats\nWHERE\n  station_rank <= 15\nORDER BY\n  station_rank ASC;",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Calculate the total number of trips originating from each station.\n2. Assign a rank to each station based on the descending order of total trips, where the station with the highest number of trips has the highest rank.\n3. Calculate the percentage of total trips that each station represents by dividing the number of trips from that station by the total number of trips from all stations.\n4. Select only those stations that rank within the top 15.\n5. Join the data of the top 15 stations with another dataset that contains additional details about each station.\n6. From the joined data, filter out stations to keep only those that are currently active.\n7. From the filtered active top 15 stations, select the station with the highest rank.\n8. Retrieve the percentage of total trips for this station and display the result.\n",
        "special_function": [
            "mathematical-functions/ROUND",
            "numbering-functions/RANK"
        ]
    },
    {
        "instance_id": "bq284",
        "db": "bigquery-public-data.bbc_news",
        "question": "Can you provide a breakdown of the total number of articles into different categories and the percentage of those articles that mention \"education\" within each category from the BBC News?",
        "SQL": "SELECT \n  category,\n  COUNT(*) AS number_total_by_category,  \n  CASE \n    WHEN category = 'tech' THEN \n          (SELECT count(*)\n                FROM `bigquery-public-data.bbc_news.fulltext`\n                WHERE (LOWER(body) LIKE '%education%') AND category = 'tech') * 100 /\n                (SELECT count(*)\n                FROM `bigquery-public-data.bbc_news.fulltext`\n                WHERE category = 'tech')\n    WHEN category = 'sport' THEN \n          (SELECT count(*)\n                FROM `bigquery-public-data.bbc_news.fulltext`\n                WHERE (LOWER(body) LIKE '%education%') AND category = 'sport') * 100 /\n                (SELECT count(*)\n                FROM `bigquery-public-data.bbc_news.fulltext`\n                WHERE category = 'sport')\n    WHEN category = 'business' THEN \n          (SELECT count(*)\n                FROM `bigquery-public-data.bbc_news.fulltext`\n                WHERE (LOWER(body) LIKE '%education%') AND category = 'business') * 100 /\n                (SELECT count(*)\n                FROM `bigquery-public-data.bbc_news.fulltext`\n                WHERE category = 'business')\n    WHEN category = 'politics' THEN \n          (SELECT count(*)\n                FROM `bigquery-public-data.bbc_news.fulltext`\n                WHERE (LOWER(body) LIKE '%education%') AND category = 'politics') * 100 /\n                (SELECT count(*)\n                FROM `bigquery-public-data.bbc_news.fulltext`\n                WHERE category = 'politics')\n    WHEN category = 'entertainment' THEN \n          (SELECT count(*)\n                FROM `bigquery-public-data.bbc_news.fulltext`\n                WHERE (LOWER(body) LIKE '%education%') AND category = 'entertainment') * 100 /\n                (SELECT count(*)\n                FROM `bigquery-public-data.bbc_news.fulltext`\n                WHERE category = 'entertainment')\n  END AS percent_education\nFROM `bigquery-public-data.bbc_news.fulltext`\nGROUP BY\n  category;",
        "external_knowledge": null,
        "plan": "1. Group all records by their category.\n2. For each category group, count the total number of records.\n3. For each category, count the number of records where the body of the text contains the word 'Education'.\n4. For each category, count the total number of records that belong to the current category.\n5. For each category, calculate the percentage of records that mention 'Education' by dividing the count of 'Education' Mentions by the count of total records and then multiply by 100 to get a percentage.\n6. Combine the total count and the calculated percentage for each category into a single output table.\n",
        "special_function": [
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq413",
        "db": "bigquery-public-data.dimensions_ai_covid19",
        "question": "Retrieve the venue titles of publications inserted from 2024 onwards, where the associated grid's city is 'Qianjiang', prioritizing the venue titles from journal first, then proceedings, book, or book series titles.",
        "SQL": "SELECT\n   COALESCE(p.journal.title, p.proceedings_title.preferred, p.book_title.preferred, p.book_series_title.preferred) AS venue,\nFROM\n   `bigquery-public-data.dimensions_ai_covid19.publications` p\nLEFT JOIN\n   UNNEST(research_orgs) AS research_orgs_grids\nLEFT JOIN\n   `bigquery-public-data.dimensions_ai_covid19.grid` grid\nON\n   grid.id=research_orgs_grids\nWHERE\n   EXTRACT(YEAR FROM date_inserted) >= 2021\n   AND\n   grid.address.city = 'Qianjiang'",
        "external_knowledge": null,
        "plan": "1. **Identify Relevant Records**:\n   - Focus on records of publications that were inserted from the year 2024 onwards. This ensures that only recent entries are considered.\n\n2. **Extract Associated Entities**:\n   - For each publication, identify the associated research organizations using a nested structure. This will help in linking the publications with their respective grids.\n\n3. **Join with Additional Data**:\n   - Join the extracted research organizations with another dataset that contains detailed grid information. This step helps in obtaining specific details about the grids, such as their city.\n\n4. **Filter Based on City**:\n   - Apply a filter to select only those grids where the city is 'Qianjiang'. This narrows down the dataset to the required geographic location.\n\n5. **Prioritize and Select Venue Titles**:\n   - For each publication, select the venue title, prioritizing journal titles first. If a journal title is not available, then consider proceedings titles, followed by book titles, and finally book series titles. This ensures that the most relevant venue title is chosen.\n\n6. **Output the Results**:\n   - Output the selected venue titles for the filtered publications. The result will list the prioritized venue titles for publications from the specified city and time frame.",
        "special_function": null
    },
    {
        "instance_id": "bq425",
        "db": "bigquery-public-data.ebi_chembl",
        "question": "List all distinct molecules associated with the company 'SanofiAventis,' along with their trade name and approval date, retaining the most recent approval date for each molecule, using data from ChEMBL Release 23.",
        "SQL": "SELECT *\n  FROM (\n  SELECT\n  molregno,\n  comp.company,\n  prod.trade_name,\n  prod.approval_date,\n  ROW_NUMBER() OVER(PARTITION BY molregno ORDER BY PARSE_DATE('%Y-%m-%d', prod.approval_date) DESC) rn\n  FROM bigquery-public-data.ebi_chembl.compound_records_23 AS cmpd_rec\n  JOIN bigquery-public-data.ebi_chembl.molecule_synonyms_23 AS ms USING (molregno)\n  JOIN bigquery-public-data.ebi_chembl.research_companies_23 AS comp USING (res_stem_id)\n  JOIN bigquery-public-data.ebi_chembl.formulations_23 AS form USING (molregno)\n  JOIN bigquery-public-data.ebi_chembl.products_23 AS prod USING (product_id)\n  ) as subq\n WHERE rn = 1 AND company = 'SanofiAventis'",
        "external_knowledge": null,
        "plan": "1. **Subquery Creation**: Start by creating a subquery that aggregates the necessary data from multiple tables.\n\n2. **Select Relevant Columns**: Select the columns for the molecule identifier, company name, trade name, and approval date.\n\n3. **Filter by Company**: Ensure that only records associated with the specified company are considered.\n\n4. **Order and Partition Data**: Use a window function to partition the data by molecule identifier and order it by approval date in descending order. This will help in identifying the most recent approval date for each molecule.\n\n5. **Rank Records**: Assign a row number to each record within its partition to facilitate the selection of the most recent record.\n\n6. **Filter for Most Recent**: Filter the results of the subquery to retain only the most recent record for each molecule.\n\n7. **Final Selection**: Select all columns from the filtered subquery where the company matches the specified name.",
        "special_function": null
    },
    {
        "instance_id": "bq430",
        "db": "bigquery-public-data.ebi_chembl",
        "question": "Find pairs of different molecules tested in the same assay and standard type, where both have 10\u201315 heavy atoms, fewer than 5 activities in that assay, fewer than 2 duplicate activities, non-null standard values, and pChEMBL values over 10. For each pair, report the maximum heavy atom count, the latest publication date (calculated based on the document's rank within the same journal and year, and map it to a synthetic month and day), the highest document ID, classify the change in standard values as 'increase', 'decrease', or 'no-change' based on their values and relations, and generate UUIDs from their activity IDs and canonical SMILES.",
        "SQL": "select \n  -- *, \n  greatest(heavy_atoms_1, heavy_atoms_2) as heavy_atoms_greatest,\n  greatest(publication_date_1, publication_date_2) as publication_date_greatest,\n  greatest(doc_id_1, doc_id_2) as doc_id_greatest,\n  case \n    when \n      standard_value_1 > standard_value_2 and \n      standard_relation_1 not in ('<', '<<') and \n      standard_relation_2 not in ('>', '>>')\n    then 'decrease'\n    when\n      standard_value_1 < standard_value_2 and \n      standard_relation_1 not in ('>', '>>') and \n      standard_relation_2 not in ('<', '<<') \n    then 'increase'\n    when\n      standard_value_1 = standard_value_2 and \n      standard_relation_1 in ('=', '~') and \n      standard_relation_2 in ('=', '~') \n    then 'no-change'\n    else null\n  end as standard_change,\n  to_hex(md5(to_json_string(struct(activity_id_1, activity_id_2)))) as mmp_delta_uuid,\n  to_hex(md5(to_json_string(struct(canonical_smiles_1, canonical_smiles_2, 5)))) as mmp_search_uuid\nfrom (\n  select \n    act.assay_id,\n    act.standard_type,\n    act.activity_id as activity_id_1,\n    cast(act.standard_value as numeric) as standard_value_1,\n    act.standard_relation as standard_relation_1,\n    cast(act.pchembl_value as numeric) as pchembl_value_1,\n    count(*) over (partition by act.assay_id) as count_activities_1,\n    count(*) over (partition by act.assay_id, act.molregno) as duplicate_activities_1,\n    act.molregno as molregno_1,\n    com.canonical_smiles as canonical_smiles_1,\n    cast(cmp.heavy_atoms as int64) as heavy_atoms_1,\n    cast(d.doc_id as int64) as doc_id_1,\n    date(\n      coalesce(cast(d.year as int64), 1970), \n      coalesce(cast(floor(percent_rank() over (\n        partition by d.journal, d.year order by SAFE_CAST(d.first_page as int64)) * 11) as int64) + 1, 1),\n      coalesce(mod(cast(floor(percent_rank() over (\n        partition by d.journal, d.year order by SAFE_CAST(d.first_page as int64)) * 308) as int64), 28) + 1, 1)) as publication_date_1\n  FROM `bigquery-public-data.ebi_chembl.activities_29` act\n  join `bigquery-public-data.ebi_chembl.compound_structures_29` com using (molregno)\n  join `bigquery-public-data.ebi_chembl.compound_properties_29` cmp using (molregno)\n  left join `bigquery-public-data.ebi_chembl.docs_29` d using (doc_id)\n  where standard_type in (select distinct standard_type from`bigquery-public-data.ebi_chembl.activities_29` where pchembl_value is not null)\n  ) a1\njoin (\n  select \n    act.assay_id,\n    act.standard_type,\n    act.activity_id as activity_id_2,\n    cast(act.standard_value as numeric) as standard_value_2,\n    act.standard_relation as standard_relation_2,\n    cast(act.pchembl_value as numeric) as pchembl_value_2,\n    count(*) over (partition by act.assay_id) as count_activities_2,\n    count(*) over (partition by act.assay_id, act.molregno) as duplicate_activities_2, \n    act.molregno as molregno_2,\n    com.canonical_smiles as canonical_smiles_2, \n    cast(cmp.heavy_atoms as int64) as heavy_atoms_2,\n    cast(d.doc_id as int64) as doc_id_2,\n    date(\n      coalesce(cast(d.year as int64), 1970), \n      coalesce(cast(floor(percent_rank() over (\n        partition by d.journal, d.year order by SAFE_CAST(d.first_page as int64)) * 11) as int64) + 1, 1),\n      coalesce(mod(cast(floor(percent_rank() over (\n        partition by d.journal, d.year order by SAFE_CAST(d.first_page as int64)) * 308) as int64), 28) + 1, 1)) as publication_date_2\n  FROM `bigquery-public-data.ebi_chembl.activities_29` act\n  join `bigquery-public-data.ebi_chembl.compound_structures_29` com using (molregno)\n  join `bigquery-public-data.ebi_chembl.compound_properties_29` cmp using (molregno)\n  left join `bigquery-public-data.ebi_chembl.docs_29` d using (doc_id)\n  where standard_type in (select distinct standard_type from`bigquery-public-data.ebi_chembl.activities_29` where pchembl_value is not null)\n  ) a2 using (assay_id, standard_type)\nwhere \n  a1.molregno_1 != a2.molregno_2 and\n  a1.count_activities_1 < 5 and \n  a2.count_activities_2 < 5 and \n  a1.heavy_atoms_1 between 10 and 15 and\n  a2.heavy_atoms_2 between 10 and 15 and\n  a1.standard_value_1 is not null and \n  a2.standard_value_2 is not null and\n  a1.duplicate_activities_1 < 2 and\n  a2.duplicate_activities_2 < 2 and\n  a1.pchembl_value_1 > 10 and\n  a2.pchembl_value_2 > 10",
        "external_knowledge": "chembl.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq023",
        "db": "bigquery-public-data.census_bureau_acs\nbigquery-public-data.fec\nbigquery-public-data.fdic_banks\nbigquery-public-data.hud_zipcode_crosswalk\nbigquery-public-data.geo_census_tracts",
        "question": "What are the average political donation amounts and median incomes for each census tract identifier in Kings County (Brooklyn), NY, using data from the 2018 ACS and 2020 FEC contributions?",
        "SQL": "with median_income_by_geo as (\n    select\n        geo_id\n        , median_income \n    from `bigquery-public-data.census_bureau_acs.censustract_2018_5yr`\n)\n\n, donations_by_zip as (\n    select\n        SUBSTRING(zip_code, 1, 5) AS zip_code\n        , transaction_amt\n    from `bigquery-public-data.fec.indiv20` \n    where state = 'NY'\n)\n\n, zip_to_geo as (\n    select\n        zip_code\n        , census_tract_geoid \n    from `bigquery-public-data.hud_zipcode_crosswalk.zipcode_to_census_tracts`\n)\n\n, avg_donations_by_geo_id as (\n    select\n        zip_to_geo.census_tract_geoid as geo_id\n        , AVG(donations_by_zip.transaction_amt) as average_donation\n    from donations_by_zip\n    join zip_to_geo\n    on zip_to_geo.zip_code = donations_by_zip.zip_code\n    group by 1\n)\n\n, census_tracts_kings as (\n    select \n        geo_id\n        , tract_ce\n        , tract_geom\n    from `bigquery-public-data.geo_census_tracts.census_tracts_new_york`\n    where county_fips_code = '047'\n    and state_fips_code = '36'\n)\n\nselect \n    census_tracts_kings.tract_ce as census_tract\n    , avg_donations_by_geo_id.average_donation\n    , median_income_by_geo.median_income\nfrom \n    census_tracts_kings\n    LEFT JOIN avg_donations_by_geo_id  \n    ON census_tracts_kings.geo_id = avg_donations_by_geo_id.geo_id\n    LEFT join median_income_by_geo\n    on census_tracts_kings.geo_id = median_income_by_geo.geo_id\norder by census_tract;",
        "external_knowledge": null,
        "plan": "1. Extract Median Income Data:\n    - Retrieve the median income for each geographical area from the census data.\n\n2. Filter Political Donations by State:\n    - Select donation amounts and corresponding ZIP codes for a specific state from political contribution records.\n\n3. Map ZIP Codes to Census Tracts:\n    - Create a mapping between ZIP codes and census tracts using a crosswalk table.\n\n4. Calculate Average Donations per Census Tract:\n    - Aggregate the donation amounts by census tract, using the mapping from ZIP codes to census tracts, to compute the average donation per tract.\n\n5. Identify Census Tracts in Target County:\n    - Select the census tracts that belong to the specific county of interest, filtering by county and state codes.\n\n6. Combine Data:\n    - Merge the census tract information with the computed average donations and the median income data, ensuring that all tracts in the target county are included, even if there are no corresponding donations or income records.\n\n7. Format and Order Results:\n    - Select the relevant fields for the final output, specifically the census tract identifier, average donation amount, and median income. Sort the results by the census tract identifier for orderly presentation.",
        "special_function": null
    },
    {
        "instance_id": "bq094",
        "db": "bigquery-public-data.census_bureau_acs\nbigquery-public-data.fec\nbigquery-public-data.fdic_banks\nbigquery-public-data.hud_zipcode_crosswalk\nbigquery-public-data.geo_census_tracts",
        "question": "Please display committees from 2016 that supported candidates and received small-dollar donations over $0 (under $200 each). For each qualifying committee, list the committee name, number of supported candidates, the candidates' names (in alphabetical order, separated by commas), and total small-dollar donations dollars.",
        "SQL": "SELECT\n  cmte.cmte_nm AS Committee_Name,\n  COUNT(DISTINCT link.linkage_id) AS Number_of_Candidates,\n  STRING_AGG(DISTINCT cand.cand_name ORDER BY cand.cand_name ASC) AS Candidates_Supported,\n  SUM(indiv.transaction_amt) AS Total_Small_Dollar_Donations_in_Millions\nFROM\n  `bigquery-public-data.fec.ccl16` AS link\nINNER JOIN\n  `bigquery-public-data.fec.cn16` AS cand\n  ON cand.cand_id = link.cand_id\nINNER JOIN\n  `bigquery-public-data.fec.cm16` AS cmte\n  ON cmte.cmte_id = link.cmte_id\nLEFT JOIN (\n  SELECT\n    cmte_id,\n    transaction_amt\n  FROM\n    `bigquery-public-data.fec.indiv16`\n  WHERE\n    transaction_amt > 0 AND transaction_amt < 200\n) AS indiv\n  ON cmte.cmte_id = indiv.cmte_id\nGROUP BY\n  cmte.cmte_nm\nHAVING\n  SUM(indiv.transaction_amt)  > 0",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq287",
        "db": "bigquery-public-data.fdic_banks\nbigquery-public-data.fec\nbigquery-public-data.census_bureau_acs\nbigquery-public-data.hud_zipcode_crosswalk\nbigquery-public-data.geo_census_tracts",
        "question": "What is the employment rate (only consider population over 16) in the Utah zip code that has the fewest number of bank locations based on American Community Survey data in 2017?",
        "SQL": "with utah_fips AS\n   (\n        SELECT\n            state_fips_code\n        FROM\n            `bigquery-public-data.census_utility.fips_codes_states`\n        WHERE\n            state_name = \"Utah\"\n    ),\n\nutah_zip AS\n    (\n        SELECT\n            z.zip_code\n        FROM\n            `bigquery-public-data.geo_us_boundaries.zip_codes` z, utah_fips u\n        WHERE\n            z.state_fips_code = u.state_fips_code\n    ),\n\nlocations AS\n    (\n        SELECT\n            COUNT(i.institution_name) AS count_locations,\n            l.zip_code\n        FROM\n            utah_zip z\n        JOIN\n            `bigquery-public-data.fdic_banks.locations` l USING (zip_code)\n        JOIN\n            `bigquery-public-data.fdic_banks.institutions` i USING (fdic_certificate_number)\n        WHERE\n            l.state IS NOT NULL\n            AND l.state_name IS NOT NULL\n            AND l.zip_code = z.zip_code\n        GROUP BY 2\n    ),\n\n    acs_2017 AS \n        (\n            SELECT\n                CAST(geo_id as STRING) AS zip_code,\n                ROUND(SAFE_DIVIDE(employed_pop, pop_16_over), 4) AS rate_employment,\n            FROM\n                `bigquery-public-data.census_bureau_acs.zip_codes_2017_5yr`\n            JOIN\n                utah_zip \n            ON \n                geo_id = zip_code\n        )\n\nSELECT\n    rate_employment\nFROM\n  utah_zip z\nJOIN\n  locations l USING (zip_code)\nJOIN\n  acs_2017 acs USING (zip_code)\nORDER BY\n  l.count_locations ASC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "1. Retrieve the FIPS code for the state of Utah.\n2. Use the Utah FIPS code to filter and obtain a list of ZIP codes associated with Utah from a dataset containing ZIP codes and their corresponding state FIPS codes.\n3. For each ZIP code in Utah, count the number of banking institution locations and ensure that only ZIP codes with valid state and state name information are considered.\n4. Join the banking locations dataset with the banking institutions dataset.\n5. For each ZIP code in Utah, calculate the employment rate using the population over 16 and the employed population.\n6. Combine the datasets containing the Utah ZIP codes, the number of bank locations per ZIP code, and the employment rates.\n7. From the combined dataset, select the employment rate for the ZIP code that has the fewest number of banking institution locations.\n8. Order the results by the number of locations in ascending order.\n9. Limit the result to just one record to find the ZIP code with the lowest number of bank locations and its corresponding employment rate.\n",
        "special_function": [
            "conversion-functions/CAST",
            "json-functions/STRING",
            "mathematical-functions/ROUND",
            "mathematical-functions/SAFE_DIVIDE",
            "timestamp-functions/STRING"
        ]
    },
    {
        "instance_id": "bq432",
        "db": "bigquery-public-data.fda_food\nbigquery-public-data.fdic_banks\nbigquery-public-data.census_utility\nbigquery-public-data.geo_census_blockgroups\nbigquery-public-data.geo_us_boundaries",
        "question": "Could you provide me with the cleansed data of food events in January 2015 as listed in the cleansing documentation?",
        "SQL": "WITH food_events_cleansed AS \n(\n  SELECT \n  report_number\n  ,SPLIT(reactions, ',') AS reactions\n  ,SPLIT(outcomes, ',') AS outcomes\n    ,CASE \n        WHEN REGEXP_INSTR(products_brand_name, '\\\\d(,\\\\d)') > 0 THEN\n          (SPLIT(REPLACE(CONCAT(LEFT(products_brand_name, REGEXP_INSTR(products_brand_name, '\\\\d(,\\\\d)') - 1), ' '\n        , RIGHT(products_brand_name, LENGTH(products_brand_name) - REGEXP_INSTR(products_brand_name, '\\\\d(,\\\\d)') ))\n        , ', ', ' -- ')))\n          ELSE (SPLIT(REPLACE(products_brand_name, ', ', ' -- ')))\n    END AS brand_name\n    ,SPLIT(REPLACE(products_industry_code, ', ', ' -- ')) AS industry_code\n    ,SPLIT(REPLACE(products_role, ', ', ' -- ')) AS role\n    ,SPLIT(REPLACE(products_industry_name, ', ', ' -- ')) AS industry_name\n  ,date_created\n  ,date_started\n  ,consumer_gender\n  ,consumer_age\n  ,consumer_age_unit\n  ,ARRAY_LENGTH(SPLIT(REPLACE(products_industry_code, ', ', ' -- '))) AS industry_code_length\n  ,ARRAY_LENGTH(CASE \n        WHEN REGEXP_INSTR(products_brand_name, '\\\\d(,\\\\d)') > 0 THEN\n          (SPLIT(REPLACE(CONCAT(LEFT(products_brand_name, REGEXP_INSTR(products_brand_name, '\\\\d(,\\\\d)') - 1), ' '\n        , RIGHT(products_brand_name, LENGTH(products_brand_name) - REGEXP_INSTR(products_brand_name, '\\\\d(,\\\\d)') ))\n        , ', ', ' -- ')))\n          ELSE (SPLIT(REPLACE(products_brand_name, ', ', ' -- ')))\n    END) AS brand_name_length\n\n  FROM `bigquery-public-data.fda_food.food_events` \n)\n\nSELECT * FROM food_events_cleansed\nWHERE date_created BETWEEN '2015-01-01' AND '2015-01-31'\nAND date_started BETWEEN '2015-01-01' AND '2015-01-31'\n;",
        "external_knowledge": "Food_Event_Cleansing_Logic.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq285",
        "db": "bigquery-public-data.fdic_banks\nbigquery-public-data.fda_food\nbigquery-public-data.census_utility\nbigquery-public-data.geo_census_blockgroups\nbigquery-public-data.geo_us_boundaries",
        "question": "Could you provide me with the zip code of the location that has the highest number of bank institutions in Florida?",
        "SQL": "with _fips AS\n    (\n        SELECT\n            state_fips_code\n        FROM\n            `bigquery-public-data.census_utility.fips_codes_states`\n        WHERE\n            state_name = \"Florida\"\n    )\n\n    ,_zip AS\n    (\n        SELECT\n            z.zip_code,\n            z.zip_code_geom,\n        FROM\n            `bigquery-public-data.geo_us_boundaries.zip_codes` z, _fips u\n        WHERE\n            z.state_fips_code = u.state_fips_code\n    )\n\n    ,locations AS\n    (\n        SELECT\n            COUNT(i.institution_name) AS count_locations,\n            l.zip_code\n        FROM\n            `bigquery-public-data.fdic_banks.institutions` i\n        JOIN\n            `bigquery-public-data.fdic_banks.locations` l \n        USING (fdic_certificate_number)\n        WHERE\n            l.state IS NOT NULL\n        AND \n            l.state_name IS NOT NULL\n        GROUP BY 2\n    )\n\n    SELECT\n        z.zip_code\n    FROM\n        _zip z\n    JOIN\n        locations l \n    USING (zip_code)\n    GROUP BY\n        z.zip_code\n    ORDER BY\n        SUM(l.count_locations) DESC\n    LIMIT 1;",
        "external_knowledge": null,
        "plan": "1. Retrieve the FIPS code for Florida from a dataset that contains FIPS codes for all states.\n2. Limit the data to zip codes that are only in the Florida state by using FIPS codes to filter the dataset.\n3. From a dataset containing institution locations, perform an aggregation to count the number of locations in each zip code.\n4. Combine the filtered list of zip codes with the aggregated location counts.\n5. Group the data from the join by zip code and sum the counts of locations. \n6. Order the results in descending order based on the sum of location counts.\n7. Limit the result to the top entry, which represents the zip code with the highest number of locations.\n",
        "special_function": null
    },
    {
        "instance_id": "bq288",
        "db": "bigquery-public-data.fdic_banks\nbigquery-public-data.fda_food\nbigquery-public-data.census_utility\nbigquery-public-data.geo_census_blockgroups\nbigquery-public-data.geo_us_boundaries",
        "question": "What is the total number of all banking institutions in the state that has the highest sum of assets from banks established between January 1, 1900, and December 31, 2000, with institution names starting with 'Bank'?",
        "SQL": "WITH state_counts AS (\n        SELECT \n            DISTINCT state_name,\n            COUNT(1) AS count_institutions\n        FROM\n            `bigquery-public-data.fdic_banks.institutions`\n        GROUP BY\n            state_name\n        ),\n\n        detailed_info AS (\n            SELECT\n                DISTINCT state_name AS state,\n                SUM(total_assets) AS sum_assets,\n            FROM\n                `bigquery-public-data.fdic_banks.institutions`\n            WHERE\n                established_date BETWEEN '1945-01-01' AND '2000-12-31'\n            AND \n                (institution_name LIKE 'Bank%')\n            GROUP BY\n                state_name\n        )\nSELECT\n    s.count_institutions AS count_institutions_in_state\nFROM\n    detailed_info d\nJOIN\n    state_counts s \nON \n    d.state = s.state_name\nORDER BY\n    d.sum_assets DESC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "1. Calculate the number of institutions per state.\n2. Gather detailed information about the total assets for each state.\n3. Join the results above based on the matching state names.\n4. Sort the results by the total assets in descending order to bring the state with the highest sum of assets to the top.\n5. Limit the output to only the top record, which corresponds to the state with the highest total assets among the filtered institutions.\n6. Display the count of institutions for the state with the highest total assets.\n",
        "special_function": null
    },
    {
        "instance_id": "bq412",
        "db": "spider2-public-data.google_ads_transparency_center",
        "question": "Please provide the page URLs, first shown time, last shown time, removal reason, violation category, and lower and upper bound shown times for the most recent five closed ads in the Croatia region which had shown higher than 10,000 and lower than 25,000, and used at least one audience criterion such as demographics, geographic location, contextual signals, customer lists, or interest topics. The region code of Croatia is HR.",
        "SQL": "SELECT\n    creative_page_url,\n    PARSE_TIMESTAMP('%Y-%m-%d', first_shown) AS first_shown,\n    PARSE_TIMESTAMP('%Y-%m-%d', last_shown) AS last_shown,\n    disapproval[0].removal_reason AS removal_reason,\n    disapproval[0].violation_category AS violation_category,\n    times_shown_lower_bound AS times_shown_lower,\n    times_shown_upper_bound AS times_shown_upper\nFROM\n    `spider2-public-data.google_ads_transparency_center.removed_creative_stats`,\n    UNNEST(region_stats)\nWHERE\n    region_code = 'HR' AND\n    times_shown_availability_date IS NULL AND\n    times_shown_lower_bound > 10000 AND\n    times_shown_upper_bound < 25000 AND\n    (audience_selection_approach_info.demographic_info != 'CRITERIA_UNUSED' OR\n     audience_selection_approach_info.geo_location != 'CRITERIA_UNUSED' OR\n     audience_selection_approach_info.contextual_signals != 'CRITERIA_UNUSED' OR\n     audience_selection_approach_info.customer_lists != 'CRITERIA_UNUSED' OR\n     audience_selection_approach_info.topics_of_interest != 'CRITERIA_UNUSED')\nORDER BY\n    last_shown DESC\nLIMIT 5",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq423",
        "db": "spider2-public-data.google_ads_transparency_center",
        "question": "Please provide the page URL of the image-type ad published by advertisers from region CY, on the topic of health, that has the highest upper bound of times shown. This ad should have demographic information, geo location, contextual signals, customer lists, and topics of interest. The ad should be shown in region Croatia (region code HR) and must have been displayed between January 1, 2023, and January 1, 2024. Additionally, the advertiser must be verified.",
        "SQL": "SELECT\n    creative_page_url,\nFROM `spider2-public-data.google_ads_transparency_center.creative_stats`,\n    UNNEST(region_stats)\n    \nWHERE\n    region_code = 'HR' AND\n    times_shown_availability_date IS NULL AND\n    audience_selection_approach_info.demographic_info != 'CRITERIA_UNUSED' AND\n    audience_selection_approach_info.geo_location != 'CRITERIA_UNUSED' AND\n    audience_selection_approach_info.contextual_signals != 'CRITERIA_UNUSED' AND\n    audience_selection_approach_info.customer_lists != 'CRITERIA_UNUSED' AND\n    audience_selection_approach_info.topics_of_interest != 'CRITERIA_UNUSED' AND\n    \n    advertiser_verification_status = 'VERIFIED' AND\n    advertiser_location = 'CY' AND\n    ad_format_type = 'IMAGE' AND\n    topic = 'Health' AND\n    \n    PARSE_TIMESTAMP('%Y-%m-%d', first_shown) > TIMESTAMP('2023-01-01') AND\n    PARSE_TIMESTAMP('%Y-%m-%d', last_shown) < TIMESTAMP('2024-01-01')\nORDER BY times_shown_upper_bound DESC\nLIMIT 1",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq070",
        "db": "spider2-public-data.idc_v17",
        "question": "Could you construct a structured clean dataset from `dicom_all` for me? It should retrieve digital slide microscopy (SM) images from the TCGA-LUAD and TCGA-LUSC datasets and meet the requirements in `dicom_dataset_selection.md`. The target labels are tissue type and cancer subtype.",
        "SQL": "WITH\n  sm_images AS (\n    SELECT\n      SeriesInstanceUID AS digital_slide_id, \n      StudyInstanceUID AS case_id,\n      ContainerIdentifier AS physical_slide_id,\n      PatientID AS patient_id,\n      TotalPixelMatrixColumns AS width, \n      TotalPixelMatrixRows AS height,\n      collection_id,\n      crdc_instance_uuid,\n      gcs_url, \n      CAST(SharedFunctionalGroupsSequence[SAFE_OFFSET(0)].\n              PixelMeasuresSequence[SAFE_OFFSET(0)].\n              PixelSpacing[SAFE_OFFSET(0)] AS FLOAT64) AS pixel_spacing, \n      CASE TransferSyntaxUID\n          WHEN '1.2.840.10008.1.2.4.50' THEN 'jpeg'\n          WHEN '1.2.840.10008.1.2.4.91' THEN 'jpeg2000'\n          ELSE 'other'\n      END AS compression,\n    FROM\n      spider2-public-data.idc_v17.dicom_all\n    WHERE\n      Modality = 'SM' AND ImageType[OFFSET(2)] = 'VOLUME'\n  ),\n\n  tissue_types AS (\n    SELECT DISTINCT *\n    FROM (\n      SELECT\n        SeriesInstanceUID AS digital_slide_id,\n        CASE steps_unnested2.CodeValue\n            WHEN '17621005' THEN 'normal' -- meaning: 'Normal' (i.e., non neoplastic)\n            WHEN '86049000' THEN 'tumor' -- meaning: 'Neoplasm, Primary'\n            ELSE 'other' -- meaning: 'Neoplasm, Metastatic'\n        END AS tissue_type\n      FROM\n        spider2-public-data.idc_v17.dicom_all\n        CROSS JOIN\n          UNNEST (SpecimenDescriptionSequence[SAFE_OFFSET(0)].PrimaryAnatomicStructureSequence) AS steps_unnested1\n        CROSS JOIN\n          UNNEST (steps_unnested1.PrimaryAnatomicStructureModifierSequence) AS steps_unnested2\n    )\n  ),\n\n  specimen_preparation_sequence_items AS (\n    SELECT DISTINCT *\n    FROM (\n      SELECT\n        SeriesInstanceUID AS digital_slide_id,\n        steps_unnested2.ConceptNameCodeSequence[SAFE_OFFSET(0)].CodeMeaning AS item_name,\n        steps_unnested2.ConceptCodeSequence[SAFE_OFFSET(0)].CodeMeaning AS item_value\n      FROM\n        spider2-public-data.idc_v17.dicom_all\n        CROSS JOIN\n          UNNEST (SpecimenDescriptionSequence[SAFE_OFFSET(0)].SpecimenPreparationSequence) AS steps_unnested1\n        CROSS JOIN\n          UNNEST (steps_unnested1.SpecimenPreparationStepContentItemSequence) AS steps_unnested2\n    )\n  )\n\n\nSELECT\n  a.*,\n  b.tissue_type,\n  (REPLACE (REPLACE(a.collection_id, 'tcga_luad', 'luad'), 'tcga_lusc', 'lscc')) AS cancer_subtype\nFROM \n  sm_images AS a\n  JOIN tissue_types AS b ON b.digital_slide_id = a.digital_slide_id\n  JOIN specimen_preparation_sequence_items AS c ON c.digital_slide_id = a.digital_slide_id \nWHERE\n  (a.collection_id = 'tcga_luad' OR a.collection_id = 'tcga_lusc')\n  AND a.compression != 'other'\n  AND (b.tissue_type = 'normal' OR b.tissue_type = 'tumor')\n  AND (c.item_name = 'Embedding medium' AND c.item_value = 'Tissue freezing medium')\nORDER BY crdc_instance_uuid",
        "external_knowledge": "dicom_dataset_selection.md",
        "plan": "1. **Filter and Select Relevant Metadata**:\n   - Create a subquery to filter and select only the digital slide microscopy images.\n   - Ensure images belong to specified collections and have the required compression types.\n   - Extract and rename relevant metadata attributes for these images.\n\n2. **Identify Tissue Types**:\n   - Create a subquery to identify and classify the tissue type of each slide as either 'normal' or 'tumor' based on specific codes.\n   - Map these codes to human-readable labels.\n\n3. **Check Preparation Method**:\n   - Create a subquery to check the preparation method of the slides.\n   - Ensure the embedding medium used is 'Tissue freezing medium'.\n\n4. **Combine and Filter Data**:\n   - Join the subqueries to combine the metadata, tissue type, and preparation method information.\n   - Filter the combined results to include only the images meeting all specified criteria.\n\n5. **Sort and Label Data**:\n   - Sort the final results by a unique identifier to ensure a consistent order.\n   - Map collection identifiers to cancer subtypes for labeling purposes.",
        "special_function": [
            "conversion-functions/CAST",
            "string-functions/REPLACE",
            "conditional-functions/CASE_EXPR",
            "other-functions/UNNEST",
            "other-functions/ARRAY_SUBSCRIPT",
            "other-functions/STRUCT_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "bq320",
        "db": "spider2-public-data.idc_v17",
        "question": "What is the total count of StudyInstanceUIDs that have a segmented property type of '15825003' and belong to the 'Community' or 'nsclc_radiomics' collections?",
        "SQL": "SELECT\n  COUNT(*) AS total_count\nFROM\n  `spider2-public-data.idc_v17.dicom_pivot` dicom_pivot\nWHERE\n  StudyInstanceUID IN (\n    SELECT\n      StudyInstanceUID\n    FROM\n      `spider2-public-data.idc_v17.dicom_pivot` dicom_pivot\n    WHERE\n      StudyInstanceUID IN (\n        SELECT\n          StudyInstanceUID\n        FROM\n          `spider2-public-data.idc_v17.dicom_pivot` dicom_pivot\n        WHERE\n          (\n            LOWER(\n              dicom_pivot.SegmentedPropertyTypeCodeSequence\n            ) LIKE LOWER('15825003')\n          )\n        GROUP BY\n          StudyInstanceUID\n        INTERSECT DISTINCT\n        SELECT\n          StudyInstanceUID\n        FROM\n          `spider2-public-data.idc_v17.dicom_pivot` dicom_pivot\n        WHERE\n          (\n            dicom_pivot.collection_id IN ('Community', 'nsclc_radiomics')\n          )\n        GROUP BY\n          StudyInstanceUID\n      )\n    GROUP BY\n      StudyInstanceUID\n  );",
        "external_knowledge": null,
        "plan": "1. **Identify the Main Query Objective:**\n   - The main goal is to count unique instances based on specific criteria involving properties and collections.\n\n2. **Specify the Conditions for Segmented Property Type:**\n   - Check for instances where a particular property type matches the given code, ignoring case sensitivity.\n\n3. **Specify the Conditions for Collection Membership:**\n   - Check for instances that belong to either of the specified collections.\n\n4. **Combine the Conditions Using Nested Queries:**\n   - Use nested queries to ensure that only instances meeting both conditions are considered.\n     - First, create a subquery to filter instances by the segmented property type.\n     - Second, create another subquery to filter instances by the collection membership.\n     - Use an intersection operation to find instances that satisfy both subquery conditions.\n\n5. **Aggregate and Count the Results:**\n   - Count the total number of unique instances from the combined results of the nested subqueries.\n\n6. **Return the Total Count:**\n   - The final output is the count of instances meeting all specified conditions.",
        "special_function": [
            "string-functions/LOWER"
        ]
    },
    {
        "instance_id": "bq321",
        "db": "spider2-public-data.idc_v17",
        "question": "How many unique StudyInstanceUIDs are there from the DWI, T2 Weighted Axial, Apparent Diffusion Coefficient series, and T2 Weighted Axial Segmentations in the 'qin_prostate_repeatability' collection?",
        "SQL": "WITH relevant_series AS (\n  SELECT \n    DISTINCT StudyInstanceUID\n  FROM \n    `spider2-public-data.idc_v17.dicom_all`\n  WHERE \n    collection_id = 'qin_prostate_repeatability'\n    AND \n         SeriesDescription IN (\n            'DWI',\n            'T2 Weighted Axial',\n            'Apparent Diffusion Coefficient',\n            'T2 Weighted Axial Segmentations',\n            'Apparent Diffusion Coefficient Segmentations'\n    )    \n),\nt2_seg_lesion_series AS (\n  SELECT \n    DISTINCT StudyInstanceUID\n  FROM \n    `spider2-public-data.idc_v17.dicom_all`\n  CROSS JOIN UNNEST(SegmentSequence) AS segSeq\n  WHERE \n    collection_id = 'qin_prostate_repeatability'\n    AND SeriesDescription = 'T2 Weighted Axial Segmentations'\n)\n\nSELECT \n    COUNT(DISTINCT StudyInstanceUID) AS total_count\nFROM (\n  SELECT \n    StudyInstanceUID \n  FROM relevant_series\n  UNION ALL\n  SELECT \n    StudyInstanceUID\n  FROM t2_seg_lesion_series\n);",
        "external_knowledge": null,
        "plan": "1. **Identify Relevant Records**:\n   - Create a temporary dataset that includes unique identifiers from records that match specific criteria within a given collection.\n   - Filter these records based on a list of specified descriptions.\n\n2. **Additional Filtering with Nested Data**:\n   - Create another temporary dataset that includes unique identifiers from records that match a different specific description within the same collection.\n   - Additionally, this step involves expanding nested data fields to match the criteria.\n\n3. **Combine Results**:\n   - Combine the unique identifiers from both temporary datasets into a single dataset.\n   - Use a union operation to ensure all relevant records are included.\n\n4. **Count Unique Identifiers**:\n   - Count the number of unique identifiers in the combined dataset to determine the total number of unique instances that meet the criteria.",
        "special_function": [
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq323",
        "db": "spider2-public-data.idc_v17",
        "question": "What is the combined overall average from Repetition Time, Echo Time, and Slice Thickness for MRI sequences labeled as t2w_prostateX (when the series description contains t2_tse_tra) and adc_prostateX (when the series description contains ADC) within the prostatex collection, ensuring the modality is MR?",
        "SQL": "WITH temp_query AS (\n  SELECT DISTINCT\n    SeriesInstanceUID,\n    StudyInstanceUID,\n    PatientID,\n    RepetitionTime,\n    EchoTime,\n    SliceThickness,\n    MagneticFieldStrength,\n    PixelSpacing[SAFE_OFFSET(0)] AS PixelSpacing,\n    CASE\n      WHEN SeriesDescription LIKE '%t2_tse_tra%' AND collection_id = 'prostatex' AND Modality = 'MR' THEN 't2w_prostateX' \n      WHEN SeriesDescription LIKE '%ADC%' AND collection_id = 'prostatex' AND Modality = 'MR' THEN 'adc_prostateX'\n      ELSE CONCAT(SeriesDescription, collection_id, Modality)\n    END AS collection_sequence_id\n  FROM \n    `spider2-public-data.idc_v17.dicom_all`\n  WHERE \n    collection_id = 'prostatex'\n),\n\naverages AS (\n  SELECT \n    AVG(CAST(RepetitionTime AS FLOAT64)) AS avg_RepetitionTime,\n    AVG(CAST(EchoTime AS FLOAT64)) AS avg_EchoTime,\n    AVG(CAST(SliceThickness AS FLOAT64)) AS avg_SliceThickness\n  FROM \n    temp_query\n  WHERE \n    collection_sequence_id IN ('t2w_prostateX', 'adc_prostateX')\n  GROUP BY collection_sequence_id\n)\n\nSELECT \n  SUM(avg_RepetitionTime + avg_EchoTime + avg_SliceThickness) AS total_avg_time\nFROM \n  averages;",
        "external_knowledge": null,
        "plan": "1. **Create a Temporary Dataset**:\n   - Define a temporary dataset that will store unique sequence identifiers and relevant attributes.\n   - Filter the dataset to include only sequences from a specified collection and with a specified modality.\n   - For sequences that match certain descriptions, label them with predefined identifiers. Otherwise, concatenate certain attributes to form a unique identifier.\n\n2. **Filter and Label Sequences**:\n   - Select distinct records and relevant attributes from the dataset.\n   - Apply conditions to label sequences appropriately based on their descriptions and ensure they belong to the specified collection and modality.\n\n3. **Calculate Averages**:\n   - From the temporary dataset, calculate the average values for three specific attributes.\n   - Ensure the calculations are performed only for sequences that have the predefined labels.\n\n4. **Sum Averages**:\n   - Sum the calculated average values of the three attributes.\n   - Output the total sum as the final result.\n\nBy following these steps, the query ensures that only the relevant sequences are considered, and the requested averages are computed and summed correctly.",
        "special_function": [
            "conversion-functions/CAST",
            "string-functions/CONCAT",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE",
            "other-functions/ARRAY_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "bq417",
        "db": "spider2-public-data.idc_v17",
        "question": "Please provide identification and descriptive data, including the storage details and size metrics of medical images for male patients over 60 years old, examined in the mediastinum, after September 1, 2014.",
        "SQL": "SELECT\n  ANY_VALUE(collection_id) AS collection_id,\n  ANY_VALUE(PatientID) AS PatientID,\n  SeriesInstanceUID,\n  ANY_VALUE(StudyInstanceUID) AS StudyInstanceUID,\n  ANY_VALUE(source_DOI) AS source_DOI,\n  ANY_VALUE(PatientAge) AS PatientAge,\n  ANY_VALUE(PatientSex) AS PatientSex,\n  ANY_VALUE(StudyDate) AS StudyDate,\n  ANY_VALUE(StudyDescription) AS StudyDescription,\n  ANY_VALUE(dicom_curated.BodyPartExamined) AS BodyPartExamined,\n  ANY_VALUE(Modality) AS Modality,\n  ANY_VALUE(Manufacturer) AS Manufacturer,\n  ANY_VALUE(ManufacturerModelName) AS ManufacturerModelName,\n  ANY_VALUE(SAFE_CAST(SeriesDate AS STRING)) AS SeriesDate,\n  ANY_VALUE(SeriesDescription) AS SeriesDescription,\n  ANY_VALUE(SeriesNumber) AS SeriesNumber,\n  COUNT(dicom_all.SOPInstanceUID) AS instanceCount,\n  ANY_VALUE(license_short_name) as license_short_name,\n  ANY_VALUE(CONCAT(\"s3://\", SPLIT(aws_url,\"/\")[SAFE_OFFSET(2)], \"/\", crdc_series_uuid, \"/*\")) AS series_aws_url,\n  ROUND(SUM(SAFE_CAST(instance_size AS float64))/1000000, 2) AS series_size_MB,\nFROM\n  spider2-public-data.idc_v17.dicom_all AS dicom_all\nJOIN\n  spider2-public-data.idc_v17.dicom_metadata_curated AS dicom_curated\nON\n  dicom_all.SOPInstanceUID = dicom_curated.SOPInstanceUID\nWHERE\n  CAST(REGEXP_EXTRACT(PatientAge, r'\\d+') AS INT64) = 18\n  AND PatientSex = 'M'\n  AND SAFE_CAST(StudyDate AS DATE) > DATE('2014-09-01')\n  AND dicom_curated.BodyPartExamined = 'MEDIASTINUM'\nGROUP BY\n  SeriesInstanceUID",
        "external_knowledge": "IDC_data_model.md",
        "plan": "1. **Select Relevant Patient Information**:\n   - Extract the patient ID, series instance identifier, study instance identifier, study description, and modality for patients meeting the specified criteria.\n\n2. **Count Number of Instances**:\n   - Count the total number of medical image instances for each series.\n\n3. **Construct Storage Path**:\n   - Build the storage path for each series using a predefined format that includes the cloud storage location and series identifier.\n\n4. **Calculate Total Size**:\n   - Calculate the total size of the medical image instances for each series, converting the size into megabytes and rounding to two decimal places.\n\n5. **Join Data Sources**:\n   - Merge information from two different data sources based on a common identifier to ensure all necessary information is included.\n\n6. **Filter by Patient Age and Sex**:\n   - Restrict the results to patients who are exactly 18 years old and male.\n\n7. **Filter by Examination Date**:\n   - Include only those examinations conducted after a specified date.\n\n8. **Filter by License**:\n   - Ensure the data includes only those instances with a specified licensing agreement.\n\n9. **Filter by Body Part Examined**:\n   - Limit the results to examinations of a specific body part.\n\n10. **Group Results**:\n    - Group the final results by series instance identifier to consolidate data appropriately.",
        "special_function": null
    },
    {
        "instance_id": "bq455",
        "db": "spider2-public-data.idc_v17",
        "question": "Find the top 5 CT scan series ID, including their series number, patient ID, and series size (in MiB), where the series are not classified as 'LOCALIZER' or have the specific JPEG compressed transfer syntaxes '1.2.840.10008.1.2.4.70' or '1.2.840.10008.1.2.4.51'. The series must have consistent slice intervals, exposure levels, image orientation, pixel spacing, image positions, and pixel dimensions. Additionally, the z-axis of the image orientation must align with the expected plane (dot product between 0.99 and 1.01).",
        "SQL": "WITH\n  -- Create a common table expression (CTE) named localizerAndJpegCompressedSeries\n  localizerAndJpegCompressedSeries AS (\n  Select SeriesInstanceUID  from \n  `spider2-public-data.idc_v17.dicom_all` bid, bid.ImageType image_type \n   where \n   image_type='LOCALIZER' OR  \n   TransferSyntaxUID  IN ( '1.2.840.10008.1.2.4.70','1.2.840.10008.1.2.4.51')\n  ),\n  -- Create a common table expression (CTE) named nonLocalizerRawData \n  nonLocalizerRawData AS (\n    SELECT\n      SeriesInstanceUID,\n      StudyInstanceUID,\n      PatientID,\n      SOPInstanceUID,\n      SliceThickness,\n      ImageType,\n      TransferSyntaxUID,\n      SeriesNumber,\n      aws_bucket,\n      crdc_series_uuid,\n      SAFE_CAST(Exposure AS FLOAT64) Exposure,\n      SAFE_CAST(ipp AS FLOAT64) AS zImagePosition,\n      CONCAT(ipp2, '/', ipp3) AS xyImagePosition,\n      LEAD(SAFE_CAST(ipp AS FLOAT64)) OVER (PARTITION BY SeriesInstanceUID ORDER BY SAFE_CAST(ipp AS FLOAT64)) - SAFE_CAST(ipp AS FLOAT64) AS slice_interval,\n      ARRAY_TO_STRING(ImageOrientationPatient, '/') AS iop,\n      (\n        SELECT ARRAY_AGG(SAFE_CAST(part AS FLOAT64))\n        FROM UNNEST(ImageOrientationPatient) part WITH OFFSET index\n        WHERE index BETWEEN 0 AND 2\n      ) AS x_vector,\n      (\n        SELECT ARRAY_AGG(SAFE_CAST(part AS FLOAT64))\n        FROM UNNEST(ImageOrientationPatient) part WITH OFFSET index\n        WHERE index BETWEEN 3 AND 5\n      ) AS y_vector,\n      ARRAY_TO_STRING(PixelSpacing, '/') AS pixelSpacing,\n      `Rows` AS pixelRows,\n      `Columns` AS pixelColumns,\n      instance_size AS instanceSize,\n    FROM\n      `spider2-public-data.idc_v17.dicom_all` bid\n    LEFT JOIN\n      UNNEST(bid.ImagePositionPatient) ipp WITH OFFSET AS axes\n    LEFT JOIN\n      UNNEST(bid.ImagePositionPatient) ipp2 WITH OFFSET AS axis1\n    LEFT JOIN\n      UNNEST(bid.ImagePositionPatient) ipp3 WITH OFFSET AS axis2\n    WHERE\n      collection_id != 'nlst' AND Modality = 'CT' AND axes = 2 AND axis1 = 0 AND axis2 = 1 \n      AND SeriesInstanceUID NOT IN (Select SeriesInstanceUID from localizerAndJpegCompressedSeries)\n  ),\ncrossProduct AS (\n  SELECT\n    SOPInstanceUID,\n    SeriesInstanceUID,\n    (SELECT AS STRUCT\n      (x_vector[OFFSET(1)] * y_vector[OFFSET(2)] - x_vector[OFFSET(2)] * y_vector[OFFSET(1)]) AS x,\n      (x_vector[OFFSET(2)] * y_vector[OFFSET(0)] - x_vector[OFFSET(0)] * y_vector[OFFSET(2)]) AS y,\n      (x_vector[OFFSET(0)] * y_vector[OFFSET(1)] - x_vector[OFFSET(1)] * y_vector[OFFSET(0)]) AS z\n    ) AS xyCrossProduct\n  FROM nonLocalizerRawData\n),\ndotProduct AS (\n  SELECT\n    SOPInstanceUID,\n    SeriesInstanceUID,\n    xyCrossProduct,\n    (\n      SELECT SUM(element1 * element2)\n      FROM UNNEST([xyCrossProduct.x, xyCrossProduct.y, xyCrossProduct.z]) element1 WITH OFFSET pos\n      JOIN UNNEST([0, 0, 1]) element2 WITH OFFSET pos\n      USING(pos)\n    ) AS xyDotProduct\n  FROM crossProduct\n),\ngeometryChecks AS (\n  SELECT\n    SeriesInstanceUID,\n    seriesNumber,\n    aws_bucket,\n    crdc_series_uuid,\n    StudyInstanceUID,\n    PatientID,\n    ARRAY_AGG(DISTINCT(slice_interval) IGNORE NULLS) AS sliceIntervalDifferences,\n    ARRAY_AGG(DISTINCT(Exposure) IGNORE NULLS) AS distinctExposures,\n    COUNT(DISTINCT iop) AS iopCount,\n    COUNT(DISTINCT pixelSpacing) AS pixelSpacingCount,\n    COUNT(DISTINCT zImagePosition) AS positionCount,\n    COUNT(DISTINCT xyImagePosition) AS xyPositionCount,\n    COUNT(DISTINCT SOPInstanceUID) AS sopInstanceCount,\n    COUNT(DISTINCT SliceThickness) AS sliceThicknessCount,\n    COUNT(DISTINCT Exposure) AS exposureCount,\n    COUNT(DISTINCT pixelRows) AS pixelRowCount,\n    COUNT(DISTINCT pixelColumns) AS pixelColumnCount,\n    MAX(xyDotProduct) AS dotProduct,\n    SUM(instanceSize) / 1024 / 1024 AS seriesSizeInMiB\n  FROM\n    nonLocalizerRawData\n  JOIN dotProduct USING (SeriesInstanceUID, SOPInstanceUID)\n  GROUP BY\n    SeriesInstanceUID, \n    seriesNumber,\n    aws_bucket,\n    crdc_series_uuid,\n    StudyInstanceUID,\n    PatientID\n  HAVING\n    iopCount = 1 \n    AND pixelSpacingCount = 1  \n    AND sopInstanceCount = positionCount \n    AND xyPositionCount = 1\n    AND pixelColumnCount = 1 \n    AND pixelRowCount = 1 \n    AND ABS(dotProduct) BETWEEN 0.99 AND 1.01\n)\nSELECT\n  SeriesInstanceUID,\n  seriesNumber,\n  PatientID,\n  seriesSizeInMiB\nFROM\n  geometryChecks\nORDER BY\n  seriesSizeInMiB DESC\nLIMIT 5;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq456",
        "db": "spider2-public-data.idc_v17",
        "question": "Can you retrieve the PatientID, StudyInstanceUID, StudyDate, and FindingSite for each patient, along with the maximum values for Elongation, Flatness, Least Axis in 3D Length, Major Axis in 3D Length, Maximum 3D Diameter of a Mesh, Minor Axis in 3D Length, Sphericity, Surface Area of Mesh, Surface to Volume Ratio, Volume from Voxel Summation, and Volume of Mesh, in 2001?",
        "SQL": "SELECT\n  da.PatientID AS PatientID,\n  StudyInstanceUID,\n  StudyDate,\n  findingSite.CodeMeaning AS FindingSite,\n  MAX(CASE\n      WHEN Quantity.CodeMeaning = 'Elongation' THEN Value\n  END) AS Elongation,\n  MAX(CASE\n      WHEN Quantity.CodeMeaning = 'Flatness' THEN Value\n  END) AS Flatness,\n  MAX(CASE\n      WHEN Quantity.CodeMeaning = 'Least Axis in 3D Length' THEN Value\n  END) AS Least_Axis_in_3D_Length,\n  MAX(CASE\n      WHEN Quantity.CodeMeaning = 'Major Axis in 3D Length' THEN Value\n  END) AS Major_Axis_in_3D_Length,\n  MAX(CASE\n      WHEN Quantity.CodeMeaning = 'Maximum 3D Diameter of a Mesh' THEN Value\n  END) AS Maximum_3D_Diameter_of_a_Mesh,\n  MAX(CASE\n      WHEN Quantity.CodeMeaning = 'Minor Axis in 3D Length' THEN Value\n  END) AS Minor_Axis_in_3D_Length,\n  MAX(CASE\n      WHEN Quantity.CodeMeaning = 'Sphericity' THEN Value\n  END) AS Sphericity,\n  MAX(CASE\n      WHEN Quantity.CodeMeaning = 'Surface Area of Mesh' THEN Value\n  END) AS Surface_Area_of_Mesh,\n  MAX(CASE\n      WHEN Quantity.CodeMeaning = 'Surface to Volume Ratio' THEN Value\n  END) AS Surface_to_Volume_Ratio,\n  MAX(CASE\n      WHEN Quantity.CodeMeaning = 'Volume from Voxel Summation' THEN Value\n  END) AS Volume_from_Voxel_Summation,\n  MAX(CASE\n      WHEN Quantity.CodeMeaning = 'Volume of Mesh' THEN Value\n  END) AS Volume_of_Mesh\nFROM\n  `spider2-public-data.idc_v17.quantitative_measurements` qm\nJOIN\n  `spider2-public-data.idc_v17.dicom_all` da\nON\n  qm.segmentationInstanceUID = da.SOPInstanceUID\nWHERE\n  StudyDate BETWEEN '2001-01-01' AND '2001-12-31'\nGROUP BY\n  1, 2, 3, 4;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq324",
        "db": "spider2-public-data.idc_v17",
        "question": "What is the total number of frames for whole slide microscopy images categorized under the 'TCGA-BRCA' collection that contain eosin staining during specimen preparation steps?",
        "SQL": "WITH specimen_preparation_sequence_items AS (\n  SELECT DISTINCT\n    SeriesInstanceUID AS digital_slide_id,\n    steps_unnested2.ConceptNameCodeSequence[SAFE_OFFSET(0)].CodeMeaning AS item_name,\n    steps_unnested2.ConceptCodeSequence[SAFE_OFFSET(0)].CodeMeaning AS item_value\n  FROM\n    `spider2-public-data.idc_v17.dicom_all`\n  CROSS JOIN\n    UNNEST(SpecimenDescriptionSequence[SAFE_OFFSET(0)].SpecimenPreparationSequence) AS steps_unnested1\n  CROSS JOIN\n    UNNEST(steps_unnested1.SpecimenPreparationStepContentItemSequence) AS steps_unnested2\n),\n\ngrouped_by_study AS (\n  SELECT\n    dicom.StudyInstanceUID AS case_id,\n    ANY_VALUE(dicom.ContainerIdentifier) AS physical_slide_id\n  FROM\n    `spider2-public-data.idc_v17.dicom_all` AS dicom\n  WHERE\n    dicom.Modality = 'SM' AND\n    EXISTS (\n      SELECT 1\n      FROM specimen_preparation_sequence_items AS specimen\n      WHERE dicom.SeriesInstanceUID = specimen.digital_slide_id\n        AND specimen.item_value LIKE '%eosin%'\n    )\n  GROUP BY\n    dicom.StudyInstanceUID\n)\n\nSELECT\n  SUM(CAST(dicom.NumberOfFrames AS INT64)) AS total_frames\nFROM\n  grouped_by_study AS study\nJOIN\n  `spider2-public-data.idc_v17.dicom_all` AS dicom\nON\n  dicom.StudyInstanceUID = study.case_id\n  AND dicom.ContainerIdentifier = study.physical_slide_id\nLEFT JOIN\n  specimen_preparation_sequence_items AS specimen\nON\n  dicom.SeriesInstanceUID = specimen.digital_slide_id\nWHERE\n  dicom.Modality = 'SM' AND\n  dicom.collection_name = 'TCGA-BRCA';",
        "external_knowledge": null,
        "plan": "1. **Extract Specific Preparation Steps:**\n   - Create a temporary subset of data that focuses on unique identifiers for digital slides and their associated preparation steps, specifically targeting the names and values of these steps.\n\n2. **Filter for Eosin Staining:**\n   - From the subset, filter out digital slides that include any preparation step mentioning 'eosin' in their description.\n\n3. **Identify Relevant Studies:**\n   - Group data by study instances to gather all unique identifiers for studies that contain digital slides with the 'eosin' staining step, ensuring each study has a corresponding physical slide identifier.\n\n4. **Join and Filter Data:**\n   - Combine the grouped study data with the main dataset to retrieve all relevant slides that match the study and physical slide identifiers. Ensure these slides are categorized under a specific modality.\n\n5. **Calculate Total Frames:**\n   - Sum the number of frames for all slides that meet the conditions of being part of the specified collection and containing the specific modality, resulting in the total frame count.",
        "special_function": null
    },
    {
        "instance_id": "bq418",
        "db": "isb-cgc-bq.targetome_versioned\nisb-cgc-bq.reactome_versioned",
        "question": "What are the counts of targets and non-targets within and outside the top 3 pathways with the highest chi-squared statistic for target species 'homo sapiens' associated with 'sorafenib' only? The targets should meet specific assay conditions (median assay value \u2264 100, with assay values below and above also \u2264 100 or NULL if not applicable), and only pathways with TAS evidence at the lowest level should be considered.",
        "SQL": "WITH\n    target_list_query AS (\n      SELECT DISTINCT inter.target_uniprotID\n      FROM `isb-cgc-bq.targetome_versioned.interactions_v1` AS inter\n      INNER JOIN `isb-cgc-bq.targetome_versioned.drug_synonyms_v1` AS drugsyn\n        ON inter.drugID = drugsyn.drugID \n      INNER JOIN `isb-cgc-bq.targetome_versioned.experiments_v1` AS exp\n        ON inter.expID = exp.expID \n      INNER JOIN `isb-cgc-bq.reactome_versioned.physical_entity_v77` AS pe\n        ON inter.target_uniprotID = pe.uniprot_id\n      WHERE\n        LOWER(drugsyn.synonym) = LOWER('sorafenib')\n        AND exp.exp_assayValueMedian <= 100\n        AND (exp.exp_assayValueLow <= 100 OR exp.exp_assayValueLow IS NULL)\n        AND (exp.exp_assayValueHigh <= 100 OR exp.exp_assayValueHigh IS NULL)\n        AND inter.targetSpecies = 'Homo sapiens'\n    ),\n    \n    target_pp_query AS (\n      SELECT\n        COUNT(DISTINCT target_list_query.target_uniprotID) AS num_targets,\n        pathway.stable_id\n      FROM target_list_query\n      INNER JOIN `isb-cgc-bq.reactome_versioned.physical_entity_v77` AS pe\n        ON target_list_query.target_uniprotID = pe.uniprot_id\n      INNER JOIN `isb-cgc-bq.reactome_versioned.pe_to_pathway_v77` AS pe2pathway\n        ON pe.stable_id = pe2pathway.pe_stable_id\n      INNER JOIN `isb-cgc-bq.reactome_versioned.pathway_v77` AS pathway\n        ON pe2pathway.pathway_stable_id = pathway.stable_id\n      WHERE pe2pathway.evidence_code = 'TAS'\n      GROUP BY pathway.stable_id\n      ORDER BY num_targets DESC\n    ),\n    \n    not_target_list_query AS (\n      SELECT DISTINCT inter.target_uniprotID AS target_uniprotID\n      FROM `isb-cgc-bq.targetome_versioned.interactions_v1` AS inter\n      WHERE inter.targetSpecies = 'Homo sapiens'\n        AND inter.target_uniprotID NOT IN (SELECT target_uniprotID FROM target_list_query)\n    ),\n    \n    not_target_pp_query AS (\n      SELECT\n        COUNT(DISTINCT not_target_list_query.target_uniprotID) AS num_not_targets,\n        pathway.stable_id\n      FROM not_target_list_query\n      INNER JOIN `isb-cgc-bq.reactome_versioned.physical_entity_v77` AS pe\n        ON not_target_list_query.target_uniprotID = pe.uniprot_id\n      INNER JOIN `isb-cgc-bq.reactome_versioned.pe_to_pathway_v77` AS pe2pathway\n        ON pe.stable_id = pe2pathway.pe_stable_id\n      INNER JOIN `isb-cgc-bq.reactome_versioned.pathway_v77` AS pathway\n        ON pe2pathway.pathway_stable_id = pathway.stable_id\n      WHERE pe2pathway.evidence_code = 'TAS'\n      GROUP BY pathway.stable_id\n      ORDER BY num_not_targets DESC\n    ),\n    \n    target_count_query AS (\n      SELECT\n        target_count,\n        not_target_count,\n        target_count + not_target_count AS total_count\n      FROM \n        (SELECT COUNT(*) AS target_count FROM target_list_query),\n        (SELECT COUNT(*) AS not_target_count FROM not_target_list_query)\n    ),\n    \n    observed_query AS (\n      SELECT\n        target_pp_query.num_targets AS in_target_in_pathway,\n        not_target_pp_query.num_not_targets AS not_target_in_pathway,\n        target_count_query.target_count - target_pp_query.num_targets AS in_target_not_pathway,\n        target_count_query.not_target_count - not_target_pp_query.num_not_targets AS not_target_not_pathway,\n        target_pp_query.stable_id\n      FROM target_pp_query, target_count_query\n      INNER JOIN not_target_pp_query\n        ON target_pp_query.stable_id = not_target_pp_query.stable_id\n    ),\n    \n    sum_query AS (\n      SELECT\n        observed_query.in_target_in_pathway + observed_query.not_target_in_pathway AS pathway_total,\n        observed_query.in_target_not_pathway + observed_query.not_target_not_pathway AS not_pathway_total,\n        observed_query.in_target_in_pathway + observed_query.in_target_not_pathway AS target_total,\n        observed_query.not_target_in_pathway + observed_query.not_target_not_pathway AS not_target_total,\n        observed_query.stable_id\n      FROM observed_query\n    ),\n    \n    expected_query AS (\n      SELECT \n        sum_query.target_total * sum_query.pathway_total / target_count_query.total_count AS exp_in_target_in_pathway,\n        sum_query.not_target_total * sum_query.pathway_total / target_count_query.total_count AS exp_not_target_in_pathway,\n        sum_query.target_total * sum_query.not_pathway_total / target_count_query.total_count AS exp_in_target_not_pathway,\n        sum_query.not_target_total * sum_query.not_pathway_total / target_count_query.total_count AS exp_not_target_not_pathway,\n        sum_query.stable_id\n      FROM sum_query, target_count_query\n    ),\n    \n    chi_squared_query AS (\n      SELECT\n        POW(ABS(observed_query.in_target_in_pathway - expected_query.exp_in_target_in_pathway) - 0.5, 2) / expected_query.exp_in_target_in_pathway \n        + POW(ABS(observed_query.not_target_in_pathway - expected_query.exp_not_target_in_pathway) - 0.5, 2) / expected_query.exp_not_target_in_pathway\n        + POW(ABS(observed_query.in_target_not_pathway - expected_query.exp_in_target_not_pathway) - 0.5, 2) / expected_query.exp_in_target_not_pathway\n        + POW(ABS(observed_query.not_target_not_pathway - expected_query.exp_not_target_not_pathway) - 0.5, 2) / expected_query.exp_not_target_not_pathway\n        AS chi_squared_stat,\n        observed_query.stable_id\n      FROM observed_query\n      INNER JOIN expected_query\n        ON observed_query.stable_id = expected_query.stable_id\n    )\n    \nSELECT\n  chi_squared_query.stable_id,\n  observed_query.in_target_in_pathway,\n  observed_query.in_target_not_pathway,\n  observed_query.not_target_in_pathway,\n  observed_query.not_target_not_pathway,\n  chi_squared_query.chi_squared_stat\nFROM chi_squared_query\nINNER JOIN observed_query\n  ON chi_squared_query.stable_id = observed_query.stable_id\nINNER JOIN `isb-cgc-bq.reactome_versioned.pathway_v77` AS pathway\n  ON chi_squared_query.stable_id = pathway.stable_id\nWHERE pathway.lowest_level = TRUE\nORDER BY chi_squared_stat DESC\nLIMIT 3;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq330",
        "db": "bigquery-public-data.fdic_banks\nbigquery-public-data.fda_food\nbigquery-public-data.census_utility\nbigquery-public-data.geo_census_blockgroups\nbigquery-public-data.geo_us_boundaries",
        "question": "Which Colorado zip code has the highest concentration of bank locations per block group, based on the overlap between zip codes and block groups?",
        "SQL": "WITH _fips AS (\n    SELECT\n        state_fips_code\n    FROM\n        `bigquery-public-data.census_utility.fips_codes_states`\n    WHERE\n        state_name = \"Colorado\"\n),\n\n_bg AS (\n    SELECT\n        b.geo_id,\n        b.blockgroup_geom,\n        ST_AREA(b.blockgroup_geom) AS bg_size\n    FROM\n        `bigquery-public-data.geo_census_blockgroups.us_blockgroups_national` b\n    JOIN\n        _fips u ON b.state_fips_code = u.state_fips_code\n),\n\n_zip AS (\n    SELECT\n        z.zip_code,\n        z.zip_code_geom\n    FROM\n        `bigquery-public-data.geo_us_boundaries.zip_codes` z\n    JOIN\n        _fips u ON z.state_fips_code = u.state_fips_code\n),\n\nbq_zip_overlap AS (\n    SELECT\n        b.geo_id,\n        z.zip_code,\n        ST_AREA(ST_INTERSECTION(b.blockgroup_geom, z.zip_code_geom)) / b.bg_size AS overlap_size,\n        b.blockgroup_geom\n    FROM\n        _zip z\n    JOIN\n        _bg b ON ST_INTERSECTS(b.blockgroup_geom, z.zip_code_geom)\n),\n\nlocations AS (\n    SELECT\n        SUM(overlap_size * count_locations) AS locations_per_bg,\n        l.zip_code\n    FROM (\n        SELECT\n            COUNT(CONCAT(institution_name, \" : \", branch_name)) AS count_locations,\n            zip_code\n        FROM\n            `bigquery-public-data.fdic_banks.locations`\n        WHERE\n            state IS NOT NULL\n            AND state_name IS NOT NULL\n        GROUP BY\n            zip_code\n    ) l\n    JOIN\n        bq_zip_overlap ON l.zip_code = bq_zip_overlap.zip_code\n    GROUP BY\n        l.zip_code\n)\n\nSELECT\n    l.zip_code\nFROM\n    locations l\nGROUP BY\n    l.zip_code\nORDER BY\n    MAX(locations_per_bg) DESC\nLIMIT 1;",
        "external_knowledge": "overlap_ratio.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq398",
        "db": "bigquery-public-data.world_bank_intl_debt\nbigquery-public-data.world_bank_wdi\nbigquery-public-data.world_bank_intl_education\nbigquery-public-data.world_bank_health_population\nbigquery-public-data.world_bank_global_population",
        "question": "What are the top three debt indicators for Russia based on the highest debt values?",
        "SQL": "WITH russia_Data as (\nSELECT distinct \n  id.country_name,\n  id.value, --format in DataStudio\n  id.indicator_name\nFROM (\n  SELECT\n    country_code,\n    region\n  FROM\n    bigquery-public-data.world_bank_intl_debt.country_summary\n  WHERE\n    region != \"\" ) cs --aggregated countries do not have a region\nINNER JOIN (\n  SELECT\n    country_code,\n    country_name,\n    value, \n    indicator_name\n  FROM\n    bigquery-public-data.world_bank_intl_debt.international_debt\n  WHERE true\n    and country_code = 'RUS'  \n     ) id\nON\n  cs.country_code = id.country_code\nWHERE value is not null\nORDER BY\n  id.value DESC\n)\nSELECT \n    indicator_name\nFROM russia_data\nLIMIT 3;",
        "external_knowledge": null,
        "plan": "1. **Filter for Specific Region**: \n   - Start by selecting countries that have a specified region to exclude aggregated country data that lacks regional information.\n\n2. **Filter for Specific Country**:\n   - From the international debt data, select records specifically related to the country of interest (in this case, Russia).\n\n3. **Join Data**:\n   - Combine the filtered country data with the filtered international debt data based on matching country identifiers to get comprehensive records for the country of interest.\n\n4. **Filter Non-Null Values**:\n   - Ensure that only records with non-null debt values are considered, as null values would not contribute to identifying top debt indicators.\n\n5. **Sort by Debt Value**:\n   - Sort the resulting records in descending order based on the debt values to prioritize higher debt amounts.\n\n6. **Select Relevant Information**:\n   - From the sorted data, select the indicators that correspond to the top debt values.\n\n7. **Limit Results**:\n   - Restrict the final output to the top three records to meet the requirement of identifying the top three debt indicators.",
        "special_function": [
            "string-functions/FORMAT"
        ]
    },
    {
        "instance_id": "bq399",
        "db": "bigquery-public-data.world_bank_intl_debt\nbigquery-public-data.world_bank_wdi\nbigquery-public-data.world_bank_intl_education\nbigquery-public-data.world_bank_health_population\nbigquery-public-data.world_bank_global_population",
        "question": "Which high-income country had the highest average crude birth rate respectively in each region, and what are their corresponding average birth rate, during the 1980s?",
        "SQL": "WITH country_data AS ( \n  SELECT \n    country_code, \n    short_name AS country,\n    region, \n    income_group \n  FROM \n    bigquery-public-data.world_bank_wdi.country_summary\n)\n, birth_rate_data AS (\n  SELECT \n    data.country_code, \n    country_data.country,\n    country_data.region,\n    AVG(value) AS avg_birth_rate\n  FROM \n    bigquery-public-data.world_bank_wdi.indicators_data data \n  LEFT JOIN \n    country_data \n  ON \n    data.country_code = country_data.country_code\n  WHERE \n    indicator_code = \"SP.DYN.CBRT.IN\" -- Birth Rate\n    AND EXTRACT(YEAR FROM PARSE_DATE('%Y', CAST(year AS STRING))) BETWEEN 1980 AND 1989 -- 1980s\n    AND country_data.income_group = \"High income\" -- High-income group\n  GROUP BY \n    data.country_code, \n    country_data.country,\n    country_data.region\n)\n, ranked_birth_rates AS (\n  SELECT\n    region,\n    country,\n    avg_birth_rate,\n    RANK() OVER(PARTITION BY region ORDER BY avg_birth_rate DESC) AS rank\n  FROM\n    birth_rate_data\n)\nSELECT \n  region, \n  country, \n  avg_birth_rate\nFROM \n  ranked_birth_rates\nWHERE \n  rank = 1\nORDER BY \n  region;",
        "external_knowledge": null,
        "plan": "1. **Define Country Data**:\n   - Select and rename relevant columns from the country summary table to identify each country, its region, and its income group.\n\n2. **Calculate Average Birth Rate**:\n   - Join the birth rate data with the previously defined country data.\n   - Filter this joined data to only include birth rate records from the 1980s.\n   - Further filter to only include countries classified as \"High income\".\n   - Exclude any entries that represent aggregated regions.\n   - Compute the average birth rate for each country over the 1980s.\n\n3. **Rank Birth Rates by Region**:\n   - For each region, rank the countries based on their average birth rate in descending order.\n\n4. **Select Top Countries**:\n   - Select the top-ranked country (highest average birth rate) for each region.\n   - Retrieve the region name, country name, and corresponding average birth rate.\n\n5. **Order Results**:\n   - Order the final results by region for better readability and organization.",
        "special_function": null
    },
    {
        "instance_id": "bq457",
        "db": "bigquery-public-data.libraries_io",
        "question": "Get details of repositories that use specific feature toggle libraries. For each repository, include the full name with owner, hosting platform type, size in bytes, primary programming language, fork source name (if any), last update timestamp, the artifact and library names of the feature toggle used, and the library's programming languages. Include repositories that depend on the specified feature toggle libraries, defined by their artifact names, library names, platforms, and languages.",
        "SQL": "SELECT DISTINCT r.name_with_owner, r.host_type, CAST(r.size * 1024 AS int64) AS bytes, r.language, r.fork_source_name_with_owner, r.updated_timestamp, libraries.artifact_name, libraries.library, libraries.languages AS library_language\nFROM `bigquery-public-data.libraries_io.projects` p\nINNER JOIN UNNEST([\n  STRUCT('unleash-client-dotnet' AS library, 'Unleash.FeatureToggle.Client' AS artifact_name, 'NuGet' AS platform, 'C#,Visual Basic' AS languages),\n  STRUCT('unleash-client' AS library, 'unleash.client' AS artifact_name, 'NuGet' AS platform, 'C#,Visual Basic' AS languages),\n  STRUCT('launchdarkly' AS library, 'LaunchDarkly.Client' AS artifact_name, 'NuGet' AS platform, 'C#,Visual Basic' AS languages),\n  STRUCT('NFeature' AS library, 'NFeature' AS artifact_name, 'NuGet' AS platform, 'C#,Visual Basic' AS languages),\n  STRUCT('FeatureToggle' AS library, 'FeatureToggle' AS artifact_name, 'NuGet' AS platform, 'C#,Visual Basic' AS languages),\n  STRUCT('FeatureSwitcher' AS library, 'FeatureSwitcher' AS artifact_name, 'NuGet' AS platform, 'C#,Visual Basic' AS languages),\n  STRUCT('Toggler' AS library, 'Toggler' AS artifact_name, 'NuGet' AS platform, 'C#,Visual Basic' AS languages),\n  STRUCT('launchdarkly' AS library, 'github.com/launchdarkly/go-client' AS artifact_name, 'Go' AS platform, 'Go' AS languages),\n  STRUCT('Toggle' AS library, 'github.com/xchapter7x/toggle' AS artifact_name, 'Go' AS platform, 'Go' AS languages),\n  STRUCT('dcdr' AS library, 'github.com/vsco/dcdr' AS artifact_name, 'Go' AS platform, 'Go' AS languages),\n  STRUCT('unleash-client-go' AS library, 'github.com/unleash/unleash-client-go' AS artifact_name, 'Go' AS platform, 'Go' AS languages),\n  STRUCT('unleash-client-node' AS library, 'unleash-client' AS artifact_name, 'NPM' AS platform, 'JavaScript,TypeScript' AS languages),\n  STRUCT('launchdarkly' AS library, 'ldclient-js' AS artifact_name, 'NPM' AS platform, 'JavaScript,TypeScript' AS languages),\n  STRUCT('ember-feature-flags' AS library, 'ember-feature-flags' AS artifact_name, 'NPM' AS platform, 'JavaScript,TypeScript' AS languages),\n  STRUCT('feature-toggles' AS library, 'feature-toggles' AS artifact_name, 'NPM' AS platform, 'JavaScript,TypeScript' AS languages),\n  STRUCT('React Feature Toggles' AS library, '@paralleldrive/react-feature-toggles' AS artifact_name, 'NPM' AS platform, 'JavaScript,TypeScript' AS languages),\n  STRUCT('launchdarkly' AS library, 'ldclient-node' AS artifact_name, 'NPM' AS platform, 'JavaScript,TypeScript' AS languages),\n  STRUCT('flipit' AS library, 'flipit' AS artifact_name, 'NPM' AS platform, 'JavaScript,TypeScript' AS languages),\n  STRUCT('fflip' AS library, 'fflip' AS artifact_name, 'NPM' AS platform, 'JavaScript,TypeScript' AS languages),\n  STRUCT('Bandiera' AS library, 'bandiera-client' AS artifact_name, 'NPM' AS platform, 'JavaScript,TypeScript' AS languages),\n  STRUCT('flopflip' AS library, '@flopflip/react-redux' AS artifact_name, 'NPM' AS platform, 'JavaScript,TypeScript' AS languages),\n  STRUCT('flopflip' AS library, '@flopflip/react-broadcast' AS artifact_name, 'NPM' AS platform, 'JavaScript,TypeScript' AS languages),\n  STRUCT('launchdarkly' AS library, 'com.launchdarkly:launchdarkly-android-client' AS artifact_name, 'Maven' AS platform, 'Kotlin,Java' AS languages),\n  STRUCT('toggle' AS library, 'cc.soham:toggle' AS artifact_name, 'Maven' AS platform, 'Kotlin,Java' AS languages),\n  STRUCT('unleash-client-java' AS library, 'no.finn.unleash:unleash-client-java' AS artifact_name, 'Maven' AS platform, 'Kotlin,Java' AS languages),\n  STRUCT('launchdarkly' AS library, 'com.launchdarkly:launchdarkly-client' AS artifact_name, 'Maven' AS platform, 'Kotlin,Java' AS languages),\n  STRUCT('Togglz' AS library, 'org.togglz:togglz-core' AS artifact_name, 'Maven' AS platform, 'Kotlin,Java' AS languages),\n  STRUCT('FF4J' AS library, 'org.ff4j:ff4j-core' AS artifact_name, 'Maven' AS platform, 'Kotlin,Java' AS languages),\n  STRUCT('Flip' AS library, 'com.tacitknowledge.flip:core' AS artifact_name, 'Maven' AS platform, 'Kotlin,Java' AS languages),\n  STRUCT('launchdarkly' AS library, 'LaunchDarkly' AS artifact_name, 'CocoaPods' AS platform, 'Objective-C,Swift' AS languages),\n  STRUCT('launchdarkly' AS library, 'launchdarkly/ios-client' AS artifact_name, 'Carthage' AS platform, 'Objective-C,Swift' AS languages),\n  STRUCT('launchdarkly' AS library, 'launchdarkly/launchdarkly-php' AS artifact_name, 'Packagist' AS platform, 'PHP' AS languages),\n  STRUCT('Symfony FeatureFlagsBundle' AS library, 'dzunke/feature-flags-bundle' AS artifact_name, 'Packagist' AS platform, 'PHP' AS languages),\n  STRUCT('rollout' AS library, 'opensoft/rollout' AS artifact_name, 'Packagist' AS platform, 'PHP' AS languages),\n  STRUCT('Bandiera' AS library, 'npg/bandiera-client-php' AS artifact_name, 'Packagist' AS platform, 'PHP' AS languages),\n  STRUCT('unleash-client-python' AS library, 'UnleashClient' AS artifact_name, 'Pypi' AS platform, 'Python' AS languages),\n  STRUCT('launchdarkly' AS library, 'ldclient-py' AS artifact_name, 'Pypi' AS platform, 'Python' AS languages),\n  STRUCT('Flask FeatureFlags' AS library, 'Flask-FeatureFlags' AS artifact_name, 'Pypi' AS platform, 'Python' AS languages),\n  STRUCT('Gutter' AS library, 'gutter' AS artifact_name, 'Pypi' AS platform, 'Python' AS languages),\n  STRUCT('Feature Ramp' AS library, 'feature_ramp' AS artifact_name, 'Pypi' AS platform, 'Python' AS languages),\n  STRUCT('flagon' AS library, 'flagon' AS artifact_name, 'Pypi' AS platform, 'Python' AS languages),\n  STRUCT('Waffle' AS library, 'django-waffle' AS artifact_name, 'Pypi' AS platform, 'Python' AS languages),\n  STRUCT('Gargoyle' AS library, 'gargoyle' AS artifact_name, 'Pypi' AS platform, 'Python' AS languages),\n  STRUCT('Gargoyle' AS library, 'gargoyle-yplan' AS artifact_name, 'Pypi' AS platform, 'Python' AS languages),\n  STRUCT('unleash-client-ruby' AS library, 'unleash' AS artifact_name, 'Rubygems' AS platform, 'Ruby' AS languages),\n  STRUCT('launchdarkly' AS library, 'ldclient-rb' AS artifact_name, 'Rubygems' AS platform, 'Ruby' AS languages),\n  STRUCT('rollout' AS library, 'rollout' AS artifact_name, 'Rubygems' AS platform, 'Ruby' AS languages),\n  STRUCT('FeatureFlipper' AS library, 'feature_flipper' AS artifact_name, 'Rubygems' AS platform, 'Ruby' AS languages),\n  STRUCT('Flip' AS library, 'flip' AS artifact_name, 'Rubygems' AS platform, 'Ruby' AS languages),\n  STRUCT('Setler' AS library, 'setler' AS artifact_name, 'Rubygems' AS platform, 'Ruby' AS languages),\n  STRUCT('Bandiera' AS library, 'bandiera-client' AS artifact_name, 'Rubygems' AS platform, 'Ruby' AS languages),\n  STRUCT('Feature' AS library, 'feature' AS artifact_name, 'Rubygems' AS platform, 'Ruby' AS languages),\n  STRUCT('Flipper' AS library, 'flipper' AS artifact_name, 'Rubygems' AS platform, 'Ruby' AS languages),\n  STRUCT('Bandiera' AS library, 'com.springernature:bandiera-client-scala_2.12' AS artifact_name, 'Maven' AS platform, 'Scala' AS languages),\n  STRUCT('Bandiera' AS library, 'com.springernature:bandiera-client-scala_2.11' AS artifact_name, 'Maven' AS platform, 'Scala' AS languages)\n]) libraries\nON p.name = libraries.artifact_name AND p.platform = libraries.platform\nINNER JOIN `bigquery-public-data.libraries_io.repository_dependencies` rd\nON rd.dependency_project_id = p.id\nINNER JOIN `bigquery-public-data.libraries_io.repositories` r\nON r.id = rd.repository_id",
        "external_knowledge": "feature_toggle_libraries.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "bq227",
        "db": "bigquery-public-data.london_crime\nbigquery-public-data.london_fire_brigade",
        "question": "Could you provide the annual percentage shares, rounded to two decimal places, of the top 5 minor crime categories from 2008 in London's total crimes, with each year displayed in one row?",
        "SQL": "WITH top5_categories AS (\n  SELECT minor_category\n  FROM `bigquery-public-data.london_crime.crime_by_lsoa`\n  WHERE year = 2008\n  GROUP BY minor_category\n  ORDER BY SUM(value) DESC\n  LIMIT 5\n),\n\ntotal_crimes_per_year AS (\n  SELECT \n    year, \n    SUM(value) AS total_crimes_year\n  FROM `bigquery-public-data.london_crime.crime_by_lsoa`\n  GROUP BY year\n),\n\ntop5_crimes_per_year AS (\n  SELECT\n    year,\n    minor_category,\n    SUM(value) AS total_crimes_category_year\n  FROM `bigquery-public-data.london_crime.crime_by_lsoa`\n  WHERE minor_category IN (SELECT minor_category FROM top5_categories)\n  GROUP BY year, minor_category\n)\n\nSELECT\n  t.year,\n  ROUND(SUM(CASE WHEN t.minor_category = (SELECT minor_category FROM top5_categories LIMIT 1 OFFSET 0) THEN t.total_crimes_category_year ELSE 0 END) / y.total_crimes_year * 100, 2) AS `Category 1`,\n  ROUND(SUM(CASE WHEN t.minor_category = (SELECT minor_category FROM top5_categories LIMIT 1 OFFSET 1) THEN t.total_crimes_category_year ELSE 0 END) / y.total_crimes_year * 100, 2) AS `Category 2`,\n  ROUND(SUM(CASE WHEN t.minor_category = (SELECT minor_category FROM top5_categories LIMIT 1 OFFSET 2) THEN t.total_crimes_category_year ELSE 0 END) / y.total_crimes_year * 100, 2) AS `Category 3`,\n  ROUND(SUM(CASE WHEN t.minor_category = (SELECT minor_category FROM top5_categories LIMIT 1 OFFSET 3) THEN t.total_crimes_category_year ELSE 0 END) / y.total_crimes_year * 100, 2) AS `Category 4`,\n  ROUND(SUM(CASE WHEN t.minor_category = (SELECT minor_category FROM top5_categories LIMIT 1 OFFSET 4) THEN t.total_crimes_category_year ELSE 0 END) / y.total_crimes_year * 100, 2) AS `Category 5`\nFROM\n  top5_crimes_per_year t\nJOIN\n  total_crimes_per_year y ON t.year = y.year\nGROUP BY\n  t.year, y.total_crimes_year\nORDER BY\n  t.year;",
        "external_knowledge": null,
        "plan": "1. Aggregate the total values for each category, year, and month combination.\n2. Identify the top categories based on aggregated values. \n3. Rank categories by their total and classify the top five categories.\n4. Summarize the values for the categorized data (top five versus others) for each year, without distinguishing by month or specific category anymore.\n5. Aggregate these sums over each year to compute the total annual value for both the top categories and others.\n6. For each year, calculate the percentage share of the total value that the top categories represent by dividing the total value of top categories by the total annual value and multiplying by 100 to convert this fraction into a percentage.\n7. From the results, filter out the data to only show the percentage values for the top categories across each year.\n8. Finally, order the resulting data by year.\n",
        "special_function": [
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq232",
        "db": "bigquery-public-data.london_crime\nbigquery-public-data.london_fire_brigade",
        "question": "Could you provide the total number of 'Other Theft' incidents within the 'Theft and Handling' category for each year in the Westminster borough?",
        "SQL": "WITH borough_data AS (\n    SELECT \n        year, \n        month, \n        borough, \n        major_category, \n        minor_category, \n        SUM(value) AS total,\n    CASE \n        WHEN \n            major_category = 'Theft and Handling' \n        THEN \n            'Theft and Handling'\n        ELSE \n            'Other' \n    END AS major_division,\n    CASE \n        WHEN \n            minor_category = 'Other Theft' THEN minor_category\n        ELSE \n            'Other'\n    END AS minor_division,\n    FROM \n        bigquery-public-data.london_crime.crime_by_lsoa\n    GROUP BY 1,2,3,4,5\n    ORDER BY 1,2\n)\n\nSELECT year, SUM(total) AS year_total\nFROM borough_data\nWHERE \n    borough = 'Westminster'\nAND\n    major_division != 'Other'\nAND \n    minor_division != 'Other'\nGROUP BY year, major_division, minor_division\nORDER BY year;",
        "external_knowledge": null,
        "plan": "1. Group data by year, month, borough, and crime categories.\n2. Sum the values in each group to get a total count of incidents per group.\n3. Categorize the crime data into two divisions for the major crime category. If the major category is specified as 'Theft and Handling', label it as such; otherwise, label it as 'Other'.\n4. Similarly, categorize the minor crime category. If the minor category matches 'Other Theft', keep the category name; otherwise, label it as 'Other'.\n5. Filter the results to focus only on the borough 'Westminster' and exclude the 'Other' categories for both major and minor divisions.\n6. Calculate the total incidents per year by summing up the totals from the filtered results. \n7. Group these results by year, major division, and minor division.\n8. Finally, order the summarized data by year.",
        "special_function": [
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq228",
        "db": "bigquery-public-data.london_crime\nbigquery-public-data.london_fire_brigade",
        "question": "Please provide a list of the top three major crime categories in the borough of Barking and Dagenham, along with the number of incidents in each category.",
        "SQL": "WITH ranked_crimes AS (\n    SELECT\n        borough,\n        major_category,\n        RANK() OVER(PARTITION BY borough ORDER BY SUM(value) DESC) AS rank_per_borough,\n        SUM(value) AS no_of_incidents\n    FROM\n        `bigquery-public-data.london_crime.crime_by_lsoa`\n    GROUP BY\n        borough,\n        major_category\n)\n\nSELECT\n    borough,\n    major_category,\n    rank_per_borough,\n    no_of_incidents\nFROM\n    ranked_crimes\nWHERE\n    rank_per_borough <= 3\nAND \n    borough = 'Barking and Dagenham'\nORDER BY\n    borough,\n    rank_per_borough;",
        "external_knowledge": null,
        "plan": "1. Calculate and rank the sum of crime incidents by major category within each borough.\n2. Group the data by both borough and major category to aggregate the total number of crime incidents for each category within each borough.\n3. Apply a ranking function to order the crime categories within each borough based on the aggregated sum of incidents, in descending order. \n4. Retrieve the borough, major category, ranking, and total number of incidents.\n5. Include only those records where the rank is within the top 3 for each borough.\n6. Further filter to include only records pertaining to Barking and Dagenham.\n7. Sort the final output first by borough and then by the ranking within that borough to show the top crime categories orderly.\n8. Display the major crime categories with the most incidents in the specified borough.\n",
        "special_function": [
            "numbering-functions/RANK"
        ]
    },
    {
        "instance_id": "bq229",
        "db": "bigquery-public-data.open_images",
        "question": "Can you provide a count of how many image URLs are categorized as \u2018cat\u2019 (with label '/m/01yrx' and full confidence) and how many contain no such cat labels(categorized as \u2018other\u2019) at all? ",
        "SQL": "WITH all_images_and_labels AS (\n  SELECT i.original_url, l.label_name, l.confidence\n  FROM `bigquery-public-data.open_images.images` i\n  JOIN `bigquery-public-data.open_images.labels` l\n  ON i.image_id = l.image_id\n),\nurls_with_labels AS (\n    SELECT original_url, label\n    FROM\n    (\n    SELECT DISTINCT original_url, 'cat' as label\n    FROM all_images_and_labels\n    WHERE confidence = 1\n    AND label_name LIKE '/m/01yrx'\n    UNION ALL\n    (\n        SELECT DISTINCT all_images.original_url, 'other' as label\n        FROM all_images_and_labels all_images\n        LEFT JOIN\n        (\n        SELECT original_url\n        FROM all_images_and_labels\n        WHERE confidence = 1\n        AND NOT (label_name LIKE '/m/01yrx')\n        ) not_cat\n        ON all_images.original_url = not_cat.original_url\n        WHERE not_cat.original_url IS NULL\n    )\n    )\n)\n\nSELECT label, COUNT(*) AS url_num\nFROM urls_with_labels\nGROUP BY label",
        "external_knowledge": null,
        "plan": "1. Join two tables to combine information about images and their associated labels. This includes the image's URL, label name, and a confidence score for each label.\n2, Filter and select distinct image URLs with label_name '/m/01yrx' where the confidence is 100%.\n3. Identify images that have any other labels with 100% confidence.\n4. Combine these two sets using a UNION ALL, where the first set gets labeled with its label and the second set gets labeled as 'other'.\n5. From the combined and categorized URLs, group by the label.\n6. Count the number of URLs associated with each label group.\n7. Return the count and label type.\n",
        "special_function": null
    },
    {
        "instance_id": "bq230",
        "db": "bigquery-public-data.usda_nass_agriculture",
        "question": "Find the total 2022 production figures in bushels for corn from the field crops category and mushrooms from the horticulture group for each U.S. state using the USDA NASS Agriculture Crops dataset? The data should include only records that show production as the statistic category, are measured at the state level, and have no missing values.",
        "SQL": "WITH \n  corn_temp AS (\n    SELECT\n      state_name,\n      commodity_desc,\n      SUM(value) AS total_produce,\n      TIMESTAMP_TRUNC(load_time, YEAR) AS year_load\n    FROM \n      `bigquery-public-data.usda_nass_agriculture.crops`\n    WHERE\n      group_desc = 'FIELD CROPS' AND\n      statisticcat_desc = 'PRODUCTION' AND\n      agg_level_desc = 'STATE' AND\n      value IS NOT NULL AND\n      unit_desc = 'BU' AND\n      commodity_desc = 'CORN'\n    GROUP BY\n      state_name,\n      commodity_desc,\n      year_load\n    ORDER BY\n      state_name,\n      commodity_desc,\n      total_produce DESC\n  ),\n  \n  mushrooms_temp AS (\n    SELECT\n      state_name,\n      commodity_desc,\n      SUM(value) AS total_produce,\n      TIMESTAMP_TRUNC(load_time, YEAR) AS year_load\n    FROM \n      `bigquery-public-data.usda_nass_agriculture.crops`\n    WHERE\n      group_desc = 'HORTICULTURE' AND\n      statisticcat_desc = 'PRODUCTION' AND\n      agg_level_desc = 'STATE' AND\n      commodity_desc = 'MUSHROOMS' AND\n      value IS NOT NULL\n    GROUP BY\n      state_name,\n      commodity_desc,\n      year_load\n  )\n\nSELECT\n  c.state_name,\n  c.total_produce AS corn_total_prod,\n  m.total_produce AS mushrooms_total_prod\nFROM\n  corn_temp c\nINNER JOIN\n  mushrooms_temp m\nON\n  c.state_name = m.state_name\nWHERE\n  c.year_load = '2022-01-01 00:00:00 UTC' AND\n  m.year_load = '2022-01-01 00:00:00 UTC'\nORDER BY\n  c.state_name;",
        "external_knowledge": null,
        "plan": "1. Select and group data by state name, commodity description, and year.\n2. Filter the data to include only records for crop group 'FIELD CROPS', data type like 'PRODUCTION', at the state level, where the measured value is not null and the units are specified as bushels.\n3. Calculate the total amount of the commodity produced per state, per commodity, per year.\n4. Filter the entries for 2018 and 'CORN'.\n5. For each state, find the maximum total produce amount recorded for that commodity.\n6. Display the state name and the maximum total produce amount for the specified commodity and year.\n7. Order the results by state name to present the data in a sorted manner by geographical location.\n",
        "special_function": [
            "timestamp-functions/TIMESTAMP_TRUNC"
        ]
    },
    {
        "instance_id": "bq326",
        "db": "bigquery-public-data.world_bank_intl_debt\nbigquery-public-data.world_bank_wdi\nbigquery-public-data.world_bank_intl_education\nbigquery-public-data.world_bank_health_population\nbigquery-public-data.world_bank_global_population",
        "question": "How many countries saw increases of more than 1% in both their population and per capita current health expenditures (adjusted for purchasing power parity) in 2018 based on the world bank health and global population data",
        "SQL": "WITH base AS (\n    SELECT country,\n    country_code,\n    unpivotted\n    FROM `bigquery-public-data.world_bank_global_population.population_by_country` a\n    \n  , UNNEST(fhoffa.x.unpivot(a, 'year')) unpivotted\n),\n\npop AS (SELECT country, \n country_code, \n CAST (RIGHT(unpivotted.key,4) AS INT64) AS as_of_year,\n CASE WHEN unpivotted.value = 'null' THEN '0' ELSE unpivotted.value END AS population \nFROM base \nWHERE CAST (RIGHT(unpivotted.key,4) AS INT64)  >= 2010 ), \n\npop_1 AS ( \n  SELECT \n   country,\n   country_code,\n   as_of_year,\n  CAST (population as FLOAT64) AS population,\n  COALESCE (LAG (CAST (population AS FLOAT64), 1)\n  OVER (PARTITION BY country_code ORDER BY as_of_year), 0) AS prev_population \n  from pop ), \nNumber1 as (\n  SELECT *, \n  COALESCE (ROUND ( population /NULLIF(prev_population,0), 2 ), 0) AS change_in_population  \n  FROM pop_1\nwhere pop_1.as_of_year = 2018),\nA AS (\n SELECT\n  country_name,\n  country_code,\n  indicator_name,\n  value as PPP,\n  year,\n LAG(value) over (partition by country_code order by year) as prePPP\n FROM `bigquery-public-data.world_bank_health_population.health_nutrition_population`\n WHERE\n  indicator_code = \"SH.XPD.CHEX.PP.CD\"),\nB AS (\n  SELECT\n   *,\n  COALESCE (ROUND ( A.PPP /NULLIF(prePPP,0), 2 ), 0) AS change_in_PPP\n  FROM A \n  WHERE year = 2018)\nSELECT \n COUNT(country) AS country_count\nFROM Number1\n left join B\n  ON Number1.country_code = B.country_code\nWHERE\n B.change_in_PPP > 1 AND Number1.change_in_population > 1;",
        "external_knowledge": null,
        "plan": "1. **Unpivot Yearly Data**:\n   - Transpose the yearly data columns into rows to create a more manageable format for further analysis.\n\n2. **Filter and Format Data**:\n   - Select relevant columns and filter the data for years starting from 2010. Convert the year and population values into appropriate data types.\n\n3. **Calculate Previous Population**:\n   - Use a window function to calculate the previous year's population for each country to enable the calculation of population changes.\n\n4. **Calculate Population Change for 2018**:\n   - Calculate the change in population for the year 2018 by comparing it to the previous year's population. Handle potential division by zero errors gracefully.\n\n5. **Filter Health Expenditure Data**:\n   - Select and filter the health expenditure per capita data for the specific indicator of interest. Use a window function to calculate the previous year's expenditure.\n\n6. **Calculate Health Expenditure Change for 2018**:\n   - Calculate the change in health expenditure per capita for the year 2018 by comparing it to the previous year's expenditure. Handle potential division by zero errors gracefully.\n\n7. **Join and Filter Results**:\n   - Join the population change data and the health expenditure change data on the country identifier. Filter the joined data to find countries that experienced both a significant population change and a significant health expenditure change in 2018.\n\n8. **Count the Countries**:\n   - Count the number of countries that meet the criteria of having both changes greater than 1 in the specified year.",
        "special_function": [
            "conversion-functions/CAST",
            "debugging-functions/ERROR",
            "mathematical-functions/ROUND",
            "navigation-functions/LAG",
            "string-functions/RIGHT",
            "conditional-functions/CASE",
            "conditional-functions/COALESCE",
            "conditional-functions/NULLIF",
            "other-functions/UNNEST",
            "other-functions/UNPIVOT"
        ]
    },
    {
        "instance_id": "bq424",
        "db": "bigquery-public-data.world_bank_intl_debt\nbigquery-public-data.world_bank_wdi\nbigquery-public-data.world_bank_intl_education\nbigquery-public-data.world_bank_health_population\nbigquery-public-data.world_bank_global_population",
        "question": "List the top 10 countries with respect to the total amount of long-term external debt in descending order, excluding those without a specified region.",
        "SQL": "SELECT DISTINCT\n  id.country_name,\n  --cs.region,\n  id.value AS debt,\n  --id.indicator_code\nFROM (\n  SELECT\n    country_code,\n    region\n  FROM\n    `bigquery-public-data.world_bank_intl_debt.country_summary`\n  WHERE\n    region != \"\" ) cs\nINNER JOIN (\n  SELECT\n    country_code,\n    country_name,\n    value,\n    indicator_code\n  FROM\n    `bigquery-public-data.world_bank_intl_debt.international_debt`\n  WHERE\n    indicator_code = \"DT.AMT.DLXF.CD\") id\n\nON\n  cs.country_code = id.country_code\nORDER BY\n  id.value DESC\n  LIMIT 10",
        "external_knowledge": null,
        "plan": "1. **Filter Countries with Specified Regions**:\n   - Extract a list of countries along with their regions, ensuring that only those countries with a specified region (i.e., non-empty region) are included.\n\n2. **Filter Debt Data by Indicator**:\n   - Extract the debt data, specifically focusing on records that correspond to long-term external debt.\n\n3. **Join Filtered Data**:\n   - Perform an inner join operation between the filtered list of countries (with specified regions) and the filtered debt data, matching on the country code to combine relevant details from both datasets.\n\n4. **Select and Rename Columns**:\n   - Select distinct country names and their corresponding debt amounts from the joined dataset. Rename the debt amount column for clarity.\n\n5. **Sort and Limit Results**:\n   - Order the resulting list of countries by their total debt amount in descending order.\n   - Limit the output to the top 10 countries to fulfill the requirement of listing only the top 10 entries.\n\nBy following these steps, the SQL query effectively identifies and lists the top 10 countries with the highest amounts of long-term external debt, while excluding those without a specified region.",
        "special_function": null
    },
    {
        "instance_id": "bq327",
        "db": "bigquery-public-data.world_bank_intl_debt\nbigquery-public-data.world_bank_wdi\nbigquery-public-data.world_bank_intl_education\nbigquery-public-data.world_bank_health_population\nbigquery-public-data.world_bank_global_population",
        "question": "How many debt indicators for Russia have a value of 0, excluding NULL values?",
        "SQL": "WITH russia_Data AS (\n  SELECT DISTINCT \n    id.country_name,\n    id.value, -- Format in DataStudio\n    id.indicator_name\n  FROM (\n    SELECT\n      country_code,\n      region\n    FROM\n      bigquery-public-data.world_bank_intl_debt.country_summary\n    WHERE\n      region != \"\" -- Aggregated countries do not have a region\n  ) cs -- Aggregated countries do not have a region\n  INNER JOIN (\n    SELECT\n      country_code,\n      country_name,\n      value, \n      indicator_name\n    FROM\n      bigquery-public-data.world_bank_intl_debt.international_debt\n    WHERE\n      country_code = 'RUS'\n  ) id\n  ON\n    cs.country_code = id.country_code\n  WHERE value IS NOT NULL\n)\n-- Count the number of indicators with a value of 0 for Russia\nSELECT \n  COUNT(*) AS number_of_indicators_with_zero\nFROM \n  russia_Data\nWHERE \n  value = 0;",
        "external_knowledge": null,
        "plan": "1. **Create a Subquery to Filter Countries with Regions:**\n   - Construct a subquery to select the country codes and regions from a summary table.\n   - Filter out rows where the region is empty to exclude aggregated countries that do not have a specific region.\n\n2. **Create a Subquery for Russia's Debt Indicators:**\n   - Construct another subquery to select the country code, country name, debt value, and indicator name from an international debt table.\n   - Filter this subquery to include only rows where the country code matches the code for Russia.\n\n3. **Join the Two Subqueries:**\n   - Perform an inner join between the two subqueries on the country code to combine the data, ensuring that only records with matching country codes from both subqueries are included.\n\n4. **Filter Out NULL Values:**\n   - Further refine the joined dataset by excluding rows where the debt value is NULL.\n\n5. **Define a Common Table Expression (CTE):**\n   - Use the results of the above steps to create a CTE that holds the filtered and joined data for Russia's debt indicators, ensuring that each combination of country name, value, and indicator name is distinct.\n\n6. **Count Debt Indicators with a Value of Zero:**\n   - Query the CTE to count the number of debt indicators where the value is exactly zero.\n   - Return this count as the final result.",
        "special_function": [
            "string-functions/FORMAT"
        ]
    },
    {
        "instance_id": "bq328",
        "db": "bigquery-public-data.world_bank_intl_debt\nbigquery-public-data.world_bank_wdi\nbigquery-public-data.world_bank_intl_education\nbigquery-public-data.world_bank_health_population\nbigquery-public-data.world_bank_global_population",
        "question": "Which region has the highest median GDP (constant 2015 US$) value?",
        "SQL": "WITH country_data AS (\n  -- CTE for country descriptive data\n  SELECT \n    country_code, \n    short_name AS country,\n    region, \n    income_group \n  FROM \n    `bigquery-public-data.world_bank_wdi.country_summary`\n),\n\ngdp_data AS (\n  -- Filter data to only include GDP values\n  SELECT \n    data.country_code, \n    country,\n    region,\n    value AS gdp_value\n  FROM \n    `bigquery-public-data.world_bank_wdi.indicators_data` data\n  LEFT JOIN country_data\n    ON data.country_code = country_data.country_code\n  WHERE indicator_code = \"NY.GDP.MKTP.KD\" -- GDP Indicator\n    AND country_data.region IS NOT NULL\n    AND country_data.income_group IS NOT NULL\n),\n\ncal_median_gdp AS (\n  -- Calculate the median GDP value for each region\n  SELECT \n    region,\n    APPROX_QUANTILES(gdp_value, 2)[OFFSET(1)] AS median_gdp\n  FROM gdp_data\n  GROUP BY region\n)\n-- Select the regions with their median GDP values\nSELECT \n  region\nFROM \n  cal_median_gdp\nORDER BY median_gdp DESC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "1. **Define Country Data**: Create a temporary dataset that includes key descriptive data for countries such as their codes, names, regions, and income groups.\n  \n2. **Filter GDP Data**: Create another temporary dataset that joins the country data with a larger dataset containing various indicators. Focus specifically on GDP values, ensuring that only relevant records with specific GDP indicators and non-null region and income group information are included.\n\n3. **Calculate Median GDP**: For each region, compute the median GDP value using an approximation function to handle large datasets efficiently. Group the results by region to ensure each region's median GDP is calculated correctly.\n\n4. **Identify Region with Highest Median GDP**: From the dataset containing median GDP values per region, sort the regions by their median GDP in descending order and select the top region, which represents the region with the highest median GDP value.\n\nBy following these steps, the original user intention to find the region with the highest median GDP value is achieved systematically and efficiently.",
        "special_function": [
            "approximate-aggregate-functions/APPROX_QUANTILES",
            "other-functions/ARRAY_SUBSCRIPT",
            "other-functions/STRUCT_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "bq331",
        "db": "spider2-public-data.meta_kaggle",
        "question": "Who are the top three users whose forum message scores are closest to the average score, based on the absolute difference between their scores and the average score across all forum topics?",
        "SQL": "WITH AverageScoreCTE AS (\n    SELECT \n        AVG(RealScore.Score) AS AvgScore\n    FROM \n        `spider2-public-data.meta_kaggle.ForumTopics` AS ForumTopics\n    INNER JOIN `spider2-public-data.meta_kaggle.ForumMessages` AS ForumMessages\n        ON ForumMessages.Id = ForumTopics.FirstForumMessageId\n    INNER JOIN (\n        SELECT \n            ForumMessageId,\n            COUNT(DISTINCT FromUserId) AS Score\n        FROM \n            `spider2-public-data.meta_kaggle.ForumMessageVotes`\n        GROUP BY ForumMessageId\n    ) AS RealScore\n        ON RealScore.ForumMessageId = ForumTopics.FirstForumMessageId\n),\nUserScoresCTE AS (\n    SELECT \n        Users.UserName,\n        RealScore.Score,\n        ABS(RealScore.Score - AverageScoreCTE.AvgScore) AS ScoreDifference\n    FROM \n        `spider2-public-data.meta_kaggle.ForumTopics` AS ForumTopics\n    INNER JOIN `spider2-public-data.meta_kaggle.ForumMessages` AS ForumMessages\n        ON ForumMessages.Id = ForumTopics.FirstForumMessageId\n    INNER JOIN `spider2-public-data.meta_kaggle.Users` AS Users\n        ON Users.Id = ForumMessages.PostUserId\n    INNER JOIN (\n        SELECT \n            ForumMessageId,\n            COUNT(DISTINCT FromUserId) AS Score\n        FROM \n            `spider2-public-data.meta_kaggle.ForumMessageVotes`\n        GROUP BY ForumMessageId\n    ) AS RealScore\n        ON RealScore.ForumMessageId = ForumTopics.FirstForumMessageId,\n    AverageScoreCTE\n)\nSELECT \n    UserName,\n    ScoreDifference\nFROM \n    UserScoresCTE\nORDER BY \n    Score DESC\nLIMIT 3;",
        "external_knowledge": null,
        "plan": "1. **Calculate Average Upvotes**:\n   - Create a temporary table to calculate the average number of upvotes for all messages.\n   - Join the relevant tables to link messages and their upvotes.\n   - Compute the average upvotes using an aggregate function.\n\n2. **Calculate Upvotes for Each Message**:\n   - Create another temporary table to calculate the upvotes for each message.\n   - Use a subquery to count distinct upvotes for each message.\n   - Join the relevant tables to link messages with their upvotes.\n\n3. **Combine User and Upvotes Data**:\n   - Create a temporary table to combine user information with their message upvotes.\n   - Join the user table with the message and upvotes data.\n   - Also, join with the average upvotes temporary table to include the average upvotes in the result set.\n\n4. **Calculate Difference from Average**:\n   - For each user, calculate the absolute difference between their message's upvotes and the average upvotes.\n   - Include this difference as a new column in the temporary table.\n\n5. **Select Top 3 Users**:\n   - Sort the results based on the difference from average upvotes in ascending order.\n   - In case of a tie, sort further by username in ascending order.\n   - Limit the results to the top 3 users.\n\n6. **Output the Usernames**:\n   - Select and display only the usernames of the top 3 users from the sorted results.",
        "special_function": null
    },
    {
        "instance_id": "bq380",
        "db": "spider2-public-data.meta_kaggle",
        "question": "What are the usernames of the top three users with the most upvotes received in the Kaggle forum, along with the number of upvotes they received and the number of upvotes they gave to others?",
        "SQL": "WITH UserUpvotes AS (\n  SELECT\n    Users.UserName,\n    COUNT(DISTINCT ForumMessageVotes.Id) AS UpvotesReceived\n  FROM spider2-public-data.meta_kaggle.ForumMessageVotes AS ForumMessageVotes\n  INNER JOIN spider2-public-data.meta_kaggle.Users AS Users\n    ON Users.Id = ForumMessageVotes.ToUserId\n  GROUP BY Users.UserName\n),\nUserGivenUpvotes AS (\n  SELECT\n    Users.UserName,\n    COUNT(DISTINCT ForumMessageVotes.Id) AS UpvotesGiven\n  FROM spider2-public-data.meta_kaggle.ForumMessageVotes AS ForumMessageVotes\n  INNER JOIN spider2-public-data.meta_kaggle.Users AS Users\n    ON Users.Id = ForumMessageVotes.FromUserId\n  GROUP BY Users.UserName\n),\nMostUpvotedUser AS (\n  SELECT\n    COALESCE(up.UserName, ug.UserName) AS UserName,\n    COALESCE(up.UpvotesReceived, 0) AS UpvotesReceived,\n    COALESCE(ug.UpvotesGiven, 0) AS UpvotesGiven\n  FROM UserUpvotes AS up\n  FULL OUTER JOIN UserGivenUpvotes AS ug\n    ON up.UserName = ug.UserName\n)\nSELECT \n  UserName AS MostUpvotedUserName,\n  UpvotesReceived,\n  UpvotesGiven\nFROM MostUpvotedUser\nORDER BY UpvotesReceived DESC\nLIMIT 3;",
        "external_knowledge": null,
        "plan": "Top 20 Giver-and-Recipient of upvotes",
        "special_function": null
    },
    {
        "instance_id": "bq370",
        "db": "spider2-public-data.wide_world_importers",
        "question": "How many customers have an equal number of orders and invoices, and where the total value of their orders matches the total value of their invoices?",
        "SQL": "WITH CustomerOrderData AS (\n    SELECT OrderID,\n           InvoiceID,\n           CustomerID,\n           COUNT(CustomerOrderCost) AS TotalNbOrders,\n           SUM(CustomerOrderCost) AS OrdersTotalValue,\n           COUNT(CustomerOrderInvoice) AS TotalNbInvoices,\n           SUM(CustomerOrderInvoice) AS InvoicesTotalValue\n    FROM (\n           SELECT Orders.*,\n                  Invoices.CustomerOrderInvoice\n           FROM (\n                  SELECT o.OrderID,\n                         Inv.InvoiceID,\n                         ol.StockItemID,\n                         cu.CustomerID,\n                         ol.Quantity * ol.UnitPrice AS CustomerOrderCost\n                  FROM `spider2-public-data.wide_world_importers.sales_Customers` cu\n                  INNER JOIN `spider2-public-data.wide_world_importers.sales_Orders` o \n                          ON cu.CustomerID = o.CustomerID\n                  INNER JOIN `spider2-public-data.wide_world_importers.sales_OrderLines` ol \n                          ON o.OrderID = ol.OrderID\n                  INNER JOIN `spider2-public-data.wide_world_importers.sales_Invoices` Inv \n                          ON ol.OrderID = Inv.OrderID\n                ) Orders\n           INNER JOIN (\n                  SELECT Inv.OrderID,\n                         Invl.InvoiceID,\n                         Invl.StockItemID,\n                         Quantity * UnitPrice AS CustomerOrderInvoice\n                  FROM `spider2-public-data.wide_world_importers.sales_InvoiceLines` Invl\n                  INNER JOIN `spider2-public-data.wide_world_importers.sales_Invoices` Inv \n                          ON Invl.InvoiceID = Inv.InvoiceID\n                ) Invoices \n           ON Orders.OrderID = Invoices.OrderID \n           AND Orders.InvoiceID = Invoices.InvoiceID \n           AND Orders.StockItemID = Invoices.StockItemID\n         ) A\n    GROUP BY OrderID,\n             InvoiceID,\n             CustomerID\n),\nCustomerSummary AS (\n    SELECT CustomerID,\n           COUNT(TotalNbOrders) AS TotalNbOrders,\n           COUNT(TotalNbInvoices) AS TotalNbInvoices,\n           SUM(OrdersTotalValue) AS OrdersTotalValue,\n           SUM(InvoicesTotalValue) AS InvoicesTotalValue,\n           (SUM(OrdersTotalValue) - SUM(InvoicesTotalValue)) AS AbsoluteValueDifference\n    FROM CustomerOrderData\n    GROUP BY CustomerID\n)\nSELECT COUNT(*) AS NumberOfCustomersWithNoDifference\nFROM CustomerSummary\nWHERE TotalNbOrders = TotalNbInvoices\n  AND OrdersTotalValue = InvoicesTotalValue;",
        "external_knowledge": null,
        "plan": "1. **Data Preparation - Aggregating Orders and Invoices**:\n    - Combine data from the order and invoice tables, joining them based on common identifiers.\n    - For each order line, calculate the cost by multiplying quantity and unit price.\n    - For each invoice line, similarly calculate the invoice value.\n\n2. **First-Level Aggregation**:\n    - Group by individual order and invoice details along with customer identifiers.\n    - Calculate the total number of orders, total order value, total number of invoices, and total invoice value for each grouping.\n    \n3. **Intermediate Result - Customer Order Data**:\n    - Create a dataset that contains these aggregated metrics for each unique combination of order, invoice, and customer.\n\n4. **Second-Level Aggregation - Customer Summary**:\n    - Group the intermediate results by customer identifier.\n    - Summarize the total number of orders, total number of invoices, total order value, and total invoice value for each customer.\n    - Compute the absolute difference between the summed order values and the summed invoice values.\n\n5. **Final Filtering and Counting**:\n    - From the customer summary, filter out customers where the total number of orders equals the total number of invoices, and the total value of orders matches the total value of invoices.\n    - Count the number of such customers who meet these criteria.\n\n6. **Output the Result**:\n    - Return the count of customers who have an equal number of orders and invoices, with matching total values. This gives the final count of customers satisfying the conditions.",
        "special_function": null
    },
    {
        "instance_id": "bq371",
        "db": "spider2-public-data.wide_world_importers",
        "question": "What is the difference between the maximum and minimum average invoice values across the quarters in the year 2013?",
        "SQL": "WITH CustomerQuarterlyData AS (\n    SELECT \n           CUS.CustomerName,\n           COALESCE(INVL.unitprice * INVL.quantity, 0) AS InvoicesTotalValue,\n           CASE\n               WHEN EXTRACT(QUARTER FROM CAST(INV.invoicedate AS TIMESTAMP)) = 1 THEN 'Q1'\n               WHEN EXTRACT(QUARTER FROM CAST(INV.invoicedate AS TIMESTAMP)) = 2 THEN 'Q2'\n               WHEN EXTRACT(QUARTER FROM CAST(INV.invoicedate AS TIMESTAMP)) = 3 THEN 'Q3'\n               WHEN EXTRACT(QUARTER FROM CAST(INV.invoicedate AS TIMESTAMP)) = 4 THEN 'Q4'\n           END AS QuarterInvoiceDate\n    FROM \n         `spider2-public-data.wide_world_importers.sales_InvoiceLines` AS INVL\n    INNER JOIN `spider2-public-data.wide_world_importers.sales_Invoices` AS INV\n            ON INVL.InvoiceID = INV.InvoiceID\n    INNER JOIN `spider2-public-data.wide_world_importers.sales_Customers` AS CUS\n            ON CUS.CustomerID = INV.CustomerID\n    WHERE EXTRACT(YEAR FROM CAST(INV.invoicedate AS TIMESTAMP)) = 2013\n),\nQuarterlyAverages AS (\n    SELECT \n           QuarterInvoiceDate,\n           AVG(InvoicesTotalValue) AS AvgInvoiceValue\n    FROM \n         CustomerQuarterlyData\n    GROUP BY QuarterInvoiceDate\n),\nMaxMinAverages AS (\n    SELECT \n           MAX(AvgInvoiceValue) AS MaxAvgValue,\n           MIN(AvgInvoiceValue) AS MinAvgValue\n    FROM \n         QuarterlyAverages\n)\nSELECT \n     MaxAvgValue - MinAvgValue AS DifferenceBetweenMaxAndMinAvg\nFROM MaxMinAverages;",
        "external_knowledge": null,
        "plan": "1. **Define the Data Subset for Analysis**:\n   - Create a temporary dataset that includes customer names, total invoice values, and the corresponding quarter for each invoice within the year 2013.\n\n2. **Calculate Invoice Values**:\n   - Compute the total value for each invoice by multiplying the unit price by the quantity. If these values are missing, treat them as zero.\n\n3. **Determine Quarter for Each Invoice**:\n   - Extract the quarter from each invoice date and assign a label (Q1, Q2, Q3, Q4) based on the quarter number.\n\n4. **Filter by Year**:\n   - Ensure that only invoices from the year 2013 are included in the dataset.\n\n5. **Compute Average Invoice Values Per Quarter**:\n   - Group the data by the quarter and calculate the average invoice value for each quarter.\n\n6. **Find Maximum and Minimum Averages**:\n   - From the quarterly average invoice values, determine the maximum and minimum values.\n\n7. **Calculate the Difference**:\n   - Compute the difference between the maximum and minimum average invoice values.\n\n8. **Return the Result**:\n   - Output the calculated difference as the final result.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/TIMESTAMP",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE",
            "conditional-functions/COALESCE"
        ]
    },
    {
        "instance_id": "bq372",
        "db": "spider2-public-data.wide_world_importers",
        "question": "Which customer category has the maximum lost order value that is closest to the average maximum loss across all categories?",
        "SQL": "WITH MaxLossPerCategory AS (\n    -- Calculate the maximum loss for each category\n    SELECT \n           CustomerCategoryName,\n           MAX(OrderValueLost) AS MaxLoss\n    FROM (\n        SELECT \n               cc.CustomerCategoryName,\n               SUM(ol.Quantity * ol.UnitPrice) AS OrderValueLost\n        FROM \n               `spider2-public-data.wide_world_importers.sales_Customers` cu\n        INNER JOIN `spider2-public-data.wide_world_importers.sales_Orders` o\n            ON cu.CustomerID = o.CustomerID\n        INNER JOIN `spider2-public-data.wide_world_importers.sales_CustomerCategories` cc\n            ON cu.CustomerCategoryID = cc.CustomerCategoryID\n        INNER JOIN `spider2-public-data.wide_world_importers.sales_OrderLines` ol\n            ON o.OrderID = ol.OrderID\n        WHERE o.OrderID NOT IN (\n            SELECT \n                 I.OrderID\n            FROM \n                 `spider2-public-data.wide_world_importers.sales_Invoices` I\n        )\n        GROUP BY cc.CustomerCategoryName, cu.CustomerID\n    ) A\n    GROUP BY A.CustomerCategoryName\n),\nAvgMaxLoss AS (\n    -- Calculate the average of the maximum losses across all categories\n    SELECT \n         AVG(MaxLoss) AS AvgLoss\n    FROM \n         MaxLossPerCategory\n),\nClosestToAvg AS (\n    -- Find the category with the maximum loss closest to the average\n    SELECT \n           ml.CustomerCategoryName,\n           ABS(ml.MaxLoss - al.AvgLoss) AS DiffFromAvg\n    FROM \n           MaxLossPerCategory ml\n    CROSS JOIN AvgMaxLoss al\n    ORDER BY DiffFromAvg ASC\n    LIMIT 1\n)\n-- Final selection of the category name\nSELECT \n     CustomerCategoryName\nFROM \n     ClosestToAvg;",
        "external_knowledge": null,
        "plan": "1. **Calculate Maximum Loss per Category:**\n    - Identify the maximum order value lost for each customer category.\n    - For each customer, compute the total order value lost by summing up the product of quantity and unit price of order lines for orders that were not invoiced.\n    - Group the results by customer category and customer, then find the maximum loss for each category by grouping the results again by customer category.\n\n2. **Compute Average Maximum Loss:**\n    - Calculate the average value of the maximum losses obtained for each customer category in the previous step.\n\n3. **Identify Closest Category to Average:**\n    - For each customer category, determine the absolute difference between its maximum loss and the calculated average maximum loss.\n    - Sort the results based on the absolute difference in ascending order and select the top result.\n\n4. **Select Final Category:**\n    - Extract the customer category name that has the maximum loss closest to the average maximum loss from the sorted results.",
        "special_function": [
            "mathematical-functions/ABS"
        ]
    },
    {
        "instance_id": "bq373",
        "db": "spider2-public-data.wide_world_importers",
        "question": "What's the median of the average monthly spending across all customers for the year 2014?",
        "SQL": "WITH MonthlyAverageSpending AS (\n    SELECT \n        AVG(Jan + Feb + Mar + Apr + May + Jun + Jul + Aug + Sep + Oct + Nov + Dec) AS AvgMonthlySpending\n    FROM (\n        SELECT \n            CustomerName,\n            IFNULL(SUM(CASE WHEN MonthInvoiceDate = 'Jan' THEN InvoicesTotalValue ELSE 0.00 END), 0.00) AS Jan,\n            IFNULL(SUM(CASE WHEN MonthInvoiceDate = 'Feb' THEN InvoicesTotalValue ELSE 0.00 END), 0.00) AS Feb,\n            IFNULL(SUM(CASE WHEN MonthInvoiceDate = 'Mar' THEN InvoicesTotalValue ELSE 0.00 END), 0.00) AS Mar,\n            IFNULL(SUM(CASE WHEN MonthInvoiceDate = 'Apr' THEN InvoicesTotalValue ELSE 0.00 END), 0.00) AS Apr,\n            IFNULL(SUM(CASE WHEN MonthInvoiceDate = 'May' THEN InvoicesTotalValue ELSE 0.00 END), 0.00) AS May,\n            IFNULL(SUM(CASE WHEN MonthInvoiceDate = 'Jun' THEN InvoicesTotalValue ELSE 0.00 END), 0.00) AS Jun,\n            IFNULL(SUM(CASE WHEN MonthInvoiceDate = 'Jul' THEN InvoicesTotalValue ELSE 0.00 END), 0.00) AS Jul,\n            IFNULL(SUM(CASE WHEN MonthInvoiceDate = 'Aug' THEN InvoicesTotalValue ELSE 0.00 END), 0.00) AS Aug,\n            IFNULL(SUM(CASE WHEN MonthInvoiceDate = 'Sep' THEN InvoicesTotalValue ELSE 0.00 END), 0.00) AS Sep,\n            IFNULL(SUM(CASE WHEN MonthInvoiceDate = 'Oct' THEN InvoicesTotalValue ELSE 0.00 END), 0.00) AS Oct,\n            IFNULL(SUM(CASE WHEN MonthInvoiceDate = 'Nov' THEN InvoicesTotalValue ELSE 0.00 END), 0.00) AS Nov,\n            IFNULL(SUM(CASE WHEN MonthInvoiceDate = 'Dec' THEN InvoicesTotalValue ELSE 0.00 END), 0.00) AS Dec\n        FROM (\n            SELECT \n                cu.CustomerName,\n                COALESCE(invl.UnitPrice * invl.Quantity, 0) AS InvoicesTotalValue,\n                FORMAT_DATE('%b', DATE(inv.InvoiceDate)) AS MonthInvoiceDate\n            FROM \n                `spider2-public-data.wide_world_importers.sales_InvoiceLines` invl\n            INNER JOIN \n                `spider2-public-data.wide_world_importers.sales_Invoices` inv\n                ON invl.InvoiceID = inv.InvoiceID\n            INNER JOIN \n                `spider2-public-data.wide_world_importers.sales_Customers` cu\n                ON cu.CustomerID = inv.CustomerID\n            WHERE \n                EXTRACT(YEAR FROM CAST(INV.invoicedate AS TIMESTAMP)) = 2014\n        ) AS SourceTable\n        GROUP BY \n            CustomerName\n    ) AS MonthlySpend\n    GROUP BY \n        CustomerName\n),\nMedianSpending AS (\n    SELECT \n        PERCENTILE_CONT(AvgMonthlySpending, 0.5) OVER() AS MedianOfAvgMonthlySpending\n    FROM \n        MonthlyAverageSpending\n)\n-- Final output of the median spending\nSELECT DISTINCT MedianOfAvgMonthlySpending\nFROM MedianSpending;",
        "external_knowledge": null,
        "plan": "1. **Calculate Monthly Totals for Each Customer:**\n   - Select customer names and the total invoice values for each month of the year 2014.\n   - Use conditional aggregation to sum invoice values for each month (January to December).\n   - Handle missing values with a default of 0.00.\n\n2. **Compute Average Monthly Spending:**\n   - For each customer, compute the average monthly spending by averaging the sums of the monthly totals.\n   - This results in a single average monthly spending value for each customer.\n\n3. **Determine the Median of Average Monthly Spendings:**\n   - Use a window function to calculate the median of the average monthly spending values obtained in the previous step.\n   - The `PERCENTILE_CONT` function is used to find the 50th percentile (median).\n\n4. **Retrieve the Median Value:**\n   - Select the distinct median value from the results.\n\nThis process ensures that the final output is the median of the average monthly spending across all customers for the specified year.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "date-functions/FORMAT_DATE",
            "datetime-functions/EXTRACT",
            "differentially-private-aggregate-functions/PERCENTILE_CONT",
            "interval-functions/EXTRACT",
            "navigation-functions/PERCENTILE_CONT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/TIMESTAMP",
            "conditional-functions/CASE",
            "conditional-functions/COALESCE",
            "conditional-functions/IFNULL"
        ]
    },
    {
        "instance_id": "bq393",
        "db": "bigquery-public-data.hacker_news",
        "question": "Can you tell me the ID and corresponding month number of the user with the highest month number who became inactive after their last recorded activity month, considering data only up until September 10, 2024?",
        "SQL": "WITH\n  all_months AS (\n    SELECT DISTINCT\n      timestamp_trunc(timestamp, month) AS month\n    FROM\n      `bigquery-public-data.hacker_news.full`\n    WHERE\n      timestamp <= '2024-09-10'  -- Date limit added\n  ),\n  \n  active_months AS (\n    SELECT DISTINCT\n      `by` AS userid,\n      timestamp_trunc(timestamp, month) AS month\n    FROM\n      `bigquery-public-data.hacker_news.full`\n    WHERE\n      timestamp <= '2024-09-10'  -- Date limit added\n  ),\n  \n  users AS (\n    SELECT\n      `by` AS userid,\n      MIN(timestamp_trunc(timestamp, month)) AS first_month,\n      MAX(timestamp_trunc(timestamp, month)) AS last_month\n    FROM\n      `bigquery-public-data.hacker_news.full`\n    WHERE\n      timestamp <= '2024-09-10'  -- Date limit added\n    GROUP BY\n      userid\n  ),\n  \n  user_activity AS (\n    SELECT\n      u.userid,\n      m.month,\n      CAST(u.first_month AS STRING) AS cohort_month_str,\n      DATE_DIFF(DATE(m.month), DATE(u.first_month), MONTH) AS month_number,\n      IF(m.month <= u.last_month, 1.0, 0.0) AS unbounded_active,\n      IF(a.month IS NOT NULL, 1.0, 0.0) AS active\n    FROM\n      users AS u\n    CROSS JOIN\n      all_months AS m\n    LEFT JOIN\n      active_months AS a\n    ON\n      a.userid = u.userid AND a.month = m.month\n    WHERE\n      m.month >= u.first_month\n      AND m.month <= '2024-09-10'  -- Date limit added\n  ),\n  \n  unbounded_active_users AS (\n    SELECT\n      userid,\n      month_number\n    FROM\n      user_activity\n    WHERE\n      unbounded_active = 1.0\n      AND active = 0.0\n  ),\n  \n  max_month_number AS (\n    SELECT\n      MAX(month_number) AS max_month_number\n    FROM\n      unbounded_active_users\n  )\n\nSELECT\n  u.userid,\n  u.month_number\nFROM\n  unbounded_active_users AS u\nJOIN\n  max_month_number AS m\nON\n  u.month_number = m.max_month_number;",
        "external_knowledge": null,
        "plan": "1. **Identify Unique Months**:\n   - Create a list of unique months by truncating the timestamps to the month level for all records up to the specified date.\n\n2. **Identify User Active Months**:\n   - Generate a list of unique user IDs and their corresponding active months by truncating the timestamps to the month level for all records up to the specified date.\n\n3. **Determine First and Last Active Month for Each User**:\n   - For each user, calculate their first and last active months by truncating the timestamps to the month level, considering only records up to the specified date.\n\n4. **Generate User Activity Timeline**:\n   - For each user, create a timeline of months from their first recorded month to the specified date. \n   - For each month in the timeline, determine if the user was active during that month.\n   - Calculate the month number relative to the user's first active month.\n\n5. **Identify Inactive Users After Last Active Month**:\n   - Filter the user activity timeline to find users who are inactive (i.e., have no recorded activity) after their last active month.\n\n6. **Find the Maximum Month Number of Inactive Users**:\n   - Determine the highest month number from the filtered list of inactive users.\n\n7. **Retrieve User ID and Month Number**:\n   - Select the user ID and the corresponding highest month number for the user who has the maximum month number of inactivity after their last recorded activity month.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "date-functions/DATE_DIFF",
            "json-functions/STRING",
            "timestamp-functions/STRING",
            "timestamp-functions/TIMESTAMP",
            "timestamp-functions/TIMESTAMP_TRUNC",
            "conditional-functions/IF"
        ]
    },
    {
        "instance_id": "bq403",
        "db": "bigquery-public-data.irs_990",
        "question": "Which three years in 2012-2017 have the smallest absolute difference between median revenue and median functional expenses for organizations filing IRS 990 forms? Please output three years and respective differences.",
        "SQL": "WITH RankedData AS (\n    SELECT\n        CONCAT(\"20\", _TABLE_SUFFIX) AS year_filed,\n        totrevenue,\n        totfuncexpns,\n        ROW_NUMBER() OVER (PARTITION BY CONCAT(\"20\", _TABLE_SUFFIX) ORDER BY totrevenue) \n        AS revenue_rank,\n        ROW_NUMBER() OVER (PARTITION BY CONCAT(\"20\", _TABLE_SUFFIX) ORDER BY totfuncexpns) \n        AS expense_rank,\n        COUNT(*) OVER (PARTITION BY CONCAT(\"20\", _TABLE_SUFFIX)) AS total_count\n    FROM \n        `bigquery-public-data.irs_990.irs_990_20*`\n),\n\nYearlyMedians AS (\n    SELECT\n        year_filed,\n        IF(MOD(total_count, 2) = 1, \n           MAX(CASE WHEN revenue_rank = (total_count + 1) / 2 THEN totrevenue END),\n           AVG(CASE WHEN revenue_rank IN ((total_count / 2), (total_count / 2) + 1) THEN totrevenue END)\n        ) AS median_revenue,\n        IF(MOD(total_count, 2) = 1, \n           MAX(CASE WHEN expense_rank = (total_count + 1) / 2 THEN totfuncexpns END),\n           AVG(CASE WHEN expense_rank IN ((total_count / 2), (total_count / 2) + 1) THEN totfuncexpns END)\n        ) AS median_expense\n    FROM\n        RankedData\n    GROUP BY\n        year_filed, total_count\n),\n\nDifferenceCalculations AS (\n    SELECT\n        year_filed,\n        median_revenue,\n        median_expense,\n        ABS(median_revenue - median_expense) AS difference\n    FROM\n        YearlyMedians\n)\n\nSELECT\n    year_filed,\n    difference\nFROM\n    DifferenceCalculations\nWHERE\n    year_filed BETWEEN '2012' AND '2017'\nORDER BY\n    difference ASC\nLIMIT 3;",
        "external_knowledge": null,
        "plan": "1. **Initial Data Preparation**:\n    - Select data for each year within a specific range.\n    - Calculate ranks for revenue and expenses within each year.\n    - Count the total number of records for each year.\n\n2. **Calculate Yearly Medians**:\n    - For each year, determine if the total number of records is odd or even.\n    - If odd, find the middle value for revenue and expenses.\n    - If even, compute the average of the two middle values for revenue and expenses.\n\n3. **Compute Differences**:\n    - Calculate the absolute difference between the median revenue and median expenses for each year.\n\n4. **Filter and Sort Results**:\n    - Select the relevant years within the specified range.\n    - Order the results by the calculated differences in ascending order.\n    - Limit the output to the top three years with the smallest differences.\n\n5. **Output**:\n    - Display the years and their respective differences.",
        "special_function": null
    },
    {
        "instance_id": "bq397",
        "db": "data-to-insights.ecommerce",
        "question": "Identify the country with the highest total transactions within each channel grouping, provided that the channel includes transactions from more than one country. What is the transaction total for that country?",
        "SQL": "WITH tmp AS (\n  SELECT DISTINCT *\n  FROM `data-to-insights.ecommerce.rev_transactions`\n  -- Removing duplicated values\n),\ntmp1 AS (\n  SELECT \n    tmp.channelGrouping,\n    tmp.geoNetwork_country,\n    SUM(tmp.totals_transactions) AS tt\n  FROM tmp\n  GROUP BY 1, 2\n),\ntmp2 AS (\n  SELECT \n    channelGrouping,\n    geoNetwork_country,\n    SUM(tt) AS TotalTransaction,\n    COUNT(DISTINCT geoNetwork_country) OVER (PARTITION BY channelGrouping) AS CountryCount\n  FROM tmp1\n  GROUP BY channelGrouping, geoNetwork_country\n),\ntmp3 AS (\n  SELECT\n    channelGrouping,\n    geoNetwork_country AS Country,\n    TotalTransaction,\n    RANK() OVER (PARTITION BY channelGrouping ORDER BY TotalTransaction DESC) AS rnk\n  FROM tmp2\n  WHERE CountryCount > 1\n)\nSELECT\n  channelGrouping,\n  Country,\n  TotalTransaction\nFROM tmp3\nWHERE rnk = 1;",
        "external_knowledge": null,
        "plan": "1. **Remove Duplicate Records**:\n   - Create a temporary table to store unique records by removing any duplicate entries from the original dataset.\n\n2. **Aggregate Transactions by Channel and Country**:\n   - Create another temporary table that groups the unique records by channel and country.\n   - For each group, calculate the total number of transactions.\n\n3. **Calculate Total Transactions and Country Count per Channel**:\n   - Create a third temporary table that further aggregates the data by channel.\n   - Calculate the total transactions for each country within each channel.\n   - Count the number of distinct countries involved in each channel.\n\n4. **Rank Countries by Total Transactions per Channel**:\n   - Create a fourth temporary table that ranks the countries within each channel based on their total transactions.\n   - Only include channels where more than one country is involved.\n\n5. **Select Top-Ranked Countries per Channel**:\n   - From the ranked temporary table, select the top-ranked country for each channel.\n   - Retrieve the channel, the top country, and the total number of transactions for that country.\n\nThis step-by-step process ensures that for each channel, the country with the highest total transactions is identified, provided that the channel involves multiple countries.",
        "special_function": null
    },
    {
        "instance_id": "bq402",
        "db": "data-to-insights.ecommerce",
        "question": "What is the conversion rate from unique visitors to purchasers, where purchasers are defined as visitors with at least one transaction? Additionally, what is the average number of transactions per purchaser?",
        "SQL": "WITH visitors AS (\n  SELECT\n    COUNT(DISTINCT fullVisitorId) AS total_visitors\n  FROM \n    `data-to-insights.ecommerce.web_analytics`\n),\n\npurchasers AS (\n  SELECT\n    COUNT(DISTINCT fullVisitorId) AS total_purchasers\n  FROM \n    `data-to-insights.ecommerce.web_analytics`\n  WHERE \n    totals.transactions IS NOT NULL\n),\n\ntransactions AS (\n  SELECT\n    COUNT(*) AS total_transactions,\n    AVG(totals.transactions) AS avg_transactions_per_purchaser\n  FROM \n    `data-to-insights.ecommerce.web_analytics`\n  WHERE \n    totals.transactions IS NOT NULL\n)\n\nSELECT\n  p.total_purchasers / v.total_visitors AS conversion_rate,\n  a.avg_transactions_per_purchaser AS avg_transactions_per_purchaser\nFROM\n  visitors v,\n  purchasers p,\n  transactions a;",
        "external_knowledge": null,
        "plan": "1. **Identify Total Visitors:**\n   - Create a temporary result set to count the distinct number of unique visitors in the dataset.\n\n2. **Identify Total Purchasers:**\n   - Create a temporary result set to count the distinct number of unique visitors who have made at least one purchase. This is determined by checking for non-null values in the transactions data.\n\n3. **Calculate Transactions Metrics:**\n   - Create a temporary result set to calculate two key metrics:\n     - The total number of transactions.\n     - The average number of transactions per purchaser, considering only visitors who have made transactions.\n\n4. **Calculate Conversion Rate:**\n   - Using the results from the first two temporary result sets, calculate the conversion rate by dividing the number of purchasers by the total number of visitors.\n\n5. **Select and Format Results:**\n   - Retrieve the calculated conversion rate.\n   - Retrieve the average number of transactions per purchaser, rounding this value to two decimal places for clarity.\n\n6. **Combine Results:**\n   - Combine the results from the previous steps into a single result set to produce the final output: the conversion rate and the average number of transactions per purchaser.",
        "special_function": null
    },
    {
        "instance_id": "bq160",
        "db": "spider2-public-data.meta_kaggle",
        "question": "Please tell me the creation date, title, parent forum, reply count, distinct user count, upvotes, and total views for the earliest five forum topics belonging to the parent forum named \"general\". Any value that is None should be regarded as 0.",
        "SQL": "WITH ForumMessagesCount AS (\n  SELECT \n    ForumTopicId,\n    COUNT(DISTINCT Id) AS ForumMessagesCount,\n    COUNT(DISTINCT PostUserId) AS ForumMessageUserRepliesCount\n  FROM `spider2-public-data.meta_kaggle.ForumMessages`\n  GROUP BY ForumTopicId\n),\nForumTopicsUpvotes AS (\n  SELECT \n    ForumTopicId,\n    SUM(Instances) AS TopicsUpvotes\n  FROM (\n    SELECT \n      innerfms.ForumTopicId,\n      COUNT(DISTINCT innerfmv.Id) AS Instances\n    FROM `spider2-public-data.meta_kaggle.ForumMessages` AS innerfms\n    INNER JOIN `spider2-public-data.meta_kaggle.ForumMessageVotes` AS innerfmv\n      ON innerfmv.ForumMessageId = innerfms.Id\n    GROUP BY innerfms.ForumTopicId\n  ) AS main\n  GROUP BY ForumTopicId\n)\nSELECT \n  ft.CreationDate AS date,\n  ft.Title AS ForumTopicTitle,\n  pfm.Title AS ParentForumTitle,\n  COALESCE(fmc.ForumMessagesCount, 0) AS ForumTopicRepliesCount,\n  COALESCE(fmc.ForumMessageUserRepliesCount, 0) AS DistinctUserRepliesCount,\n  COALESCE(ftu.TopicsUpvotes, 0) AS TopicUpvotes,\n  ft.TotalViews\nFROM `spider2-public-data.meta_kaggle.ForumTopics` AS ft\nLEFT JOIN `spider2-public-data.meta_kaggle.Forums` AS fm\n  ON fm.Id = ft.ForumId\nLEFT JOIN `spider2-public-data.meta_kaggle.Forums` AS pfm\n  ON pfm.Id = fm.ParentForumId\nLEFT JOIN ForumMessagesCount AS fmc\n  ON fmc.ForumTopicId = ft.Id\nLEFT JOIN ForumTopicsUpvotes AS ftu\n  ON ftu.ForumTopicId = ft.Id\nWHERE pfm.Title = 'General'\nORDER BY ft.CreationDate\nLIMIT 5;",
        "external_knowledge": null,
        "plan": "1. **Aggregate Forum Messages Data**:\n   - Create a temporary result set that counts the total messages and distinct user replies for each forum topic.\n   - Group the data by the forum topic identifier.\n\n2. **Aggregate Forum Message Votes**:\n   - Create another temporary result set that counts the upvotes for each forum topic.\n   - This involves joining forum messages with their corresponding votes and grouping by the forum topic identifier.\n\n3. **Main Query Setup**:\n   - Select the relevant fields: creation date, title, parent forum title, reply count, distinct user count, upvotes, and total views from the main forum topics table.\n\n4. **Join with Parent Forum**:\n   - Perform a left join to associate each forum topic with its parent forum to retrieve the parent forum title.\n\n5. **Join with Aggregated Messages Data**:\n   - Perform a left join with the temporary result set containing the count of messages and distinct user replies to include these counts for each forum topic.\n\n6. **Join with Aggregated Votes Data**:\n   - Perform a left join with the temporary result set containing the count of upvotes to include these counts for each forum topic.\n\n7. **Filter by Parent Forum Title**:\n   - Filter the results to include only those forum topics that belong to the parent forum named \"general\".\n\n8. **Handle Null Values**:\n   - Use a function to replace any null values in the reply count, distinct user count, and upvotes fields with 0.\n\n9. **Order and Limit Results**:\n   - Order the results by the creation date to get the earliest topics.\n   - Limit the result set to the first five entries to get the earliest five forum topics.\n\n10. **Output the Final Results**:\n    - Select and display the final set of fields as requested in the instruction.",
        "special_function": null
    },
    {
        "instance_id": "bq167",
        "db": "spider2-public-data.meta_kaggle",
        "question": "Please find the giver-and-recipient pair with the most Kaggle forum upvotes. Display their usernames and the respective number of upvotes they gave to each other.",
        "SQL": "WITH UserPairUpvotes AS (\n  SELECT\n    ToUsers.UserName AS ToUserName,\n    FromUsers.UserName AS FromUserName,\n    COUNT(DISTINCT ForumMessageVotes.Id) AS UpvoteCount\n  FROM `spider2-public-data.meta_kaggle.ForumMessageVotes` AS ForumMessageVotes\n  INNER JOIN `spider2-public-data.meta_kaggle.Users` AS FromUsers\n    ON FromUsers.Id = ForumMessageVotes.FromUserId\n  INNER JOIN `spider2-public-data.meta_kaggle.Users` AS ToUsers\n    ON ToUsers.Id = ForumMessageVotes.ToUserId\n  GROUP BY\n    ToUsers.UserName,\n    FromUsers.UserName\n),\nTopPairs AS (\n  SELECT\n    ToUserName,\n    FromUserName,\n    UpvoteCount,\n    ROW_NUMBER() OVER (ORDER BY UpvoteCount DESC) AS Rank\n  FROM UserPairUpvotes\n),\nReciprocalUpvotes AS (\n  SELECT\n    t.ToUserName,\n    t.FromUserName,\n    t.UpvoteCount AS UpvotesReceived,\n    COALESCE(u.UpvoteCount, 0) AS UpvotesGiven\n  FROM TopPairs t\n  LEFT JOIN UserPairUpvotes u\n    ON t.ToUserName = u.FromUserName AND t.FromUserName = u.ToUserName\n  WHERE t.Rank = 1\n)\nSELECT\n  ToUserName AS UpvotedUserName,\n  FromUserName AS UpvotingUserName,\n  UpvotesReceived AS UpvotesReceivedByUpvotedUser,\n  UpvotesGiven AS UpvotesGivenByUpvotedUser\nFROM ReciprocalUpvotes\nORDER BY UpvotesReceived DESC, UpvotesGiven DESC;",
        "external_knowledge": null,
        "plan": "1. **Identify User Pair Upvotes:**\n   - Create a temporary result set that includes pairs of users where one user has upvoted the other.\n   - For each pair, count the distinct number of upvotes given by one user to the other.\n\n2. **Rank User Pairs by Upvotes:**\n   - Generate a ranking for each user pair based on the number of upvotes they received, in descending order.\n\n3. **Match Reciprocal Upvotes:**\n   - For the top-ranked user pair (the one with the most upvotes), find if there are reciprocal upvotes from the second user back to the first user.\n   - Use a left join to include pairs where the reciprocal upvote count might be zero.\n\n4. **Select and Display Results:**\n   - Select the usernames of the top-ranked pair and display the number of upvotes received by the upvoted user and the number of upvotes given by the upvoted user.\n   - Order the final results by the number of upvotes received and then by the number of upvotes given, both in descending order.",
        "special_function": null
    },
    {
        "instance_id": "bq171",
        "db": "spider2-public-data.meta_kaggle",
        "question": "Whose Forum message upvotes are closest to the average in 2019? If there\u2019s a tie, tell me the one with the alphabetically first username.",
        "SQL": "WITH UserStats2019 AS (\n    SELECT \n        Users.UserName,\n        COUNT(DISTINCT ForumMessageVotes.Id) AS Upvote\n    FROM \n        `spider2-public-data.meta_kaggle.ForumMessageVotes` AS ForumMessageVotes\n    INNER JOIN `spider2-public-data.meta_kaggle.Users` AS Users\n        ON Users.Id = ForumMessageVotes.FromUserId\n    WHERE \n        EXTRACT(YEAR FROM VoteDate) = 2019\n    GROUP BY \n        Users.UserName\n),\nAverageUpvotes2019 AS (\n    SELECT \n        AVG(Upvote) AS AvgUpvote2019\n    FROM UserStats2019\n),\nUserClosestToAverage AS (\n    SELECT \n        UserStats2019.UserName,\n        ABS(UserStats2019.Upvote - AverageUpvotes2019.AvgUpvote2019) AS UpvoteDifference\n    FROM \n        UserStats2019, AverageUpvotes2019\n)\nSELECT \n    UserName\nFROM \n    UserClosestToAverage\nORDER BY \n    UpvoteDifference ASC,\n    UserName ASC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "1. **Filter Data by Year**:\n   - Extract and consider only the records from the specific year (2019).\n\n2. **Count Upvotes Per User**:\n   - Group the records by user and count the distinct upvotes each user received within the specified year.\n\n3. **Calculate Average Upvotes**:\n   - Compute the average number of upvotes across all users for the specified year.\n\n4. **Determine Upvote Difference**:\n   - For each user, calculate the absolute difference between their upvote count and the computed average upvotes.\n\n5. **Sort and Select User**:\n   - Sort the users first by the smallest difference in upvotes, then alphabetically by username in case of ties.\n   - Select the first user from the sorted list, which will be the one with upvotes closest to the average, and if tied, the alphabetically first username.",
        "special_function": null
    },
    {
        "instance_id": "bq118",
        "db": "spider2-public-data.death",
        "question": "How much higher is the average number of white people dying from discharges (excluding urethral discharge, firework discharge, and legal intervention involving firearm discharge) compared to vehicle-related incidents averaged across different ages?",
        "SQL": "WITH WhiteRace AS (\n    SELECT CAST(Code AS INT64)\n    FROM `spider2-public-data.death.Race` \n    WHERE LOWER(Description) LIKE '%white%'\n),\nVehicleDeaths AS (\n    SELECT \n        d.Age,\n        SUM(CASE WHEN d.Race IN (SELECT * FROM WhiteRace) THEN 1 ELSE 0 END) AS Vehicle_White\n    FROM `spider2-public-data.death.DeathRecords` d\n    JOIN `spider2-public-data.death.EntityAxisConditions` e ON d.id = e.DeathRecordId\n    JOIN `spider2-public-data.death.Icd10Code` c ON e.Icd10Code = c.code\n    WHERE LOWER(c.Description) LIKE '%vehicle%'\n    GROUP BY d.Age\n),\nGunDeaths AS (\n    SELECT \n        d.Age,\n        SUM(CASE WHEN d.Race IN (SELECT * FROM WhiteRace) THEN 1 ELSE 0 END) AS Gun_White\n    FROM `spider2-public-data.death.DeathRecords` d\n    JOIN `spider2-public-data.death.EntityAxisConditions` e ON d.id = e.DeathRecordId\n    JOIN `spider2-public-data.death.Icd10Code` c ON e.Icd10Code = c.code\n    WHERE LOWER(c.Description) LIKE '%discharge%'\n      AND c.Description NOT IN (\n          'Urethral discharge', \n          'Discharge of firework', \n          'Legal intervention involving firearm discharge'\n      )\n    GROUP BY d.Age\n)\nSELECT \n    (SELECT AVG(Gun_White) FROM GunDeaths) \n    - \n    (SELECT AVG(Vehicle_White) FROM VehicleDeaths)",
        "external_knowledge": null,
        "plan": "1. **Identify Relevant Race Codes**:\n   - Retrieve and convert the race codes associated with \"white\" people from a specific dataset.\n\n2. **Calculate Vehicle-Related Deaths**:\n   - For each age group, count the number of deaths where the race is identified as \"white\" and the cause of death is related to vehicles.\n\n3. **Calculate Gun-Related Deaths (Excluding Specific Causes)**:\n   - For each age group, count the number of deaths where the race is identified as \"white\" and the cause of death is related to discharges, excluding urethral discharge, firework discharge, and legal intervention involving firearm discharge.\n\n4. **Compute Averages**:\n   - Calculate the average number of \"white\" people dying from vehicle-related incidents across all age groups.\n   - Calculate the average number of \"white\" people dying from the specified types of discharges across all age groups.\n\n5. **Determine Difference**:\n   - Subtract the average number of vehicle-related deaths from the average number of specified discharge-related deaths to find out how much higher the latter is compared to the former.",
        "special_function": null
    },
    {
        "instance_id": "bq072",
        "db": "spider2-public-data.death",
        "question": "Please tell me the total and Black deaths due to vehicle-related incidents and firearms separately, for each age from 12 to 18.",
        "SQL": "WITH BlackRace AS (\n    SELECT CAST(Code AS INT64)\n    FROM `spider2-public-data.death.Race` \n    WHERE LOWER(Description) LIKE '%black%'\n)\nselect \n    v.Age, v.Total as Vehicle_Total, v.Black as Vehicle_Black,\n    g.Total as Gun_Total, g.Black as Gun_Black\nfrom (\n  select \n      Age, count(*) as Total, COUNTIF(Race IN (SELECT * FROM BlackRace)) as Black\n  from `spider2-public-data.death.DeathRecords` d,\n     (\n     select \n       distinct e.DeathRecordId as id \n       from \n         `spider2-public-data.death.EntityAxisConditions` e,\n         (\n            select * \n            from `spider2-public-data.death.Icd10Code` where LOWER(Description) like '%vehicle%'\n         ) as c \n        where e.Icd10Code = c.code\n     ) as f\n    where d.id = f.id and Age between 12 and 18\n    group by Age\n) as v, -- Vehicle\n\n(\n  select \n      Age, count(*) as Total, COUNTIF(Race IN (SELECT * FROM BlackRace)) as Black\n  from `spider2-public-data.death.DeathRecords` d,\n      (select \n           distinct e.DeathRecordId as id \n           from \n              `spider2-public-data.death.EntityAxisConditions` e,\n              (\n                -- Every Firearm discharge, except Legal intervention\n                select \n                   Code, Description \n                   from `spider2-public-data.death.Icd10Code`\n                   where Description like '%firearm%' \n              ) as c \n          where e.Icd10Code = c.code\n      ) as f\n  where d.id = f.id and Age between 12 and 18\n  group by Age\n) as g\nwhere g.Age = v.Age;",
        "external_knowledge": null,
        "plan": "1. **Filter Black Race Codes**:\n   - Extract codes corresponding to the Black race by filtering descriptions containing the word \"black\".\n\n2. **Prepare Vehicle-Related Incidents Data**:\n   - Create a subquery to identify records with vehicle-related incidents by matching against descriptions containing the word \"vehicle\".\n   - Join the death records with these identified records.\n   - Filter the data to include only individuals aged between 12 and 18.\n   - Calculate the total number of deaths and the number of deaths where the race matches the previously identified Black race codes.\n   - Group the results by age.\n\n3. **Prepare Firearm-Related Incidents Data**:\n   - Create a subquery to identify records with firearm-related incidents by matching against descriptions containing the word \"firearm\".\n   - Join the death records with these identified records.\n   - Filter the data to include only individuals aged between 12 and 18.\n   - Calculate the total number of deaths and the number of deaths where the race matches the previously identified Black race codes.\n   - Group the results by age.\n\n4. **Combine Results**:\n   - Perform a join on the age field between the vehicle-related incidents data and the firearm-related incidents data.\n   - Select the age, total deaths due to vehicle-related incidents, Black deaths due to vehicle-related incidents, total deaths due to firearm-related incidents, and Black deaths due to firearm-related incidents for each age group from 12 to 18.",
        "special_function": null
    },
    {
        "instance_id": "ga001",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "I want to know the preferences of customers who purchased the Google Navy Speckled Tee in December 2020. What other product was purchased with the highest total quantity alongside this item?",
        "SQL": "WITH\n  Params AS (\n    SELECT 'Google Navy Speckled Tee' AS selected_product\n  ),\n  PurchaseEvents AS (\n    SELECT\n      user_pseudo_id,\n      items\n    FROM\n      `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\n    WHERE\n      _TABLE_SUFFIX BETWEEN '20201201' AND '20201231'\n      AND event_name = 'purchase'\n  ),\n  ProductABuyers AS (\n    SELECT DISTINCT\n      user_pseudo_id\n    FROM\n      Params,\n      PurchaseEvents,\n      UNNEST(items) AS items\n    WHERE\n      items.item_name = selected_product\n  )\nSELECT\n  items.item_name AS item_name,\n  SUM(items.quantity) AS item_quantity\nFROM\n  Params,\n  PurchaseEvents,\n  UNNEST(items) AS items\nWHERE\n  user_pseudo_id IN (SELECT user_pseudo_id FROM ProductABuyers)\n  AND items.item_name != selected_product\nGROUP BY 1\nORDER BY item_quantity DESC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "1. Focus on the item named \"Google Navy Speckled Tee.\"\n2. Select all purchase-type events from December 2020.\n3. Extract the IDs of individuals who purchased the \"Google Navy Speckled Tee\" during these events.\n4. Calculate all items purchased by these IDs and retain the top 10 items by purchase volume.",
        "special_function": [
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "ga002",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Tell me the most purchased other products and their quantities by customers who bought the Google Red Speckled Tee each month for the three months starting from November 2020.",
        "SQL": "WITH\nParams AS (\n  SELECT 'Google Red Speckled Tee' AS selected_product\n),\nDateRanges AS (\n  SELECT '20201101' AS start_date, '20201130' AS end_date, '202011' AS period UNION ALL\n  SELECT '20201201', '20201231', '202012' UNION ALL\n  SELECT '20210101', '20210131', '202101'\n),\nPurchaseEvents AS (\n  SELECT\n    period,\n    user_pseudo_id,\n    items\n  FROM\n    DateRanges\n  JOIN\n    `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\n    ON _TABLE_SUFFIX BETWEEN start_date AND end_date\n  WHERE\n    event_name = 'purchase'\n),\nProductABuyers AS (\n  SELECT DISTINCT\n    period,\n    user_pseudo_id\n  FROM\n    Params,\n    PurchaseEvents,\n    UNNEST(items) AS items\n  WHERE\n    items.item_name = selected_product\n),\nTopProducts AS (\n  SELECT\n    pe.period,\n    items.item_name AS item_name,\n    SUM(items.quantity) AS item_quantity\n  FROM\n    Params,\n    PurchaseEvents pe,\n    UNNEST(items) AS items\n  WHERE\n    user_pseudo_id IN (SELECT user_pseudo_id FROM ProductABuyers pb WHERE pb.period = pe.period)\n    AND items.item_name != selected_product\n  GROUP BY\n    pe.period, items.item_name\n),\nTopProductPerPeriod AS (\n  SELECT\n    period,\n    item_name,\n    item_quantity\n  FROM (\n    SELECT\n      period,\n      item_name,\n      item_quantity,\n      RANK() OVER (PARTITION BY period ORDER BY item_quantity DESC) AS rank\n    FROM\n      TopProducts\n  )\n  WHERE\n    rank = 1\n)\nSELECT\n  period,\n  item_name,\n  item_quantity\nFROM\n  TopProductPerPeriod\nORDER BY\n  period;",
        "external_knowledge": null,
        "plan": "1. **Define Parameters:**\n   - Establish a parameter for the selected product, which in this case is 'Google Red Speckled Tee'.\n\n2. **Set Date Ranges:**\n   - Define the date ranges for the three consecutive months starting from November 2020.\n\n3. **Identify Purchase Events:**\n   - Extract purchase events from the dataset for the defined date ranges.\n   - Ensure to include the event period, user identifier, and items purchased.\n\n4. **Identify Buyers of Selected Product:**\n   - Identify distinct users who purchased the selected product within each period.\n   - This step filters out only those users who bought the 'Google Red Speckled Tee'.\n\n5. **Calculate Top Products Purchased by These Buyers:**\n   - For each period, calculate the total quantity of other products bought by the users identified in the previous step.\n   - Exclude the selected product from this count.\n\n6. **Rank Products by Quantity:**\n   - Rank the products for each period based on the total quantity purchased.\n   - Use a ranking function to order products in descending order of quantity.\n\n7. **Select Top Product Per Period:**\n   - Select the top-ranked product for each period.\n   - This ensures only the most purchased product (excluding the selected product) is chosen for each period.\n\n8. **Output Results:**\n   - Retrieve and display the period, product name, and quantity of the top purchased product for each period.\n   - Order the results by period to maintain chronological order.\n\nThis plan effectively breaks down the SQL query into logical steps to achieve the user\u2019s goal of identifying the most purchased other products by customers who bought a specific product within a given timeframe.",
        "special_function": [
            "numbering-functions/RANK",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "ga003",
        "db": "firebase-public-project.analytics_153293282",
        "question": "I'm trying to evaluate which board types were most effective on September 15, 2018. Can you find out the average scores for each board type from the quick play mode completions on that day?",
        "SQL": "WITH EventData AS (\n    SELECT \n        user_pseudo_id, \n        event_timestamp, \n        param\n    FROM \n        `firebase-public-project.analytics_153293282.events_20180915`,\n        UNNEST(event_params) AS param\n    WHERE \n        event_name = \"level_complete_quickplay\"\n        AND (param.key = \"value\" OR param.key = \"board\")\n),\nProcessedData AS (\n    SELECT \n        user_pseudo_id, \n        event_timestamp, \n        MAX(IF(param.key = \"value\", param.value.int_value, NULL)) AS score,\n        MAX(IF(param.key = \"board\", param.value.string_value, NULL)) AS board_type\n    FROM \n        EventData\n    GROUP BY \n        user_pseudo_id, \n        event_timestamp\n)\nSELECT \n    ANY_VALUE(board_type) AS board, \n    AVG(score) AS average_score\nFROM \n    ProcessedData\nGROUP BY \n    board_type",
        "external_knowledge": null,
        "plan": "1. Extract all data for the \"level_complete_quickplay\" mode.\n2. Focus on the board type, grouping by user ID and event timestamp to summarize board type and corresponding scores.\n3. Calculate the average score for each board type.",
        "special_function": [
            "aggregate-functions/ANY_VALUE",
            "conditional-functions/IF",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "ga004",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Can you figure out the average difference in pageviews between users who bought something and those who didn\u2019t in December 2020? Just label anyone who was involved in purchase events as a purchaser.",
        "SQL": "WITH\n  UserInfo AS (\n    SELECT\n      user_pseudo_id,\n      COUNTIF(event_name = 'page_view') AS page_view_count,\n      COUNTIF(event_name IN ('in_app_purchase', 'purchase')) AS purchase_event_count\n    FROM `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\n    WHERE _TABLE_SUFFIX BETWEEN '20201201' AND '20201231'\n    GROUP BY 1\n  ),\n  Averages AS (\n    SELECT\n      (purchase_event_count > 0) AS purchaser,\n      COUNT(*) AS user_count,\n      SUM(page_view_count) AS total_page_views,\n      SUM(page_view_count) / COUNT(*) AS avg_page_views\n    FROM UserInfo\n    GROUP BY 1\n  )\n\nSELECT\n  MAX(CASE WHEN purchaser THEN avg_page_views ELSE 0 END) -\n  MAX(CASE WHEN NOT purchaser THEN avg_page_views ELSE 0 END) AS avg_page_views_difference\nFROM Averages;",
        "external_knowledge": null,
        "plan": "1. Segment user activities into page views and purchase events for December 2020.\n2. Classify users based on whether they made any purchases.\n3. Calculate average page views for purchasers and non-purchasers.\n4. Determine the difference in average page views between these two groups.",
        "special_function": [
            "aggregate-functions/COUNTIF",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "ga008",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Can you give me the average page views per buyer and total page views among those buyers for each day in November 2020?",
        "SQL": "WITH\n  UserInfo AS (\n    SELECT\n      user_pseudo_id,\n      PARSE_DATE('%Y%m%d', event_date) AS event_date,\n      COUNTIF(event_name = 'page_view') AS page_view_count,\n      COUNTIF(event_name = 'purchase') AS purchase_event_count\n    FROM `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\n    WHERE _TABLE_SUFFIX BETWEEN '20201101' AND '20201130'\n    GROUP BY 1, 2\n  )\nSELECT\n  event_date,\n  SUM(page_view_count) / COUNT(*) AS avg_page_views,\n  SUM(page_view_count)\nFROM UserInfo\nWHERE purchase_event_count > 0\nGROUP BY event_date\nORDER BY event_date;",
        "external_knowledge": null,
        "plan": "1. **Define a Temporary Data Set:**\n   - Create a temporary data set to collect user activity information.\n   - Parse the date format to a standard date type.\n   - Count the occurrences of 'page_view' events for each user per day.\n   - Count the occurrences of 'purchase' related events for each user per day.\n   - Filter the data for the specified date range (November 2020).\n\n2. **Aggregate User Activity:**\n   - From the temporary data set, select the date, the sum of 'page_view' counts, and the average 'page_view' counts per user who made a purchase.\n   - Ensure to include only users who have made at least one purchase-related event.\n\n3. **Compute Metrics:**\n   - For each day in November 2020:\n     - Calculate the average number of 'page_view' events per purchasing user.\n     - Calculate the total number of 'page_view' events.\n\n4. **Group and Order Results:**\n   - Group the results by date to get daily metrics.\n   - Order the results by date to maintain chronological order.",
        "special_function": [
            "aggregate-functions/COUNTIF",
            "date-functions/PARSE_DATE"
        ]
    },
    {
        "instance_id": "ga017",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "How many distinct users viewed the most frequently visited page during January 2021?",
        "SQL": "WITH unnested_events AS (\n  SELECT\n    MAX(CASE WHEN event_params.key = 'page_location' THEN event_params.value.string_value END) AS page_location,\n    user_pseudo_id,\n    event_timestamp\n  FROM\n    `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`,\n    UNNEST(event_params) AS event_params\n  WHERE\n    _TABLE_SUFFIX BETWEEN '20210101' AND '20210131'\n    AND event_name = 'page_view'\n  GROUP BY user_pseudo_id,event_timestamp\n),\ntemp AS (\n    SELECT\n    page_location,\n    COUNT(*) AS event_count,\n    COUNT(DISTINCT user_pseudo_id) AS users\n    FROM\n    unnested_events\n    GROUP BY page_location\n    ORDER BY event_count DESC\n)\n\nSELECT users \nFROM\ntemp\nLIMIT 1",
        "external_knowledge": null,
        "plan": "1. Extract `page_view` events for January 2021 and unnest their parameters to access individual event details.\n2. Aggregate these details to identify the location of each page viewed, grouping by user and event timestamp.\n3. Count occurrences and distinct users per page, then order by the frequency of visits to each page.\n4. Select the number of distinct users for the top visited page.",
        "special_function": [
            "conditional-functions/CASE",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "ga007",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Please find out what percentage of the page views on January 2, 2021, were for PDP type pages.",
        "SQL": "WITH base_table AS (\n-- pulls relevant columns from relevant dates to decrease the size of data scanned\n  SELECT\n    event_name,\n    event_date,\n    event_timestamp,\n    user_pseudo_id,\n    user_id,\n    device,\n    geo,\n    traffic_source,\n    event_params,\n    user_properties\n  FROM\n    `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\n  WHERE\n    _table_suffix = '20210102'\n  AND event_name IN ('page_view')\n)\n, unnested_events AS (\n-- unnests event parameters to get to relevant keys and values\n  SELECT\n    event_date AS date,\n    event_timestamp AS event_timestamp_microseconds,\n    user_pseudo_id,\n    MAX(CASE WHEN c.key = 'ga_session_id' THEN c.value.int_value END) AS visitID,\n    MAX(CASE WHEN c.key = 'ga_session_number' THEN c.value.int_value END) AS visitNumber,\n    MAX(CASE WHEN c.key = 'page_title' THEN c.value.string_value END) AS page_title,\n    MAX(CASE WHEN c.key = 'page_location' THEN c.value.string_value END) AS page_location\n  FROM \n    base_table,\n    UNNEST (event_params) c\n  GROUP BY 1,2,3\n)\n,unnested_events_categorised AS (\n-- categorizing Page Titles into PDPs and PLPs\n  SELECT\n  *,\n  CASE WHEN ARRAY_LENGTH(SPLIT(page_location, '/')) >= 5 \n            AND\n            CONTAINS_SUBSTR(ARRAY_REVERSE(SPLIT(page_location, '/'))[SAFE_OFFSET(0)], '+')\n            AND (LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(4)]) IN \n                                        ('accessories','apparel','brands','campus+collection','drinkware',\n                                          'electronics','google+redesign',\n                                          'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                          'office','shop+by+brand','small+goods','stationery','wearables'\n                                          )\n                  OR\n                  LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(3)]) IN \n                                        ('accessories','apparel','brands','campus+collection','drinkware',\n                                          'electronics','google+redesign',\n                                          'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                          'office','shop+by+brand','small+goods','stationery','wearables'\n                                          )\n            )\n            THEN 'PDP'\n            WHEN NOT(CONTAINS_SUBSTR(ARRAY_REVERSE(SPLIT(page_location, '/'))[SAFE_OFFSET(0)], '+'))\n            AND (LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(4)]) IN \n                                        ('accessories','apparel','brands','campus+collection','drinkware',\n                                          'electronics','google+redesign',\n                                          'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                          'office','shop+by+brand','small+goods','stationery','wearables'\n                                          )\n                  OR \n                  LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(3)]) IN \n                                          ('accessories','apparel','brands','campus+collection','drinkware',\n                                            'electronics','google+redesign',\n                                            'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                            'office','shop+by+brand','small+goods','stationery','wearables'\n                                            )\n            )\n            THEN 'PLP'\n        ELSE page_title\n        END AS page_title_adjusted \n\n  FROM \n    unnested_events\n)\n\nSELECT (SELECT COUNT(*) FROM unnested_events_categorised WHERE page_title_adjusted='PDP') / (SELECT COUNT(*) FROM unnested_events_categorised)*100;",
        "external_knowledge": "ga4_page_category.md",
        "plan": "1. query the event data to retrieve all unique event names\n2. Selects events data from the Google Analytics 4 (GA4) sample e-commerce dataset for the specific date (20210102)\n3. Filter to include only events named 'page_view', which represent page views.\n4. flatten the nested event_params array and extract values for ga_session_id, ga_session_number, page_title, and page_location. This allows the analysis of individual page views within each user's session.\n5. Further processes the unnested event data to classify pages based on URL depth and specific keywords into either Product Detail Pages (PDP) or Product Listing Pages (PLP).\n6. Calculate the total proportion of PDP",
        "special_function": [
            "array-functions/ARRAY_LENGTH",
            "array-functions/ARRAY_REVERSE",
            "date-functions/DATE",
            "string-functions/CONTAINS_SUBSTR",
            "string-functions/LOWER",
            "string-functions/SPLIT",
            "conditional-functions/CASE",
            "other-functions/UNNEST",
            "other-functions/ARRAY_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "ga013",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "I want to know all the pages visited by user 1402138.5184246691 on January 2, 2021. Please show the names of these pages and adjust the names to PDP or PLP where necessary.",
        "SQL": "WITH base_table AS (\n  SELECT\n    event_name,\n    event_date,\n    event_timestamp,\n    user_pseudo_id,\n    user_id,\n    device,\n    geo,\n    traffic_source,\n    event_params,\n    user_properties\n  FROM\n    `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\n  WHERE\n    _table_suffix ='20210102'\n  AND user_pseudo_id='1402138.5184246691'\n  AND event_name IN ('page_view')\n)\n, unnested_events AS (\n-- unnests event parameters to get to relevant keys and values\n  SELECT\n    event_date AS date,\n    event_timestamp AS event_timestamp_microseconds,\n    user_pseudo_id,\n    MAX(CASE WHEN c.key = 'ga_session_id' THEN c.value.int_value END) AS visitID,\n    MAX(CASE WHEN c.key = 'ga_session_number' THEN c.value.int_value END) AS visitNumber,\n    MAX(CASE WHEN c.key = 'page_title' THEN c.value.string_value END) AS page_title,\n    MAX(CASE WHEN c.key = 'page_location' THEN c.value.string_value END) AS page_location\n  FROM \n    base_table,\n    UNNEST (event_params) c\n  GROUP BY 1,2,3\n)\n\n\n  SELECT\n  *,\n  CASE WHEN ARRAY_LENGTH(SPLIT(page_location, '/')) >= 5 \n            AND\n            CONTAINS_SUBSTR(ARRAY_REVERSE(SPLIT(page_location, '/'))[SAFE_OFFSET(0)], '+')\n            AND (LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(4)]) IN \n                                        ('accessories','apparel','brands','campus+collection','drinkware',\n                                          'electronics','google+redesign',\n                                          'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                          'office','shop+by+brand','small+goods','stationery','wearables'\n                                          )\n                  OR\n                  LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(3)]) IN \n                                        ('accessories','apparel','brands','campus+collection','drinkware',\n                                          'electronics','google+redesign',\n                                          'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                          'office','shop+by+brand','small+goods','stationery','wearables'\n                                          )\n            )\n            THEN 'PDP'\n            WHEN NOT(CONTAINS_SUBSTR(ARRAY_REVERSE(SPLIT(page_location, '/'))[SAFE_OFFSET(0)], '+'))\n            AND (LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(4)]) IN \n                                        ('accessories','apparel','brands','campus+collection','drinkware',\n                                          'electronics','google+redesign',\n                                          'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                          'office','shop+by+brand','small+goods','stationery','wearables'\n                                          )\n                  OR \n                  LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(3)]) IN \n                                          ('accessories','apparel','brands','campus+collection','drinkware',\n                                            'electronics','google+redesign',\n                                            'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                            'office','shop+by+brand','small+goods','stationery','wearables'\n                                            )\n            )\n            THEN 'PLP'\n        ELSE page_title\n        END AS page_title_adjusted \n\n  FROM \n    unnested_events",
        "external_knowledge": "ga4_page_category.md",
        "plan": "1. **Create a Base Table**:\n   - Select necessary columns from the dataset.\n   - Filter the data to include only records for the specified date.\n   - Further filter to include only records for the specified user ID.\n   - Ensure that only records with the specified event type are selected.\n\n2. **Unnest Event Parameters**:\n   - Unnest the event parameters to access key-value pairs.\n   - Extract relevant values using conditional logic based on the key names.\n   - Group the data by date, timestamp, and user ID to ensure each event is uniquely identified.\n\n3. **Classify Page Names**:\n   - For each event, analyze the page location URL.\n   - Split the URL into its components for further analysis.\n   - Determine the type of page (PDP or PLP) based on predefined conditions:\n     - If the URL structure and specific keywords match certain patterns, classify the page as 'PDP'.\n     - If different patterns are matched, classify the page as 'PLP'.\n   - If none of the patterns are matched, retain the original page title.\n\n4. **Final Output**:\n   - Select all relevant fields from the unnested events.\n   - Include an additional field that contains the adjusted page title based on the classification logic.",
        "special_function": [
            "array-functions/ARRAY_LENGTH",
            "array-functions/ARRAY_REVERSE",
            "date-functions/DATE",
            "string-functions/CONTAINS_SUBSTR",
            "string-functions/LOWER",
            "string-functions/SPLIT",
            "conditional-functions/CASE",
            "other-functions/UNNEST",
            "other-functions/ARRAY_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "ga018",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "I'd like to analyze the appeal of our products to users. Can you calculate the percentage of times users go from browsing the product list pages to clicking into the product detail pages during a single session on January 2nd, 2021?",
        "SQL": "WITH base_table AS (\n  SELECT\n    event_name,\n    event_date,\n    event_timestamp,\n    user_pseudo_id,\n    user_id,\n    device,\n    geo,\n    traffic_source,\n    event_params,\n    user_properties\n  FROM\n    `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\n  WHERE\n    _table_suffix = '20210102'\n  AND event_name IN ('page_view')\n)\n, unnested_events AS (\n-- unnests event parameters to get to relevant keys and values\n  SELECT\n    event_date AS date,\n    event_timestamp AS event_timestamp_microseconds,\n    user_pseudo_id,\n    MAX(CASE WHEN c.key = 'ga_session_id' THEN c.value.int_value END) AS visitID,\n    MAX(CASE WHEN c.key = 'ga_session_number' THEN c.value.int_value END) AS visitNumber,\n    MAX(CASE WHEN c.key = 'page_title' THEN c.value.string_value END) AS page_title,\n    MAX(CASE WHEN c.key = 'page_location' THEN c.value.string_value END) AS page_location\n  FROM \n    base_table,\n    UNNEST (event_params) c\n  GROUP BY 1,2,3\n)\n\n, unnested_events_categorised AS (\n-- categorizing Page Titles into PDPs and PLPs\n  SELECT\n  *,\n  CASE WHEN ARRAY_LENGTH(SPLIT(page_location, '/')) >= 5 \n            AND\n            CONTAINS_SUBSTR(ARRAY_REVERSE(SPLIT(page_location, '/'))[SAFE_OFFSET(0)], '+')\n            AND (LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(4)]) IN \n                                        ('accessories','apparel','brands','campus+collection','drinkware',\n                                          'electronics','google+redesign',\n                                          'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                          'office','shop+by+brand','small+goods','stationery','wearables'\n                                          )\n                  OR\n                  LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(3)]) IN \n                                        ('accessories','apparel','brands','campus+collection','drinkware',\n                                          'electronics','google+redesign',\n                                          'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                          'office','shop+by+brand','small+goods','stationery','wearables'\n                                          )\n            )\n            THEN 'PDP'\n            WHEN NOT(CONTAINS_SUBSTR(ARRAY_REVERSE(SPLIT(page_location, '/'))[SAFE_OFFSET(0)], '+'))\n            AND (LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(4)]) IN \n                                        ('accessories','apparel','brands','campus+collection','drinkware',\n                                          'electronics','google+redesign',\n                                          'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                          'office','shop+by+brand','small+goods','stationery','wearables'\n                                          )\n                  OR \n                  LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(3)]) IN \n                                          ('accessories','apparel','brands','campus+collection','drinkware',\n                                            'electronics','google+redesign',\n                                            'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                            'office','shop+by+brand','small+goods','stationery','wearables'\n                                            )\n            )\n            THEN 'PLP'\n        ELSE page_title\n        END AS page_title_adjusted \n\n  FROM \n    unnested_events\n)\n\n\n, ranked_screens AS (\n  SELECT\n    *,\n    LAG(page_title_adjusted,1) OVER (PARTITION BY  user_pseudo_id, visitID ORDER BY event_timestamp_microseconds ASC) previous_page,\n    LEAD(page_title_adjusted,1) OVER (PARTITION BY  user_pseudo_id, visitID ORDER BY event_timestamp_microseconds ASC)  next_page\n  FROM \n    unnested_events_categorised\n\n)\n\n,PLPtoPDPTransitions AS (\n  SELECT\n    user_pseudo_id,\n    visitID\n  FROM\n    ranked_screens\n  WHERE\n    page_title_adjusted = 'PLP' AND next_page = 'PDP'\n)\n\n,TotalPLPViews AS (\n  SELECT\n    COUNT(*) AS total_plp_views\n  FROM\n    ranked_screens\n  WHERE\n    page_title_adjusted = 'PLP'\n)\n\n,TotalTransitions AS (\n  SELECT\n    COUNT(*) AS total_transitions\n  FROM\n    PLPtoPDPTransitions\n)\n\nSELECT\n  (total_transitions * 100.0) / total_plp_views AS percentage\nFROM\n  TotalTransitions, TotalPLPViews;",
        "external_knowledge": "ga4_page_category.md",
        "plan": "1. query the event data to retrieve all unique event names\n2. Selects events data from the Google Analytics 4 (GA4) sample e-commerce dataset for the specific date (20210102)\n3. Filter to include only events named 'page_view', which represent page views.\n4. flatten the nested event_params array and extract values for ga_session_id, ga_session_number, page_title, and page_location. This allows the analysis of individual page views within each user's session.\n5. Further processes the unnested event data to classify pages based on URL depth and specific keywords into either Product Detail Pages (PDP) or Product Listing Pages (PLP).\n6. Applies window functions to the categorized data to calculate the previous and next pages for each session per user, facilitating analysis of navigation paths between pages.\n7. Filters sessions where the current page is a PLP and the next page is a PDP.\n8. Counts the number of sessions transitioning from PLP to PDP and divides this by the total views of PLP pages to calculate the conversion rate.",
        "special_function": [
            "array-functions/ARRAY_LENGTH",
            "array-functions/ARRAY_REVERSE",
            "date-functions/DATE",
            "navigation-functions/LAG",
            "navigation-functions/LEAD",
            "string-functions/CONTAINS_SUBSTR",
            "string-functions/LOWER",
            "string-functions/SPLIT",
            "conditional-functions/CASE",
            "other-functions/UNNEST",
            "other-functions/ARRAY_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "ga031",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "I want to know our user session conversion rate on January 2nd, 2021, calculated as the percentage ratio of user visits that reached both the Home and Checkout Confirmation page in one session to those landed on the Home page.",
        "SQL": "WITH base_table AS (\n  SELECT\n    event_name,\n    event_date,\n    event_timestamp,\n    user_pseudo_id,\n    user_id,\n    device,\n    geo,\n    traffic_source,\n    event_params,\n    user_properties\n  FROM\n    `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\n  WHERE\n    _table_suffix = '20210102'\n    AND event_name IN ('page_view')\n),\nunnested_events AS (\n  SELECT\n    event_date AS date,\n    event_timestamp AS event_timestamp_microseconds,\n    user_pseudo_id,\n    MAX(CASE WHEN c.key = 'ga_session_id' THEN c.value.int_value END) AS visitID,\n    MAX(CASE WHEN c.key = 'ga_session_number' THEN c.value.int_value END) AS visitNumber,\n    MAX(CASE WHEN c.key = 'page_title' THEN c.value.string_value END) AS page_title,\n    MAX(CASE WHEN c.key = 'page_location' THEN c.value.string_value END) AS page_location\n  FROM \n    base_table,\n    UNNEST (event_params) c\n  GROUP BY 1,2,3\n),\nhome_visits AS (\n  SELECT\n    user_pseudo_id,\n    visitID\n  FROM\n    unnested_events\n  WHERE\n    page_title = 'Home'\n),\ncheckout_visits AS (\n  SELECT\n    user_pseudo_id,\n    visitID\n  FROM\n    unnested_events\n  WHERE\n    page_title = 'Checkout Confirmation'\n),\nhome_to_checkout AS (\n  SELECT\n    h.user_pseudo_id,\n    h.visitID\n  FROM\n    home_visits h\n  JOIN\n    checkout_visits c ON h.user_pseudo_id = c.user_pseudo_id AND h.visitID = c.visitID\n),\ntotal_home_visits AS (\n  SELECT\n    COUNT(*) AS total_home\n  FROM\n    home_visits\n),\ntotal_checkout_visits AS (\n  SELECT\n    COUNT(*) AS total_checkout\n  FROM\n    home_to_checkout\n)\n\nSELECT\n  (total_checkout * 100.0) / total_home AS conversion_rate\nFROM\n  total_home_visits,\n  total_checkout_visits;",
        "external_knowledge": null,
        "plan": "1. Extract and prepare event data from the Google Analytics 4 (GA4) dataset, specifically for page views on January 2, 2021.\n2. Unnest the event parameters to retrieve values for session identifiers and page identifiers, such as page title and location.\n3. Identify and count the initial visits to the Home page and the successful visits to the Checkout Confirmation page.\n4. Join the data to match sessions that start on the Home page and end at the Checkout Confirmation page.\n5. Calculate the user conversion rate for January by comparing the number of successful checkout visits to the total home page visits.",
        "special_function": [
            "date-functions/DATE",
            "conditional-functions/CASE",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "ga032",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Can you pull up the sequence of pages our customer 1362228 visited on January 28th 2021, linking them with '>>' between each page? I want to see their navigation flow through our site. Please refer to the docs to convert the corresponding page title to \"PDP\" or \"PLP\" if necessary and merge adjacent identical page titles into one.",
        "SQL": "WITH base_table AS (\n-- pulls relevant columns from relevant dates to decrease the size of data scanned\n  SELECT\n    event_name,\n    event_date,\n    event_timestamp,\n    user_pseudo_id,\n    user_id,\n    device,\n    geo,\n    traffic_source,\n    event_params,\n    user_properties\n  FROM\n    `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\n  WHERE\n    _table_suffix = '20210128' \n  AND event_name IN ('page_view')\n)\n, unnested_events AS (\n-- unnests event parameters to get to relevant keys and values\n  SELECT\n    event_date AS date,\n    event_timestamp AS event_timestamp_microseconds,\n    user_pseudo_id,\n    MAX(CASE WHEN c.key = 'ga_session_id' THEN c.value.int_value END) AS visitID,\n    MAX(CASE WHEN c.key = 'ga_session_number' THEN c.value.int_value END) AS visitNumber,\n    MAX(CASE WHEN c.key = 'page_title' THEN c.value.string_value END) AS page_title,\n    MAX(CASE WHEN c.key = 'page_location' THEN c.value.string_value END) AS page_location\n  FROM \n    base_table,\n    UNNEST (event_params) c\n  WHERE user_pseudo_id='1362228.4966015575'\n  GROUP BY 1,2,3\n)\n\n, unnested_events_categorised AS (\n-- categorizing Page Titles into PDPs and PLPs\n  SELECT\n  *,\n  CASE WHEN ARRAY_LENGTH(SPLIT(page_location, '/')) >= 5 \n            AND\n            CONTAINS_SUBSTR(ARRAY_REVERSE(SPLIT(page_location, '/'))[SAFE_OFFSET(0)], '+')\n            AND (LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(4)]) IN \n                                        ('accessories','apparel','brands','campus+collection','drinkware',\n                                          'electronics','google+redesign',\n                                          'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                          'office','shop+by+brand','small+goods','stationery','wearables'\n                                          )\n                  OR\n                  LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(3)]) IN \n                                        ('accessories','apparel','brands','campus+collection','drinkware',\n                                          'electronics','google+redesign',\n                                          'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                          'office','shop+by+brand','small+goods','stationery','wearables'\n                                          )\n            )\n            THEN 'PDP'\n            WHEN NOT(CONTAINS_SUBSTR(ARRAY_REVERSE(SPLIT(page_location, '/'))[SAFE_OFFSET(0)], '+'))\n            AND (LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(4)]) IN \n                                        ('accessories','apparel','brands','campus+collection','drinkware',\n                                          'electronics','google+redesign',\n                                          'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                          'office','shop+by+brand','small+goods','stationery','wearables'\n                                          )\n                  OR \n                  LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(3)]) IN \n                                          ('accessories','apparel','brands','campus+collection','drinkware',\n                                            'electronics','google+redesign',\n                                            'lifestyle','nest','new+2015+logo','notebooks+journals',\n                                            'office','shop+by+brand','small+goods','stationery','wearables'\n                                            )\n            )\n            THEN 'PLP'\n        ELSE page_title\n        END AS page_title_adjusted \n\n  FROM \n    unnested_events\n),\n\nmerged_pages AS (\n  SELECT\n    *,\n    LAG(page_title_adjusted, 1) OVER (\n      PARTITION BY user_pseudo_id, visitID\n      ORDER BY event_timestamp_microseconds ASC\n    ) AS prev_page_title\n  FROM\n    unnested_events_categorised\n),\nnavigation_flow AS (\n  SELECT\n    user_pseudo_id,\n    visitID,\n    event_timestamp_microseconds,\n    page_title_adjusted\n  FROM\n    merged_pages\n  WHERE\n    page_title_adjusted != prev_page_title OR prev_page_title IS NULL\n),\nfinal_output AS (\n  SELECT\n    user_pseudo_id,\n    visitID,\n    STRING_AGG(page_title_adjusted, '>>' ORDER BY event_timestamp_microseconds) AS screens_on_a_visit\n  FROM\n    navigation_flow\n  GROUP BY\n    user_pseudo_id,\n    visitID\n)\nSELECT\n  screens_on_a_visit\nFROM\n  final_output;",
        "external_knowledge": "ga4_page_category.md",
        "plan": "1. Select and filter page_view events on January 28th, 2021, specifically for user 1362228 from the GA4 dataset.\n2. Unpack the event_params array to extract details such as session ID, session number, page title, and page location.\n3. Determine page categories based on URL structure and keywords to classify pages as either Product Detail Pages (PDPs), Product Listing Pages (PLPs), or other types.\n4. Rank pages based on the event timestamp and use window functions to establish the sequence of page visits within each session.\n5. concatenate the page titles visited in sequence, separated by '>>', representing the user's navigation flow through the site.",
        "special_function": [
            "aggregate-functions/ARRAY_AGG",
            "array-functions/ARRAY",
            "array-functions/ARRAY_LENGTH",
            "array-functions/ARRAY_REVERSE",
            "array-functions/ARRAY_TO_STRING",
            "date-functions/DATE",
            "mathematical-functions/POWER",
            "navigation-functions/LAG",
            "navigation-functions/LEAD",
            "numbering-functions/DENSE_RANK",
            "string-functions/CONTAINS_SUBSTR",
            "string-functions/LOWER",
            "string-functions/SPLIT",
            "conditional-functions/CASE",
            "other-functions/UNNEST",
            "other-functions/ARRAY_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "ga006",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Provide the IDs and the average purchase value (in USD) per session for users who were engaged in multiple purchase sessions in November 2020.",
        "SQL": "WITH\n  events AS (\n    SELECT\n      session.value.int_value AS session_id,\n      event.ecommerce.purchase_revenue_in_usd AS purchase_revenue,\n      event.*\n    FROM `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*` AS event\n    LEFT JOIN UNNEST(event.event_params) AS session\n      ON session.key = 'ga_session_id'\n    WHERE _TABLE_SUFFIX BETWEEN '20201101' and '20201130'\n    AND ecommerce.purchase_revenue_in_usd IS NOT NULL\n    AND event_name = 'purchase'\n  ),\n\nSESSTION_INFO AS (\n  SELECT\n    user_pseudo_id,\n    COUNT(DISTINCT session_id) AS session_count,\n    SUM(purchase_revenue) / COUNT(DISTINCT session_id) AS avg_purchase_revenue_per_session_by_user\n  FROM events\n  WHERE session_id IS NOT NULL\n  GROUP BY user_pseudo_id\n)\n\nSELECT user_pseudo_id, avg_purchase_revenue_per_session_by_user\nFROM\nSESSTION_INFO\nWHERE session_count > 1\nORDER BY user_pseudo_id",
        "external_knowledge": null,
        "plan": "1. **Define Events Data**:\n   - Create a temporary table to store event data.\n   - Extract session ID and spending value from event parameters.\n   - Include all columns from the original event data.\n   - Filter the event data to include only records from November 2020.\n\n2. **Calculate Session Info**:\n   - Create another temporary table to calculate session-related metrics for each user.\n   - Count the distinct session IDs for each user to determine the number of purchase sessions.\n   - Calculate the average spending per session by dividing the total spending by the number of distinct sessions for each user.\n   - Only include events that represent a purchase and have a valid session ID.\n\n3. **Filter and Select**:\n   - Select the user IDs and their corresponding average spending per session from the session info table.\n   - Include only users who have engaged in more than one purchase session.\n\nBy following these steps, the query effectively identifies users with multiple purchase sessions in the specified timeframe and calculates their average spending per session.",
        "special_function": [
            "date-functions/DATE",
            "range-functions/RANGE",
            "string-functions/REPLACE",
            "conditional-functions/COALESCE",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "ga009",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "I want to know the average number of engaged sessions per user of December 2020.",
        "SQL": "SELECT\n  engaged_sessions_number / user_number AS engaged_sessions_per_user\nFROM (\n  SELECT\n    COUNT(\n      DISTINCT CASE\n        WHEN (SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'session_engaged') = '1' THEN\n          CONCAT(\n            user_pseudo_id, \n            CAST(\n              (SELECT value.int_value FROM UNNEST(event_params) WHERE key = 'ga_session_id') AS STRING\n            )\n          )\n      END\n    ) AS engaged_sessions_number,\n    COUNT(DISTINCT user_pseudo_id) AS user_number\n  FROM\n    `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\n  WHERE\n    _TABLE_SUFFIX BETWEEN '20201201' and '20201231'\n) AS summary",
        "external_knowledge": null,
        "plan": "1. **Identify the Time Frame:**\n   - The query focuses on data from December 2020.\n   - Filter the dataset to include only records within this time period.\n\n2. **Engaged Sessions Calculation:**\n   - Determine the number of unique engaged sessions.\n   - An engaged session is identified by a specific event parameter indicating engagement.\n   - For each engaged session, concatenate a unique user identifier with a session identifier to ensure distinct session counts.\n\n3. **User Count Calculation:**\n   - Count the number of unique users within the specified time frame.\n   - Use a distinct user identifier to ensure each user is only counted once.\n\n4. **Calculate Averages:**\n   - Calculate the average number of engaged sessions per user.\n   - Divide the total number of engaged sessions by the total number of unique users.\n\n5. **Result Presentation:**\n   - Present the result as the average number of engaged sessions per user.",
        "special_function": [
            "conversion-functions/CAST",
            "json-functions/STRING",
            "string-functions/CONCAT",
            "timestamp-functions/STRING",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "ga010",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Can you give me an overview of our website traffic for December 2020? I'm particularly interested in the channel with the fourth highest number of sessions.",
        "SQL": "WITH prep AS (\n  SELECT\n    user_pseudo_id,\n    (SELECT value.int_value FROM UNNEST(event_params) WHERE key = 'ga_session_id') AS session_id,\n    ARRAY_AGG((SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'source') IGNORE NULLS \n              ORDER BY event_timestamp)[SAFE_OFFSET(0)] AS source,\n    ARRAY_AGG((SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'medium') IGNORE NULLS \n              ORDER BY event_timestamp)[SAFE_OFFSET(0)] AS medium,\n    ARRAY_AGG((SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'campaign') IGNORE NULLS \n              ORDER BY event_timestamp)[SAFE_OFFSET(0)] AS campaign\n  FROM\n    `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\n  WHERE\n    _TABLE_SUFFIX BETWEEN '20201201' AND '20201231'\n  GROUP BY\n    user_pseudo_id,\n    session_id\n)\nSELECT\n  -- session default channel grouping (dimension | the channel group associated with a session) \n  CASE \n    WHEN source = '(direct)' AND (medium IN ('(not set)','(none)')) THEN 'Direct'\n    WHEN REGEXP_CONTAINS(campaign, 'cross-network') THEN 'Cross-network'\n    WHEN (REGEXP_CONTAINS(source,'alibaba|amazon|google shopping|shopify|etsy|ebay|stripe|walmart')\n        OR REGEXP_CONTAINS(campaign, '^(.*(([^a-df-z]|^)shop|shopping).*)$'))\n        AND REGEXP_CONTAINS(medium, '^(.*cp.*|ppc|paid.*)$') THEN 'Paid Shopping'\n    WHEN REGEXP_CONTAINS(source,'baidu|bing|duckduckgo|ecosia|google|yahoo|yandex')\n        AND REGEXP_CONTAINS(medium,'^(.*cp.*|ppc|paid.*)$') THEN 'Paid Search'\n    WHEN REGEXP_CONTAINS(source,'badoo|facebook|fb|instagram|linkedin|pinterest|tiktok|twitter|whatsapp')\n        AND REGEXP_CONTAINS(medium,'^(.*cp.*|ppc|paid.*)$') THEN 'Paid Social'\n    WHEN REGEXP_CONTAINS(source,'dailymotion|disneyplus|netflix|youtube|vimeo|twitch|vimeo|youtube')\n        AND REGEXP_CONTAINS(medium,'^(.*cp.*|ppc|paid.*)$') THEN 'Paid Video'\n    WHEN medium IN ('display', 'banner', 'expandable', 'interstitial', 'cpm') THEN 'Display'\n    WHEN REGEXP_CONTAINS(source,'alibaba|amazon|google shopping|shopify|etsy|ebay|stripe|walmart')\n        OR REGEXP_CONTAINS(campaign, '^(.*(([^a-df-z]|^)shop|shopping).*)$') THEN 'Organic Shopping'\n    WHEN REGEXP_CONTAINS(source,'badoo|facebook|fb|instagram|linkedin|pinterest|tiktok|twitter|whatsapp')\n        OR medium IN ('social','social-network','social-media','sm','social network','social media') THEN 'Organic Social'\n    WHEN REGEXP_CONTAINS(source,'dailymotion|disneyplus|netflix|youtube|vimeo|twitch|vimeo|youtube')\n        OR REGEXP_CONTAINS(medium,'^(.*video.*)$') THEN 'Organic Video'\n    WHEN REGEXP_CONTAINS(source,'baidu|bing|duckduckgo|ecosia|google|yahoo|yandex')\n        OR medium = 'organic' THEN 'Organic Search'\n    WHEN REGEXP_CONTAINS(source,'email|e-mail|e_mail|e mail')\n        OR REGEXP_CONTAINS(medium,'email|e-mail|e_mail|e mail') THEN 'Email'\n    WHEN medium = 'affiliate' THEN 'Affiliates'\n    WHEN medium = 'referral' THEN 'Referral'\n    WHEN medium = 'audio' THEN 'Audio'\n    WHEN medium = 'sms' THEN 'SMS'\n    WHEN medium LIKE '%push'\n        OR REGEXP_CONTAINS(medium,'mobile|notification') THEN 'Mobile Push Notifications'\n    ELSE 'Unassigned' \n  END AS channel_grouping_session\nFROM\n  prep\nGROUP BY\n  channel_grouping_session\nORDER BY\n  COUNT(DISTINCT CONCAT(user_pseudo_id, session_id)) DESC\nLIMIT 1 OFFSET 3",
        "external_knowledge": "ga4_dimensions_and_metrics.md",
        "plan": "1.First, read the document to understand how traffic is divided into 18 channel groups, primarily based on the metrics of source, medium, and campaign.\n2.Extract all visits from the database for December, each visit having a unique user ID and session ID. Retrieve the source, medium, and campaign for each visit.\n3.Based on the classification standards for channel groups in the document, write conditional statements to determine which channel each set of data belongs to, mainly using regular expressions. If the data source (source) contains any of the 4.following: 'badoo', 'facebook', 'fb', 'instagram', 'linkedin', 'pinterest', 'tiktok', 'twitter', or 'whatsapp', and the medium (medium) includes 'cp', 'ppc', or starts with 'paid', then categorize it as 'Paid Social'.\n5.Calculate the number of sessions for each channel based on the channel grouping.\n6.Select the name of the channel ranked fourth as the answer.",
        "special_function": [
            "aggregate-functions/ARRAY_AGG",
            "aggregate-functions/GROUPING",
            "string-functions/CONCAT",
            "string-functions/REGEXP_CONTAINS",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE",
            "other-functions/SET",
            "other-functions/UNNEST",
            "other-functions/ARRAY_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "ga014",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Please tell me the number of sessions for each website traffic channel in December 2020.",
        "SQL": "WITH prep AS (\n  SELECT\n    user_pseudo_id,\n    (SELECT value.int_value FROM UNNEST(event_params) WHERE key = 'ga_session_id') AS session_id,\n    ARRAY_AGG((SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'source') IGNORE NULLS \n              ORDER BY event_timestamp)[SAFE_OFFSET(0)] AS source,\n    ARRAY_AGG((SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'medium') IGNORE NULLS \n              ORDER BY event_timestamp)[SAFE_OFFSET(0)] AS medium,\n    ARRAY_AGG((SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'campaign') IGNORE NULLS \n              ORDER BY event_timestamp)[SAFE_OFFSET(0)] AS campaign\n  FROM\n    `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\n  WHERE\n    _TABLE_SUFFIX BETWEEN '20201201' AND '20201231'\n  GROUP BY\n    user_pseudo_id,\n    session_id\n)\nSELECT\n  -- session default channel grouping (dimension | the channel group associated with a session) \n  CASE \n    WHEN source = '(direct)' AND (medium IN ('(not set)','(none)')) THEN 'Direct'\n    WHEN REGEXP_CONTAINS(campaign, 'cross-network') THEN 'Cross-network'\n    WHEN (REGEXP_CONTAINS(source,'alibaba|amazon|google shopping|shopify|etsy|ebay|stripe|walmart')\n        OR REGEXP_CONTAINS(campaign, '^(.*(([^a-df-z]|^)shop|shopping).*)$'))\n        AND REGEXP_CONTAINS(medium, '^(.*cp.*|ppc|paid.*)$') THEN 'Paid Shopping'\n    WHEN REGEXP_CONTAINS(source,'baidu|bing|duckduckgo|ecosia|google|yahoo|yandex')\n        AND REGEXP_CONTAINS(medium,'^(.*cp.*|ppc|paid.*)$') THEN 'Paid Search'\n    WHEN REGEXP_CONTAINS(source,'badoo|facebook|fb|instagram|linkedin|pinterest|tiktok|twitter|whatsapp')\n        AND REGEXP_CONTAINS(medium,'^(.*cp.*|ppc|paid.*)$') THEN 'Paid Social'\n    WHEN REGEXP_CONTAINS(source,'dailymotion|disneyplus|netflix|youtube|vimeo|twitch|vimeo|youtube')\n        AND REGEXP_CONTAINS(medium,'^(.*cp.*|ppc|paid.*)$') THEN 'Paid Video'\n    WHEN medium IN ('display', 'banner', 'expandable', 'interstitial', 'cpm') THEN 'Display'\n    WHEN REGEXP_CONTAINS(source,'alibaba|amazon|google shopping|shopify|etsy|ebay|stripe|walmart')\n        OR REGEXP_CONTAINS(campaign, '^(.*(([^a-df-z]|^)shop|shopping).*)$') THEN 'Organic Shopping'\n    WHEN REGEXP_CONTAINS(source,'badoo|facebook|fb|instagram|linkedin|pinterest|tiktok|twitter|whatsapp')\n        OR medium IN ('social','social-network','social-media','sm','social network','social media') THEN 'Organic Social'\n    WHEN REGEXP_CONTAINS(source,'dailymotion|disneyplus|netflix|youtube|vimeo|twitch|vimeo|youtube')\n        OR REGEXP_CONTAINS(medium,'^(.*video.*)$') THEN 'Organic Video'\n    WHEN REGEXP_CONTAINS(source,'baidu|bing|duckduckgo|ecosia|google|yahoo|yandex')\n        OR medium = 'organic' THEN 'Organic Search'\n    WHEN REGEXP_CONTAINS(source,'email|e-mail|e_mail|e mail')\n        OR REGEXP_CONTAINS(medium,'email|e-mail|e_mail|e mail') THEN 'Email'\n    WHEN medium = 'affiliate' THEN 'Affiliates'\n    WHEN medium = 'referral' THEN 'Referral'\n    WHEN medium = 'audio' THEN 'Audio'\n    WHEN medium = 'sms' THEN 'SMS'\n    WHEN medium LIKE '%push'\n        OR REGEXP_CONTAINS(medium,'mobile|notification') THEN 'Mobile Push Notifications'\n    ELSE 'Unassigned' \n  END AS channel_grouping_session,\n  COUNT(DISTINCT CONCAT(user_pseudo_id, session_id)) AS sessions\nFROM\n  prep\nGROUP BY\n  channel_grouping_session",
        "external_knowledge": "ga4_dimensions_and_metrics.md",
        "plan": "1. **Create a Preparation Step**: \n   - Define a subquery to prepare the necessary data.\n   - Select unique user identifiers.\n   - Extract session identifiers from event parameters.\n   - Aggregate source, medium, and campaign information for each session, ordering them by event timestamp to ensure correct sequence.\n\n2. **Filter for Specific Time Period**:\n   - Limit data to events occurring in December 2020 by specifying the date range.\n\n3. **Group Data by User and Session**:\n   - Group the prepared data by unique user and session identifiers to consolidate session information.\n\n4. **Classify Sessions into Channels**:\n   - Use a `CASE` statement to categorize each session into a traffic channel based on the source, medium, and campaign information.\n   - Define various conditions for channel grouping, such as direct traffic, cross-network campaigns, paid and organic shopping, search, social, video, email, affiliate, referral, audio, SMS, and mobile push notifications.\n   - Assign any sessions that do not match the predefined conditions to an \"Unassigned\" category.\n\n5. **Count Sessions per Channel**:\n   - Count the number of distinct sessions for each traffic channel by combining user and session identifiers.\n   - Group the results by the traffic channel to get the session count for each channel.\n\n6. **Return the Results**:\n   - Output the channel grouping and the corresponding session count for each channel.",
        "special_function": [
            "aggregate-functions/ARRAY_AGG",
            "aggregate-functions/GROUPING",
            "string-functions/CONCAT",
            "string-functions/REGEXP_CONTAINS",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE",
            "other-functions/SET",
            "other-functions/UNNEST",
            "other-functions/ARRAY_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "ga011",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "What is the highest number of page views for different pages under website \"shop.googlemerchandisestore.com\" in December 2020?",
        "SQL": "SELECT\n    REGEXP_EXTRACT(\n        (\n            SELECT\n                REGEXP_REPLACE(value.string_value, r'/+', '/')\n            FROM\n                UNNEST(event_params)\n            WHERE \n                key = 'page_location'\n                AND REGEXP_CONTAINS(value.string_value, r'^https?://shop\\.googlemerchandisestore\\.com')\n        ),\n        r'^https?:/[^/]+(/[^\\\\?]*)'\n    ) AS page_path,\n    COUNT(*) AS page_views\nFROM\n    `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\nWHERE\n    _TABLE_SUFFIX BETWEEN '20201201' AND '20201231'\n    AND event_name = 'page_view'\nGROUP BY\n    page_path\nORDER BY\n    page_views DESC\nLIMIT 1",
        "external_knowledge": null,
        "plan": "1. **Identify the Data Source**: Access a dataset that contains event information with a specific naming pattern for tables. Focus on data from December 2020.\n\n2. **Filter by Date and Event Type**: Select records where the date falls within December 2020 and the event type is 'page_view'.\n\n3. **Extract Page Location**: For each event, access a nested structure to find the parameter indicating the page location. Ensure this page belongs to a specific website using a regular expression match.\n\n4. **Normalize URL Paths**: Replace multiple slashes in the URL path with a single slash to normalize it.\n\n5. **Extract Page Path**: Use a regular expression to extract the path part of the URL from the page location, ignoring protocol and domain.\n\n6. **Count Page Views**: Group the data by the extracted page path and count the number of views for each path.\n\n7. **Order and Limit Results**: Sort the results by the number of page views in descending order and select the top result to find the page with the highest views.",
        "special_function": [
            "aggregate-functions/COUNTIF",
            "string-functions/CONCAT",
            "string-functions/SPLIT",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE",
            "other-functions/UNNEST",
            "other-functions/ARRAY_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "ga012",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Find the transaction IDs, total item quantities, and purchase revenues for the item category with the highest tax rate on November 30, 2020.",
        "SQL": "WITH top_category AS (\n  SELECT\n    product.item_category,\n    SUM(ecommerce.tax_value_in_usd) / SUM(ecommerce.purchase_revenue_in_usd) AS tax_rate\n  FROM\n    bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20201130,\n    UNNEST(items) AS product\n  WHERE\n    event_name = 'purchase'\n  GROUP BY\n    product.item_category\n  ORDER BY\n    tax_rate DESC\n  LIMIT 1\n)\n\nSELECT\n    ecommerce.transaction_id,\n    SUM(ecommerce.total_item_quantity) AS total_item_quantity,\n    SUM(ecommerce.purchase_revenue_in_usd) AS purchase_revenue_in_usd,\n    SUM(ecommerce.purchase_revenue) AS purchase_revenue\nFROM\n    bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20201130, \n    UNNEST(items) AS product\nJOIN top_category\nON product.item_category = top_category.item_category\nWHERE\n    event_name = 'purchase'\nGROUP BY\n    ecommerce.transaction_id;",
        "external_knowledge": null,
        "plan": "1. **Identify the Top Tax Rate Category:**\n    - Create a common table expression (CTE) to calculate the tax rate for each item category.\n    - Calculate the tax rate as the ratio of the sum of tax values to the sum of purchase revenues for each category.\n    - Select the item category with the highest tax rate by ordering the results in descending order of tax rate and limiting the result to one.\n\n2. **Select Required Transaction Details:**\n    - Using the main table and the CTE, retrieve transaction details for the item category with the highest tax rate.\n    - Filter the dataset to include only purchase events.\n    - Join the main table with the CTE on the item category to ensure only transactions from the top tax rate category are considered.\n    - Group the results by transaction ID.\n\n3. **Calculate Aggregates:**\n    - For each transaction, calculate the total item quantity by summing the quantities of items.\n    - Calculate the total purchase revenue in the specified currency by summing the purchase revenue fields.\n    - Ensure the results include transaction IDs, total item quantities, and purchase revenues.\n\n4. **Output the Results:**\n    - Return the transaction ID, the aggregated total item quantity, and the aggregated purchase revenue for transactions within the top tax rate category on the specified date.",
        "special_function": [
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "ga019",
        "db": "firebase-public-project.analytics_153293282",
        "question": "Could you determine what percentage of users either did not uninstall our app within seven days or never uninstalled it after installing during August and September 2018?",
        "SQL": "WITH\n--List of users who installed\nsept_cohort AS (\n  SELECT DISTINCT user_pseudo_id,\n  FORMAT_DATE('%Y-%m-%d', PARSE_DATE('%Y%m%d', event_date)) AS date_first_open,\n  FROM `firebase-public-project.analytics_153293282.events_*`\n  WHERE event_name = 'first_open'\n  AND _TABLE_SUFFIX BETWEEN '20180801' and '20180930'\n),\n--Get the list of users who uninstalled\nuninstallers AS (\n  SELECT DISTINCT user_pseudo_id,\n  FORMAT_DATE('%Y-%m-%d', PARSE_DATE('%Y%m%d', event_date)) AS date_app_remove,\n  FROM `firebase-public-project.analytics_153293282.events_*`\n  WHERE event_name = 'app_remove'\n  AND _TABLE_SUFFIX BETWEEN '20180801' and '20180930'\n),\n--Join the 2 tables and compute for # of days to uninstall\njoined AS (\n  SELECT a.*,\n  b.date_app_remove,\n  DATE_DIFF(DATE(b.date_app_remove), DATE(a.date_first_open), DAY) AS days_to_uninstall\n  FROM sept_cohort a\n  LEFT JOIN uninstallers b\n  ON a.user_pseudo_id = b.user_pseudo_id\n)\n--Compute for the percentage\nSELECT\nCOUNT(DISTINCT\nCASE WHEN days_to_uninstall > 7 OR days_to_uninstall IS NULL THEN user_pseudo_id END) /\nCOUNT(DISTINCT user_pseudo_id)\nAS percent_users_7_days\nFROM joined",
        "external_knowledge": null,
        "plan": "1. Extract distinct user IDs and their first open dates from events labeled as 'first_open' for the months of August and September 2020.\n2. Gather distinct user IDs and their app removal dates from events labeled 'app_remove' during the same timeframe.\n3. Join the installation data with the uninstallation data on user ID to calculate the number of days between app installation and removal.\n4. Determine the percentage of users who uninstalled the app within seven days of installation by comparing the number of users who uninstalled within this period to the total number of users who installed the app.",
        "special_function": [
            "date-functions/DATE",
            "date-functions/DATE_DIFF",
            "date-functions/FORMAT_DATE",
            "date-functions/PARSE_DATE",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "ga030",
        "db": "firebase-public-project.analytics_153293282",
        "question": "Can you group users by the week they first used the app starting from July 2, 2018 and show which group has the most active users remained in the next four weeks, with each group named by the Monday date of that week? Please answer in the format of \" YYYY-MM-DD\".",
        "SQL": "WITH dates AS (\n    SELECT \n        DATE('2018-07-02') AS start_date,\n        DATE('2018-10-02') AS end_date,\n        DATE_ADD(DATE_TRUNC(DATE('2018-10-02'), WEEK(MONDAY)), INTERVAL -4 WEEK) AS min_date\n),\n\ndate_table AS (\n    SELECT DISTINCT \n        PARSE_DATE('%Y%m%d', `event_date`) AS event_date,\n        user_pseudo_id,\n        CASE \n            WHEN DATE_DIFF(PARSE_DATE('%Y%m%d', `event_date`), DATE(TIMESTAMP_MICROS(user_first_touch_timestamp)), DAY) = 0 \n            THEN 1 \n            ELSE 0 \n        END AS is_new_user\n    FROM `firebase-public-project.analytics_153293282.events_*` \n    WHERE event_name = 'session_start'\n    AND PARSE_DATE('%Y%m%d', `event_date`) >= DATE('2018-07-02')\n),\n\nnew_user_list AS (\n    SELECT DISTINCT \n        user_pseudo_id,\n        event_date\n    FROM date_table\n    WHERE is_new_user = 1\n),\n\ndays_since_start_table AS (\n    SELECT DISTINCT \n        is_new_user,\n        nu.event_date AS date_cohort,\n        dt.user_pseudo_id,\n        dt.event_date,\n        DATE_DIFF(dt.event_date, nu.event_date, DAY) AS days_since_start\n    FROM date_table dt\n    JOIN new_user_list nu ON dt.user_pseudo_id = nu.user_pseudo_id\n),\n\nweeks_retention AS (\n    SELECT \n        date_cohort,\n        DATE_TRUNC(date_cohort, WEEK(MONDAY)) AS week_cohort,\n        user_pseudo_id,\n        days_since_start,\n        CASE \n            WHEN days_since_start = 0 THEN 0 \n            ELSE CEIL(days_since_start / 7) \n        END AS weeks_since_start\n    FROM days_since_start_table\n),\n\nretention_counts AS (\n    SELECT \n        week_cohort,\n        weeks_since_start,\n        COUNT(DISTINCT user_pseudo_id) AS retained_users\n    FROM weeks_retention\n    WHERE week_cohort >= DATE('2018-07-02') AND week_cohort <= (SELECT min_date FROM dates) \n    GROUP BY \n        week_cohort,\n        weeks_since_start\n)\n\nSELECT \n    initial.week_cohort\nFROM \n    retention_counts AS initial\nLEFT JOIN \n    retention_counts AS four_week ON initial.week_cohort = four_week.week_cohort AND four_week.weeks_since_start = 4\nWHERE \n    initial.weeks_since_start = 0\nORDER BY \n    IFNULL(four_week.retained_users / initial.retained_users, 0)\nDESC\nLIMIT 1\n;",
        "external_knowledge": "retention_rate.md",
        "plan": "1. **Define Date Range and Calculate Minimum Date**:\n    - Establish a specific start date and end date.\n    - Determine the minimum date by subtracting four weeks from the end date truncated to the start of the week.\n\n2. **Prepare Event Data**:\n    - Convert event dates to a standard date format.\n    - Identify whether a user is using the app for the first time on that date.\n\n3. **Extract New Users**:\n    - Create a list of users who are new, along with their first event date.\n\n4. **Calculate Days Since Start for Each User**:\n    - Determine the number of days since a user\u2019s first app use.\n    - Associate each event date with the respective user and their first event date.\n\n5. **Calculate Weeks Since Start**:\n    - Group users by the week they started using the app.\n    - Calculate the number of weeks since each user's initial use.\n\n6. **Count Retained Users by Week**:\n    - For each starting week, count the number of distinct users who are still active, grouped by the number of weeks since they started.\n\n7. **Find the Week with Most Retained Users After Four Weeks**:\n    - Identify the initial user cohorts for each week.\n    - Determine the number of users still active after four weeks for each cohort.\n    - Calculate the retention rate (users active after four weeks / initial users).\n    - Sort the results by retention rate in descending order.\n    - Select the week with the highest retention rate.\n\nThis step-by-step plan ensures that users are grouped by their initial week of usage, and the retention rates are calculated to find the week with the highest user retention after four weeks.",
        "special_function": [
            "date-functions/DATE",
            "date-functions/DATE_ADD",
            "date-functions/DATE_DIFF",
            "date-functions/DATE_TRUNC",
            "date-functions/PARSE_DATE",
            "mathematical-functions/CEIL",
            "timestamp-functions/TIMESTAMP_MICROS",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE",
            "conditional-functions/IFNULL"
        ]
    },
    {
        "instance_id": "ga005",
        "db": "firebase-public-project.analytics_153293282",
        "question": "Please conduct a weekly cohort analysis for user retention starting July 9, 2018. Group users by their first week of app use and calculate the retention rates for each cohort over the next two weeks, showing the rate of the original cohort that returned each week. The data is available up to October 2, 2018.",
        "SQL": "WITH dates AS (\n    SELECT \n        DATE('2018-07-09') AS start_date,\n        DATE('2018-10-02') AS end_date,\n        DATE_ADD(DATE_TRUNC(DATE('2018-10-02'), WEEK(MONDAY)), INTERVAL -2 WEEK) AS min_date\n),\n\ndate_table AS (\n    SELECT DISTINCT \n        PARSE_DATE('%Y%m%d', `event_date`) AS event_date,\n        user_pseudo_id,\n        CASE \n            WHEN DATE_DIFF(PARSE_DATE('%Y%m%d', `event_date`), DATE(TIMESTAMP_MICROS(user_first_touch_timestamp)), DAY) = 0 \n            THEN 1 \n            ELSE 0 \n        END AS is_new_user\n    FROM `firebase-public-project.analytics_153293282.events_*` \n    WHERE event_name = 'session_start'\n    AND PARSE_DATE('%Y%m%d', `event_date`) >= DATE('2018-07-09')\n),\n\nnew_user_list AS (\n    SELECT DISTINCT \n        user_pseudo_id,\n        event_date\n    FROM date_table\n    WHERE is_new_user = 1\n),\n\ndays_since_start_table AS (\n    SELECT DISTINCT \n        is_new_user,\n        nu.event_date AS date_cohort,\n        dt.user_pseudo_id,\n        dt.event_date,\n        DATE_DIFF(dt.event_date, nu.event_date, DAY) AS days_since_start\n    FROM date_table dt\n    JOIN new_user_list nu ON dt.user_pseudo_id = nu.user_pseudo_id\n),\n\nweeks_retention AS (\n    SELECT \n        date_cohort,\n        DATE_TRUNC(date_cohort, WEEK(MONDAY)) AS week_cohort,\n        user_pseudo_id,\n        days_since_start,\n        CASE \n            WHEN days_since_start = 0 THEN 0 \n            ELSE CEIL(days_since_start / 7) \n        END AS weeks_since_start\n    FROM days_since_start_table\n),\n\nretention_counts AS (\n    SELECT \n        week_cohort,\n        weeks_since_start,\n        COUNT(DISTINCT user_pseudo_id) AS retained_users\n    FROM weeks_retention\n    WHERE week_cohort >= DATE('2018-07-02') AND week_cohort <= (SELECT min_date FROM dates) \n    GROUP BY \n        week_cohort,\n        weeks_since_start\n)\n\nSELECT \n    initial.week_cohort,\n    IFNULL(four_week.retained_users / initial.retained_users, 0) AS retention_rate\nFROM \n    retention_counts AS initial\nLEFT JOIN \n    retention_counts AS four_week ON initial.week_cohort = four_week.week_cohort AND four_week.weeks_since_start = 2\nWHERE \n    initial.weeks_since_start = 0\nORDER BY\nweek_cohort",
        "external_knowledge": "retention_rate.md",
        "plan": "1. **Define Date Range and Minimum Date**:\n   - Create a CTE (Common Table Expression) to define the start date, end date, and the minimum date (which is two weeks before the end date).\n\n2. **Create Date Table**:\n   - Extract the event date and user ID from the events table.\n   - Determine if the event date is the first time the user used the app by comparing it with the user's first touch timestamp.\n   - Filter events that occurred on or after July 9, 2018, and where the event name is 'session_start'.\n\n3. **Identify New Users**:\n   - Create a list of users who are new (i.e., their first use of the app) by selecting distinct user IDs and their first event dates.\n\n4. **Calculate Days Since Start for Each User**:\n   - Calculate the number of days since each new user started using the app.\n   - This involves joining the date table with the new user list to get the event date differences for each user's subsequent events.\n\n5. **Calculate Weeks Since Start for Each User**:\n   - Group the users into weekly cohorts based on their first event date.\n   - Calculate the number of weeks since the user first used the app, where the week starts on Monday.\n\n6. **Count Retained Users**:\n   - Count the number of distinct users retained in each weekly cohort for each week since they first used the app.\n   - Ensure the counts are within the defined date range.\n\n7. **Calculate Retention Rate**:\n   - Join the retention counts for the initial week cohort with the counts for the second week (two weeks since the start).\n   - Calculate the retention rate by dividing the number of users retained in the second week by the number of users in the initial cohort.\n   - Ensure to handle cases where no users are retained by using an IFNULL function.\n\n8. **Output and Order Results**:\n   - Select the initial week cohort and the calculated retention rate.\n   - Filter to include only the initial cohorts and order the results by the week cohort for clear presentation.",
        "special_function": [
            "date-functions/DATE",
            "date-functions/DATE_ADD",
            "date-functions/DATE_DIFF",
            "date-functions/DATE_TRUNC",
            "date-functions/PARSE_DATE",
            "mathematical-functions/CEIL",
            "timestamp-functions/TIMESTAMP_MICROS",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE",
            "conditional-functions/IFNULL"
        ]
    },
    {
        "instance_id": "ga028",
        "db": "firebase-public-project.analytics_153293282",
        "question": "Please perform a 7-day retention analysis for users who first used the app during the week starting on July 2, 2018. Provide the total number of these new users and the number of retained users for each week from Week 0 (the initial week) through Week 4.",
        "SQL": "WITH dates AS (\n    SELECT \n        DATE('2018-07-02') AS start_date,\n        DATE('2018-10-02') AS end_date,\n        DATE_ADD(DATE_TRUNC(DATE('2018-10-02'), WEEK(TUESDAY)), INTERVAL -4 WEEK) AS min_date\n),\n\ndate_table AS (\n    SELECT DISTINCT \n        PARSE_DATE('%Y%m%d', `event_date`) AS event_date,\n        user_pseudo_id,\n        CASE \n            WHEN DATE_DIFF(PARSE_DATE('%Y%m%d', `event_date`), DATE(TIMESTAMP_MICROS(user_first_touch_timestamp)), DAY) = 0 \n            THEN 1 \n            ELSE 0 \n        END AS is_new_user\n    FROM \n        `firebase-public-project.analytics_153293282.events_*` \n    WHERE \n        event_name = 'session_start'\n),\n\nnew_user_list AS (\n    SELECT DISTINCT \n        user_pseudo_id,\n        event_date\n    FROM \n        date_table\n    WHERE \n        is_new_user = 1\n),\n\ndays_since_start_table AS (\n    SELECT DISTINCT \n        is_new_user,\n        nu.event_date AS date_cohort,\n        dt.user_pseudo_id,\n        dt.event_date,\n        DATE_DIFF(dt.event_date, nu.event_date, DAY) AS days_since_start\n    FROM \n        date_table dt\n    JOIN \n        new_user_list nu ON dt.user_pseudo_id = nu.user_pseudo_id\n),\n\nweeks_retention AS (\n    SELECT \n        date_cohort,\n        DATE_TRUNC(date_cohort, WEEK(MONDAY)) AS week_cohort,\n        user_pseudo_id,\n        days_since_start,\n        CASE \n            WHEN days_since_start = 0 \n            THEN 0 \n            ELSE CEIL(days_since_start / 7) \n        END AS weeks_since_start\n    FROM \n        days_since_start_table\n),\nRETENTION_INFO AS (\n  SELECT \n      week_cohort,\n      weeks_since_start,\n      COUNT(DISTINCT user_pseudo_id) AS retained_users\n  FROM \n      weeks_retention\n  WHERE \n      week_cohort <= (SELECT min_date FROM dates)\n  GROUP BY \n      week_cohort,\n      weeks_since_start\n  HAVING \n      weeks_since_start <= 4\n  ORDER BY \n      week_cohort,\n      weeks_since_start\n)\n\nSELECT weeks_since_start, retained_users\nFROM RETENTION_INFO\nWHERE week_cohort = DATE('2018-07-02')",
        "external_knowledge": "retention_rate.md",
        "plan": "1. **Define the Date Range**:\n   - Establish the start date and end date for the analysis period.\n   - Calculate the minimum date needed for the 4-week retention analysis.\n\n2. **Create Event Date Table**:\n   - Parse the event dates.\n   - Determine if a user is a new user by comparing the event date with the user's first touch timestamp.\n\n3. **Identify New Users**:\n   - Extract a list of new users who first used the app on their respective event dates.\n\n4. **Calculate Days Since First Use**:\n   - For each user, calculate the number of days since their first use.\n   - Associate each user with their event dates and the calculated days since their first use.\n\n5. **Group by Week and Calculate Retention**:\n   - Group the users by the week of their first use.\n   - Calculate the number of weeks since the user's first use.\n   - Use the ceiling function to group days into weeks, starting from 0.\n\n6. **Aggregate Retention Data**:\n   - Count the number of distinct retained users for each week since their first use.\n   - Group the data by the week cohort and the weeks since the start.\n   - Ensure that the data is limited to the defined date range and only includes up to 4 weeks.\n\n7. **Filter and Sort the Retention Data**:\n   - Filter the retention data to show only the week starting on the specified start date.\n   - Sort the data by the week cohort and the number of weeks since the start.\n\n8. **Select Final Output**:\n   - Select and display the number of retained users for each week from 0 to 4 weeks since the user's first use.",
        "special_function": [
            "date-functions/DATE",
            "date-functions/DATE_ADD",
            "date-functions/DATE_DIFF",
            "date-functions/DATE_TRUNC",
            "date-functions/PARSE_DATE",
            "mathematical-functions/CEIL",
            "timestamp-functions/TIMESTAMP_MICROS",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "ga020",
        "db": "firebase-public-project.analytics_153293282",
        "question": "Which quickplay event type had the lowest user retention rate during the second week after their initial engagement, for users who first engaged between August 1 and August 15, 2018?",
        "SQL": "-- Define the date range and calculate the minimum date for filtering results\nWITH dates AS (\n    SELECT \n        DATE('2018-08-01') AS start_date,\n        DATE('2018-08-15') AS end_date\n),\n-- Create a table of active dates for each user within the specified date range\ndates_active_table AS (\n    SELECT\n        user_pseudo_id,\n        PARSE_DATE('%Y%m%d', `event_date`) AS user_active_date\n    FROM \n        `firebase-public-project.analytics_153293282.events_*` \n    WHERE \n        event_name = 'session_start'\n        AND PARSE_DATE('%Y%m%d', `event_date`) BETWEEN (SELECT start_date FROM dates) AND (SELECT end_date FROM dates)\n    GROUP BY \n        user_pseudo_id, user_active_date\n),\n-- Create a table of the earliest quickplay event date for each user within the specified date range\nevent_table AS (\n    SELECT \n        user_pseudo_id,\n        event_name,\n        MIN(PARSE_DATE('%Y%m%d', `event_date`)) AS event_cohort_date\n    FROM \n        `firebase-public-project.analytics_153293282.events_*` \n    WHERE \n        event_name IN ('level_start_quickplay', 'level_end_quickplay', 'level_complete_quickplay', \n                       'level_fail_quickplay', 'level_reset_quickplay', 'level_retry_quickplay')\n        AND PARSE_DATE('%Y%m%d', `event_date`) BETWEEN (SELECT start_date FROM dates) AND (SELECT end_date FROM dates)\n    GROUP BY \n        user_pseudo_id, event_name\n),\n-- Calculate the number of days since each user's initial quickplay event\ndays_since_event_table AS (\n    SELECT\n        events.user_pseudo_id,\n        events.event_name AS event_cohort,\n        events.event_cohort_date,\n        days.user_active_date,\n        DATE_DIFF(days.user_active_date, events.event_cohort_date, DAY) AS days_since_event\n    FROM \n        event_table events\n    LEFT JOIN \n        dates_active_table days ON events.user_pseudo_id = days.user_pseudo_id\n    WHERE \n        events.event_cohort_date <= days.user_active_date\n),\n-- Calculate the weeks since each user's initial quickplay event and count the active days in each week\nweeks_retention AS (\n    SELECT\n        event_cohort,\n        user_pseudo_id,\n        CAST(CASE WHEN days_since_event = 0 THEN 0 ELSE CEIL(days_since_event / 7) END AS INTEGER) AS weeks_since_event,\n        COUNT(DISTINCT days_since_event) AS days_active_since_event -- Count Days Active in Week\n    FROM \n        days_since_event_table\n    GROUP BY \n        event_cohort, user_pseudo_id, weeks_since_event\n),\n-- Aggregate the weekly retention data\naggregated_weekly_retention_table AS (\n    SELECT\n        event_cohort,\n        weeks_since_event,\n        SUM(days_active_since_event) AS weekly_days_active,\n        COUNT(DISTINCT user_pseudo_id) AS retained_users\n    FROM \n        weeks_retention\n    GROUP BY \n        event_cohort, weeks_since_event\n),\nRETENTION_INFO AS (\n-- Select and calculate the weekly retention rate for each event cohort\nSELECT\n    event_cohort,\n    weeks_since_event,\n    weekly_days_active,\n    retained_users,\n    (retained_users / MAX(retained_users) OVER (PARTITION BY event_cohort)) AS retention_rate\nFROM \n    aggregated_weekly_retention_table\nORDER BY \n    event_cohort, weeks_since_event\n)\n\nSELECT event_cohort\nFROM\nRETENTION_INFO\nWHERE weeks_since_event = 2\nORDER BY retention_rate\nLIMIT 1",
        "external_knowledge": "retention_rate.md",
        "plan": "1. Extract unique users who installed the app in September 2018, recording the date of first use.\n2. Collect data on users who uninstalled the app within a little over a month from installation, noting the uninstallation dates.\n3. Retrieve crash data for the same users during the specified timeframe to determine app stability issues.\n4. Combine the installation, uninstallation, and crash data into a single dataset using user ID as the key.\n5. Calculate the proportion of users who experienced a crash within a week of installation out of those who uninstalled the app within a week, providing insight into potential issues affecting user retention.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "date-functions/DATE_DIFF",
            "date-functions/PARSE_DATE",
            "mathematical-functions/CEIL",
            "range-functions/RANGE",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "ga021",
        "db": "firebase-public-project.analytics_153293282",
        "question": "What is the retention rate for users two weeks after their initial quickplay event within the period from July 2, 2018, to July 16, 2018, calculated separately for each quickplay event type?",
        "SQL": "-- Define the date range and calculate the minimum date for filtering results\nWITH dates AS (\n    SELECT \n        DATE('2018-07-02') AS start_date,\n        DATE('2018-07-16') AS end_date\n),\n-- Create a table of active dates for each user within the specified date range\ndates_active_table AS (\n    SELECT\n        user_pseudo_id,\n        PARSE_DATE('%Y%m%d', `event_date`) AS user_active_date\n    FROM \n        `firebase-public-project.analytics_153293282.events_*` \n    WHERE \n        event_name = 'session_start'\n        AND PARSE_DATE('%Y%m%d', `event_date`) BETWEEN (SELECT start_date FROM dates) AND (SELECT end_date FROM dates)\n    GROUP BY \n        user_pseudo_id, user_active_date\n),\n-- Create a table of the earliest quickplay event date for each user within the specified date range\nevent_table AS (\n    SELECT \n        user_pseudo_id,\n        event_name,\n        MIN(PARSE_DATE('%Y%m%d', `event_date`)) AS event_cohort_date\n    FROM \n        `firebase-public-project.analytics_153293282.events_*` \n    WHERE \n        event_name IN ('level_start_quickplay', 'level_end_quickplay', 'level_complete_quickplay', \n                       'level_fail_quickplay', 'level_reset_quickplay', 'level_retry_quickplay')\n        AND PARSE_DATE('%Y%m%d', `event_date`) BETWEEN (SELECT start_date FROM dates) AND (SELECT end_date FROM dates)\n    GROUP BY \n        user_pseudo_id, event_name\n),\n-- Calculate the number of days since each user's initial quickplay event\ndays_since_event_table AS (\n    SELECT\n        events.user_pseudo_id,\n        events.event_name AS event_cohort,\n        events.event_cohort_date,\n        days.user_active_date,\n        DATE_DIFF(days.user_active_date, events.event_cohort_date, DAY) AS days_since_event\n    FROM \n        event_table events\n    LEFT JOIN \n        dates_active_table days ON events.user_pseudo_id = days.user_pseudo_id\n    WHERE \n        events.event_cohort_date <= days.user_active_date\n),\n-- Calculate the weeks since each user's initial quickplay event and count the active days in each week\nweeks_retention AS (\n    SELECT\n        event_cohort,\n        user_pseudo_id,\n        CAST(CASE WHEN days_since_event = 0 THEN 0 ELSE CEIL(days_since_event / 7) END AS INTEGER) AS weeks_since_event,\n        COUNT(DISTINCT days_since_event) AS days_active_since_event -- Count Days Active in Week\n    FROM \n        days_since_event_table\n    GROUP BY \n        event_cohort, user_pseudo_id, weeks_since_event\n),\n-- Aggregate the weekly retention data\naggregated_weekly_retention_table AS (\n    SELECT\n        event_cohort,\n        weeks_since_event,\n        SUM(days_active_since_event) AS weekly_days_active,\n        COUNT(DISTINCT user_pseudo_id) AS retained_users\n    FROM \n        weeks_retention\n    GROUP BY \n        event_cohort, weeks_since_event\n),\nRETENTION_INFO AS (\nSELECT\n    event_cohort,\n    weeks_since_event,\n    weekly_days_active,\n    retained_users,\n    (retained_users / MAX(retained_users) OVER (PARTITION BY event_cohort)) AS retention_rate\nFROM \n    aggregated_weekly_retention_table\nORDER BY \n    event_cohort, weeks_since_event\n)\n\nSELECT event_cohort, retention_rate\nFROM\nRETENTION_INFO\nWHERE weeks_since_event = 2",
        "external_knowledge": "retention_rate.md",
        "plan": "1. **Define Date Range:**\n   - Establish the start and end dates for the period of interest.\n\n2. **Identify Active Dates:**\n   - Select user identifiers and their active dates (based on session start events) within the defined date range.\n\n3. **Identify Initial Quickplay Events:**\n   - For each user, determine the earliest date of specific quickplay events within the defined date range.\n\n4. **Calculate Days Since Initial Event:**\n   - For each user and each initial quickplay event type, calculate the number of days since the initial quickplay event until each subsequent active date.\n\n5. **Determine Weeks Since Initial Event:**\n   - Convert the days since the initial quickplay event into weeks and count the number of active days in each week for each user and each quickplay event type.\n\n6. **Aggregate Weekly Retention Data:**\n   - Summarize the weekly retention data by counting the active days and the number of retained users for each quickplay event type and each week.\n\n7. **Calculate Retention Rate:**\n   - Calculate the retention rate by dividing the number of retained users in each week by the maximum number of retained users in the first week for each quickplay event type.\n\n8. **Filter for Two Weeks:**\n   - Select and return the retention rates for users two weeks after their initial quickplay event for each quickplay event type.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "date-functions/DATE_DIFF",
            "date-functions/PARSE_DATE",
            "mathematical-functions/CEIL",
            "range-functions/RANGE",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "ga022",
        "db": "firebase-public-project.analytics_153293282",
        "question": "Could you please help me get the weekly customer retention rate in September 2018 for new customers who first used our app within the first week starting from September 1st, 2018 (timezone in Shanghai)? The retention rates should cover the following 3-week period after the initial use and display them in column format.",
        "SQL": "WITH analytics_data AS (\n  SELECT user_pseudo_id, event_timestamp, event_name, \n    UNIX_MICROS(TIMESTAMP(\"2018-09-01 00:00:00\", \"+8:00\")) AS start_day,\n    3600*1000*1000*24*7 AS one_week_micros\n  FROM `firebase-public-project.analytics_153293282.events_*`\n  WHERE _table_suffix BETWEEN '20180901' AND '20180930'\n)\n\nSELECT\n week_1_cohort / week_0_cohort AS week_1_pct,\n week_2_cohort / week_0_cohort AS week_2_pct,\n week_3_cohort / week_0_cohort AS week_3_pct\nFROM (\n  WITH week_3_users AS (\n    SELECT DISTINCT user_pseudo_id\n    FROM analytics_data\n    WHERE event_timestamp BETWEEN start_day+(3*one_week_micros) AND start_day+(4*one_week_micros)\n  ),\n  week_2_users AS (\n    SELECT DISTINCT user_pseudo_id\n    FROM analytics_data\n    WHERE event_timestamp BETWEEN start_day+(2*one_week_micros) AND start_day+(3*one_week_micros)\n  ),\n  week_1_users AS (\n    SELECT DISTINCT user_pseudo_id\n    FROM analytics_data\n    WHERE event_timestamp BETWEEN start_day+(1*one_week_micros) AND start_day+(2*one_week_micros)\n  ), \n  week_0_users AS (\n    SELECT DISTINCT user_pseudo_id\n    FROM analytics_data\n    WHERE event_name = 'first_open'\n      AND event_timestamp BETWEEN start_day AND start_day+(1*one_week_micros)\n  )\n  SELECT \n    (SELECT count(*) \n     FROM week_0_users) AS week_0_cohort,\n    (SELECT count(*) \n     FROM week_1_users \n     JOIN week_0_users USING (user_pseudo_id)) AS week_1_cohort,\n    (SELECT count(*) \n     FROM week_2_users \n     JOIN week_0_users USING (user_pseudo_id)) AS week_2_cohort,\n    (SELECT count(*) \n     FROM week_3_users \n     JOIN week_0_users USING (user_pseudo_id)) AS week_3_cohort\n)",
        "external_knowledge": "retention_rate.md",
        "plan": "1. Select from the table series to filter out the data in September 2018, and record their timestamp and app version.\n2. Browse the document to understand the weekly customer retention rate is the percentage of customers who continue to use the app across a specified number of weeks.\n3. Get all the data of the users who first open the app in the first week (new customers).\n4. Get all the data of the users who are active using the app in the 2nd, 3rd and 4th week.\n5. Combine them to find out the users who first come in the first week and are still active in the following weeks.\n6. Calculate the weekly retention rate for each week according to the retention rate formula.",
        "special_function": null
    },
    {
        "instance_id": "ga025",
        "db": "firebase-public-project.analytics_153293282",
        "question": "For all users who first opened the app in September 2018 and then uninstalled within seven days, I want to know what percentage of them experienced an app crash.",
        "SQL": "WITH\n-- List of users who installed in Sept\nsept_cohort AS (\n    SELECT DISTINCT \n        user_pseudo_id,\n        FORMAT_DATE('%Y-%m-%d', PARSE_DATE('%Y%m%d', event_date)) AS date_first_open\n    FROM \n        `firebase-public-project.analytics_153293282.events_*`\n    WHERE \n        event_name = 'first_open'\n        AND _TABLE_SUFFIX BETWEEN '20180901' AND '20180930'\n),\n-- Get the list of users who uninstalled\nuninstallers AS (\n    SELECT DISTINCT \n        user_pseudo_id,\n        FORMAT_DATE('%Y-%m-%d', PARSE_DATE('%Y%m%d', event_date)) AS date_app_remove\n    FROM \n        `firebase-public-project.analytics_153293282.events_*`\n    WHERE \n        event_name = 'app_remove'\n        AND _TABLE_SUFFIX BETWEEN '20180901' AND '20181007'\n),\n-- Get the list of users who experienced crashes\nusers_crashes AS (\n    SELECT DISTINCT \n        user_pseudo_id,\n        FORMAT_DATE('%Y-%m-%d', PARSE_DATE('%Y%m%d', event_date)) AS date_crash\n    FROM \n        `firebase-public-project.analytics_153293282.events_*`,\n        UNNEST(event_params) e\n    WHERE \n        event_name = 'app_exception'\n        AND _TABLE_SUFFIX BETWEEN '20180901' AND '20181007'\n),\n-- Join the 3 tables\njoined AS (\n    SELECT \n        a.user_pseudo_id,\n        a.date_first_open,\n        b.date_app_remove,\n        DATE_DIFF(DATE(b.date_app_remove), DATE(a.date_first_open), DAY) AS days_to_uninstall,\n        c.date_crash\n    FROM \n        sept_cohort a\n    LEFT JOIN \n        uninstallers b ON a.user_pseudo_id = b.user_pseudo_id\n    LEFT JOIN \n        users_crashes c ON a.user_pseudo_id = c.user_pseudo_id\n)\n-- Compute the percentage\nSELECT\n    COUNT(DISTINCT CASE WHEN days_to_uninstall <= 7 AND date_crash IS NOT NULL THEN user_pseudo_id END) / \n    COUNT(DISTINCT CASE WHEN days_to_uninstall <= 7 THEN user_pseudo_id END) AS percent_users_crashes\nFROM \n    joined;",
        "external_knowledge": null,
        "plan": "1. Extract users who installed the app in September 2018.\n2. Extract users who uninstalled the app between September 1 and October 7, 2018.\n3. Extract users who experienced app crashes between September 1 and October 7, 2018.\n4. Combine these datasets to analyze the relationship between crashes and uninstallations.\n5. Calculate the percentage of users who uninstalled within a week and experienced crashes to determine the impact of app crashes on user retention.",
        "special_function": [
            "date-functions/DATE",
            "date-functions/DATE_DIFF",
            "date-functions/FORMAT_DATE",
            "date-functions/PARSE_DATE",
            "conditional-functions/CASE",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "local002",
        "db": "E_commerce",
        "question": "Can you calculate the 5-day symmetric moving average of predicted toy sales for December 5 to 8, 2018, using daily sales data from January 1, 2017, to August 29, 2018, with a simple linear regression model? Provide the total of the moving averages for those four days.",
        "SQL": "WITH DailySalesPerCategory AS (\n    SELECT\n        DATE(order_purchase_timestamp) AS date,\n        -- Days since 2017-01-01\n        CAST(JULIANDAY(order_purchase_timestamp) - JULIANDAY('2017-01-01') AS INTEGER) AS day,\n        product_category_name_english AS category,\n        SUM(price) AS sales\n    FROM\n        orders\n        JOIN order_items USING (order_id)\n        JOIN products USING (product_id)\n        JOIN product_category_name_translation USING (product_category_name)\n    WHERE\n        order_purchase_timestamp BETWEEN '2017-01-01' AND '2018-08-29'\n        AND category = 'toys'\n    GROUP BY\n        day,\n        product_category_name_english\n),\nLmPerCategory AS (\n    SELECT\n        category,\n        -- Slope\n        (COUNT(*) * SUM(day * sales) - SUM(day) * SUM(sales)) / \n            (COUNT(*) * SUM(day * day) - SUM(day) * SUM(day))\n            AS slope,\n        -- Intercept\n        (SUM(sales) -\n            ((COUNT(*) * SUM(day * sales) - SUM(day) * SUM(sales)) / \n            (COUNT(*) * SUM(day * day) - SUM(day) * SUM(day))) *\n            SUM(day)) / COUNT(*)\n            AS intercept\n    FROM\n        DailySalesPerCategory\n    GROUP BY\n        category\n),\nForecastedSales AS (\n    SELECT\n        DATE(date, '+1 year') AS date,\n        category,\n        -- Increase in predicted sales * sales 1 year ago\n        (intercept + slope * (day + CAST(JULIANDAY('2018-12-31') - JULIANDAY('2017-12-31') AS INTEGER)))\n            / (intercept + slope * day) * sales\n            AS forecasted_sales\n    FROM DailySalesPerCategory\n        JOIN LmPerCategory USING (category)\n    -- Filter for days of December 2018\n    WHERE day + CAST(JULIANDAY('2018-12-31') - JULIANDAY('2017-12-31') AS INTEGER)\n        BETWEEN CAST(JULIANDAY('2018-12-01') - JULIANDAY('2017-01-01') AS INTEGER)\n        AND CAST(JULIANDAY('2018-12-31') - JULIANDAY('2017-01-01') AS INTEGER)\n),\nAVGForecastedSales AS (\n  SELECT\n    -- 5-day moving average\n    AVG(forecasted_sales)\n      OVER (PARTITION BY category ORDER BY date ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING) AS moving_avg_sales,\n    date\n  FROM ForecastedSales\n  WHERE date IN ('2018-12-05', '2018-12-06', '2018-12-07', '2018-12-08')\n)\n\nSELECT SUM(moving_avg_sales) AS total_sales\nFROM AVGForecastedSales",
        "external_knowledge": null,
        "plan": "Calculate the 5-day moving average of forecasted sales for December 2018 for toys category based on a linear regression model of daily sales data from January 1, 2017, to August 29, 2018.",
        "special_function": null
    },
    {
        "instance_id": "local003",
        "db": "E_commerce",
        "question": "According to the RFM definition document, how much is the average sales per order for each customer within distinct RFM segments, considering only 'delivered' orders? Please rank the customers into segments to analyze differences in average sales across these segments",
        "SQL": "WITH RecencyScore AS (\n    SELECT customer_unique_id,\n           MAX(order_purchase_timestamp) AS last_purchase,\n           NTILE(5) OVER (ORDER BY MAX(order_purchase_timestamp) DESC) AS recency\n    FROM orders\n        JOIN customers USING (customer_id)\n    WHERE order_status = 'delivered'\n    GROUP BY customer_unique_id\n),\nFrequencyScore AS (\n    SELECT customer_unique_id,\n           COUNT(order_id) AS total_orders,\n           NTILE(5) OVER (ORDER BY COUNT(order_id) DESC) AS frequency\n    FROM orders\n        JOIN customers USING (customer_id)\n    WHERE order_status = 'delivered'\n    GROUP BY customer_unique_id\n),\nMonetaryScore AS (\n    SELECT customer_unique_id,\n           SUM(price) AS total_spent,\n           NTILE(5) OVER (ORDER BY SUM(price) DESC) AS monetary\n    FROM orders\n        JOIN order_items USING (order_id)\n        JOIN customers USING (customer_id)\n    WHERE order_status = 'delivered'\n    GROUP BY customer_unique_id\n),\n\n-- 2. Assign each customer to a group\nRFM AS (\n    SELECT last_purchase, total_orders, total_spent,\n        CASE\n            WHEN recency = 1 AND frequency + monetary IN (1, 2, 3, 4) THEN \"Champions\"\n            WHEN recency IN (4, 5) AND frequency + monetary IN (1, 2) THEN \"Can't Lose Them\"\n            WHEN recency IN (4, 5) AND frequency + monetary IN (3, 4, 5, 6) THEN \"Hibernating\"\n            WHEN recency IN (4, 5) AND frequency + monetary IN (7, 8, 9, 10) THEN \"Lost\"\n            WHEN recency IN (2, 3) AND frequency + monetary IN (1, 2, 3, 4) THEN \"Loyal Customers\"\n            WHEN recency = 3 AND frequency + monetary IN (5, 6) THEN \"Needs Attention\"\n            WHEN recency = 1 AND frequency + monetary IN (7, 8) THEN \"Recent Users\"\n            WHEN recency = 1 AND frequency + monetary IN (5, 6) OR\n                recency = 2 AND frequency + monetary IN (5, 6, 7, 8) THEN \"Potentital Loyalists\"\n            WHEN recency = 1 AND frequency + monetary IN (9, 10) THEN \"Price Sensitive\"\n            WHEN recency = 2 AND frequency + monetary IN (9, 10) THEN \"Promising\"\n            WHEN recency = 3 AND frequency + monetary IN (7, 8, 9, 10) THEN \"About to Sleep\"\n        END AS RFM_Bucket\n    FROM RecencyScore\n        JOIN FrequencyScore USING (customer_unique_id)\n        JOIN MonetaryScore USING (customer_unique_id)\n)\n\nSELECT RFM_Bucket, \n       AVG(total_spent / total_orders) AS avg_sales_per_customer\nFROM RFM\nGROUP BY RFM_Bucket",
        "external_knowledge": "RFM.md",
        "plan": "Classify customers into RFM buckets based on recency, frequency, and monetary scores, then calculate and display group statistics, including average days since the last purchase, average sales per customer, and customer count for each RFM bucket.",
        "special_function": null
    },
    {
        "instance_id": "local004",
        "db": "E_commerce",
        "question": "Could you tell me the number of orders, average payment per order and customer lifespan in weeks of the 3 custumers with the highest average payment per order. Attention: I want the lifespan in float number if it's longer than one week, otherwise set it to be 1.0.",
        "SQL": "WITH CustomerData AS (\n    SELECT\n        customer_unique_id,\n        COUNT(DISTINCT orders.order_id) AS order_count,\n        SUM(payment_value) AS total_payment,\n        JULIANDAY(MIN(order_purchase_timestamp)) AS first_order_day,\n        JULIANDAY(MAX(order_purchase_timestamp)) AS last_order_day\n    FROM customers\n        JOIN orders USING (customer_id)\n        JOIN order_payments USING (order_id)\n    GROUP BY customer_unique_id\n)\nSELECT\n    customer_unique_id,\n    order_count AS PF,\n    ROUND(total_payment / order_count, 2) AS AOV,\n    CASE\n        WHEN (last_order_day - first_order_day) < 7 THEN\n            1\n        ELSE\n            (last_order_day - first_order_day) / 7\n        END AS ACL\nFROM CustomerData\nORDER BY AOV DESC\nLIMIT 3",
        "external_knowledge": null,
        "plan": "Calculate each customer's order frequency (PF), average order value (AOV), and average customer lifespan (ACL) based on their order history, zip code prefix, and payment data.",
        "special_function": null
    },
    {
        "instance_id": "local007",
        "db": "Baseball",
        "question": "Could you help me calculate the average single career span value in years for all baseball players? Please precise the result as a float number. If it's a full year, we count it as one year. If it's less than a full year but full months, we consider 12 months as one year. If it's less than a month, we consider 365 days as one year.",
        "SQL": "with player_career_span as (\n    select \n        b.player_name,\n        b.debut,\n        b.final_game,\n        round(b.year, 2) + round(round(b.month, 2) / round(12, 2), 2) + round(round(b.day, 2) / round(365, 2), 2) as career_span\n    from (\n        select\n            a.player_name,\n            a.debut,\n            a.final_game,\n            case when a.year > 0 then a.year else -a.year end as year,\n            case when a.month > 0 then a.month else -a.month end as month,\n            case when a.days > 0 then a.days else -a.days end as day\n        from (\n            select \n                name_given as player_name,\n                debut,\n                final_game,\n                (strftime('%Y', final_game) - strftime('%Y', debut)) as year,\n                (strftime('%m', final_game) - strftime('%m', debut)) as month,\n                (strftime('%d', final_game) - strftime('%d', debut)) as days\n            from player\n        ) a\n    ) b\n)\n\nselect \n    avg(career_span) as average_career_span\nfrom \n    player_career_span;",
        "external_knowledge": null,
        "plan": "Calculate the career span of baseball players, including their debut and final game dates, by extracting the year, month, and day differences between the debut and final game dates, and combining these into a single career span value.",
        "special_function": null
    },
    {
        "instance_id": "local008",
        "db": "Baseball",
        "question": "I would like to know the given names of baseball players who have achieved the highest value of games played, runs, hits, and home runs, with their corresponding score values.",
        "SQL": "WITH player_stats AS (\n    SELECT\n        b.player_id,\n        p.name_given AS player_name,\n        SUM(b.g) AS games_played,\n        SUM(b.r) AS runs,\n        SUM(b.h) AS hits,\n        SUM(b.hr) AS home_runs\n    FROM player p\n    JOIN batting b ON p.player_id = b.player_id\n    GROUP BY b.player_id, p.name_given\n)\n\nSELECT 'Games Played' AS Category, player_name AS Player_Name, games_played AS Batting_Table_Topper\nFROM player_stats\nWHERE games_played = (SELECT MAX(games_played) FROM player_stats)\n\nUNION ALL\n\nSELECT 'Runs' AS Category, player_name AS Player_Name, runs AS Batting_Table_Topper\nFROM player_stats\nWHERE runs = (SELECT MAX(runs) FROM player_stats)\n\nUNION ALL\n\nSELECT 'Hits' AS Category, player_name AS Player_Name, hits AS Batting_Table_Topper\nFROM player_stats\nWHERE hits = (SELECT MAX(hits) FROM player_stats)\n\nUNION ALL\n\nSELECT 'Home Runs' AS Category, player_name AS Player_Name, home_runs AS Batting_Table_Topper\nFROM player_stats\nWHERE home_runs = (SELECT MAX(home_runs) FROM player_stats);",
        "external_knowledge": null,
        "plan": "1. **Calculate Aggregate Statistics for Each Player:**\n   - Create a temporary dataset that includes each player\u2019s unique identifier, name, and the total number of games played, runs, hits, and home runs they have achieved.\n   - This involves joining the player information with their performance data and summarizing these performance metrics.\n\n2. **Identify Top Performer in Games Played:**\n   - Query the temporary dataset to find the player(s) with the highest total number of games played.\n   - Label the category appropriately and select the player\u2019s name and their total games played.\n\n3. **Identify Top Performer in Runs:**\n   - Query the temporary dataset to find the player(s) with the highest total number of runs scored.\n   - Label the category appropriately and select the player\u2019s name and their total runs.\n\n4. **Identify Top Performer in Hits:**\n   - Query the temporary dataset to find the player(s) with the highest total number of hits.\n   - Label the category appropriately and select the player\u2019s name and their total hits.\n\n5. **Identify Top Performer in Home Runs:**\n   - Query the temporary dataset to find the player(s) with the highest total number of home runs.\n   - Label the category appropriately and select the player\u2019s name and their total home runs.\n\n6. **Combine Results into a Single Output:**\n   - Use a union operation to combine the results of the top performers in each category into a single output set.\n   - Ensure each row in the final output specifies the category, the player's name, and the corresponding top performance metric.",
        "special_function": null
    },
    {
        "instance_id": "local009",
        "db": "Airlines",
        "question": "What is the distance of the longest route where Abakan is either the departure or destination city (in kilometers)?",
        "SQL": "WITH FLIGHT_INFO AS (\n    SELECT    \n        flights.flight_id,\n        json_extract(departure.city, '$.en') AS from_city,\n        CAST(SUBSTR(departure.coordinates, 2, INSTR(departure.coordinates, ',') - 2) AS REAL) AS from_longitude,\n        CAST(SUBSTR(departure.coordinates, INSTR(departure.coordinates, ',') + 1, LENGTH(departure.coordinates) - INSTR(departure.coordinates, ',') - 2) AS REAL) AS from_latitude,\n        json_extract(arrival.city, '$.en') AS to_city,\n        CAST(SUBSTR(arrival.coordinates, 2, INSTR(arrival.coordinates, ',') - 2) AS REAL) AS to_longitude,\n        CAST(SUBSTR(arrival.coordinates, INSTR(arrival.coordinates, ',') + 1, LENGTH(arrival.coordinates) - INSTR(arrival.coordinates, ',') - 2) AS REAL) AS to_latitude\n    FROM\n        flights \n    LEFT JOIN airports_data AS departure ON flights.departure_airport = departure.airport_code\n    LEFT JOIN airports_data AS arrival ON flights.arrival_airport = arrival.airport_code\n),\nDISTANCES AS (\n    SELECT\n        flight_id,\n        from_city,\n        to_city,\n        CASE\n            WHEN from_city < to_city THEN from_city ELSE to_city END AS city1,\n            CASE\n            WHEN from_city < to_city THEN to_city ELSE from_city END AS city2,\n        2 * 6371 * ASIN(SQRT(\n            POWER(SIN(RADIANS((to_latitude - from_latitude) / 2)), 2) +\n            COS(RADIANS(from_latitude)) * COS(RADIANS(to_latitude)) *\n            POWER(SIN(RADIANS((to_longitude - from_longitude) / 2)), 2)\n        )) AS distance_km\n    FROM FLIGHT_INFO\n),\nALL_Route AS (\n    SELECT\n        city1,\n        city2,\n        distance_km,\n        COUNT(*) AS number_of_flights -- Count flights for both directions\n    FROM DISTANCES\n    WHERE (city1 = 'Abakan' OR city2 = 'Abakan')\n    GROUP BY city1, city2\n)\nSELECT \n    distance_km\nFROM ALL_Route\nORDER BY distance_km DESC\nLIMIT 1;",
        "external_knowledge": "haversine_formula.md",
        "plan": "Calculate the average distance for each unique combination of from_city and to_city",
        "special_function": null
    },
    {
        "instance_id": "local010",
        "db": "Airlines",
        "question": "Distribute all the unique city pairs into the distance ranges 0, 1000, 2000, 3000, 4000, 5000, and 6000+, based on their average distance of all routes between them. Then how many pairs are there in the distance range with the fewest unique city paires?",
        "SQL": "WITH FLIGHT_INFO AS (\n    SELECT    \n        flights.flight_id,\n        json_extract(departure.city, '$.en') AS from_city,\n        CAST(SUBSTR(departure.coordinates, 2, INSTR(departure.coordinates, ',') - 2) AS REAL) AS from_longitude,\n        CAST(SUBSTR(departure.coordinates, INSTR(departure.coordinates, ',') + 1, LENGTH(departure.coordinates) - INSTR(departure.coordinates, ',') - 2) AS REAL) AS from_latitude,\n        json_extract(arrival.city, '$.en') AS to_city,\n        CAST(SUBSTR(arrival.coordinates, 2, INSTR(arrival.coordinates, ',') - 2) AS REAL) AS to_longitude,\n        CAST(SUBSTR(arrival.coordinates, INSTR(arrival.coordinates, ',') + 1, LENGTH(arrival.coordinates) - INSTR(arrival.coordinates, ',') - 2) AS REAL) AS to_latitude\n    FROM\n        flights \n    LEFT JOIN airports_data AS departure ON flights.departure_airport = departure.airport_code\n    LEFT JOIN airports_data AS arrival ON flights.arrival_airport = arrival.airport_code\n)\n\n\n-- Create a histogram distribution of average_distance_km\nSELECT group_count FROM\n(\nSELECT\n    FLOOR(average_distance_km / 1000) * 1000 AS distance_range,\n    COUNT(*) AS group_count\nFROM (\n    -- Calculate the average distance for each unique combination of from_city and to_city\n    SELECT\n        from_city,\n        to_city,\n        AVG(distance_km) AS average_distance_km\n    FROM (\n        -- Subquery to calculate the distances as before\n        SELECT\n            from_city,\n            to_city,\n            -- Calculate the distance using the Haversine formula\n            2 * 6371 * ASIN(SQRT(\n                POWER(SIN(RADIANS((to_latitude - from_latitude) / 2)), 2) +\n                COS(RADIANS(from_latitude)) * COS(RADIANS(to_latitude)) *\n                POWER(SIN(RADIANS((to_longitude - from_longitude) / 2)), 2)\n            )) AS distance_km\n        FROM FLIGHT_INFO\n    ) AS subquery\n    GROUP BY from_city, to_city\n) AS distances\nGROUP BY distance_range\nORDER BY group_count\nLIMIT 1\n)",
        "external_knowledge": "haversine_formula.md",
        "plan": "Create a histogram distribution of the average distances (in km) between city pairs, grouping the distances into ranges of 1000 km.",
        "special_function": null
    },
    {
        "instance_id": "local015",
        "db": "California_Traffic_Collision",
        "question": "Help me respectively caulculate the percentage of motorcycle accident fatalities involving riders who were wearing helmets and those who weren't?",
        "SQL": "WITH base AS (\n    SELECT \n        col.case_id AS case_id,\n        col.motorcyclist_killed_count AS motorcyclist_killed_count,\n        CASE WHEN party.party_safety_equipment_1 = 'driver, motorcycle helmet used' THEN 1\n             WHEN party.party_safety_equipment_2 = 'driver, motorcycle helmet used' THEN 1\n             WHEN party.party_safety_equipment_1 = 'passenger, motorcycle helmet used' THEN 1\n             WHEN party.party_safety_equipment_2 = 'passenger, motorcycle helmet used' THEN 1\n             ELSE 0 END AS helmet_used,\n        CASE WHEN party.party_safety_equipment_1 = 'driver, motorcycle helmet not used' THEN 1\n             WHEN party.party_safety_equipment_2 = 'driver, motorcycle helmet not used' THEN 1\n             WHEN party.party_safety_equipment_1 = 'passenger, motorcycle helmet not used' THEN 1\n             WHEN party.party_safety_equipment_2 = 'passenger, motorcycle helmet not used' THEN 1\n             ELSE 0 END AS helmet_not_used\n    FROM collisions col\n    JOIN parties party\n        ON col.case_id = party.case_id\n    WHERE \n        col.motorcycle_collision = 1\n        AND party.party_age IS NOT NULL\n    GROUP BY 1, 2\n)\nSELECT \n    ROUND(SUM(CASE WHEN helmet_used = 1 THEN motorcyclist_killed_count ELSE 0 END) * 100.0 / NULLIF(COUNT(CASE WHEN helmet_used = 1 THEN case_id END), 0), 2) AS percent_killed_helmet_used,\n    ROUND(SUM(CASE WHEN helmet_not_used = 1 THEN motorcyclist_killed_count ELSE 0 END) * 100.0 / NULLIF(COUNT(CASE WHEN helmet_not_used = 1 THEN case_id END), 0), 2) AS percent_killed_helmet_not_used\nFROM \n    base",
        "external_knowledge": null,
        "plan": "Calculate the percentage of motorcycle collisions resulting in fatalities where either the driver or passenger was not wearing a helmet.",
        "special_function": null
    },
    {
        "instance_id": "local017",
        "db": "California_Traffic_Collision",
        "question": "In which year were the two most common causes of traffic accidents different from those in other years?",
        "SQL": "WITH AnnualTotals AS (\n    SELECT \n        STRFTIME('%Y', collision_date) AS Year, \n        COUNT(case_id) AS AnnualTotal\n    FROM \n        collisions\n    GROUP BY \n        Year\n),\nCategoryTotals AS (\n    SELECT \n        STRFTIME('%Y', collision_date) AS Year,\n        pcf_violation_category AS Category,\n        COUNT(case_id) AS Subtotal\n    FROM \n        collisions\n    GROUP BY \n        Year, Category\n),\nCategoryPercentages AS (\n    SELECT \n        ct.Year,\n        ct.Category,\n        ROUND((ct.Subtotal * 100.0) / at.AnnualTotal, 1) AS PercentageOfAnnualRoadIncidents\n    FROM \n        CategoryTotals ct\n    JOIN \n        AnnualTotals at ON ct.Year = at.Year\n),\nRankedCategories AS (\n    SELECT\n        Year,\n        Category,\n        PercentageOfAnnualRoadIncidents,\n        ROW_NUMBER() OVER (PARTITION BY Year ORDER BY PercentageOfAnnualRoadIncidents DESC) AS Rank\n    FROM\n        CategoryPercentages\n),\nTopTwoCategories AS (\n    SELECT\n        Year,\n        GROUP_CONCAT(Category, ', ') AS TopCategories\n    FROM\n        RankedCategories\n    WHERE\n        Rank <= 2\n    GROUP BY\n        Year\n),\nUniqueYear AS (\n    SELECT\n        Year\n    FROM\n        TopTwoCategories\n    GROUP BY\n        TopCategories\n    HAVING COUNT(Year) = 1\n),\nresults AS (\nSELECT \n    rc.Year, \n    rc.Category, \n    rc.PercentageOfAnnualRoadIncidents\nFROM \n    UniqueYear u\nJOIN \n    RankedCategories rc ON u.Year = rc.Year\nWHERE \n    rc.Rank <= 2\n)\n\nSELECT distinct Year FROM results",
        "external_knowledge": null,
        "plan": "Retrieve the top three categories of traffic violations for each year based on their percentage of total road incidents, calculated by dividing the number of incidents in each category by the annual total of road incidents.",
        "special_function": null
    },
    {
        "instance_id": "local018",
        "db": "California_Traffic_Collision",
        "question": "For the most common cause of traffic accidents in 2021, how much did its share (percentage in the annual roal incidents) decrease compared to 10 years earlier?",
        "SQL": "WITH AnnualTotals AS (\n    SELECT \n        STRFTIME('%Y', collision_date) AS Year, \n        COUNT(case_id) AS AnnualTotal\n    FROM \n        collisions\n    GROUP BY \n        Year\n),\nCategoryTotals AS (\n    SELECT \n        STRFTIME('%Y', collision_date) AS Year,\n        pcf_violation_category AS Category,\n        COUNT(case_id) AS Subtotal\n    FROM \n        collisions\n    GROUP BY \n        Year, Category\n),\nCategoryPercentages AS (\n    SELECT \n        ct.Year,\n        ct.Category,\n        ((ct.Subtotal * 100.0) / at.AnnualTotal) AS PercentageOfAnnualRoadIncidents\n    FROM \n        CategoryTotals ct\n    JOIN \n        AnnualTotals at ON ct.Year = at.Year\n),\nRankedCategories AS (\n    SELECT\n        Year,\n        Category,\n        PercentageOfAnnualRoadIncidents,\n        ROW_NUMBER() OVER (PARTITION BY Year ORDER BY PercentageOfAnnualRoadIncidents DESC) AS Rank\n    FROM\n        CategoryPercentages\n),\nTopCategories AS (\nSELECT \n    Year, \n    Category, \n    PercentageOfAnnualRoadIncidents\nFROM \n    RankedCategories\nWHERE\n    Rank = 1\n),\nTopCategoryIn2021 AS (\n    SELECT \n        Category, \n        PercentageOfAnnualRoadIncidents\n    FROM \n        TopCategories\n    WHERE \n        Year = '2021'\n),\nCategoryPercentageIn2011 AS (\n    SELECT \n        PercentageOfAnnualRoadIncidents\n    FROM \n        RankedCategories\n    WHERE \n        Year = '2011'\n        AND Category = (SELECT Category FROM TopCategoryIn2021)\n)\nSELECT \n    (SELECT PercentageOfAnnualRoadIncidents FROM TopCategoryIn2021)\n            -\n    (SELECT PercentageOfAnnualRoadIncidents FROM CategoryPercentageIn2011) AS answer;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local019",
        "db": "WWE",
        "question": "For the NXT title that had the shortest match (excluding titles with \"title change\"), what were the names of the two wrestlers involved?",
        "SQL": "WITH MatchDetails AS (\n    SELECT\n        b.name AS titles,\n        m.duration AS match_duration,\n        w1.name || ' vs ' || w2.name AS matches,\n        m.win_type AS win_type,\n        l.name AS location,\n        e.name AS event,\n        ROW_NUMBER() OVER (PARTITION BY b.name ORDER BY m.duration ASC) AS rank\n    FROM \n        Belts b\n    INNER JOIN Matches m ON m.title_id = b.id\n    INNER JOIN Wrestlers w1 ON w1.id = m.winner_id\n    INNER JOIN Wrestlers w2 ON w2.id = m.loser_id\n    INNER JOIN Cards c ON c.id = m.card_id\n    INNER JOIN Locations l ON l.id = c.location_id\n    INNER JOIN Events e ON e.id = c.event_id\n    INNER JOIN Promotions p ON p.id = c.promotion_id\n    WHERE\n        p.name = 'NXT'\n        AND m.duration <> ''\n        AND b.name <> ''\n        AND b.name NOT IN (\n            SELECT name \n            FROM Belts \n            WHERE name LIKE '%title change%'\n        )\n),\nRank1 AS (\nSELECT \n    titles,\n    match_duration,\n    matches,\n    win_type,\n    location,\n    event\nFROM \n    MatchDetails\nWHERE \n    rank = 1\n)\nSELECT\n    SUBSTR(matches, 1, INSTR(matches, ' vs ') - 1) AS wrestler1,\n    SUBSTR(matches, INSTR(matches, ' vs ') + 4) AS wrestler2\nFROM\nRank1\nORDER BY match_duration \nLIMIT 1",
        "external_knowledge": null,
        "plan": "Retrieve the shortest match details for each title in the NXT promotion, excluding matches with empty durations and titles containing \"title change\". Include the title name, match duration, wrestlers involved, win type, location, and event name.",
        "special_function": null
    },
    {
        "instance_id": "local026",
        "db": "IPL",
        "question": "Please help me find the top 3 bowlers who conceded the maximum runs in a single over, along with the corresponding matches.",
        "SQL": "WITH combined_runs AS (\n    SELECT match_id, over_id, ball_id, innings_no, runs_scored AS runs\n    FROM batsman_scored\n    UNION ALL\n    SELECT match_id, over_id, ball_id, innings_no, extra_runs AS runs\n    FROM extra_runs\n),\nover_runs AS (\n    SELECT match_id, innings_no, over_id, SUM(runs) AS runs_scored\n    FROM combined_runs\n    GROUP BY match_id, innings_no, over_id\n),\nmax_over_runs AS (\n    SELECT match_id, MAX(runs_scored) AS max_runs\n    FROM over_runs\n    GROUP BY match_id\n),\ntop_overs AS (\n    SELECT o.match_id, o.innings_no, o.over_id, o.runs_scored\n    FROM over_runs o\n    JOIN max_over_runs m ON o.match_id = m.match_id AND o.runs_scored = m.max_runs\n),\ntop_bowlers AS (\n    SELECT\n        bb.match_id,\n        t.runs_scored AS maximum_runs,\n        bb.bowler\n    FROM ball_by_ball bb\n    JOIN top_overs t ON bb.match_id = t.match_id\n    AND bb.innings_no = t.innings_no\n    AND bb.over_id = t.over_id\n    GROUP BY bb.match_id, t.runs_scored, bb.bowler\n)\nSELECT\n    b.match_id,\n    p.player_name\nFROM (\n    SELECT *\n    FROM top_bowlers\n    ORDER BY maximum_runs DESC\n    LIMIT 3\n) b\nJOIN player p ON p.player_id = b.bowler\nORDER BY b.maximum_runs DESC, b.match_id, p.player_name;",
        "external_knowledge": null,
        "plan": "1. Combine runs scored by player and extra runs given into a single dataset named combined_runs.\n2. Aggregate the combined runs by match, innings, and over to calculate the total runs scored in each over, creating the over_runs dataset.\n3. Identify the maximum runs scored in a single over for each match and store this information in the max_over_runs dataset.\n4. Retrieve the details of overs where the maximum runs were scored for each match by joining over_runs and max_over_runs, resulting in the top_overs dataset.\n5. Determine the bowlers who bowled the overs with the maximum runs by joining ball_by_ball and top_overs, and group by match, maximum runs scored, and bowler to create the top_bowlers dataset.\n6. Select the top 3 overs with the highest maximum runs scored, ordering by maximum runs in descending order.\n7. Join the top_bowlers dataset with the player table to get the player names for the bowlers.\n8. Output the match ID and player names of the top 3 bowlers, ordered by maximum runs scored in descending order, match ID, and player name.",
        "special_function": null
    },
    {
        "instance_id": "local020",
        "db": "IPL",
        "question": "Which bowler has the lowest bowling average per wicket taken?",
        "SQL": "WITH runs_and_wickets AS (\n    SELECT \n        bowler,\n        SUM(CASE WHEN wt.match_id IS NOT NULL THEN 1 ELSE 0 END) AS wickets_taken,\n        SUM(COALESCE(er.extra_runs, 0) + COALESCE(bs.runs_scored, 0)) AS total_runs_given\n    FROM \n        ball_by_ball bb\n    LEFT JOIN \n        wicket_taken wt ON bb.match_id = wt.match_id \n            AND bb.over_id = wt.over_id \n            AND bb.ball_id = wt.ball_id \n            AND bb.innings_no = wt.innings_no\n    LEFT JOIN \n        extra_runs er ON bb.match_id = er.match_id \n            AND bb.over_id = er.over_id \n            AND bb.ball_id = er.ball_id \n            AND bb.innings_no = er.innings_no\n    LEFT JOIN \n        batsman_scored bs ON bb.match_id = bs.match_id \n            AND bb.over_id = bs.over_id \n            AND bb.ball_id = bs.ball_id \n            AND bb.innings_no = bs.innings_no\n    GROUP BY \n        bowler\n),\nbowling_averages AS (\n    SELECT \n        bowler AS player_id, \n        ROUND(total_runs_given / NULLIF(wickets_taken, 0), 3) AS bowling_avg,\n        ROW_NUMBER() OVER (ORDER BY total_runs_given / NULLIF(wickets_taken, 0)) AS rn\n    FROM \n        runs_and_wickets\n    WHERE \n        wickets_taken > 0\n)\nSELECT \n    p.player_name \nFROM \n    player p\nJOIN \n    bowling_averages ba ON p.player_id = ba.player_id \nWHERE \n    ba.rn = 1\nORDER BY \n    p.player_name;",
        "external_knowledge": null,
        "plan": "1. Join the ball_by_ball dataset with the wicket_taken, extra_runs, and batsman_scored datasets to identify wickets taken, extra runs given, and runs scored for each ball.\n2. Aggregate the data by bowler to calculate the total number of wickets taken and the total runs given.\n3. Calculate the bowling average for each bowler by dividing the total runs given by the number of wickets taken.\n4. Assign a rank to each bowler based on their bowling average, with the lowest average ranked first.\n5. Join the player table with the bowling_averages CTE to get the player names.\n6. Filter to select the player with the best (lowest) bowling average.",
        "special_function": null
    },
    {
        "instance_id": "local021",
        "db": "IPL",
        "question": "Could you show me the average total score of strikers who have scored more than 50 runs in at least one match?",
        "SQL": "WITH derived AS (\n    SELECT \n        bbb.match_id, \n        bbb.over_id, \n        bbb.ball_id, \n        bbb.innings_no, \n        bbb.striker, \n        bs.runs_scored AS runs \n    FROM \n        ball_by_ball bbb\n    JOIN \n        batsman_scored bs\n    ON \n        bbb.match_id = bs.match_id \n        AND bbb.over_id = bs.over_id \n        AND bbb.ball_id = bs.ball_id \n        AND bbb.innings_no = bs.innings_no\n),\nfifty_plus_scorers AS (\n    SELECT DISTINCT\n        striker AS player_id \n    FROM \n        derived \n    GROUP BY \n        match_id, striker \n    HAVING \n        SUM(runs) > 50\n),\ntotal_runs AS (\n    SELECT \n        striker AS player_id, \n        SUM(runs) AS total_runs \n    FROM \n        derived \n    GROUP BY \n        striker\n),\nqualified_scorers AS (\n    SELECT \n        tr.player_id, \n        tr.total_runs \n    FROM \n        total_runs tr\n    JOIN \n        fifty_plus_scorers fps\n    ON \n        tr.player_id = fps.player_id\n)\nSELECT \n    AVG(total_runs) AS average_runs \nFROM \n    qualified_scorers;",
        "external_knowledge": null,
        "plan": "Get the total runs scored by players who have scored more than 50 runs in at least one match.\n List the names of all players and their runs scored (who have scored at least 50 runs in any match). Order result in decreasing order of runs scored. Resolve ties alphabetically. ",
        "special_function": null
    },
    {
        "instance_id": "local022",
        "db": "IPL",
        "question": "Show me the names of strikers who scored no less than 100 runs in a match, but their team lost the game?",
        "SQL": "-- Step 1: Calculate players' total runs in each match\nWITH player_runs AS (\n    SELECT \n        bbb.striker AS player_id, \n        bbb.match_id, \n        SUM(bsc.runs_scored) AS total_runs \n    FROM \n        ball_by_ball AS bbb\n    JOIN \n        batsman_scored AS bsc\n    ON \n        bbb.match_id = bsc.match_id \n        AND bbb.over_id = bsc.over_id \n        AND bbb.ball_id = bsc.ball_id \n        AND bbb.innings_no = bsc.innings_no\n    GROUP BY \n        bbb.striker, bbb.match_id\n    HAVING \n        SUM(bsc.runs_scored) >= 100\n),\n\n-- Step 2: Identify losing teams for each match\nlosing_teams AS (\n    SELECT \n        match_id, \n        CASE \n            WHEN match_winner = team_1 THEN team_2 \n            ELSE team_1 \n        END AS loser \n    FROM \n        match\n),\n\n-- Step 3: Combine the above results to get players who scored 100 or more runs in losing teams\nplayers_in_losing_teams AS (\n    SELECT \n        pr.player_id, \n        pr.match_id \n    FROM \n        player_runs AS pr\n    JOIN \n        losing_teams AS lt\n    ON \n        pr.match_id = lt.match_id\n    JOIN \n        player_match AS pm\n    ON \n        pr.player_id = pm.player_id \n        AND pr.match_id = pm.match_id \n        AND lt.loser = pm.team_id\n)\n\n-- Step 4: Select distinct player names from the player table\nSELECT DISTINCT \n    p.player_name \nFROM \n    player AS p\nJOIN \n    players_in_losing_teams AS plt\nON \n    p.player_id = plt.player_id\nORDER BY \n    p.player_name;",
        "external_knowledge": null,
        "plan": "List the player names who scored a century but their teams lost the match. Order results alphabetically.\nFind players who scored more than 100 points in a game where their team lost, sorted by player name",
        "special_function": null
    },
    {
        "instance_id": "local023",
        "db": "IPL",
        "question": "Please help me find the names of top 5 players with the highest average runs per match in season 5, along with their batting averages.",
        "SQL": "WITH runs_scored AS (\n    SELECT \n        bb.striker AS player_id,\n        bb.match_id,\n        bs.runs_scored AS runs\n    FROM \n        ball_by_ball AS bb\n    JOIN \n        batsman_scored AS bs ON bb.match_id = bs.match_id \n            AND bb.over_id = bs.over_id \n            AND bb.ball_id = bs.ball_id \n            AND bb.innings_no = bs.innings_no\n    WHERE \n        bb.match_id IN (SELECT match_id FROM match WHERE season_id = 5)\n),\ntotal_runs AS (\n    SELECT \n        player_id, \n        match_id, \n        SUM(runs) AS total_runs \n    FROM \n        runs_scored \n    GROUP BY \n        player_id, match_id\n),\nbatting_averages AS (\n    SELECT \n        player_id, \n        SUM(total_runs) AS runs, \n        COUNT(match_id) AS num_matches,\n        ROUND(SUM(total_runs) / CAST(COUNT(match_id) AS FLOAT), 3) AS batting_avg\n    FROM \n        total_runs \n    GROUP BY \n        player_id \n    ORDER BY \n        batting_avg DESC \n    LIMIT 5\n)\nSELECT \n    p.player_name,\n    b.batting_avg\nFROM \n    player AS p\nJOIN \n    batting_averages AS b ON p.player_id = b.player_id\nORDER BY \n    b.batting_avg DESC;",
        "external_knowledge": null,
        "plan": "List the names of top 10 players who have the best batting average in season 5. Batting average can be calculated according to the following formula:\nbatting average(player) = Number of runs scored by player/ Number of matches player has batted in                         [2 marks]\nThe output should contain exactly 10 rows. Report results upto 3 decimal places. Resolve ties alpha- betically\nFind the top 10 players with the highest batting averages in season 5, sorted by batting average and player name",
        "special_function": null
    },
    {
        "instance_id": "local024",
        "db": "IPL",
        "question": "Can you help me find the top 5 countries with the highest average runs per match for all players across all seasons, and also include their batting averages?",
        "SQL": "WITH player_runs AS (\n    SELECT \n        b.striker AS player_id, \n        b.match_id, \n        SUM(bs.runs_scored) AS total_runs \n    FROM \n        ball_by_ball b\n        JOIN batsman_scored bs\n        ON b.match_id = bs.match_id \n        AND b.over_id = bs.over_id \n        AND b.ball_id = bs.ball_id \n        AND b.innings_no = bs.innings_no\n    GROUP BY \n        b.striker, b.match_id\n), \nplayer_averages AS (\n    SELECT \n        player_id, \n        AVG(total_runs) AS batting_avg \n    FROM \n        player_runs \n    GROUP BY \n        player_id\n), \ncountry_averages AS (\n    SELECT \n        p.country_name, \n        AVG(pa.batting_avg) AS country_batting_avg \n    FROM \n        player_averages pa\n        JOIN player p\n        ON pa.player_id = p.player_id\n    GROUP BY \n        p.country_name\n) \nSELECT \n    country_name, country_batting_avg\nFROM \n    country_averages \nORDER BY \n    country_batting_avg DESC\nLIMIT 5;",
        "external_knowledge": null,
        "plan": "Using the formula provided in the previous query, find out the batting average of all players across all seasons. The batting average of a country X is:\n                country batting average = \udbff\udc00 Summationover n, n\u2208player-of-country-X [ batting average(n)/ |player of country X| ]        [3 marks]\nList the top 5 countries with the highest country batting average. Report results upto 3 decimal places. Output may contain 5 or more rows",
        "special_function": null
    },
    {
        "instance_id": "local025",
        "db": "IPL",
        "question": "Please calculate the average of the highest runs conceded in a single over for each match.",
        "SQL": "WITH consolidated_runs AS (\n    SELECT\n        match_id,\n        over_id,\n        innings_no,\n        runs_scored AS runs\n    FROM\n        batsman_scored\n    UNION ALL\n    SELECT\n        match_id,\n        over_id,\n        innings_no,\n        extra_runs AS runs\n    FROM\n        extra_runs\n),\nsummarized_runs AS (\n    SELECT\n        match_id,\n        innings_no,\n        over_id,\n        SUM(runs) AS total_runs\n    FROM\n        consolidated_runs\n    GROUP BY\n        match_id, innings_no, over_id\n),\nmax_runs_per_match AS (\n    SELECT\n        match_id,\n        MAX(total_runs) AS max_runs\n    FROM\n        summarized_runs\n    GROUP BY\n        match_id\n),\nover_details AS (\n    SELECT\n        sr.total_runs,\n        bb.bowler\n    FROM\n        summarized_runs sr\n    JOIN\n        max_runs_per_match mr ON sr.match_id = mr.match_id AND sr.total_runs = mr.max_runs\n    JOIN\n        ball_by_ball bb ON sr.match_id = bb.match_id AND sr.innings_no = bb.innings_no AND sr.over_id = bb.over_id\n    GROUP BY\n        sr.match_id, sr.innings_no, sr.over_id, sr.total_runs, bb.bowler\n),\nfinal_result AS (\n    SELECT\n        od.total_runs AS maximum_runs\n    FROM\n        over_details od\n    JOIN\n        player p ON od.bowler = p.player_id\n)\nSELECT\n    AVG(maximum_runs) AS average_maximum_runs\nFROM\n    final_result;",
        "external_knowledge": null,
        "plan": "Get the bowler and their player name for each match who conceded the maximum runs in a single over, along with the match ID and the number of runs scored.\nFor each match id, list the maximum runs scored in any over and the bowler bowling in that over. If there is more than one over having maximum runs, return all of them and order them in increasing order of over id. Order results in increasing order of match ids. (<match id, maximum runs, player name>) [3 marks]",
        "special_function": null
    },
    {
        "instance_id": "local028",
        "db": "Brazilian_E_Commerce",
        "question": "Could you generate a report that shows the number of delivered orders for each month in the years 2016, 2017, and 2018? Each column represents a year, and each row represents a month",
        "SQL": "SELECT\n  month AS month_no,\n  SUM(CASE WHEN a.year = '2016' THEN 1 ELSE 0 END) AS Year2016,\n  SUM(CASE WHEN a.year = '2017' THEN 1 ELSE 0 END) AS Year2017,\n  SUM(CASE WHEN a.year = '2018' THEN 1 ELSE 0 END) AS Year2018\nFROM\n(\n  SELECT \n    customer_id,\n    order_id,\n    order_delivered_customer_date,\n    order_status,\n    strftime('%Y', order_delivered_customer_date) AS Year,\n    strftime('%m', order_delivered_customer_date) AS Month\n  FROM olist_orders\n  WHERE order_status = 'delivered' \n    AND order_delivered_customer_date IS NOT NULL\n  GROUP BY customer_id, order_id, order_delivered_customer_date\n  ORDER BY order_delivered_customer_date ASC\n) a\nGROUP BY month\nORDER BY month_no ASC;",
        "external_knowledge": null,
        "plan": "1. **Filter and Extract Data:**\n   - Select records where the order status is 'delivered' and the delivery date is not null.\n   - From these records, extract the year and month of the delivery date.\n\n2. **Subquery Formation:**\n   - Group the filtered data by a unique combination of customer ID, order ID, and delivery date to avoid duplicates.\n   - Order the grouped data by the delivery date in ascending order.\n\n3. **Main Query Aggregation:**\n   - In the main query, group the results by the month extracted from the delivery date.\n\n4. **Conditional Summation:**\n   - For each month, count the number of orders delivered in the years 2016, 2017, and 2018 separately.\n   - Use conditional aggregation to sum up the counts for each specified year.\n\n5. **Order the Results:**\n   - Ensure the final results are ordered by the month number in ascending order.\n\nThis plan systematically filters, groups, and aggregates the data to generate a monthly report of delivered orders for each of the specified years.",
        "special_function": null
    },
    {
        "instance_id": "local031",
        "db": "Brazilian_E_Commerce",
        "question": "What is the highest monthly delivered orders volume in the year with the lowest annual delivered orders volume among 2016, 2017, and 2018?",
        "SQL": "WITH monthly_order_counts AS (\n    SELECT\n        strftime('%Y', order_delivered_customer_date) AS Year,\n        strftime('%m', order_delivered_customer_date) AS Month,\n        COUNT(*) AS MonthlyOrderCount\n    FROM olist_orders\n    WHERE order_status = 'delivered'\n      AND order_delivered_customer_date IS NOT NULL\n    GROUP BY Year, Month\n),\nyearly_order_counts AS (\n    SELECT\n        Year,\n        SUM(MonthlyOrderCount) AS TotalOrderCount\n    FROM monthly_order_counts\n    WHERE Year IN ('2016', '2017', '2018')\n    GROUP BY Year\n),\nmin_order_year AS (\n    SELECT\n        Year\n    FROM yearly_order_counts\n    ORDER BY TotalOrderCount ASC\n    LIMIT 1\n)\nSELECT\n    MAX(MonthlyOrderCount) AS max_monthly_order_count\nFROM monthly_order_counts\nWHERE Year = (SELECT Year FROM min_order_year);",
        "external_knowledge": null,
        "plan": "1. **Identify Monthly Order Counts**:\n   - Extract the year and month from the order delivery date.\n   - Count the total number of delivered orders for each month.\n   - Consider only delivered orders with a non-null delivery date.\n\n2. **Calculate Annual Order Counts**:\n   - Sum the monthly order counts to get the total number of orders for each year.\n   - Focus only on the years 2016, 2017, and 2018.\n\n3. **Determine the Year with the Lowest Annual Order Count**:\n   - From the calculated annual order counts, find the year with the smallest total number of orders.\n   - Select this year for further analysis.\n\n4. **Find the Highest Monthly Order Count in the Selected Year**:\n   - Within the year identified in the previous step, find the maximum monthly order count.\n   - Return this maximum value as the highest monthly delivered orders volume for the year with the lowest annual delivered orders volume.",
        "special_function": null
    },
    {
        "instance_id": "local029",
        "db": "Brazilian_E_Commerce",
        "question": "Please calculate the average payment value, city, and state for the top 3 customers with the most delivered orders.",
        "SQL": "WITH customer_orders AS (\n    SELECT\n        c.customer_unique_id,\n        COUNT(o.order_id) AS Total_Orders_By_Customers,\n        AVG(p.payment_value) AS Average_Payment_By_Customer,\n        c.customer_city,\n        c.customer_state\n    FROM olist_customers c\n    JOIN olist_orders o ON c.customer_id = o.customer_id\n    JOIN olist_order_payments p ON o.order_id = p.order_id\n    WHERE o.order_status = 'delivered'\n    GROUP BY c.customer_unique_id, c.customer_city, c.customer_state\n)\n\nSELECT \n    Average_Payment_By_Customer,\n    customer_city,\n    customer_state\nFROM customer_orders\nORDER BY Total_Orders_By_Customers DESC\nLIMIT 3;",
        "external_knowledge": null,
        "plan": "Calculate the total number of orders, average payment value, city, and state for each unique customer who had their orders delivered.",
        "special_function": null
    },
    {
        "instance_id": "local030",
        "db": "Brazilian_E_Commerce",
        "question": "Can you find the average payments and order counts for the five cities with the lowest total payments from delivered orders?",
        "SQL": "WITH bottom_five_cities AS (\n    SELECT \n        c.customer_city,\n        SUM(p.payment_value) AS Total_Payment_By_Customers,\n        COUNT(o.order_id) AS Total_Number_Of_Orders\n    FROM olist_customers c\n    JOIN olist_orders o ON c.customer_id = o.customer_id\n    JOIN olist_order_payments p ON o.order_id = p.order_id\n    WHERE o.order_status = 'delivered'\n    GROUP BY c.customer_city\n    ORDER BY Total_Payment_By_Customers ASC\n    LIMIT 5\n)\nSELECT \n    AVG(Total_Payment_By_Customers) AS Average_Total_Payment,\n    AVG(Total_Number_Of_Orders) AS Average_Total_Orders\nFROM bottom_five_cities;",
        "external_knowledge": null,
        "plan": "Retrieve the top 5 cities and states with the highest total customer payments, including the total number of orders and total payments by customers, for orders that have been delivered.",
        "special_function": null
    },
    {
        "instance_id": "local032",
        "db": "Brazilian_E_Commerce",
        "question": "Could you help me find the sellers respectively with the highest number of distinct customers, highest profit, highest number of distinct orders, and most 5-star ratings, in delivered orders, along with their corresponding values? ",
        "SQL": "SELECT * FROM (\n    SELECT\n        'Seller with most unique customers :-' AS Description,\n        s.seller_id AS Seller_ID,\n        COUNT(DISTINCT c.customer_unique_id) AS Value\n    FROM\n        olist_orders o\n        JOIN olist_order_items i ON o.order_id = i.order_id\n        JOIN olist_sellers s ON i.seller_id = s.seller_id\n        JOIN olist_customers c ON o.customer_id = c.customer_id\n    WHERE\n        o.order_status = 'delivered'\n    GROUP BY\n        s.seller_id\n    ORDER BY\n        Value DESC\n    LIMIT 1\n)\n\nUNION ALL\n\nSELECT * FROM (\n    SELECT\n        'Seller with highest Profit :-' AS Description,\n        s.seller_id AS Seller_ID,\n        SUM(i.price - i.freight_value) AS Value\n    FROM\n        olist_orders o\n        JOIN olist_order_items i ON o.order_id = i.order_id\n        JOIN olist_sellers s ON i.seller_id = s.seller_id\n    WHERE\n        o.order_status = 'delivered'\n    GROUP BY\n        s.seller_id\n    ORDER BY\n        Value DESC\n    LIMIT 1\n)\n\nUNION ALL\n\nSELECT * FROM (\n    SELECT\n        'Seller with most unique orders :-' AS Description,\n        s.seller_id AS Seller_ID,\n        COUNT(DISTINCT o.order_id) AS Value\n    FROM\n        olist_orders o\n        JOIN olist_order_items i ON o.order_id = i.order_id\n        JOIN olist_sellers s ON i.seller_id = s.seller_id\n    WHERE\n        o.order_status = 'delivered'\n    GROUP BY\n        s.seller_id\n    ORDER BY\n        Value DESC\n    LIMIT 1\n)\n\nUNION ALL\n\nSELECT * FROM (\n    SELECT\n        'Seller with most 5 star ratings :-' AS Description,\n        s.seller_id AS Seller_ID,\n        COUNT(*) AS Value\n    FROM\n        olist_orders o\n        JOIN olist_order_items i ON o.order_id = i.order_id\n        JOIN olist_sellers s ON i.seller_id = s.seller_id\n        JOIN olist_order_reviews r ON o.order_id = r.order_id\n    WHERE\n        o.order_status = 'delivered'\n        AND r.review_score = 5\n    GROUP BY\n        s.seller_id\n    ORDER BY\n        Value DESC\n    LIMIT 1\n);\n-- select * from\n-- (\n--     select\n--         'Seller with most unique customers :-' as Description,\n--         raw.seller_id as Seller_ID,\n--         count(distinct customer_unique_id) as Value\n--     from (\n--         select\n--             c.customer_id,\n--             c.customer_unique_id,\n--             s.seller_id,\n--             o.order_id,\n--             o.order_status,\n--             i.order_item_id,\n--             i.product_id,\n--             i.price,\n--             i.freight_value,\n--             s.seller_city,\n--             s.seller_state,\n--             c.customer_city,\n--             c.customer_state,\n--             o.order_delivered_customer_date\n--         from olist_orders o\n--         join olist_customers c\n--             on o.customer_id=c.customer_id\n--         join olist_order_items i\n--             on o.order_id=i.order_id\n--         join olist_sellers s\n--             on i.seller_id=s.seller_id\n--         where order_status='delivered'\n--         group by 1,2,3,5,6,7,8,9,10,11,12,13\n--     ) raw\n-- )\n-- union all\n-- select * from (\n--     select\n--         'Seller with highest Profit :-' as Description,\n--         raw.seller_id as Seller_ID,\n--         sum(raw.profit) as Value\n--     from (\n--         select\n--             c.customer_id,\n--             c.customer_unique_id,\n--             s.seller_id,\n--             o.order_id,\n--             o.order_status,\n--             i.order_item_id,\n--             i.product_id,\n--             i.price,\n--             i.freight_value,\n--             i.price - i.freight_value as profit,\n--             s.seller_city,\n--             s.seller_state,\n--             c.customer_city,\n--             c.customer_state,\n--             o.order_delivered_customer_date\n--         from olist_orders o\n--         join olist_customers c\n--             on o.customer_id=c.customer_id\n--         join olist_order_items i\n--             on o.order_id=i.order_id\n--         join olist_sellers s\n--             on i.seller_id=s.seller_id\n--         where order_status='delivered'\n--         group by 1,2,3,5,6,7,8,9,10,11,12,13\n--     ) raw\n-- )\n-- union all\n-- select * from (\n--     select\n--         'Seller with most unique orders :-' as Description,\n--         raw.seller_id as Seller_ID,\n--         count(distinct order_id) as Value\n--     from (\n--         select\n--             c.customer_id,\n--             c.customer_unique_id,\n--             s.seller_id,\n--             o.order_id,\n--             o.order_status,\n--             i.order_item_id,\n--             i.product_id,\n--             i.price,\n--             i.freight_value,\n--             s.seller_city,\n--             s.seller_state,\n--             c.customer_city,\n--             c.customer_state,\n--             o.order_delivered_customer_date\n--         from olist_orders o\n--         join olist_customers c\n--             on o.customer_id=c.customer_id\n--         join olist_order_items i\n--             on o.order_id=i.order_id\n--         join olist_sellers s\n--             on i.seller_id=s.seller_id\n--         where order_status='delivered'\n--         group by 1,2,3,5,6,7,8,9,10,11,12,13\n--     ) raw\n-- )\n-- union all\n-- select * from (\n--     select\n--         'Seller with most 5 star ratings :-' as Description,\n--         raw.seller_id as Seller_ID,\n--         count(raw.review_score) as Value\n--     from (\n--         select\n--             c.customer_id,\n--             c.customer_unique_id,\n--             s.seller_id,\n--             o.order_id,\n--             o.order_status,\n--             r.review_score,\n--             i.order_item_id,\n--             i.product_id,\n--             i.price,\n--             i.freight_value,\n--             s.seller_city,\n--             s.seller_state,\n--             c.customer_city,\n--             c.customer_state,\n--             o.order_delivered_customer_date\n--         from olist_orders o\n--         join olist_customers c\n--             on o.customer_id=c.customer_id\n--         join olist_order_items i\n--             on o.order_id=i.order_id\n--         join olist_sellers s\n--             on i.seller_id=s.seller_id\n--         join olist_order_reviews r\n--             on o.order_id=r.order_id\n--         where order_status='delivered'\n--         group by 1,2,3,5,6,7,8,9,10,11,12,13\n--         having r.review_score > 4\n--     ) raw\n-- )",
        "external_knowledge": null,
        "plan": "1. **Identify Sellers with Delivered Orders:**\n   - Begin by filtering the data to include only those orders which have been marked as 'delivered'.\n   - Join relevant tables to gather necessary information about customers, sellers, and order details.\n\n2. **Seller with Most Unique Customers:**\n   - Group the filtered data by seller ID.\n   - Count the distinct customer IDs associated with each seller.\n   - Select the seller with the highest count of unique customers.\n\n3. **Seller with Highest Profit:**\n   - Group the filtered data by seller ID.\n   - Calculate the profit for each order item by subtracting the freight value from the price.\n   - Sum these profit values for each seller.\n   - Select the seller with the highest total profit.\n\n4. **Seller with Most Unique Orders:**\n   - Group the filtered data by seller ID.\n   - Count the distinct order IDs associated with each seller.\n   - Select the seller with the highest count of unique orders.\n\n5. **Seller with Most 5-Star Ratings:**\n   - Further filter the delivered orders to include only those with a review score higher than 4.\n   - Group the filtered data by seller ID.\n   - Count the review scores associated with each seller.\n   - Select the seller with the highest count of 5-star ratings.\n\n6. **Combine Results:**\n   - Use a union operation to combine the results from the four separate queries.\n   - Each result should include a description, the seller ID, and the corresponding value.\n\n7. **Output the Combined Results:**\n   - The final output will list the sellers who meet each of the specified criteria along with their respective values.",
        "special_function": null
    },
    {
        "instance_id": "local034",
        "db": "Brazilian_E_Commerce",
        "question": "Could you help me calculate the average of the total payment count for the most preferred payment method in each product category?",
        "SQL": "WITH category_payments AS (\n  SELECT\n    final.product_category_name AS Category_Name,\n    CASE MAX(final.Credit_Card_Payment, final.Boleto_Payment, final.Voucher_Payment, final.Debit_Card_Payment, final.Not_defined_Payment)\n      WHEN final.Credit_Card_Payment THEN 'Credit_Card_Payment'\n      WHEN final.Boleto_Payment THEN 'Boleto_Payment'\n      WHEN final.Voucher_Payment THEN 'Voucher_Payment'\n      WHEN final.Debit_Card_Payment THEN 'Debit_Card_Payment'\n      WHEN final.Not_defined_Payment THEN 'Not_defined_Payment'\n    END AS Preferred_option,\n    MAX(final.Credit_Card_Payment, final.Boleto_Payment, final.Voucher_Payment, final.Debit_Card_Payment, final.Not_defined_Payment) AS Total_Payments\n  FROM\n    (SELECT\n      result.product_category_name,\n      SUM(result.Credit_Card_Payment) AS Credit_Card_Payment,\n      SUM(result.Boleto_Payment) AS Boleto_Payment,\n      SUM(result.Voucher_Payment) AS Voucher_Payment,\n      SUM(result.Debit_Card_Payment) AS Debit_Card_Payment,\n      SUM(result.Not_defined_Payment) AS Not_defined_Payment\n    FROM (\n      SELECT\n        raw.product_category_name,\n        SUM(CASE WHEN raw.payment_type = 'credit_card' THEN 1 ELSE 0 END) AS Credit_Card_Payment,\n        SUM(CASE WHEN raw.payment_type = 'boleto' THEN 1 ELSE 0 END) AS Boleto_Payment,\n        SUM(CASE WHEN raw.payment_type = 'voucher' THEN 1 ELSE 0 END) AS Voucher_Payment,\n        SUM(CASE WHEN raw.payment_type = 'debit_card' THEN 1 ELSE 0 END) AS Debit_Card_Payment,\n        SUM(CASE WHEN raw.payment_type = 'not_defined' THEN 1 ELSE 0 END) AS Not_defined_Payment\n      FROM (\n        SELECT\n          pd.product_category_name,\n          p.order_id,\n          p.payment_type\n        FROM olist_order_payments p\n        JOIN olist_orders o ON o.order_id = p.order_id\n        JOIN olist_order_items i ON i.order_id = o.order_id\n        JOIN olist_products_dataset pd ON i.product_id = pd.product_id\n        GROUP BY 1, 2, 3\n      ) raw\n      GROUP BY 1\n      ORDER BY 1 ASC\n    ) result\n    GROUP BY 1\n  ) final\n  GROUP BY 1\n)\n\nSELECT AVG(Total_Payments) AS Average_Most_Used_Payment_Count\nFROM category_payments;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local037",
        "db": "Brazilian_E_Commerce",
        "question": "What are the top three product categories with the highest number of payments in single payment type, and how many payments were made in each category?",
        "SQL": "WITH payment_counts AS (\n  SELECT\n    raw.product_category_name,\n    raw.payment_type,\n    COUNT(raw.order_id) AS payment_count\n  FROM (\n    SELECT \n      p.product_category_name,\n      p.order_id,\n      p.payment_type\n    FROM olist_order_payments p\n    JOIN olist_orders o ON o.order_id = p.order_id\n    JOIN olist_order_items i ON i.order_id = o.order_id\n    JOIN olist_products_dataset p ON i.product_id = p.product_id\n    GROUP BY 1, 2, 3\n  ) raw\n  GROUP BY 1, 2\n),\nranked_payments AS (\n  SELECT\n    payment_counts.product_category_name,\n    payment_counts.payment_type,\n    payment_counts.payment_count,\n    ROW_NUMBER() OVER (PARTITION BY payment_counts.product_category_name ORDER BY payment_counts.payment_count DESC) AS rn\n  FROM payment_counts\n),\ntop_payments AS (\n  SELECT\n    product_category_name,\n    payment_type,\n    payment_count\n  FROM ranked_payments\n  WHERE rn = 1\n  ORDER BY payment_count DESC\n  LIMIT 3\n)\nSELECT \n  top_payments.product_category_name AS Category_Name,\n  top_payments.payment_count AS Payment_count\nFROM top_payments\nORDER BY Payment_count DESC;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local035",
        "db": "Brazilian_E_Commerce",
        "question": "Please help me find two adjacent cities with the greatest distance between them.",
        "SQL": "select city_one, city_two from (\n  Select\n    a.geolocation_city as city_one,\n    lag(a.geolocation_city) over(order by a.geolocation_state,a.geolocation_city,a.geolocation_zip_code_prefix,a.geolocation_lat,a.geolocation_lng asc) as city_two,\n    a.distance_between_two_cities - lag(a.distance_between_two_cities) over(order by a.geolocation_state,a.geolocation_city,a.geolocation_zip_code_prefix,a.geolocation_lat,a.geolocation_lng asc) as Distance\n  from (\n    SELECT *,(6371 * acos(\n      cos( radians(geolocation_lat) )\n      * cos( radians(lag(geolocation_lat) over(order by geolocation_state,geolocation_city,geolocation_zip_code_prefix,geolocation_lat,geolocation_lng asc) ) )\n      * cos( radians(lag(geolocation_lng) over(order by geolocation_state,geolocation_city,geolocation_zip_code_prefix,geolocation_lat,geolocation_lng asc) ) - radians(geolocation_lng) )\n      + sin( radians(lag(geolocation_lng) over(order by geolocation_state,geolocation_city,geolocation_zip_code_prefix,geolocation_lat,geolocation_lng asc)) )\n      * sin( radians(lag(geolocation_lat) over(order by geolocation_state,geolocation_city,geolocation_zip_code_prefix,geolocation_lat,geolocation_lng asc) ) )\n    ) ) as distance_between_two_cities from olist_geolocation\n  ) a\n  group by 1\n) result\ngroup by 1,2\norder by result.Distance desc\nLIMIT 1",
        "external_knowledge": "spherical_law.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local038",
        "db": "Pagila",
        "question": "Could you help me find the actor who appeared most in English G or PG-rated children's movies no longer than 2 hours, released between 2000 and 2010\uff1fGive me a full name.",
        "SQL": "SELECT\n    actor.first_name || ' ' || actor.last_name AS full_name\nFROM\n    actor\nINNER JOIN film_actor ON actor.actor_id = film_actor.actor_id\nINNER JOIN film ON film_actor.film_id = film.film_id\nINNER JOIN film_category ON film.film_id = film_category.film_id\nINNER JOIN category ON film_category.category_id = category.category_id\n-- Join with the language table\nINNER JOIN language ON film.language_id = language.language_id\nWHERE\n    category.name = 'Children' AND\n    film.release_year BETWEEN 2000 AND 2010 AND\n    film.rating IN ('G', 'PG') AND\n    language.name = 'English' AND\n    film.length <= 120\nGROUP BY\n    actor.actor_id, actor.first_name, actor.last_name\nORDER BY\n    COUNT(film.film_id) DESC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "Display the top 3 actors who have most appeared in films in the \u201cChildren\u201d category. If several actors have the same number of films, output all",
        "special_function": null
    },
    {
        "instance_id": "local039",
        "db": "Pagila",
        "question": "Please help me find the film category with the highest total rental hours in cities where the city's name either starts with \"A\" or contains a hyphen. ",
        "SQL": "SELECT\n    category.name\nFROM\n    category\nINNER JOIN film_category USING (category_id)\nINNER JOIN film USING (film_id)\nINNER JOIN inventory USING (film_id)\nINNER JOIN rental USING (inventory_id)\nINNER JOIN customer USING (customer_id)\nINNER JOIN address USING (address_id)\nINNER JOIN city USING (city_id)\nWHERE\n    LOWER(city.city) LIKE 'a%' OR city.city LIKE '%-%'\nGROUP BY\n    category.name\nORDER BY\n    SUM(CAST((julianday(rental.return_date) - julianday(rental.rental_date)) * 24 AS INTEGER)) DESC\nLIMIT\n    1;",
        "external_knowledge": null,
        "plan": "Output the category of movies that has the largest number of hours of total rental in cities (customer.address_id in this city), and that starts with the letter \u201ca\u201d. Do the same for cities in which there is a \u201c-\u201d symbol. Write everything in one request",
        "special_function": null
    },
    {
        "instance_id": "local040",
        "db": "modern_data",
        "question": "Which three boroughs have the highest number of trees, and what is the average mean income for each, considering only areas where both median and mean income estimates are greater than zero, and using the available ZIP code income data when tree ZIP codes are missing?",
        "SQL": "WITH joined_trees AS (\n    SELECT\n        t.idx,\n        t.tree_id,\n        t.tree_dbh,\n        t.stump_diam,\n        t.status,\n        t.health,\n        t.spc_latin,\n        t.spc_common,\n        t.address,\n        COALESCE(t.zipcode, it.zipcode) AS zipcode,\n        t.borocode,\n        t.boroname,\n        t.nta_name,\n        t.state,\n        t.latitude,\n        t.longitude,\n        COALESCE(it.Estimate_Total, 0) AS estimate_total,\n        COALESCE(it.Margin_of_Error_Total, 0) AS margin_of_error_total,\n        COALESCE(it.Estimate_Median_income, 0) AS estimate_median_income,\n        COALESCE(it.Margin_of_Error_Median_income, 0) AS margin_of_error_median_income,\n        COALESCE(it.Estimate_Mean_income, 0) AS estimate_mean_income,\n        COALESCE(it.Margin_of_Error_Mean_income, 0) AS margin_of_error_mean_income\n    FROM \n        trees t\n    LEFT JOIN \n        income_trees it ON t.zipcode = it.zipcode\n\n    UNION\n\n    SELECT\n        NULL AS idx,\n        NULL AS tree_id,\n        NULL AS tree_dbh,\n        NULL AS stump_diam,\n        NULL AS status,\n        NULL AS health,\n        NULL AS spc_latin,\n        NULL AS spc_common,\n        NULL AS address,\n        it.zipcode AS zipcode,\n        NULL AS borocode,\n        NULL AS boroname,\n        NULL AS nta_name,\n        NULL AS state,\n        NULL AS latitude,\n        NULL AS longitude,\n        it.Estimate_Total AS estimate_total,\n        it.Margin_of_Error_Total AS margin_of_error_total,\n        it.Estimate_Median_income AS estimate_median_income,\n        it.Margin_of_Error_Median_income AS margin_of_error_median_income,\n        it.Estimate_Mean_income AS estimate_mean_income,\n        it.Margin_of_Error_Mean_income AS margin_of_error_mean_income\n    FROM \n        income_trees it\n    LEFT JOIN \n        trees t ON t.zipcode = it.zipcode\n    WHERE \n        t.zipcode IS NULL\n)\n\n\nSELECT\n    jt.boroname,\n    AVG(jt.estimate_mean_income) AS mean_income\nFROM\n    joined_trees jt\nJOIN (\n    SELECT\n        boroname,\n        COUNT(tree_id) AS count_trees\n    FROM\n        joined_trees\n    WHERE\n        estimate_median_income > 0 AND estimate_mean_income > 0 AND boroname IS NOT NULL\n    GROUP BY\n        boroname\n    ORDER BY\n        count_trees DESC\n    LIMIT 3\n) AS tree_counts\nON jt.boroname = tree_counts.boroname\nWHERE\n    jt.estimate_mean_income > 0\nGROUP BY\n    jt.boroname\nORDER BY\n    mean_income DESC;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local041",
        "db": "modern_data",
        "question": "What percentage of trees in the Bronx have a health status of Good?",
        "SQL": "WITH Borough_Health_Tree_Counts AS (\n    SELECT\n        t.health,\n        COUNT(*) AS tree_count\n    FROM trees t\n    WHERE t.boroname = 'Bronx' AND t.health IS NOT NULL AND t.health != ''\n    GROUP BY t.health\n),\nBorough_Health_Total_Trees AS (\n    SELECT\n        COUNT(*) AS total_trees\n    FROM trees t\n    WHERE t.boroname = 'Bronx'\n)\n\nSELECT\n    ROUND(\n        (\n            SELECT tree_count \n            FROM Borough_Health_Tree_Counts\n            WHERE health = 'Good'\n        ) * 100.0 / \n        (\n            SELECT total_trees \n            FROM Borough_Health_Total_Trees\n        ), 2\n    ) AS Percentage\n;",
        "external_knowledge": null,
        "plan": "1. **Filter Data by Location and Health Status:**\n   - Create a temporary dataset that includes only the trees located in the specified area with a non-null and non-empty health status.\n\n2. **Count Trees by Health Status:**\n   - Within the temporary dataset, group the trees by their health status and count the number of trees in each group.\n\n3. **Count Total Trees in the Area:**\n   - Create another temporary dataset that counts the total number of trees in the specified area, without considering their health status.\n\n4. **Calculate the Percentage:**\n   - Select the count of trees with a specific health status from the first temporary dataset.\n   - Select the total count of trees from the second temporary dataset.\n   - Calculate the percentage by dividing the count of trees with the specific health status by the total count of trees and multiplying by 100.\n\n5. **Round the Result:**\n   - Round the calculated percentage to two decimal places for a more readable output.\n\n6. **Return the Result:**\n   - Output the rounded percentage as the final result.",
        "special_function": null
    },
    {
        "instance_id": "local049",
        "db": "modern_data",
        "question": "Can you help me calculate the average number of new unicorn companies per year in the top industry from 2019 to 2021?",
        "SQL": "WITH top_industry AS\n(\n    SELECT i.industry\n    FROM companies_industries AS i\n    INNER JOIN companies_dates AS d\n        ON i.company_id = d.company_id\n    WHERE strftime('%Y', d.date_joined) IN ('2019', '2020', '2021')\n    GROUP BY i.industry\n    ORDER BY COUNT(*) DESC\n    LIMIT 1\n),\n\nyearly_counts AS\n(\n    SELECT strftime('%Y', d.date_joined) AS year,\n           COUNT(*) AS num_unicorns\n    FROM companies_industries AS i\n    INNER JOIN companies_dates AS d\n        ON i.company_id = d.company_id\n    WHERE strftime('%Y', d.date_joined) IN ('2019', '2020', '2021')\n      AND i.industry = (SELECT industry FROM top_industry)\n    GROUP BY year\n)\n\nSELECT ROUND(AVG(num_unicorns), 2) AS average_new_unicorns\nFROM yearly_counts;",
        "external_knowledge": null,
        "plan": "Determine which industries have the highest average valuations from 2019 to 2021, and tell me what new unicorns are in these industries?",
        "special_function": null
    },
    {
        "instance_id": "local054",
        "db": "chinook",
        "question": "Could you tell me the first names of customers who spent less than $1 on albums by the best-selling artist, along with the amounts they spent?",
        "SQL": "WITH BEST_SELLING_ARTIST AS (\n    SELECT \n        ARTISTS.ARTISTID AS ARTIST_ID, \n        ARTISTS.NAME AS ARTIST_NAME,\n        SUM(INVOICE_ITEMS.UNITPRICE * INVOICE_ITEMS.QUANTITY) AS TOTAL_SALES\n    FROM \n        INVOICE_ITEMS\n    JOIN \n        TRACKS ON TRACKS.TRACKID = INVOICE_ITEMS.TRACKID\n    JOIN \n        ALBUMS ON ALBUMS.ALBUMID = TRACKS.ALBUMID\n    JOIN \n        ARTISTS ON ARTISTS.ARTISTID = ALBUMS.ARTISTID\n    GROUP BY \n        ARTISTS.ARTISTID, ARTISTS.NAME\n    ORDER BY \n        TOTAL_SALES DESC\n    LIMIT 1\n),\nCUSTOMER_SPENDING AS (\n    SELECT\n        CUSTOMERS.FIRSTNAME,\n        SUM(INVOICE_ITEMS.UNITPRICE * INVOICE_ITEMS.QUANTITY) AS AMOUNT_SPENT\n    FROM\n        INVOICES\n    JOIN\n        CUSTOMERS ON CUSTOMERS.CUSTOMERID = INVOICES.CUSTOMERID\n    JOIN\n        INVOICE_ITEMS ON INVOICE_ITEMS.INVOICEID = INVOICES.INVOICEID\n    JOIN\n        TRACKS ON TRACKS.TRACKID = INVOICE_ITEMS.TRACKID\n    JOIN\n        ALBUMS ON ALBUMS.ALBUMID = TRACKS.ALBUMID\n    JOIN\n        BEST_SELLING_ARTIST ON BEST_SELLING_ARTIST.ARTIST_ID = ALBUMS.ARTISTID\n    GROUP BY\n        CUSTOMERS.FIRSTNAME\n)\nSELECT \n    FIRSTNAME,\n    AMOUNT_SPENT\nFROM \n    CUSTOMER_SPENDING \nWHERE\n    AMOUNT_SPENT < 1;",
        "external_knowledge": null,
        "plan": "Find how much amount spent by each customer on artists. Write a query to return customer name, artist name and total spent.",
        "special_function": null
    },
    {
        "instance_id": "local055",
        "db": "chinook",
        "question": "What is the difference in average spending between customers who bought albums from the best-selling artist and those who bought from the least-selling artist? If there is a tie for either best-selling or lowest-selling, choose the artist whose name comes first alphabetically.",
        "SQL": "WITH ARTIST_SALES AS (\n    SELECT \n        ARTISTS.ARTISTID AS ARTIST_ID, \n        ARTISTS.NAME AS ARTIST_NAME,\n        SUM(INVOICE_ITEMS.UNITPRICE * INVOICE_ITEMS.QUANTITY) AS TOTAL_SALES\n    FROM \n        INVOICE_ITEMS\n    JOIN \n        TRACKS ON TRACKS.TRACKID = INVOICE_ITEMS.TRACKID\n    JOIN \n        ALBUMS ON ALBUMS.ALBUMID = TRACKS.ALBUMID\n    JOIN \n        ARTISTS ON ARTISTS.ARTISTID = ALBUMS.ARTISTID\n    GROUP BY \n        ARTISTS.ARTISTID, ARTISTS.NAME\n),\nBEST_SELLING_ARTIST AS (\n    SELECT \n        ARTIST_ID, \n        ARTIST_NAME\n    FROM \n        ARTIST_SALES\n    ORDER BY \n        TOTAL_SALES DESC,\n        ARTIST_NAME ASC\n    LIMIT 1\n),\nLEAST_SELLING_ARTIST AS (\n    SELECT \n        ARTIST_ID, \n        ARTIST_NAME\n    FROM \n        ARTIST_SALES\n    ORDER BY \n        TOTAL_SALES ASC,\n        ARTIST_NAME ASC\n    LIMIT 1\n),\nCUSTOMER_SPENDING AS (\n    SELECT\n        CUSTOMERS.FIRSTNAME,\n        ARTISTS.NAME,\n        SUM(INVOICE_ITEMS.UNITPRICE * INVOICE_ITEMS.QUANTITY) AS AMOUNT_SPENT\n    FROM\n        INVOICES\n    JOIN\n        CUSTOMERS ON CUSTOMERS.CUSTOMERID = INVOICES.CUSTOMERID\n    JOIN\n        INVOICE_ITEMS ON INVOICE_ITEMS.INVOICEID = INVOICES.INVOICEID\n    JOIN\n        TRACKS ON TRACKS.TRACKID = INVOICE_ITEMS.TRACKID\n    JOIN\n        ALBUMS ON ALBUMS.ALBUMID = TRACKS.ALBUMID\n    JOIN\n        ARTISTS ON ARTISTS.ARTISTID = ALBUMS.ARTISTID\n    GROUP BY\n        CUSTOMERS.FIRSTNAME, ARTISTS.NAME\n),\nAVERAGE_SPENDING AS (\n    SELECT \n        NAME,\n        AVG(AMOUNT_SPENT) AS AVERAGE_SPENT\n    FROM \n        CUSTOMER_SPENDING\n    WHERE \n        NAME IN (SELECT ARTIST_NAME FROM BEST_SELLING_ARTIST UNION SELECT ARTIST_NAME FROM LEAST_SELLING_ARTIST)\n    GROUP BY \n        NAME\n)\nSELECT \n    ABS(\n        (SELECT AVERAGE_SPENT FROM AVERAGE_SPENDING WHERE NAME = (SELECT ARTIST_NAME FROM BEST_SELLING_ARTIST))\n        -\n        (SELECT AVERAGE_SPENT FROM AVERAGE_SPENDING WHERE NAME = (SELECT ARTIST_NAME FROM LEAST_SELLING_ARTIST))\n    ) AS DIFFERENCE;",
        "external_knowledge": null,
        "plan": "1. **Calculate Total Sales for Each Artist:**\n   - Aggregate the total sales amount for each artist by multiplying unit prices by quantities.\n   - Group the results by artist ID and name to obtain a total sales figure for each artist.\n\n2. **Identify Best-Selling Artist:**\n   - Order the artists by total sales in descending order.\n   - In case of a tie, order the artists alphabetically by name.\n   - Select the top artist from this ordered list as the best-selling artist.\n\n3. **Identify Least-Selling Artist:**\n   - Order the artists by total sales in ascending order.\n   - In case of a tie, order the artists alphabetically by name.\n   - Select the top artist from this ordered list as the least-selling artist.\n\n4. **Calculate Customer Spending per Artist:**\n   - Aggregate the total amount spent by each customer on each artist by multiplying unit prices by quantities.\n   - Group the results by customer name and artist name to obtain the total amount spent per customer for each artist.\n\n5. **Calculate Average Spending for Best and Least-Selling Artists:**\n   - Filter the customer spending data to include only the best-selling and least-selling artists.\n   - Calculate the average amount spent by customers on each of these artists.\n\n6. **Compute the Difference in Average Spending:**\n   - Retrieve the average spending for the best-selling artist.\n   - Retrieve the average spending for the least-selling artist.\n   - Calculate the absolute difference between these two average spending amounts.\n\n7. **Output the Difference:**\n   - Return the computed difference in average spending between customers who bought from the best-selling artist and those who bought from the least-selling artist.",
        "special_function": null
    },
    {
        "instance_id": "local198",
        "db": "chinook",
        "question": "Using the sales data, what is the median value of total sales made in countries where the number of customers is greater than 4?",
        "SQL": "WITH tcs AS \n  (\n    SELECT \n      c.CustomerID AS customer_id,\n      SUM(ii.UnitPrice) AS total_sales\n    FROM invoice_items AS ii\n    JOIN Invoices AS i ON i.InvoiceID = ii.InvoiceID\n    JOIN Customers AS c ON c.CustomerID = i.CustomerID\n    GROUP BY c.CustomerID\n  ),    \ncountry_sales AS\n  (\n    SELECT\n      c.Country AS country,\n      COUNT(c.CustomerID) AS customer_count,\n      SUM(tcs.total_sales) AS total_sales\n    FROM customers AS c\n    JOIN tcs ON tcs.customer_id = c.CustomerID\n    GROUP BY c.Country\n    HAVING COUNT(c.CustomerID) > 4\n  ),  \nranked_sales AS\n  (\n    SELECT\n      country,\n      total_sales,\n      ROW_NUMBER() OVER (ORDER BY total_sales) AS sales_rank,\n      COUNT(*) OVER () AS total_count\n    FROM country_sales\n  )  \nSELECT\n  AVG(total_sales) AS median_total_sales\nFROM ranked_sales\nWHERE sales_rank IN \n  (\n    (total_count + 1) / 2,\n    (total_count + 2) / 2\n  )\nORDER BY total_sales;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local056",
        "db": "sqlite-sakila",
        "question": "Which customer has the highest average monthly change in payment amounts? Provide the customer's full name.",
        "SQL": "WITH result_table AS (\n    SELECT \n        strftime('%m', pm.payment_date) AS pay_mon, \n        cust.first_name || ' ' || cust.last_name AS fullname, \n        SUM(pm.amount) AS pay_amount \n    FROM \n        payment AS pm \n    JOIN \n        customer AS cust \n    ON \n        pm.customer_id = cust.customer_id \n    GROUP BY \n        1, \n        2\n), \ndifference_per_mon AS (\n    SELECT \n        rt.fullname, \n        ABS(rt.pay_amount - LAG(rt.pay_amount) OVER (PARTITION BY rt.fullname ORDER BY rt.pay_mon)) AS diff \n    FROM \n        result_table rt\n), \naverage_difference AS (\n    SELECT \n        fullname, \n        AVG(diff) AS avg_diff\n    FROM \n        difference_per_mon \n    WHERE \n        diff IS NOT NULL\n    GROUP BY \n        fullname\n)\nSELECT \n    fullname\nFROM \n    average_difference\nORDER BY \n    avg_diff DESC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "1. **Extract Monthly Payments and Customer Names**: Create a temporary table that calculates the total payment amount for each customer for each month. Additionally, concatenate the customer's first and last names into a full name.\n\n2. **Calculate Monthly Payment Differences**: From the temporary table, compute the absolute difference in payment amounts between consecutive months for each customer. Use a window function to achieve this, ensuring the calculation is partitioned by customer and ordered by month.\n\n3. **Compute Average Monthly Differences**: Create another temporary table that calculates the average of these monthly differences for each customer. Exclude any null differences to ensure accurate averaging.\n\n4. **Identify Customer with Highest Average Difference**: Select the customer with the highest average monthly difference from the final temporary table. Order the results in descending order of average differences and limit the output to the top result.\n\nBy following these steps, you accurately determine which customer has the highest average change in their monthly payment amounts and return the corresponding full name.",
        "special_function": null
    },
    {
        "instance_id": "local058",
        "db": "education_business",
        "question": "Can you provide a list of hardware product segments along with their unique product counts for 2020 in the output, ordered by the highest percentage increase in unique fact sales products from 2020 to 2021?",
        "SQL": "WITH UniqueProducts2020 AS (\n    SELECT\n        dp.segment,\n        COUNT(DISTINCT fsm.product_code) AS unique_products_2020\n    FROM\n        hardware_fact_sales_monthly fsm\n    JOIN\n        hardware_dim_product dp ON fsm.product_code = dp.product_code\n    WHERE\n        fsm.fiscal_year = 2020\n    GROUP BY\n        dp.segment\n),\nUniqueProducts2021 AS (\n    SELECT\n        dp.segment,\n        COUNT(DISTINCT fsm.product_code) AS unique_products_2021\n    FROM\n        hardware_fact_sales_monthly fsm\n    JOIN\n        hardware_dim_product dp ON fsm.product_code = dp.product_code\n    WHERE\n        fsm.fiscal_year = 2021\n    GROUP BY\n        dp.segment\n)\nSELECT\n    spc.segment,\n    spc.unique_products_2020 AS product_count_2020\nFROM\n    UniqueProducts2020 spc\nJOIN\n    UniqueProducts2021 fup ON spc.segment = fup.segment\nORDER BY\n    ((fup.unique_products_2021 - spc.unique_products_2020) * 100.0) / (spc.unique_products_2020) DESC;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local059",
        "db": "education_business",
        "question": "For the calendar year 2021, what is the overall average quantity sold of the top three best-selling hardware products (by total quantity sold) in each division?",
        "SQL": "WITH RankedProducts AS (\n    SELECT\n        dp.division,\n        fsm.product_code,\n        dp.product,\n        SUM(fsm.sold_quantity) AS total_sold_quantity,\n        ROW_NUMBER() OVER(PARTITION BY dp.division ORDER BY SUM(fsm.sold_quantity) DESC) AS rank_order\n    FROM\n        hardware_fact_sales_monthly fsm\n    JOIN\n        hardware_dim_product dp ON fsm.product_code = dp.product_code\n    WHERE\n        strftime('%Y', fsm.date) = '2021'\n    GROUP BY\n        dp.division, fsm.product_code, dp.product\n),\nTop3Products AS (\n    SELECT\n        division,\n        product_code,\n        product,\n        total_sold_quantity,\n        rank_order\n    FROM\n        RankedProducts\n    WHERE\n        rank_order <= 3\n)\nSELECT\n    division,\n    AVG(total_sold_quantity) AS avg_sold_quantity\nFROM\n    Top3Products\nGROUP BY\n    division\nORDER BY\n    avg_sold_quantity DESC;",
        "external_knowledge": null,
        "plan": "1. **Identify Relevant Data:** \n   - Filter the sales records to include only those from the year 2021.\n\n2. **Aggregate Sales Data:** \n   - Group the filtered records by division and product, and calculate the total quantity sold for each product within each division.\n\n3. **Rank Products by Sales:** \n   - For each division, rank the products based on their total quantity sold in descending order.\n\n4. **Filter Top Products:** \n   - Select the top 3 products from each division based on their rank.\n\n5. **Calculate Averages:** \n   - For each division, compute the average quantity sold for the top 3 products.\n\n6. **Order Results:** \n   - Sort the divisions by the calculated average quantity sold in descending order to meet the user\u2019s requirement of listing the results by average quantity.\n\nBy following these steps, the query efficiently answers the original user intention by accurately computing and displaying the required average quantities in the specified order.",
        "special_function": null
    },
    {
        "instance_id": "local060",
        "db": "complex_oracle",
        "question": "What is the change in each product\u2019s share of total sales in the top 20% of products (by sales), between Q4 of 2019 and 2020, in the US? Include only products with no promotions in Q4 2019 or Q4 2020, and provide results in decreasing order of the change in sales share.",
        "SQL": "WITH prod_sales AS (\n    -- All products sales for city subset\n    SELECT \n        s.prod_id,\n        SUM(amount_sold) AS total_sales,\n        -- Using RANK() instead of CUME_DIST()\n        RANK() OVER (ORDER BY SUM(amount_sold) DESC) AS rank\n    FROM sales s\n    JOIN customers c ON s.cust_id = c.cust_id\n    JOIN channels ch ON s.channel_id = ch.channel_id\n    JOIN products p ON s.prod_id = p.prod_id\n    JOIN times t ON s.time_id = t.time_id\n    WHERE \n        s.promo_id = 999 AND\n        c.cust_city_id IN (\n            -- Top 20% of cities\n            SELECT cust_city_id\n            FROM (\n                SELECT \n                    cust_city_id,\n                    (new_cust_sales - old_cust_sales) / old_cust_sales AS pct_change,\n                    old_cust_sales\n                FROM (\n                    SELECT \n                        cust_city_id,\n                        SUM(CASE WHEN t.calendar_quarter_id = 1776 THEN amount_sold ELSE 0 END) AS new_cust_sales,\n                        SUM(CASE WHEN t.calendar_quarter_id = 1772 THEN amount_sold ELSE 0 END) AS old_cust_sales\n                    FROM sales s\n                    JOIN customers c ON s.cust_id = c.cust_id\n                    JOIN channels ch ON s.channel_id = ch.channel_id\n                    JOIN products p ON s.prod_id = p.prod_id\n                    JOIN times t ON s.time_id = t.time_id\n                    WHERE \n                        c.country_id = 52790 AND\n                        s.promo_id = 999 AND\n                        (t.calendar_quarter_id = 1776 OR t.calendar_quarter_id = 1772)\n                    GROUP BY cust_city_id\n                ) cust_sales\n                WHERE old_cust_sales > 0\n            )\n            WHERE pct_change >= 0.20\n        )\n    GROUP BY s.prod_id\n),\ntop_prod_sales AS (\n    -- Top 20% of products\n    SELECT \n        prod_id\n    FROM prod_sales\n    WHERE rank <= (SELECT COUNT(*) * 0.2 FROM prod_sales)\n),\ntotal_sales AS (\n    -- Total sales for country in each period\n    SELECT \n        SUM(CASE WHEN t.calendar_quarter_id = 1776 THEN amount_sold ELSE 0 END) AS new_tot_sales,\n        SUM(CASE WHEN t.calendar_quarter_id = 1772 THEN amount_sold ELSE 0 END) AS old_tot_sales\n    FROM sales s\n    JOIN times t ON s.time_id = t.time_id\n    JOIN channels ch ON s.channel_id = ch.channel_id\n    JOIN customers c ON s.cust_id = c.cust_id\n    JOIN countries co ON c.country_id = co.country_id\n    JOIN products p ON s.prod_id = p.prod_id\n    WHERE\n        s.promo_id = 999 AND\n        (t.calendar_quarter_id = 1776 OR t.calendar_quarter_id = 1772)\n)\n-- Main query block\nSELECT \n    p.prod_name,\n    ((new_subset_sales / (SELECT new_tot_sales FROM total_sales)) - \n     (old_subset_sales / (SELECT old_tot_sales FROM total_sales))) * 100 AS share_changes\nFROM (\n    SELECT \n        s.prod_id,\n        SUM(CASE WHEN t.calendar_quarter_id = 1776 THEN amount_sold ELSE 0 END) AS new_subset_sales,\n        SUM(CASE WHEN t.calendar_quarter_id = 1772 THEN amount_sold ELSE 0 END) AS old_subset_sales\n    FROM sales s\n    JOIN customers c ON s.cust_id = c.cust_id\n    JOIN countries co ON c.country_id = co.country_id\n    JOIN channels ch ON s.channel_id = ch.channel_id\n    JOIN times t ON s.time_id = t.time_id\n    WHERE \n        s.promo_id = 999 AND\n        (t.calendar_quarter_id = 1776 OR t.calendar_quarter_id = 1772) AND\n        s.prod_id IN (SELECT prod_id FROM top_prod_sales)\n    GROUP BY s.prod_id\n) sub\nJOIN products p ON sub.prod_id = p.prod_id\nORDER BY share_changes DESC;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local063",
        "db": "complex_oracle",
        "question": "Which product has the smallest change in sales share for each product from the top 20% of products by total sales between Q4 in 2019 and 2020 in US without any promotion?",
        "SQL": "WITH prod_sales AS (\n    -- All products sales for city subset\n    SELECT \n        s.prod_id,\n        SUM(amount_sold) AS total_sales,\n        -- Using RANK() instead of CUME_DIST()\n        RANK() OVER (ORDER BY SUM(amount_sold) DESC) AS rank\n    FROM sales s\n    JOIN customers c ON s.cust_id = c.cust_id\n    JOIN channels ch ON s.channel_id = ch.channel_id\n    JOIN products p ON s.prod_id = p.prod_id\n    JOIN times t ON s.time_id = t.time_id\n    WHERE \n            s.promo_id = 999 \n        AND c.cust_city_id IN (\n            -- Top 20% of cities\n            SELECT cust_city_id\n            FROM (\n                SELECT \n                    cust_city_id,\n                    (new_cust_sales - old_cust_sales) / old_cust_sales AS pct_change,\n                    old_cust_sales\n                FROM (\n                    SELECT \n                        cust_city_id,\n                        SUM(CASE WHEN t.calendar_quarter_id = 1776 THEN amount_sold ELSE 0 END) AS new_cust_sales,\n                        SUM(CASE WHEN t.calendar_quarter_id = 1772 THEN amount_sold ELSE 0 END) AS old_cust_sales\n                    FROM sales s\n                    JOIN customers c ON s.cust_id = c.cust_id\n                    JOIN channels ch ON s.channel_id = ch.channel_id\n                    JOIN products p ON s.prod_id = p.prod_id\n                    JOIN times t ON s.time_id = t.time_id\n                    WHERE \n                            c.country_id = 52790 \n                        AND s.promo_id = 999 \n                        AND (t.calendar_quarter_id = 1776 OR t.calendar_quarter_id = 1772)\n                    GROUP BY cust_city_id\n                ) cust_sales\n                WHERE old_cust_sales > 0\n            )\n            WHERE pct_change >= 0.20\n        )\n    GROUP BY s.prod_id\n),\ntop_prod_sales AS (\n    -- Top 20% of products\n    SELECT \n        prod_id\n    FROM prod_sales\n    WHERE rank <= (SELECT COUNT(*) * 0.2 FROM prod_sales)\n),\ntotal_sales AS (\n    -- Total sales for country in each period\n    SELECT \n        SUM(CASE WHEN t.calendar_quarter_id = 1776 THEN amount_sold ELSE 0 END) AS new_tot_sales,\n        SUM(CASE WHEN t.calendar_quarter_id = 1772 THEN amount_sold ELSE 0 END) AS old_tot_sales\n    FROM sales s\n    JOIN times t ON s.time_id = t.time_id\n    JOIN channels ch ON s.channel_id = ch.channel_id\n    JOIN customers c ON s.cust_id = c.cust_id\n    JOIN countries co ON c.country_id = co.country_id\n    JOIN products p ON s.prod_id = p.prod_id\n    WHERE \n        s.promo_id = 999 \n        AND (t.calendar_quarter_id = 1776 OR t.calendar_quarter_id = 1772)\n),\nprod_sales_change AS (\n    -- Calculate change in sales share for each product\n    SELECT \n        s.prod_id,\n        SUM(CASE WHEN t.calendar_quarter_id = 1776 THEN s.amount_sold ELSE 0 END) AS new_subset_sales,\n        SUM(CASE WHEN t.calendar_quarter_id = 1772 THEN s.amount_sold ELSE 0 END) AS old_subset_sales,\n        ((SUM(CASE WHEN t.calendar_quarter_id = 1776 THEN s.amount_sold ELSE 0 END) / (SELECT new_tot_sales FROM total_sales)) - \n         (SUM(CASE WHEN t.calendar_quarter_id = 1772 THEN s.amount_sold ELSE 0 END) / (SELECT old_tot_sales FROM total_sales))) * 100 AS share_change\n    FROM sales s\n    JOIN customers c ON s.cust_id = c.cust_id\n    JOIN countries co ON c.country_id = co.country_id\n    JOIN channels ch ON s.channel_id = ch.channel_id\n    JOIN times t ON s.time_id = t.time_id\n    WHERE \n        s.promo_id = 999 \n        AND (t.calendar_quarter_id = 1776 OR t.calendar_quarter_id = 1772) \n        AND s.prod_id IN (SELECT prod_id FROM top_prod_sales)\n    GROUP BY s.prod_id\n)\n-- Select product with smallest change in sales share\nSELECT \n    p.prod_name\nFROM prod_sales_change psc\nJOIN products p ON psc.prod_id = p.prod_id\nORDER BY share_change ASC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local061",
        "db": "complex_oracle",
        "question": "What is the average monthly projected sales in USD for France in 2021? Please use data from 2019 and 2020 for projection. Ensure all values are converted to USD based on the 2021 exchange rates.",
        "SQL": "WITH prod_sales_mo AS (\n   SELECT cn.country_name AS c,\n          s.prod_id AS p, \n          t.calendar_year AS y,\n          t.calendar_month_number AS m,\n          SUM(s.amount_sold) AS s\n   FROM sales s\n   JOIN customers c ON s.cust_id = c.cust_id\n   JOIN times t ON s.time_id = t.time_id\n   JOIN countries cn ON c.country_id = cn.country_id\n   JOIN promotions p ON s.promo_id = p.promo_id\n   JOIN channels ch ON s.channel_id = ch.channel_id\n   WHERE p.promo_total_id = 1\n     AND ch.channel_total_id = 1\n     AND cn.country_name = 'France'\n     AND t.calendar_year IN (2019, 2020, 2021)\n   GROUP BY cn.country_name,\n            s.prod_id, \n            t.calendar_year, \n            t.calendar_month_number\n),\ntime_summary AS (\n   SELECT DISTINCT calendar_year AS cal_y, \n                   calendar_month_number AS cal_m\n   FROM times\n   WHERE calendar_year IN (2019, 2020, 2021)\n),\nbase_data AS (\n   SELECT ts.cal_y AS y, \n          ts.cal_m AS m, \n          ps.c AS c, \n          ps.p AS p,\n          COALESCE(ps.s, 0) AS s,\n          (SELECT AVG(s) FROM prod_sales_mo ps2 \n           WHERE ps2.c = ps.c AND ps2.p = ps.p \n             AND ps2.y = ps.y \n             AND ps2.m BETWEEN 1 AND 12) AS avg_s\n   FROM time_summary ts\n   LEFT JOIN prod_sales_mo ps ON ts.cal_y = ps.y AND ts.cal_m = ps.m\n),\nprojected_data AS (\n   SELECT c, p, y, m, s,\n          CASE\n             WHEN y = 2021 THEN ROUND(\n                (((SELECT s FROM base_data WHERE c = b.c AND p = b.p AND y = 2020 AND m = b.m) - (SELECT s FROM base_data WHERE c = b.c AND p = b.p AND y = 2019 AND m = b.m)) / \n                 (SELECT s FROM base_data WHERE c = b.c AND p = b.p AND y = 2019 AND m = b.m)) * \n                (SELECT s FROM base_data WHERE c = b.c AND p = b.p AND y = 2020 AND m = b.m) + \n                (SELECT s FROM base_data WHERE c = b.c AND p = b.p AND y = 2020 AND m = b.m)\n             , 2)\n             ELSE avg_s\n          END AS nr\n   FROM base_data b\n),\nmonthly_avg_projection AS (\n   SELECT \n       m AS month,\n       ROUND(AVG(nr * COALESCE((SELECT to_us FROM currency WHERE country = c AND year = y AND month = m), 1)), 2) AS avg_monthly_projected_sales_in_usd\n   FROM projected_data\n   WHERE y = 2021\n   GROUP BY m\n)\nSELECT month,\n       avg_monthly_projected_sales_in_usd\nFROM monthly_avg_projection\nORDER BY month;",
        "external_knowledge": "projection_calculation.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local050",
        "db": "complex_oracle",
        "question": "What is the median value from average monthly projected sales in USD for France in 2021? Please use data from 2019 and 2020 for projection.",
        "SQL": "WITH prod_sales_mo AS (\n    SELECT cn.country_name AS c,\n           s.prod_id AS p, \n           t.calendar_year AS y,\n           t.calendar_month_number AS m,\n           SUM(s.amount_sold) AS s\n    FROM sales s\n    JOIN customers c ON s.cust_id = c.cust_id\n    JOIN times t ON s.time_id = t.time_id\n    JOIN countries cn ON c.country_id = cn.country_id\n    JOIN promotions p ON s.promo_id = p.promo_id\n    JOIN channels ch ON s.channel_id = ch.channel_id\n    WHERE p.promo_total_id = 1\n      AND ch.channel_total_id = 1\n      AND cn.country_name = 'France'\n      AND t.calendar_year IN (2019, 2020, 2021)\n    GROUP BY cn.country_name,\n             s.prod_id, \n             t.calendar_year, \n             t.calendar_month_number\n),\ntime_summary AS (\n    SELECT DISTINCT calendar_year AS cal_y, \n                    calendar_month_number AS cal_m\n    FROM times\n    WHERE calendar_year IN (2019, 2020, 2021)\n),\nbase_data AS (\n    SELECT ts.cal_y AS y, \n           ts.cal_m AS m, \n           ps.c AS c, \n           ps.p AS p,\n           COALESCE(ps.s, 0) AS s,\n           (SELECT AVG(s) FROM prod_sales_mo ps2 \n            WHERE ps2.c = ps.c AND ps2.p = ps.p \n              AND ps2.y = ps.y \n              AND ps2.m BETWEEN 1 AND 12) AS avg_s\n    FROM time_summary ts\n    LEFT JOIN prod_sales_mo ps ON ts.cal_y = ps.y AND ts.cal_m = ps.m\n),\nprojected_data AS (\n    SELECT c, p, y, m, s,\n           CASE\n              WHEN y = 2021 THEN ROUND(\n                 (((SELECT s FROM base_data WHERE c = b.c AND p = b.p AND y = 2020 AND m = b.m) - (SELECT s FROM base_data WHERE c = b.c AND p = b.p AND y = 2019 AND m = b.m)) / \n                  (SELECT s FROM base_data WHERE c = b.c AND p = b.p AND y = 2019 AND m = b.m)) * \n                 (SELECT s FROM base_data WHERE c = b.c AND p = b.p AND y = 2020 AND m = b.m) + \n                 (SELECT s FROM base_data WHERE c = b.c AND p = b.p AND y = 2020 AND m = b.m)\n              , 2)\n              ELSE avg_s\n           END AS nr\n    FROM base_data b\n),\nmonthly_avg_projection AS (\n    SELECT \n        m AS month,\n        AVG(nr * COALESCE((SELECT to_us FROM currency WHERE country = c AND year = y AND month = m), 1)) AS avg_monthly_projected_sales_in_usd\n    FROM projected_data\n    WHERE y = 2021\n    GROUP BY m\n),\nranked_months AS (\n    SELECT month,\n           avg_monthly_projected_sales_in_usd,\n           ROW_NUMBER() OVER (ORDER BY avg_monthly_projected_sales_in_usd) AS row_num,\n           COUNT(*) OVER () AS total_count\n    FROM monthly_avg_projection\n),\nmedian_month AS (\n    SELECT avg_monthly_projected_sales_in_usd\n    FROM ranked_months\n    WHERE row_num = (total_count + 1) / 2 OR (total_count % 2 = 0 AND row_num IN (total_count / 2, total_count / 2 + 1))\n)\nSELECT AVG(avg_monthly_projected_sales_in_usd) AS median\nFROM median_month;",
        "external_knowledge": "projection_calculation.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local062",
        "db": "complex_oracle",
        "question": "Can you segment Italian customers into ten profitability buckets for December 2021, using equal profit intervals, and calculate the following for each bucket in December 2021: the number of customers, maximum profit, and minimum profit?",
        "SQL": "-- profit by cust, prod, day, channel, promo\nWITH cust_prod_mon_profit AS (\n   SELECT s.cust_id, \n          s.prod_id, \n          s.time_id,\n          s.channel_id, \n          s.promo_id,\n          s.quantity_sold * (c.unit_price - c.unit_cost) AS profit,\n          s.amount_sold AS dol_sold, \n          c.unit_price AS price, \n          c.unit_cost AS cost\n   FROM sales s\n   JOIN costs c ON s.prod_id = c.prod_id\n               AND s.time_id = c.time_id\n               AND s.promo_id = c.promo_id\n               AND s.channel_id = c.channel_id\n   WHERE s.cust_id IN (\n         SELECT cust_id FROM customers\n         WHERE country_id = 52770\n   )\n   AND s.time_id IN (\n         SELECT time_id FROM times\n         WHERE calendar_month_desc = '2021-12'\n   )\n),\ncust_daily_trans_amt AS (\n   SELECT cust_id, \n          time_id, \n          channel_id, \n          SUM(dol_sold) AS cust_daily_trans_amt\n   FROM cust_prod_mon_profit\n   GROUP BY cust_id, time_id, channel_id\n),\ncust_purchase_cnt AS (\n   SELECT cust_id, \n          COUNT(*) AS cust_purchase_cnt\n   FROM cust_daily_trans_amt\n   GROUP BY cust_id\n),\ncust_mon_profit AS (\n   SELECT cust_id, \n          SUM(profit) AS cust_profit\n   FROM cust_prod_mon_profit\n   GROUP BY cust_id\n),\n-- Calculate min and max profit\nmin_max_p AS (\n   SELECT MIN(cust_profit) AS min_p, \n          MAX(cust_profit) AS max_p\n   FROM cust_mon_profit\n),\n-- Creating a simplified bucket system due to lack of `width_bucket` function\ncust_bucket AS (\n   SELECT cust_id, cust_profit,\n          CASE\n             WHEN cust_profit <= min_p + (max_p - min_p) / 10 THEN 1\n             WHEN cust_profit <= min_p + 2 * (max_p - min_p) / 10 THEN 2\n             WHEN cust_profit <= min_p + 3 * (max_p - min_p) / 10 THEN 3\n             WHEN cust_profit <= min_p + 4 * (max_p - min_p) / 10 THEN 4\n             WHEN cust_profit <= min_p + 5 * (max_p - min_p) / 10 THEN 5\n             WHEN cust_profit <= min_p + 6 * (max_p - min_p) / 10 THEN 6\n             WHEN cust_profit <= min_p + 7 * (max_p - min_p) / 10 THEN 7\n             WHEN cust_profit <= min_p + 8 * (max_p - min_p) / 10 THEN 8\n             WHEN cust_profit <= min_p + 9 * (max_p - min_p) / 10 THEN 9\n             ELSE 10\n          END AS bucket\n   FROM cust_mon_profit, min_max_p\n),\n-- Aggregated data needed for each bucket\nhisto_data AS (\n   SELECT bucket,\n          (bucket * (max_p - min_p) / 10) AS top_end,\n          COUNT(*) AS histo_count\n   FROM cust_bucket, min_max_p\n   GROUP BY bucket\n),\n-- Profitability of each product sold within each bucket\nbucket_prod_profits AS (\n   SELECT bucket, \n          prod_id, \n          SUM(profit) AS tot_prod_profit\n   FROM cust_bucket\n   JOIN cust_prod_mon_profit ON cust_bucket.cust_id = cust_prod_mon_profit.cust_id\n   GROUP BY bucket, prod_id\n),\n-- Most and least profitable product by bucket\nprod_profit AS (\n   SELECT bucket,\n          MIN(tot_prod_profit) AS min_profit_prod,\n          MAX(tot_prod_profit) AS max_profit_prod\n   FROM bucket_prod_profits\n   GROUP BY bucket\n)\n-- Main query block\nSELECT histo_data.bucket AS bucket, \n       histo_data.histo_count AS customer_count,\n       prod_profit.min_profit_prod AS min_profit, \n       prod_profit.max_profit_prod AS max_profit\nFROM histo_data\nJOIN prod_profit ON histo_data.bucket = prod_profit.bucket\nORDER BY bucket ASC;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local067",
        "db": "complex_oracle",
        "question": "Can you provide the highest and lowest profits for Italian customers segmented into ten evenly divided tiers based on their December 2021 sales profits?",
        "SQL": "WITH customer_profit AS (\n    SELECT\n        c.cust_id,\n        SUM(s.amount_sold - cs.unit_cost * s.quantity_sold) AS total_profit\n    FROM\n        customers c\n        JOIN countries co ON c.country_id = co.country_id\n        JOIN sales s ON c.cust_id = s.cust_id\n        JOIN costs cs ON s.prod_id = cs.prod_id AND s.time_id = cs.time_id AND s.promo_id = cs.promo_id AND s.channel_id = cs.channel_id\n    WHERE\n        co.country_name = 'Italy'\n        AND s.time_id BETWEEN DATE('2021-12-01') AND DATE('2021-12-31')\n    GROUP BY\n        c.cust_id\n),\nbucketed_profits AS (\n    SELECT\n        cust_id,\n        total_profit,\n        NTILE(10) OVER (ORDER BY total_profit DESC) AS bucket\n    FROM\n        customer_profit\n)\n\nSELECT\n    bucket,\n    MAX(total_profit) AS max_profit,\n    MIN(total_profit) AS min_profit\nFROM\n    bucketed_profits\nGROUP BY\n    bucket",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local070",
        "db": "city_legislation",
        "question": "Please examine our records for Chinese cities in July 2021 and identify both the shortest and longest streaks of consecutive date entries. List the dates along with their corresponding city names, capitalizing the first letter of each city name, for these streaks.",
        "SQL": "WITH get_dates AS (\n    SELECT\n        insert_date,\n        city_name\n    FROM (\n        SELECT\n            insert_date,\n            city_name,\n            ROW_NUMBER() OVER (PARTITION BY insert_date ORDER BY insert_date) AS row_num\n        FROM\n            cities\n        WHERE\n            country_code_2 = 'cn'\n            AND date(insert_date) BETWEEN '2021-07-01' AND '2021-07-31'\n    )\n    WHERE row_num = 1\n),\nget_diff AS (\n    SELECT\n        city_name,\n        insert_date,\n        CAST(strftime('%d', insert_date) AS INTEGER) - ROW_NUMBER() OVER (ORDER BY insert_date) AS diff\n    FROM (\n        SELECT\n            city_name,\n            insert_date,\n            ROW_NUMBER() OVER (ORDER BY insert_date) AS row_num\n        FROM\n            get_dates\n    )\n),\nget_diff_count AS (\n    SELECT\n        city_name,\n        insert_date,\n        COUNT(*) OVER (PARTITION BY diff) AS diff_count\n    FROM\n        get_diff\n),\nget_max_rank AS (\n    SELECT\n        DENSE_RANK() OVER (ORDER BY diff_count ASC) AS rnk,\n        insert_date,\n        city_name\n    FROM\n        get_diff_count\n),\nget_min_rank AS (\n    SELECT\n        DENSE_RANK() OVER (ORDER BY diff_count DESC) AS rnk,\n        insert_date,\n        city_name\n    FROM\n        get_diff_count\n)\n\nSELECT\n    insert_date AS most_consecutive_dates,\n    UPPER(SUBSTR(city_name, 1, 1)) || LOWER(SUBSTR(city_name, 2)) AS city_name\nFROM\n    get_max_rank\nWHERE\n    rnk = 1\nUNION\nSELECT\n    insert_date AS most_consecutive_dates,\n    UPPER(SUBSTR(city_name, 1, 1)) || LOWER(SUBSTR(city_name, 2)) AS city_name\nFROM\n    get_min_rank\nWHERE\n    rnk = 1\nORDER BY\n    insert_date;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local071",
        "db": "city_legislation",
        "question": "Could you review our records in June 2022 and identify which countries have the longest streak of consecutive inserted city dates? Please list the 2-letter length country codes of these countries.",
        "SQL": "WITH get_dates AS (\n    SELECT\n        insert_date,\n        country_code_2\n    FROM (\n        SELECT\n            insert_date,\n            country_code_2,\n            ROW_NUMBER() OVER (PARTITION BY insert_date, country_code_2 ORDER BY insert_date) AS row_num\n        FROM\n            cities\n        WHERE\n            insert_date BETWEEN '2022-06-01' AND '2022-06-30'\n    )\n    WHERE row_num = 1\n),\nget_diff AS (\n    SELECT\n        country_code_2,\n        insert_date,\n        CAST(strftime('%d', insert_date) AS INTEGER) - ROW_NUMBER() OVER (PARTITION BY country_code_2 ORDER BY insert_date) AS diff\n    FROM (\n        SELECT\n            country_code_2,\n            insert_date,\n            ROW_NUMBER() OVER (PARTITION BY country_code_2 ORDER BY insert_date) AS row_num\n        FROM\n            get_dates\n    )\n),\nget_diff_count AS (\n    SELECT\n        country_code_2,\n        insert_date,\n        COUNT(*) OVER (PARTITION BY country_code_2, diff) AS diff_count\n    FROM\n        get_diff\n),\nget_rank AS (\n    SELECT\n        country_code_2,\n        DENSE_RANK() OVER (PARTITION BY country_code_2 ORDER BY diff_count DESC) AS rnk,\n        insert_date\n    FROM\n        get_diff_count\n),\ncount_rank AS(\n\tSELECT\n\t\tcountry_code_2,\n\t\tCOUNT(rnk) AS diff_count\n\tFROM\n\t\tget_rank\n\tGROUP BY \n\t\tcountry_code_2,\n\t\trnk\n)\nSELECT\n    country_code_2 AS country\nFROM\n    count_rank\nWHERE\n\tdiff_count = (\n\t\tSELECT\n            MAX(diff_count)\n        FROM\n            count_rank\n\t);",
        "external_knowledge": null,
        "plan": "1. **Filter Records for June 2022:**\n   - Extract records with dates between June 1, 2022, and June 30, 2022.\n   - Ensure each date-country pair is unique by using row numbering and filtering on the first occurrence.\n\n2. **Calculate Date Differences:**\n   - Compute the difference between the day of the month and the row number for each country to identify potential streaks.\n\n3. **Count Consecutive Dates:**\n   - For each country, count how many times each difference value appears. This helps to identify streaks of consecutive dates for each country.\n\n4. **Rank Streak Lengths:**\n   - For each country, assign a rank based on the length of these streaks, with the longest streak getting the highest rank.\n\n5. **Count Streaks by Rank:**\n   - For each country, count the number of times each rank appears. This step helps to identify the countries with the longest streaks.\n\n6. **Identify Countries with Longest Streak:**\n   - Select the countries with the maximum streak length by comparing the counts from the previous step.\n   - Return the country codes of these countries.",
        "special_function": null
    },
    {
        "instance_id": "local072",
        "db": "city_legislation",
        "question": "Identify the country with data inserted on nine different days in January 2022. Then, find the longest consecutive period with data insertions for this country during January 2022, and calculate the proportion of entries that are from its capital city within this longest consecutive insertion period.",
        "SQL": "WITH COUNTRY AS (\n    SELECT\n        insert_date,\n        country_code_2 AS target_country\n    FROM (\n        SELECT\n            insert_date,\n            country_code_2,\n            ROW_NUMBER() OVER (PARTITION BY insert_date, country_code_2 ORDER BY insert_date) AS row_num\n        FROM\n            cities\n        WHERE\n            insert_date BETWEEN '2022-01-01' AND '2022-01-31'\n    )\n    WHERE row_num = 1\n    GROUP BY country_code_2\n    HAVING COUNT(DISTINCT insert_date) == 9\n),\nget_dates AS (\n    SELECT\n        insert_date,\n        city_name,\n        capital\n    FROM (\n        SELECT\n            insert_date,\n            city_name,\n            capital,\n            ROW_NUMBER() OVER (PARTITION BY insert_date ORDER BY insert_date) AS row_num\n        FROM\n            cities\n        WHERE\n            country_code_2 in (SELECT target_country FROM COUNTRY)\n            AND insert_date BETWEEN '2022-01-01' AND '2022-01-31'\n    )\n    WHERE row_num = 1\n),\nget_diff AS (\n    SELECT\n        city_name,\n        insert_date,\n        capital,\n        CAST(strftime('%d', insert_date) AS INTEGER) - ROW_NUMBER() OVER (ORDER BY insert_date) AS diff\n    FROM (\n        SELECT\n            city_name,\n            insert_date,\n            capital,\n            ROW_NUMBER() OVER (ORDER BY insert_date) AS row_num\n        FROM\n            get_dates\n    )\n),\nget_diff_count AS (\n    SELECT\n        city_name,\n        insert_date,\n        capital,\n        COUNT(*) OVER (PARTITION BY diff) AS diff_count\n    FROM\n        get_diff\n),\nget_rank AS (\n    SELECT\n        DENSE_RANK() OVER (ORDER BY diff_count DESC) AS rnk,\n        insert_date,\n        city_name,\n        capital\n    FROM\n        get_diff_count\n),\nget_capital AS (\n        SELECT \n                capital\n        FROM\n                cities\n        WHERE \n                country_code_2 in (SELECT target_country FROM COUNTRY)\n        AND \n                insert_date IN (\n                SELECT \n                        insert_date\n                FROM \n                        get_rank\n                WHERE \n                        rnk = 1\n                ORDER BY \n                    insert_date\n        )\n)\nSELECT SUM(capital)/(COUNT(capital)*1.00) AS percentage_of_capital\nFROM\n        get_capital;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local068",
        "db": "city_legislation",
        "question": "Calculate the number of new cities inserted each April, May, and June, along with the year-over-year growth percentage for each month from 2021-2023. List the year, month, the total number of cities added, the running cumulative total, and the year-over-year growth percentage (including \"%\") for both the monthly total and the running total. Ensure that 2021 data is used only as a baseline for calculating growth rates, and exclude it from the final output.",
        "SQL": "WITH monthly_counts AS (\n    SELECT\n        strftime('%Y', insert_date) AS year,\n        strftime('%m', insert_date) AS month,\n        COUNT(*) AS monthly_count\n    FROM \n        cities\n    WHERE \n        strftime('%m', insert_date) IN ('04', '05', '06')\n    GROUP BY \n        year, month\n),\nrunning_totals AS (\n    SELECT\n        year,\n        month,\n        monthly_count,\n        SUM(monthly_count) OVER (PARTITION BY month ORDER BY year) AS running_total\n    FROM\n        monthly_counts\n),\nyear_over_year_growth AS (\n    SELECT\n        year,\n        month,\n        monthly_count,\n        running_total,\n        100.0 * (monthly_count - LAG(monthly_count, 1) OVER (PARTITION BY month ORDER BY year)) / \n            LAG(monthly_count, 1) OVER (PARTITION BY month ORDER BY year) AS year_over_year,\n        100.0 * (running_total - LAG(running_total, 1) OVER (PARTITION BY month ORDER BY year)) / \n            LAG(running_total, 1) OVER (PARTITION BY month ORDER BY year) AS total_year_over_year\n    FROM\n        running_totals\n)\nSELECT\n    year,\n    CASE month\n        WHEN '04' THEN 'April'\n        WHEN '05' THEN 'May'\n        WHEN '06' THEN 'June'\n    END AS month_name,\n    monthly_count AS cities_inserted,\n    running_total AS running_total,\n    COALESCE(year_over_year, 0) || '%' AS year_over_year,\n    COALESCE(total_year_over_year, 0) || '%' AS total_year_over_year\nFROM\n    year_over_year_growth\nWHERE year > '2021'\nORDER BY \n    year, month;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local073",
        "db": "modern_data",
        "question": "Let's generate a report for each pizza order that lists the pizza name followed by \": \", then all the ingredients in alphabetical order. If any ingredient is ordered more than once, indicate it with '2x' directly in front of the ingredient without a space.",
        "SQL": "WITH id_customer_orders AS (\n    SELECT\n        ROW_NUMBER() OVER() AS row_id,\n        order_id,\n        customer_id,\n        pizza_id,\n        exclusions,\n        extras,\n        order_time\n    FROM\n        pizza_clean_customer_orders co\n),\nget_pizza_toppings AS (\n    WITH RECURSIVE split_toppings AS (\n        SELECT\n            row_id,\n            order_id,\n            TRIM(SUBSTR(toppings, 1, INSTR(toppings || ',', ',') - 1)) AS single_toppings,\n            SUBSTR(toppings || ',', INSTR(toppings || ',', ',') + 1) AS remaining_toppings\n        FROM\n            id_customer_orders c\n        JOIN\n            pizza_recipes pr\n        ON\n            c.pizza_id = pr.pizza_id\n        UNION ALL\n        SELECT\n            row_id,\n            order_id,\n            TRIM(SUBSTR(remaining_toppings, 1, INSTR(remaining_toppings, ',') - 1)) AS single_toppings,\n            SUBSTR(remaining_toppings, INSTR(remaining_toppings, ',') + 1) AS remaining_toppings\n        FROM\n            split_toppings\n        WHERE\n            remaining_toppings <> ''\n    )\n    SELECT\n        row_id,\n        order_id,\n        single_toppings,\n        COUNT(*) OVER (PARTITION BY row_id, order_id) AS topping_count\n    FROM\n        split_toppings\n),\ningredients AS (\n    WITH temp AS (\n        SELECT\n            t2.row_id,\n            t2.order_id,\n            t2.customer_id,\n            t2.order_time,\n            t1.pizza_name,\n            (\n                SELECT\n                    GROUP_CONCAT(topping_name, ', ')\n                FROM\n                    pizza_toppings\n                WHERE\n                    topping_id IN (\n                        SELECT single_toppings FROM get_pizza_toppings WHERE row_id = t2.row_id\n                    )\n                    AND topping_id NOT IN (\n                        SELECT exclusions FROM pizza_get_exclusions WHERE order_id = t2.order_id\n                    )\n            ) AS all_toppings,\n            (\n                SELECT\n                    GROUP_CONCAT(topping_name, ', ')\n                FROM\n                    pizza_toppings\n                WHERE\n                    topping_id IN (\n                        SELECT extras FROM pizza_get_extras WHERE order_id = t2.order_id\n                    )\n            ) AS all_extras\n        FROM\n            pizza_names t1\n        JOIN\n            id_customer_orders t2\n        ON\n            t2.pizza_id = t1.pizza_id\n        ORDER BY\n            t2.row_id\n    )\n    SELECT *, \n        COALESCE(all_toppings, '') || ',' || COALESCE(all_extras, '') AS all_ingredients\n    FROM temp\n),\nsplit_ingredients AS (\n    SELECT\n        row_id,\n        order_id,\n        customer_id,\n        pizza_name,\n        order_time,\n        all_extras,\n        TRIM(SUBSTR(all_ingredients, 1, INSTR(all_toppings || ',', ',') - 1)) AS each_ingredient,\n        SUBSTR(all_ingredients || ',', INSTR(all_toppings || ',', ',') + 1) AS remaining_ingredients\n    FROM\n        ingredients\n    UNION ALL\n    SELECT\n        row_id,\n        order_id,\n        customer_id,\n        pizza_name,\n        order_time,\n        all_extras,\n        TRIM(SUBSTR(remaining_ingredients, 1, INSTR(remaining_ingredients, ',') - 1)) AS each_ingredient,\n        SUBSTR(remaining_ingredients, INSTR(remaining_ingredients, ',') + 1)\n    FROM\n        split_ingredients\n    WHERE\n        remaining_ingredients <> ''\n),\ncreate_strings AS (\n    SELECT\n        row_id,\n        order_id,\n        customer_id,\n        pizza_name,\n        order_time,\n        CASE\n            WHEN COUNT(each_ingredient) > 1 THEN '2x' || each_ingredient\n            WHEN each_ingredient != '' THEN each_ingredient\n        END AS new_ingredient\n    FROM\n        split_ingredients\n    GROUP BY \n        row_id,\n        order_id,\n        customer_id,\n        pizza_name,\n        order_time,\n        each_ingredient\n    ORDER BY\n        new_ingredient\n)\nSELECT\n    order_id,\n    customer_id,\n    CASE\n        WHEN pizza_name = 'Meatlovers' THEN 1\n        ELSE 2\n    END AS pizza_id,\n    order_time,\n    row_id AS original_row_number,\n    pizza_name || ': ' || GROUP_CONCAT(new_ingredient, ', ') AS toppings\nFROM\n    create_strings\nGROUP BY\n    row_id,\n    order_id,\n    customer_id,\n    pizza_name,\n    order_time\nORDER BY\n    row_id;",
        "external_knowledge": null,
        "plan": "1. **Generate Unique Identifiers for Orders**:\n   - Assign a unique row number to each order to facilitate tracking and manipulation.\n\n2. **Extract Toppings for Each Pizza**:\n   - Use a recursive query to split the list of toppings for each pizza into individual toppings.\n   - For each topping, trim any whitespace and separate it from the remaining toppings.\n\n3. **Count Each Topping**:\n   - For each topping extracted, count its occurrences within the same order.\n\n4. **Compile All Toppings and Extras**:\n   - For each order, compile a list of all toppings and extras, excluding any specified exclusions.\n\n5. **Combine Toppings and Extras**:\n   - Concatenate the compiled list of toppings and extras into a single string of ingredients for each order.\n\n6. **Split Combined Ingredients**:\n   - Split the combined list of ingredients back into individual ingredients for further processing.\n\n7. **Format Ingredients with Quantity**:\n   - For each ingredient, check its count. If it appears more than once, prepend '2x' to the ingredient name.\n   - Ensure each ingredient is formatted correctly, even if it appears only once.\n\n8. **Generate Final Report**:\n   - Group the formatted ingredients back into a single string for each order.\n   - Include the order ID, customer ID, pizza name, and order time in the final output.\n   - Sort the ingredients alphabetically within each order's list.\n\n9. **Output the Report**:\n   - Produce the final report with all necessary details, sorted by the unique identifier for each order.",
        "special_function": null
    },
    {
        "instance_id": "local066",
        "db": "modern_data",
        "question": "Based on our customer pizza order information, summarize the total quantity of each ingredient used in the pizzas we delivered. Output the name and quantity for each ingredient.",
        "SQL": "WITH cte_cleaned_customer_orders AS (\n    SELECT\n        *,\n        ROW_NUMBER() OVER () AS original_row_number\n    FROM \n        pizza_clean_customer_orders\n),\nsplit_regular_toppings AS (\n    SELECT\n        pizza_id,\n        TRIM(SUBSTR(toppings, 1, INSTR(toppings || ',', ',') - 1)) AS topping_id,\n        SUBSTR(toppings || ',', INSTR(toppings || ',', ',') + 1) AS remaining_toppings\n    FROM \n        pizza_recipes\n    UNION ALL\n    SELECT\n        pizza_id,\n        TRIM(SUBSTR(remaining_toppings, 1, INSTR(remaining_toppings, ',') - 1)) AS topping_id,\n        SUBSTR(remaining_toppings, INSTR(remaining_toppings, ',') + 1) AS remaining_toppings\n    FROM \n        split_regular_toppings\n    WHERE\n        remaining_toppings <> ''\n),\ncte_base_toppings AS (\n    SELECT\n        t1.order_id,\n        t1.customer_id,\n        t1.pizza_id,\n        t1.order_time,\n        t1.original_row_number,\n        t2.topping_id\n    FROM \n        cte_cleaned_customer_orders AS t1\n    LEFT JOIN \n        split_regular_toppings AS t2\n    ON \n        t1.pizza_id = t2.pizza_id\n),\nsplit_exclusions AS (\n    SELECT\n        order_id,\n        customer_id,\n        pizza_id,\n        order_time,\n        original_row_number,\n        TRIM(SUBSTR(exclusions, 1, INSTR(exclusions || ',', ',') - 1)) AS topping_id,\n        SUBSTR(exclusions || ',', INSTR(exclusions || ',', ',') + 1) AS remaining_exclusions\n    FROM \n        cte_cleaned_customer_orders\n    WHERE \n        exclusions IS NOT NULL\n    UNION ALL\n    SELECT\n        order_id,\n        customer_id,\n        pizza_id,\n        order_time,\n        original_row_number,\n        TRIM(SUBSTR(remaining_exclusions, 1, INSTR(remaining_exclusions, ',') - 1)) AS topping_id,\n        SUBSTR(remaining_exclusions, INSTR(remaining_exclusions, ',') + 1) AS remaining_exclusions\n    FROM \n        split_exclusions\n    WHERE\n        remaining_exclusions <> ''\n),\nsplit_extras AS (\n    SELECT\n        order_id,\n        customer_id,\n        pizza_id,\n        order_time,\n        original_row_number,\n        TRIM(SUBSTR(extras, 1, INSTR(extras || ',', ',') - 1)) AS topping_id,\n        SUBSTR(extras || ',', INSTR(extras || ',', ',') + 1) AS remaining_extras\n    FROM \n        cte_cleaned_customer_orders\n    WHERE \n        extras IS NOT NULL\n    UNION ALL\n    SELECT\n        order_id,\n        customer_id,\n        pizza_id,\n        order_time,\n        original_row_number,\n        TRIM(SUBSTR(remaining_extras, 1, INSTR(remaining_extras, ',') - 1)) AS topping_id,\n        SUBSTR(remaining_extras, INSTR(remaining_extras, ',') + 1) AS remaining_extras\n    FROM \n        split_extras\n    WHERE\n        remaining_extras <> ''\n),\ncte_combined_orders AS (\n    SELECT \n        order_id,\n        customer_id,\n        pizza_id,\n        order_time,\n        original_row_number,\n        topping_id\n    FROM \n        cte_base_toppings\n    WHERE topping_id NOT IN (SELECT topping_id FROM split_exclusions WHERE split_exclusions.order_id = cte_base_toppings.order_id)\n    UNION ALL\n    SELECT \n        order_id,\n        customer_id,\n        pizza_id,\n        order_time,\n        original_row_number,\n        topping_id\n    FROM \n        split_extras\n)\nSELECT\n    t2.topping_name,\n    COUNT(*) AS topping_count\nFROM \n    cte_combined_orders AS t1\nJOIN \n    pizza_toppings AS t2\nON \n    t1.topping_id = t2.topping_id\nGROUP BY \n    t2.topping_name\nORDER BY \n    topping_count DESC;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local065",
        "db": "modern_data",
        "question": "Calculate the total income from Meat Lovers pizzas priced at $12 and Vegetarian pizzas at $10. Include any extra toppings charged at $1 each. Ensure that canceled orders are filtered out. How much money has Pizza Runner earned in total?",
        "SQL": "WITH get_extras_count AS (\n    WITH RECURSIVE split_extras AS (\n        SELECT\n            order_id,\n            TRIM(SUBSTR(extras, 1, INSTR(extras || ',', ',') - 1)) AS each_extra,\n            SUBSTR(extras || ',', INSTR(extras || ',', ',') + 1) AS remaining_extras\n        FROM\n            pizza_clean_customer_orders\n        UNION ALL\n        SELECT\n            order_id,\n            TRIM(SUBSTR(remaining_extras, 1, INSTR(remaining_extras, ',') - 1)) AS each_extra,\n            SUBSTR(remaining_extras, INSTR(remaining_extras, ',') + 1)\n        FROM\n            split_extras\n        WHERE\n            remaining_extras <> ''\n    )\n    SELECT\n        order_id,\n        COUNT(each_extra) AS total_extras\n    FROM\n        split_extras\n    GROUP BY\n        order_id\n),\ncalculate_totals AS (\n    SELECT\n        t1.order_id,\n        t1.pizza_id,\n        SUM(\n            CASE\n                WHEN pizza_id = 1 THEN 12\n                WHEN pizza_id = 2 THEN 10\n            END\n        ) AS total_price,\n        t3.total_extras\n    FROM \n        pizza_clean_customer_orders AS t1\n    JOIN\n        pizza_clean_runner_orders AS t2 \n    ON\n        t2.order_id = t1.order_id\n    LEFT JOIN\n        get_extras_count AS t3\n    ON\n        t3.order_id = t1.order_id\n    WHERE\n        t2.cancellation IS NULL\n    GROUP BY \n        t1.order_id,\n        t1.pizza_id,\n        t3.total_extras\n)\nSELECT \n    SUM(total_price) + SUM(total_extras) AS total_income\nFROM \n    calculate_totals;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local074",
        "db": "bank_sales_trading",
        "question": "Please generate a summary of the closing balances at the end of each month for each customer transactions, show the monthly changes and monthly cumulative bank account balances. Ensure that even if a customer has no account activity in a given month, the balance for that month is still included in the output.",
        "SQL": "WITH RECURSIVE generate_series AS (\n    SELECT 0 AS value\n    UNION ALL\n    SELECT value + 1\n    FROM generate_series\n    WHERE value < 3\n),\nclosing_balance AS (\n    SELECT\n        customer_id,\n        strftime('%Y-%m', DATE(txn_date, 'start of month')) AS txn_month,\n        SUM(\n            CASE\n                WHEN txn_type = 'deposit' THEN txn_amount\n                ELSE -txn_amount\n            END\n        ) AS transaction_amount\n    FROM\n        customer_transactions\n    GROUP BY\n        customer_id,\n        txn_month\n    ORDER BY\n        customer_id\n),\ngenerate_months_cte AS (\n    SELECT DISTINCT\n        customer_id,\n        strftime('%Y-%m', DATE('2020-01-01', '+' || value || ' month')) AS generated_month\n    FROM\n        customer_transactions, generate_series\n)\nSELECT \n    t1.customer_id,\n    t1.generated_month,\n    COALESCE(t2.transaction_amount, 0) AS balance_activity,\n    SUM(COALESCE(t2.transaction_amount, 0)) OVER (\n        PARTITION BY t1.customer_id\n        ORDER BY t1.generated_month\n    ) AS month_end_balance\nFROM\n    generate_months_cte AS t1\nLEFT JOIN \n    closing_balance AS t2\nON\n    t1.generated_month = t2.txn_month\nAND\n    t1.customer_id = t2.customer_id;",
        "external_knowledge": null,
        "plan": "1. **Generate a Series of Values**:\n    - Create a recursive Common Table Expression (CTE) to generate a sequence of numbers starting from 0 up to 3. This will be used later to generate months.\n\n2. **Calculate Monthly Transaction Amounts**:\n    - Create another CTE to calculate the net transaction amount for each customer at the end of each month. \n    - Convert transaction dates to 'YYYY-MM' format to group transactions by month.\n    - Sum the transaction amounts where deposits are added and withdrawals are subtracted.\n    - Group the results by customer and month to get the closing balance for each month.\n\n3. **Generate a List of Months**:\n    - Create a CTE to generate a list of months for each customer starting from a fixed date, adding the values from the generated series to this date.\n    - This ensures that even months without transactions are included for each customer.\n\n4. **Combine Transactions with Generated Months**:\n    - Perform a left join between the generated months and the monthly transaction amounts.\n    - For each customer, combine the list of all possible months with the actual transaction data, ensuring that all months are represented.\n\n5. **Calculate Monthly Balances**:\n    - For each customer and each month, calculate the balance activity (net transaction amount) using the joined data.\n    - Use a window function to calculate the cumulative sum of these monthly balances to get the month-end balance for each month.\n\n6. **Output the Results**:\n    - Select the customer ID, generated month, net transaction amount (balance activity), and the cumulative month-end balance.\n    - Ensure that even if there are no transactions in a month, it is represented with a balance activity of zero. \n\nThis process generates a summary of monthly balances for each customer, showing both the monthly changes and the cumulative balances at the end of each month.",
        "special_function": null
    },
    {
        "instance_id": "local064",
        "db": "bank_sales_trading",
        "question": "What is the difference in average month-end balance between the month with the most and the month with the fewest customers having a positive balance in 2020?",
        "SQL": "WITH RECURSIVE generate_series AS (\n    SELECT 0 AS value\n    UNION ALL\n    SELECT value + 1\n    FROM generate_series\n    WHERE value < 3\n),\ngenerate_months_cte AS (\n    SELECT DISTINCT\n        customer_id,\n        strftime('%Y-%m', DATE('2020-01-01', '+' || value || ' month')) AS generated_month\n    FROM\n        customer_transactions, generate_series\n    WHERE\n        strftime('%Y', DATE('2020-01-01', '+' || value || ' month')) = '2020'\n),\nclosing_balance AS (\n    SELECT\n        customer_id,\n        strftime('%Y-%m', DATE(txn_date, 'start of month')) AS txn_month,\n        SUM(\n            CASE\n                WHEN txn_type = 'deposit' THEN txn_amount\n                ELSE -txn_amount\n            END\n        ) AS transaction_amount\n    FROM\n        customer_transactions\n    WHERE\n        strftime('%Y', txn_date) = '2020'\n    GROUP BY\n        customer_id,\n        txn_month\n),\nfinal_balance AS (\n    SELECT \n        t1.customer_id,\n        t1.generated_month,\n        COALESCE(SUM(t2.transaction_amount), 0) AS month_end_balance\n    FROM\n        generate_months_cte AS t1\n    LEFT JOIN \n        closing_balance AS t2\n    ON\n        t1.generated_month = t2.txn_month\n        AND t1.customer_id = t2.customer_id\n    GROUP BY\n        t1.customer_id,\n        t1.generated_month\n),\npositive_balance_counts AS (\n    SELECT\n        generated_month,\n        COUNT(DISTINCT customer_id) AS positive_balance_count\n    FROM\n        final_balance\n    WHERE\n        month_end_balance > 0\n    GROUP BY\n        generated_month\n),\nmost_positive_month AS (\n    SELECT\n        generated_month\n    FROM\n        positive_balance_counts\n    ORDER BY\n        positive_balance_count DESC\n    LIMIT 1\n),\nleast_positive_month AS (\n    SELECT\n        generated_month\n    FROM\n        positive_balance_counts\n    ORDER BY\n        positive_balance_count ASC\n    LIMIT 1\n),\naverage_balance AS (\n    SELECT\n        'most_positive' AS month_type,\n        AVG(month_end_balance) AS avg_balance\n    FROM\n        final_balance\n    WHERE\n        generated_month = (SELECT generated_month FROM most_positive_month)\n    UNION ALL\n    SELECT\n        'least_positive' AS month_type,\n        AVG(month_end_balance) AS avg_balance\n    FROM\n        final_balance\n    WHERE\n        generated_month = (SELECT generated_month FROM least_positive_month)\n)\nSELECT\n    (SELECT avg_balance FROM average_balance WHERE month_type = 'most_positive') -\n    (SELECT avg_balance FROM average_balance WHERE month_type = 'least_positive') AS balance_diff;",
        "external_knowledge": null,
        "plan": "1. **Generate Series of Months**:\n   - Create a recursive series of numbers to represent months in the year 2020.\n\n2. **Generate Month-Year Pairs**:\n   - Use the series of numbers to generate distinct month-year pairs for each customer based on a fixed start date in 2020.\n\n3. **Calculate Monthly Transaction Totals**:\n   - Compute the net transaction amounts for each customer by month for the year 2020. This involves summing up deposits and withdrawals.\n\n4. **Calculate Month-End Balance**:\n   - Combine the generated month-year pairs with the monthly transaction totals to compute the month-end balance for each customer. If there are no transactions for a month, the balance is assumed to be zero.\n\n5. **Count Positive Balances**:\n   - Count the number of customers with a positive month-end balance for each generated month.\n\n6. **Identify Months with Most and Fewest Positive Balances**:\n   - Determine the month with the highest count of customers with positive balances.\n   - Determine the month with the lowest count of customers with positive balances.\n\n7. **Compute Average Balances**:\n   - Calculate the average month-end balance for the month with the highest count of positive balances.\n   - Calculate the average month-end balance for the month with the lowest count of positive balances.\n\n8. **Calculate Difference**:\n   - Subtract the average balance of the month with the fewest positive balances from the average balance of the month with the most positive balances to find the difference.",
        "special_function": null
    },
    {
        "instance_id": "local297",
        "db": "bank_sales_trading",
        "question": "I\u2019d like to know the percentage of users whose closing balances showed a growth rate of more than 5% in the most recent month (measured as of the 1st of each month). If the previous month\u2019s balance is zero, calculate the growth rate by multiplying the current balance by 100.",
        "SQL": "WITH customer_balance AS (\n    SELECT *,\n           SUM(txn_amount) OVER (PARTITION BY customer_id ORDER BY month_ ASC) AS balance\n    FROM (\n        SELECT customer_id, strftime('%Y-%m-01', txn_date) AS month_, SUM(txn_group) AS txn_amount \n        FROM (\n            SELECT *,\n                   CASE WHEN txn_type = 'deposit' THEN txn_amount\n                        ELSE -1 * txn_amount END AS txn_group\n            FROM Customer_Transactions\n            ORDER BY customer_id, txn_date\n        ) AS update_txn_amount\n        GROUP BY customer_id, strftime('%Y-%m-01', txn_date)\n    ) AS monthly_totals\n),\ngrowth_rates AS (\n    SELECT customer_id, month_, balance, previous_month_balance, \n           CASE WHEN previous_month_balance IS NULL THEN NULL\n                WHEN previous_month_balance = 0 THEN balance * 100\n                ELSE ROUND(((balance - previous_month_balance) * 1.0 / ABS(previous_month_balance)) * 100, 1) END AS growth_rate,\n           ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY month_ DESC) AS balance_index\n    FROM (\n        SELECT *,\n               LAG(balance) OVER (PARTITION BY customer_id ORDER BY month_) AS previous_month_balance\n        FROM customer_balance\n    ) AS add_previous_month_balance\n),\ncust_last_balance AS (\n    SELECT customer_id, month_, growth_rate,\n           CASE WHEN growth_rate > 5 THEN 1 ELSE 0 END AS growth_rate_check \n    FROM growth_rates\n    WHERE balance_index = 1\n)\nSELECT (SUM(growth_rate_check) * 1.0 / COUNT(*) * 100) AS percentage_growth_check\nFROM cust_last_balance;",
        "external_knowledge": null,
        "plan": "1. Calculate the running balance for each customer by month.\n2. Calculate the monthly balance growth rate for each customer.\n3. Select the latest balance growth rate for each customer and flag whether it exceeds 5%.\n4. Calculate the percentage of customers whose last month's growth rate exceeds 5%.",
        "special_function": null
    },
    {
        "instance_id": "local298",
        "db": "bank_sales_trading",
        "question": "For each month, calculate the total balance from all users for the previous month (measured as of the 1st of each month), replacing any negative balances with zero. Ensure that data from the first month is used only as a baseline for calculating previous total balance, and exclude it from the final output. Sort the results in ascending order by month. ",
        "SQL": "WITH customer_balance AS (\n    SELECT *,\n           SUM(txn_amount) OVER (PARTITION BY customer_id ORDER BY month_ ASC) AS balance\n    FROM (\n        SELECT customer_id, \n               strftime('%Y-%m-01', txn_date) AS month_, \n               SUM(txn_group) AS txn_amount \n        FROM (\n            SELECT *,\n                   CASE \n                     WHEN txn_type = 'deposit' THEN txn_amount\n                     ELSE -1 * txn_amount \n                   END AS txn_group\n            FROM Customer_Transactions\n            ORDER BY customer_id, txn_date\n        ) AS update_txn_amount\n        GROUP BY customer_id, strftime('%Y-%m-01', txn_date)\n    ) AS monthly_totals\n),\ngrowth_rates AS (\n    SELECT customer_id, \n           month_, \n           balance, \n           previous_month_balance,\n           CASE \n             WHEN previous_month_balance < 0 THEN 0 \n             ELSE previous_month_balance \n           END AS data_storage,\n           ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY month_) AS rn\n    FROM (\n        SELECT *, \n               LAG(balance) OVER (PARTITION BY customer_id ORDER BY month_) AS previous_month_balance\n        FROM customer_balance\n    ) AS add_previous_month_balance\n)\nSELECT month_, \n       SUM(data_storage) AS total_data_storage\nFROM growth_rates\nWHERE rn > 1\nGROUP BY month_\nORDER BY month_;",
        "external_knowledge": null,
        "plan": "1. **Calculate Monthly Transaction Amounts**:\n    - Create a subquery that categorizes each transaction as either a deposit or withdrawal.\n    - Calculate the net transaction amount for each month by summing categorized transactions, grouped by customer and month.\n\n2. **Compute Running Balance**:\n    - Use a window function to calculate a running balance for each customer, ordered by month.\n    - Store this running balance in a common table expression (CTE).\n\n3. **Determine Previous Month\u2019s Balance**:\n    - In another CTE, calculate the balance of the previous month for each customer using a window function.\n    - Use the `LAG` function to fetch the balance from the previous row within the same customer partition.\n\n4. **Prepare Data Storage Calculation**:\n    - Add a case statement to handle negative balances by setting them to zero, as negative balances are not considered for data allocation.\n    - Assign a row number to each month per customer to help filter out the first month (as it does not have a previous month for comparison).\n\n5. **Aggregate Data Storage Requirements**:\n    - Filter out the first month for each customer since it lacks a previous month balance.\n    - Group the data by month and sum the data storage requirements across all customers.\n\n6. **Order Results**:\n    - Order the final results by month to provide a chronological summary of total data storage requirements.",
        "special_function": null
    },
    {
        "instance_id": "local299",
        "db": "bank_sales_trading",
        "question": "Could you calculate each user\u2019s average balance over the past 30 days, computed daily? Then, for each month (based on the 1st of each month), find the highest of these daily averages for each user. Add up these maximum values across all users for each month as the final result. Please use the first month as a baseline for previous balances and exclude it from the output.",
        "SQL": "WITH RECURSIVE customer_date_series AS (\n    SELECT customer_id, MIN(txn_date) AS date_series, MAX(txn_date) AS last_date\n    FROM customer_transactions\n    GROUP BY customer_id\n    UNION ALL\n    SELECT customer_id, date(date_series, '+1 day'), last_date\n    FROM customer_date_series\n    WHERE date(date_series, '+1 day') <= last_date\n),\ncustomer_txn AS (\n    SELECT *,\n           CASE WHEN txn_type = 'deposit' THEN txn_amount\n                ELSE -1 * txn_amount END AS txn_group\n    FROM customer_transactions\n),\ncustomer_balance AS (\n    SELECT s.customer_id, s.date_series, COALESCE(b.txn_group, 0) AS txn_group,\n           SUM(COALESCE(b.txn_group, 0)) OVER (PARTITION BY s.customer_id ORDER BY s.date_series) AS balance\n    FROM customer_date_series s\n    LEFT JOIN customer_txn b ON s.customer_id = b.customer_id AND s.date_series = b.txn_date\n    ORDER BY s.customer_id, s.date_series\n),\ncustomer_data AS (\n    SELECT customer_id, date_series,\n           CASE WHEN txn_row < 30 THEN NULL\n                WHEN avg_last_30 < 0 THEN 0\n                ELSE avg_last_30 END AS data_storage\n    FROM (\n        SELECT *,\n               AVG(balance) OVER (PARTITION BY customer_id ORDER BY date_series ROWS BETWEEN 30 PRECEDING AND CURRENT ROW) AS avg_last_30,\n               ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY date_series) AS txn_row\n        FROM customer_balance\n    ) AS tmp\n),\nmonthly_data AS (\n    SELECT customer_id,\n           strftime('%Y-%m', date_series) AS month,\n           MAX(data_storage) AS data_allocation,\n           ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY strftime('%Y-%m', date_series)) AS month_row\n    FROM customer_data\n    GROUP BY customer_id, month\n)\nSELECT month, SUM(data_allocation) AS total_allocation\nFROM monthly_data\nWHERE month_row > 1\nGROUP BY month;",
        "external_knowledge": null,
        "plan": "1. **Generate Date Series**:\n    - Create a recursive CTE to generate a date series for each user, starting from their earliest transaction date to their latest transaction date.\n\n2. **Classify Transactions**:\n    - Create a CTE to classify transactions into positive (deposits) or negative (withdrawals) amounts.\n\n3. **Calculate Daily Balances**:\n    - Create a CTE to calculate the daily balance for each user by summing up the classified transactions up to each date in the date series.\n\n4. **Calculate 30-Day Average Balance**:\n    - Create a CTE to calculate the 30-day rolling average balance for each user. This involves averaging the daily balances over the past 30 days.\n    - Ensure that the average balance is only considered valid after the first 30 days.\n    - Replace negative average balances with zero.\n\n5. **Identify Monthly Maximum Average Balance**:\n    - Create a CTE to identify the maximum average balance for each user for each month. This involves grouping the data by user and month and selecting the highest valid 30-day average balance for each user.\n\n6. **Sum Highest Monthly Averages**:\n    - Calculate the total required data allocation by summing the highest average balances for all users for each month.\n    - Exclude the first month of data for each user to focus on the subsequent months starting from February.\n\n7. **Output the Results**:\n    - Group the results by month and order them chronologically to get the final total data allocation needed for each month, rounded to one decimal place.",
        "special_function": null
    },
    {
        "instance_id": "local300",
        "db": "bank_sales_trading",
        "question": "Could you calculate the highest daily balance each customer had within each month? Treat any negative daily balances as zero. Then, for each month, add up these maximum daily balances across all customers to get a monthly total.",
        "SQL": "WITH RECURSIVE customer_date_series AS (\n    SELECT customer_id, MIN(txn_date) AS date_series, MAX(txn_date) AS last_date\n    FROM customer_transactions\n    GROUP BY customer_id\n    UNION ALL\n    SELECT customer_id, date(date_series, '+1 day') AS date_series, last_date\n    FROM customer_date_series\n    WHERE date(date_series, '+1 day') <= last_date\n),\ncustomer_txn AS (\n    SELECT *,\n           CASE WHEN txn_type = 'deposit' THEN txn_amount\n                ELSE -1 * txn_amount END AS txn_group\n    FROM customer_transactions\n),\ncustomer_balance AS (\n    SELECT s.customer_id, s.date_series, COALESCE(b.txn_group, 0) AS txn_group,\n           SUM(COALESCE(b.txn_group, 0)) OVER (PARTITION BY s.customer_id ORDER BY s.date_series) AS balance\n    FROM customer_date_series s\n    LEFT JOIN customer_txn b ON s.customer_id = b.customer_id AND s.date_series = b.txn_date\n    ORDER BY s.customer_id, s.date_series\n),\ncustomer_data AS (\n    SELECT customer_id, date_series,\n           CASE WHEN balance < 0 THEN 0\n                ELSE balance END AS data_storage\n    FROM customer_balance\n)\nSELECT month, SUM(data_allocation) AS total_allocation\nFROM (\n    SELECT customer_id,\n           strftime('%Y-%m', date_series) AS month,\n           MAX(data_storage) AS data_allocation\n    FROM customer_data\n    GROUP BY customer_id, strftime('%Y-%m', date_series)\n) AS tmp\nGROUP BY month;",
        "external_knowledge": null,
        "plan": "1. **Generate Date Series:**\n   - Create a date series for each user starting from their earliest transaction date to their latest transaction date.\n   - Use a recursive CTE to incrementally add each day within the range for each user.\n\n2. **Categorize Transactions:**\n   - Categorize each transaction by determining if it is a deposit or a withdrawal.\n   - Convert withdrawal amounts to negative values to reflect deduction from the balance.\n\n3. **Calculate Daily Balances:**\n   - Combine the generated date series with the categorized transactions.\n   - Compute the running balance for each user by summing the transaction amounts day-by-day.\n\n4. **Adjust Negative Balances:**\n   - Ensure that any negative balances are set to zero, as negative balances are not considered in the final data storage calculation.\n\n5. **Extract Monthly Maximum Balances:**\n   - For each user, and for each month, find the highest daily balance.\n   - This represents the peak resource requirement for each user in a given month.\n\n6. **Summarize Monthly Data:**\n   - Sum the highest daily balances across all users for each month.\n   - Round the result to one decimal place for clarity.\n\n7. **Order Results:**\n   - Order the summarized data by month to observe trends over time.",
        "special_function": null
    },
    {
        "instance_id": "local075",
        "db": "bank_sales_trading",
        "question": "Can you provide a breakdown of how many times each product was viewed, how many times they were added to the shopping cart, and how many times they were left in the cart without being purchased? Also, give me the count of actual purchases for each product. Ensure that products with a page id in (1, 2, 12, 13) are filtered out.",
        "SQL": "WITH product_viewed AS (\n    SELECT\n        t1.page_id,\n        SUM(CASE WHEN event_type = 1 THEN 1 ELSE 0 END) AS n_page_views,\n        SUM(CASE WHEN event_type = 2 THEN 1 ELSE 0 END) AS n_added_to_cart\n    FROM\n        shopping_cart_page_hierarchy AS t1\n    JOIN\n        shopping_cart_events AS t2\n    ON\n        t1.page_id = t2.page_id\n    WHERE\n        t1.product_id IS NOT NULL\n    GROUP BY\n        t1.page_id\n),\nproduct_purchased AS (\n    SELECT\n        t2.page_id,\n        SUM(CASE WHEN event_type = 2 THEN 1 ELSE 0 END) AS purchased_from_cart\n    FROM\n        shopping_cart_page_hierarchy AS t1\n    JOIN\n        shopping_cart_events AS t2\n    ON\n        t1.page_id = t2.page_id\n    WHERE\n        t1.product_id IS NOT NULL\n        AND EXISTS (\n            SELECT\n                visit_id\n            FROM\n                shopping_cart_events\n            WHERE\n                event_type = 3\n                AND t2.visit_id = visit_id\n        )\n        AND t1.page_id NOT IN (1, 2, 12, 13)\n    GROUP BY\n        t2.page_id\n),\nproduct_abandoned AS (\n    SELECT\n        t2.page_id,\n        SUM(CASE WHEN event_type = 2 THEN 1 ELSE 0 END) AS abandoned_in_cart\n    FROM\n        shopping_cart_page_hierarchy AS t1\n    JOIN\n        shopping_cart_events AS t2\n    ON\n        t1.page_id = t2.page_id\n    WHERE\n        t1.product_id IS NOT NULL\n        AND NOT EXISTS (\n            SELECT\n                visit_id\n            FROM\n                shopping_cart_events\n            WHERE\n                event_type = 3\n                AND t2.visit_id = visit_id\n        )\n        AND t1.page_id NOT IN (1, 2, 12, 13)\n    GROUP BY\n        t2.page_id\n)\nSELECT\n    t1.page_id,\n    t1.page_name,\n    t2.n_page_views AS 'number of product being viewed',\n    t2.n_added_to_cart AS 'number added to the cart',\n    t4.abandoned_in_cart AS 'without being purchased in cart',\n    t3.purchased_from_cart AS 'count of actual purchases'\nFROM\n    shopping_cart_page_hierarchy AS t1\nJOIN\n    product_viewed AS t2 \nON\n    t2.page_id = t1.page_id\nJOIN\n    product_purchased AS t3 \nON \n    t3.page_id = t1.page_id\nJOIN\n    product_abandoned AS t4 \nON \n    t4.page_id = t1.page_id;",
        "external_knowledge": null,
        "plan": "1. **Identify Product Views and Adds to Cart:**\n   - Create a temporary dataset that counts the number of times each product page was viewed and added to the cart.\n   - This involves filtering the data to include only those records where the product ID is not null.\n   - Aggregate the data by the page identifier to get counts of views and adds to the cart.\n\n2. **Identify Purchases from Cart:**\n   - Create another temporary dataset to count the number of times products were actually purchased from the cart.\n   - This involves filtering the data to include only those records where the product ID is not null and ensuring the product was purchased (by checking for a specific purchase event type).\n   - Exclude certain page identifiers from this dataset.\n   - Aggregate the data by the page identifier to get counts of purchases from the cart.\n\n3. **Identify Abandoned Carts:**\n   - Create a third temporary dataset to count the number of times products were added to the cart but not purchased.\n   - This involves filtering the data to include only those records where the product ID is not null and ensuring the product was not purchased (by checking for the absence of a specific purchase event type).\n   - Exclude certain page identifiers from this dataset.\n   - Aggregate the data by the page identifier to get counts of abandoned carts.\n\n4. **Combine Results:**\n   - Join the three temporary datasets with the main dataset containing page information.\n   - This involves matching the page identifiers across all datasets.\n   - Select and format the relevant counts for views, adds to cart, abandoned carts, and purchases.\n\n5. **Output the Results:**\n   - Produce a final result set that includes the page identifier, page name, and the counts for each type of customer engagement: views, adds to cart, abandoned carts, and purchases.\n   - Ensure the output is clear and organized for analysis.",
        "special_function": null
    },
    {
        "instance_id": "local077",
        "db": "bank_sales_trading",
        "question": "Please review our interest data from September 2018 to August 2019. I need to know the max average composition value for each month, as well as the three-month rolling average. Ensure the output includes the date, the interest name, the max index composition for that month, the rolling average, and the top-ranking interests from the one month ago and two months ago with their names.",
        "SQL": "WITH get_top_avg_composition AS (\n    SELECT\n        t1.month_year,\n        t1.interest_id,\n        t2.interest_name,\n        ROUND((t1.composition / t1.index_value), 2) AS avg_composition,\n        RANK() OVER (\n            PARTITION BY month_year \n            ORDER BY ROUND((t1.composition / t1.index_value), 2) DESC\n        ) AS rnk\n    FROM\n        interest_metrics AS t1\n    JOIN\n        interest_map AS t2\n    ON \n        t2.id = t1.interest_id\n    ORDER BY\n        month_year, avg_composition DESC\n),\nget_moving_avg AS (\n    SELECT\n        month_year,\n        interest_name,\n        avg_composition AS max_index_composition,\n        ROUND(AVG(avg_composition) OVER ( \n            ORDER BY substr(month_year, 4, 4) || '-' || substr(month_year, 1, 2)\n            ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n        ), 2) AS \"3_month_moving_avg\"\n    FROM\n        get_top_avg_composition\n    WHERE\n        rnk = 1\n),\nget_lag_avg AS (\n    SELECT\n        *,\n        LAG(interest_name, 1) OVER (\n            ORDER BY substr(month_year, 4, 4) || '-' || substr(month_year, 1, 2)\n        ) AS interest_1_name,\n        LAG(interest_name, 2) OVER (\n            ORDER BY substr(month_year, 4, 4) || '-' || substr(month_year, 1, 2)\n        ) AS interest_2_name,\n        LAG(max_index_composition, 1) OVER (\n            ORDER BY substr(month_year, 4, 4) || '-' || substr(month_year, 1, 2)\n        ) AS interest_1_avg,\n        LAG(max_index_composition, 2) OVER (\n            ORDER BY substr(month_year, 4, 4) || '-' || substr(month_year, 1, 2)\n        ) AS interest_2_avg\n    FROM \n        get_moving_avg\n)\nSELECT\n    month_year,\n    interest_name,\n    max_index_composition,\n    \"3_month_moving_avg\",\n    interest_1_avg AS \"1_month_ago\",\n    interest_1_name AS \"1_month_ago_interest_name\",\n    interest_2_avg AS \"2_month_ago\",\n    interest_2_name AS \"2_month_ago_interest_name\"\nFROM \n    get_lag_avg\nWHERE\n    substr(month_year, 4, 4) || '-' || substr(month_year, 1, 2) BETWEEN '2018-09' AND '2019-08';",
        "external_knowledge": null,
        "plan": "1. **Calculate Average Composition and Rank:**\n   - Create a temporary dataset that calculates the average composition for each interest per month.\n   - Rank the interests within each month based on their average composition in descending order.\n\n2. **Determine Monthly Maximum and Rolling Average:**\n   - From the ranked dataset, select only the top-ranked interest for each month.\n   - Calculate a three-month rolling average of the top-ranked average compositions.\n\n3. **Add Lagged Interest Data:**\n   - Enhance the dataset by including interest names and average compositions from the previous one and two months for each month.\n\n4. **Final Selection:**\n   - Filter the enhanced dataset to include only the specified date range from September 2018 to August 2019.\n   - Select and display the month, interest name, maximum average composition for the month, three-month rolling average, and the top interests from one and two months ago with their corresponding names and average compositions.",
        "special_function": null
    },
    {
        "instance_id": "local078",
        "db": "bank_sales_trading",
        "question": "Identify the top 10 and bottom 10 interest categories based on their highest composition values across all months. For each category, display the time(MM-YYYY), interest name, and the composition value",
        "SQL": "WITH get_interest_rank AS (\n    SELECT\n        t1.month_year,\n        t2.interest_name,\n        t1.composition,\n        RANK() OVER (\n            PARTITION BY t2.interest_name\n            ORDER BY t1.composition DESC\n        ) AS interest_rank\n    FROM \n        interest_metrics AS t1\n    JOIN \n        interest_map AS t2\n    ON \n        t1.interest_id = t2.id\n    WHERE \n        t1.month_year IS NOT NULL\n),\nget_top_10 AS (\n    SELECT\n        month_year,\n        interest_name,\n        composition\n    FROM \n        get_interest_rank\n    WHERE \n        interest_rank = 1\n    ORDER BY \n        composition DESC\n    LIMIT 10\n),\nget_bottom_10 AS (\n    SELECT\n        month_year,\n        interest_name,\n        composition\n    FROM \n        get_interest_rank\n    WHERE \n        interest_rank = 1\n    ORDER BY \n        composition ASC\n    LIMIT 10\n)\nSELECT * \nFROM \n    get_top_10\nUNION\nSELECT * \nFROM \n    get_bottom_10\nORDER BY \n    composition DESC;",
        "external_knowledge": null,
        "plan": "1. **Rank Interest Categories**:\n    - Create a temporary result set to rank each interest category based on its composition value across all months.\n    - For each category, rank the composition values in descending order.\n\n2. **Filter Top Composition Values**:\n    - From the ranked result set, filter out the highest composition value for each interest category.\n    - This ensures that we are considering only the peak composition value for each category.\n\n3. **Select Top 10 Categories**:\n    - From the filtered set of highest composition values, select the top 10 categories with the highest values.\n    - Order these top 10 categories by their composition values in descending order.\n\n4. **Select Bottom 10 Categories**:\n    - Similarly, from the filtered set of highest composition values, select the bottom 10 categories with the lowest values.\n    - Order these bottom 10 categories by their composition values in ascending order.\n\n5. **Combine Results**:\n    - Combine the top 10 and bottom 10 categories into a single result set.\n    - Use a union operation to merge these two sets.\n\n6. **Final Ordering**:\n    - Order the combined result set by the composition values in descending order to provide a coherent view of the data.\n\n7. **Output**:\n    - Display the time, interest category name, and the composition value for the selected top 10 and bottom 10 categories.",
        "special_function": null
    },
    {
        "instance_id": "local081",
        "db": "northwind",
        "question": "How many customers were in each spending group in 1998, and what percentage of the total customer base does each group represent?",
        "SQL": "WITH orders_1998 AS (\n    SELECT\n        o.customerid,\n        COALESCE(SUM(od.unitprice * od.quantity), 0) AS total\n    FROM\n        orders o\n    INNER JOIN\n        order_details od ON o.orderid = od.orderid\n    WHERE\n        CAST(STRFTIME('%Y', o.orderdate) AS INTEGER) = 1998\n    GROUP BY\n        o.customerid\n), customer_groups AS (\n    SELECT\n        cgt.groupname AS `group`\n    FROM\n        orders_1998 o\n    INNER JOIN\n        customergroupthreshold cgt ON o.total BETWEEN cgt.rangebottom AND cgt.rangetop\n), groups_count AS (\n    SELECT\n        cg.`group`,\n        COUNT(cg.`group`) AS group_total\n    FROM\n        customer_groups cg\n    GROUP BY\n        cg.`group`\n)\nSELECT\n    gc.`group`,\n    gc.group_total AS total_customer,\n    ((100.0 * gc.group_total / (SELECT SUM(group_total) FROM groups_count))) AS percentage\nFROM\n    groups_count gc\nORDER BY\n    gc.group_total DESC;",
        "external_knowledge": null,
        "plan": "1. **Filter Orders by Year**:\n   - Select orders from the specified year.\n   - Calculate the total spending for each customer by summing up the product of unit price and quantity for all their orders within that year.\n   - Group the data by customer to get the total spending per customer.\n\n2. **Classify Customers into Spending Groups**:\n   - Using the total spending per customer, classify each customer into a predefined spending group based on their total spending amount.\n   - This classification is based on predefined thresholds for each group.\n\n3. **Count Customers per Spending Group**:\n   - Count the number of customers in each spending group.\n   - Group the data by spending group to get the total number of customers in each group.\n\n4. **Calculate Percentage Representation**:\n   - For each spending group, calculate the percentage of the total customer base that the group represents.\n   - This is done by dividing the number of customers in each group by the total number of customers across all groups and converting it to a percentage.\n\n5. **Output Results**:\n   - Select the spending group, the total number of customers in each group, and the percentage representation of each group.\n   - Order the results by the number of customers in each group in descending order to show the most populous groups first.",
        "special_function": null
    },
    {
        "instance_id": "local085",
        "db": "northwind",
        "question": "Can you tell me the ID of the top 3 employees who have the highest percentage of orders delivered late, considering only those with more than 50 total orders? Also provide their respective number of late orders and the percentage. ",
        "SQL": "WITH late_orders AS (\n  SELECT\n    o.employeeid,\n    COUNT(o.orderid) AS late_order_count\n  FROM\n    orders o\n  WHERE\n    o.requireddate <= o.shippeddate\n  GROUP BY\n    o.employeeid\n),\ntot_orders AS (\n  SELECT\n    o.employeeid,\n    COUNT(o.orderid) AS total_order_count\n  FROM\n    orders o\n  GROUP BY\n    o.employeeid\n),\nemployee_performance AS (\n  SELECT\n    tot.employeeid,\n    tot.total_order_count,\n    lo.late_order_count,\n    ((lo.late_order_count * 100.0) / tot.total_order_count) AS late_order_percentage\n  FROM\n    tot_orders tot\n  LEFT JOIN\n    late_orders lo ON tot.employeeid = lo.employeeid\n)\nSELECT\n  ep.employeeid,\n  ep.late_order_count,\n  ep.late_order_percentage\nFROM\n  employee_performance ep\nWHERE\n  ep.total_order_count > 50\nORDER BY\n  ep.late_order_percentage DESC\nLIMIT 3;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local096",
        "db": "Db-IMDB",
        "question": "I'm interested in knowing the proportion of films that had exclusively female actors for each year. Show the proportion of female-actor-only films and the total number of all films for each of those years.",
        "SQL": "WITH\n    MOVIES_WITH_NON_FEMALES AS (\n        SELECT DISTINCT\n            TRIM(MC.MID) AS MID\n        FROM\n            M_Cast MC\n            JOIN Person P ON TRIM(MC.PID) = TRIM(P.PID)\n        WHERE\n            TRIM(P.Gender) IN ('Male', 'None') -- Considering None as not female.\n    ),\n    NUM_OF_MOV_WITH_ONLY_F_BY_YR AS (\n        SELECT\n            CAST(SUBSTR(M.year, -4) AS UNSIGNED) AS YEAR,\n            COUNT(DISTINCT TRIM(MID)) AS NUM_OF_MOV_WITH_ONLY_FEMALES\n        FROM\n            Movie M\n        WHERE\n            TRIM(MID) NOT IN (SELECT MID FROM MOVIES_WITH_NON_FEMALES)\n        GROUP BY \n            CAST(SUBSTR(M.year, -4) AS UNSIGNED)\n    ),\n    TOTAL_NUM_OF_MOV_BY_YR AS (\n        SELECT\n            CAST(SUBSTR(M.year, -4) AS UNSIGNED) AS YEAR,\n            COUNT(DISTINCT TRIM(MID)) AS TOTAL_NUM_OF_MOV\n        FROM\n            Movie M\n        GROUP BY\n            CAST(SUBSTR(M.year, -4) AS UNSIGNED)\n    )\nSELECT\n    TOT_MOV.YEAR,\n    TOT_MOV.TOTAL_NUM_OF_MOV,\n    ((IFNULL(MOV_F.NUM_OF_MOV_WITH_ONLY_FEMALES, 0) * 100.0) / TOT_MOV.TOTAL_NUM_OF_MOV) AS PERCENT_OF_MOV_WITH_ONLY_F\nFROM\n    TOTAL_NUM_OF_MOV_BY_YR TOT_MOV\n    LEFT OUTER JOIN NUM_OF_MOV_WITH_ONLY_F_BY_YR MOV_F\n    ON TRIM(TOT_MOV.YEAR) = TRIM(MOV_F.YEAR)\nORDER BY\n    PERCENT_OF_MOV_WITH_ONLY_F DESC;",
        "external_knowledge": null,
        "plan": "1. **Identify Movies with Non-Female Actors:**\n   - Create a list of unique movie IDs that have at least one actor who is either male or of unspecified gender.\n   - This is achieved by joining the cast and actor information tables and filtering based on gender.\n\n2. **Count Movies with Only Female Actors by Year:**\n   - Select movie IDs that are not in the list of movies identified in step 1 (i.e., movies without any non-female actors).\n   - Extract the year from the movie's release date and count the distinct movie IDs for each year.\n   - This gives the number of movies with only female actors for each year.\n\n3. **Total Movie Count by Year:**\n   - Extract the year from the release date for all movies.\n   - Count the total number of distinct movie IDs for each year.\n   - This provides the total number of movies released each year.\n\n4. **Calculate Proportion of Movies with Only Female Actors:**\n   - Join the results from steps 2 and 3 on the year.\n   - For each year, calculate the proportion of movies with only female actors by dividing the count from step 2 by the total count from step 3.\n   - Format the proportion as a percentage.\n\n5. **Output the Results:**\n   - Select the year, total movie count, and the calculated percentage of movies with only female actors.\n   - Order the results by the percentage in descending order to highlight the years with the highest proportion of such movies.",
        "special_function": null
    },
    {
        "instance_id": "local097",
        "db": "Db-IMDB",
        "question": "Could you analyze our data and identify which any consecutive ten-year period had the largest number of films? Only output the start year and the total count for that specific period.",
        "SQL": "WITH DISTINCT_YEARS AS\n(\nSELECT\n    DISTINCT\n    CAST(SUBSTR(year,-4) AS UNSIGNED) YEAR,\n    CAST(SUBSTR(year,-4) AS UNSIGNED) START_OF_DECADE,\n    CAST(SUBSTR(year,-4) AS UNSIGNED)+9 END_OF_DECADE,\n    SUBSTR(year,-4)   DECADE_OF\nFROM\n    Movie\n),\nNUMBER_OF_MOV_BY_YR AS\n(\nSELECT\nCOUNT(DISTINCT MID) NUM_OF_MOV,\nCAST(SUBSTR(year,-4) AS UNSIGNED) YEAR\nFROM\n\tMovie\nGROUP BY\n\tCAST(SUBSTR(year,-4) AS UNSIGNED)\n),\nNUM_OF_MOV_IN_DECADE AS \n(\nSELECT\n\tSUM(NUM_OF_MOV) TOTAL_MOVIES,\n\tDY.DECADE_OF\nFROM\n\tNUMBER_OF_MOV_BY_YR NM,\n\tDISTINCT_YEARS DY\nWHERE\n\tNM.YEAR BETWEEN DY.START_OF_DECADE AND DY.END_OF_DECADE\nGROUP BY\n\tDY.DECADE_OF\n)\nSELECT\n\tDECADE_OF,\n\tTOTAL_MOVIES\nFROM\n\tNUM_OF_MOV_IN_DECADE\nWHERE\n\tTOTAL_MOVIES = (\n\t\tSELECT\n\t\t\tMAX(TOTAL_MOVIES)\n\t\tFROM\n\t\t\tNUM_OF_MOV_IN_DECADE\n\t\t)",
        "external_knowledge": null,
        "plan": "1. Create a table containing each distinct year and its corresponding decade (start year and end year).\n2. Calculate the number of movies for each year.\n3. Calculate the total number of movies within each decade.\n4. Identify the decade with the highest number of movies and output the decade and its total movie count.",
        "special_function": null
    },
    {
        "instance_id": "local098",
        "db": "Db-IMDB",
        "question": "I'd like to know how many actors have managed to avoid long breaks in their careers. Could you check our records to see how many actors haven't been out of work for more than three years at any point?",
        "SQL": "WITH\n    NUM_OF_MOV_FOR_AN_ACTR_BY_YR AS (\n        SELECT\n            TRIM(MC.PID) AS PID,\n            CAST(SUBSTR(year, -4) AS UNSIGNED) AS YEAR,\n            COUNT(DISTINCT TRIM(M.MID)) AS NUM_OF_MOV\n        FROM\n            M_Cast MC\n            JOIN Movie M ON TRIM(MC.MID) = TRIM(M.MID)\n        GROUP BY\n            TRIM(MC.PID),\n            CAST(SUBSTR(year, -4) AS UNSIGNED)\n        ORDER BY\n            NUM_OF_MOV DESC\n    ),\n    ACTRS_FOR_MORE_THAN_ONE_YR AS (\n        SELECT\n            PID,\n            COUNT(YEAR) AS NUM_OF_YEARS,\n            MIN(YEAR) AS MIN_YEAR,\n            MAX(YEAR) AS MAX_YEAR\n        FROM\n            NUM_OF_MOV_FOR_AN_ACTR_BY_YR\n        GROUP BY\n            PID\n    ),\n    NUM_OF_FOR_ACTR_W_MRE_THN_1_YR AS (\n        SELECT\n            NM.PID,\n            NM.YEAR,\n            NM.YEAR + 4 AS YEAR_PLUS_4,\n            NM.NUM_OF_MOV,\n            AY.MIN_YEAR,\n            AY.MAX_YEAR\n        FROM\n            NUM_OF_MOV_FOR_AN_ACTR_BY_YR NM\n            JOIN ACTRS_FOR_MORE_THAN_ONE_YR AY ON NM.PID = AY.PID\n    ),\n    NUM_OF_MOV_TILL_DATE_BY_ACTOR AS (\n        SELECT\n            NA.PID,\n            NY.YEAR,\n            SUM(NA.NUM_OF_MOV) AS NUM_OF_MOV_TILL_THAT_YEAR\n        FROM\n            NUM_OF_FOR_ACTR_W_MRE_THN_1_YR NA\n            JOIN NUM_OF_FOR_ACTR_W_MRE_THN_1_YR NY ON NA.PID = NY.PID\n        WHERE\n            NA.YEAR BETWEEN NY.MIN_YEAR AND NY.YEAR\n        GROUP BY\n            NA.PID,\n            NY.YEAR\n    ),\n    NUM_OF_MV_BY_ACTR_BY_YR_PLS_4 AS (\n        SELECT\n            NA.PID,\n            NY.YEAR,\n            SUM(NA.NUM_OF_MOV) AS NUM_OF_MOV_TILL_AS_OF_YR_PLS_4\n        FROM\n            NUM_OF_FOR_ACTR_W_MRE_THN_1_YR NA\n            JOIN NUM_OF_FOR_ACTR_W_MRE_THN_1_YR NY ON NA.PID = NY.PID\n        WHERE\n            NA.YEAR BETWEEN NY.MIN_YEAR AND NY.YEAR_PLUS_4\n            AND NY.YEAR_PLUS_4 <= NY.MAX_YEAR\n        GROUP BY\n            NA.PID,\n            NY.YEAR\n    )\nSELECT\n    COUNT(DISTINCT TRIM(P.Name)) AS NUM_OF_ACTORS_NEVER_UNEMPLOYED_FOR_MORE_THAN_3_YRS\nFROM\n    Person P\nWHERE\n    TRIM(P.PID) NOT IN (\n        SELECT DISTINCT\n            NMT.PID\n        FROM\n            NUM_OF_MOV_TILL_DATE_BY_ACTOR NMT\n            JOIN NUM_OF_MV_BY_ACTR_BY_YR_PLS_4 NMP ON NMT.PID = NMP.PID AND NMT.YEAR = NMP.YEAR\n        WHERE\n            NMT.NUM_OF_MOV_TILL_THAT_YEAR = NMP.NUM_OF_MOV_TILL_AS_OF_YR_PLS_4\n    );",
        "external_knowledge": null,
        "plan": "1. **Calculate Annual Movie Count**:\n   - Extract the year and count the number of distinct movies an actor has appeared in for each year.\n\n2. **Identify Multi-Year Actors**:\n   - Determine which actors have worked for more than one year by counting the distinct years and finding the earliest and latest year they worked.\n\n3. **Prepare Yearly Range Data**:\n   - For each actor, calculate a range of years from each year to three years ahead. This helps in checking employment continuity within this range.\n\n4. **Summarize Movie Count Till Each Year**:\n   - For each actor and each year, calculate the cumulative number of movies up to that year. This helps track the actor's work history year by year.\n\n5. **Summarize Movie Count Till Year Plus Three**:\n   - For each actor and each year, calculate the cumulative number of movies up to three years after that year. This helps in checking if the actor continued to work without a break in the subsequent years.\n\n6. **Identify Actors with Breaks**:\n   - Identify actors who have the same cumulative number of movies in a given year and three years later, indicating a break in their career.\n\n7. **Count Actors Without Long Breaks**:\n   - Count the number of actors who do not appear in the list of actors with breaks. This gives the total number of actors who have not had breaks longer than three years.\n\n8. **Final Result**:\n   - Return the count of actors who have never been unemployed for more than three years at any point in their careers.",
        "special_function": null
    },
    {
        "instance_id": "local099",
        "db": "Db-IMDB",
        "question": "I need you to look into the actor collaborations and tell me how many actors have made more films with Yash Chopra than with any other director. This will help us understand his influence on the industry better.",
        "SQL": "WITH YASH_CHOPRAS_PID AS (\n    SELECT\n        TRIM(P.PID) AS PID\n    FROM\n        Person P\n    WHERE\n        TRIM(P.Name) = 'Yash Chopra'\n),\nNUM_OF_MOV_BY_ACTOR_DIRECTOR AS (\n    SELECT\n        TRIM(MC.PID) AS ACTOR_PID,\n        TRIM(MD.PID) AS DIRECTOR_PID,\n        COUNT(DISTINCT TRIM(MD.MID)) AS NUM_OF_MOV\n    FROM\n        M_Cast MC\n    JOIN\n        M_Director MD ON TRIM(MC.MID) = TRIM(MD.MID)\n    GROUP BY\n        ACTOR_PID,\n        DIRECTOR_PID\n),\nNUM_OF_MOVIES_BY_YC AS (\n    SELECT\n        NM.ACTOR_PID,\n        NM.DIRECTOR_PID,\n        NM.NUM_OF_MOV AS NUM_OF_MOV_BY_YC\n    FROM\n        NUM_OF_MOV_BY_ACTOR_DIRECTOR NM\n    JOIN\n        YASH_CHOPRAS_PID YCP ON NM.DIRECTOR_PID = YCP.PID\n),\nMAX_MOV_BY_OTHER_DIRECTORS AS (\n    SELECT\n        ACTOR_PID,\n        MAX(NUM_OF_MOV) AS MAX_NUM_OF_MOV\n    FROM\n        NUM_OF_MOV_BY_ACTOR_DIRECTOR NM\n    JOIN\n        YASH_CHOPRAS_PID YCP ON NM.DIRECTOR_PID <> YCP.PID\n    GROUP BY\n        ACTOR_PID\n),\nACTORS_MOV_COMPARISION AS (\n    SELECT\n        NMY.ACTOR_PID,\n        CASE WHEN NMY.NUM_OF_MOV_BY_YC > IFNULL(NMO.MAX_NUM_OF_MOV, 0) THEN 'Y' ELSE 'N' END AS MORE_MOV_BY_YC\n    FROM\n        NUM_OF_MOVIES_BY_YC NMY\n    LEFT OUTER JOIN\n        MAX_MOV_BY_OTHER_DIRECTORS NMO ON NMY.ACTOR_PID = NMO.ACTOR_PID\n)\nSELECT\n    COUNT(DISTINCT TRIM(P.PID)) AS \"Number of actor\"\nFROM\n    Person P\nWHERE\n    TRIM(P.PID) IN (\n        SELECT\n            DISTINCT ACTOR_PID\n        FROM\n            ACTORS_MOV_COMPARISION\n        WHERE\n            MORE_MOV_BY_YC = 'Y'\n    );",
        "external_knowledge": null,
        "plan": "1. **Identify the Director**: Extract the unique identifier for the director named 'Yash Chopra'.\n\n2. **Count Collaborations**: For each actor and director pair, count the number of distinct movies they have collaborated on.\n\n3. **Filter Yash Chopra Collaborations**: From the previous results, isolate the records where the director is 'Yash Chopra' and store the number of movies each actor has done with him.\n\n4. **Max Collaborations with Other Directors**: For each actor, find the maximum number of movies they have done with any director other than 'Yash Chopra'.\n\n5. **Comparison**: Compare the number of movies each actor has done with 'Yash Chopra' to the maximum number of movies they have done with any other director. Mark actors who have more movies with 'Yash Chopra'.\n\n6. **Final Count**: Count the number of actors who have collaborated more frequently with 'Yash Chopra' than with any other director.",
        "special_function": null
    },
    {
        "instance_id": "local100",
        "db": "Db-IMDB",
        "question": "Find out how many actors have a 'Shahrukh number' of 2? This means they acted in a film with someone who acted with Shahrukh Khan, but not directly with him.",
        "SQL": "WITH \n    SHAHRUKH_0 AS (\n        SELECT \n            TRIM(P.PID) AS PID\n        FROM \n            Person P\n        WHERE \n            TRIM(P.Name) LIKE '%Shahrukh%'\n    ),\n    SHAHRUKH_1_MOVIES AS (\n        SELECT \n            DISTINCT TRIM(MC.MID) AS MID, \n            S0.PID\n        FROM \n            M_Cast MC\n        JOIN \n            SHAHRUKH_0 S0 ON TRIM(MC.PID) = S0.PID\n    ),\n    SHAHRUKH_1_ACTORS AS (\n        SELECT \n            DISTINCT TRIM(MC.PID) AS PID\n        FROM \n            M_Cast MC\n        JOIN \n            SHAHRUKH_1_MOVIES S1M ON TRIM(MC.MID) = S1M.MID\n        WHERE \n            TRIM(MC.PID) <> S1M.PID\n    ),\n    SHAHRUKH_2_MOVIES AS (\n        SELECT \n            DISTINCT TRIM(MC.MID) AS MID, \n            S1A.PID\n        FROM \n            M_Cast MC\n        JOIN \n            SHAHRUKH_1_ACTORS S1A ON TRIM(MC.PID) = S1A.PID\n    )\nSELECT \n    COUNT(DISTINCT TRIM(MC.PID)) AS 'Number of actor'\nFROM \n    Person P\nJOIN \n    M_Cast MC ON TRIM(MC.PID) = TRIM(P.PID)\nJOIN \n    SHAHRUKH_2_MOVIES S2M ON TRIM(MC.MID) = S2M.MID\nWHERE \n    TRIM(MC.PID) <> S2M.PID;",
        "external_knowledge": null,
        "plan": "1. **Identify the Target Actor**:\n   - Create a list of IDs for actors whose name includes 'Shahrukh'.\n\n2. **Find First Degree Connections**:\n   - Identify movies featuring the target actor.\n   - List all unique movies along with the target actor's ID.\n\n3. **Find Co-Actors in the Same Movies**:\n   - Determine actors who acted in the same movies as the target actor.\n   - Exclude the target actor from this list.\n   - Generate a unique list of these co-actors.\n\n4. **Find Second Degree Connections**:\n   - Identify movies featuring these first-degree co-actors.\n   - List all unique movies along with these co-actors' IDs.\n\n5. **Count Second Degree Actors**:\n   - Join the main actor list with the second-degree movie list to find actors who acted in these movies.\n   - Ensure these actors are not the same as the first-degree co-actors.\n   - Count the distinct number of these second-degree actors.\n\n6. **Final Result**:\n   - Output the count of distinct actors who have a 'Shahrukh number' of 2, i.e., acted with someone who acted with the target actor but not directly with the target actor.",
        "special_function": null
    },
    {
        "instance_id": "local114",
        "db": "education_business",
        "question": "Provide a detailed web sales report for each region, including the number of orders, total sales amount, and the name and sales amount of the top-selling sales representative in each region",
        "SQL": "WITH region_sales AS (\n    SELECT \n        r.name AS region,\n        s.name AS rep,\n        SUM(o.total_amt_usd) AS total_usd\n    FROM \n        web_orders o\n    JOIN \n        web_accounts a ON o.account_id = a.id\n    JOIN \n        web_sales_reps s ON a.sales_rep_id = s.id\n    JOIN \n        web_region r ON s.region_id = r.id\n    GROUP BY \n        r.name, s.name\n),\nmax_rep_sales AS (\n    SELECT \n        region,\n        MAX(total_usd) AS max_sales\n    FROM \n        region_sales\n    GROUP BY \n        region\n),\nregion_totals AS (\n    SELECT \n        region,\n        COUNT(*) AS total_orders,\n        SUM(total_usd) AS total_sales\n    FROM \n        region_sales\n    GROUP BY \n        region\n)\nSELECT \n    rt.region,\n    rt.total_orders,\n    rt.total_sales,\n    rs.rep AS top_selling_rep,\n    ms.max_sales\nFROM \n    region_totals rt\nJOIN \n    max_rep_sales ms ON rt.region = ms.region\nJOIN \n    region_sales rs ON rs.region = ms.region AND rs.total_usd = ms.max_sales;",
        "external_knowledge": null,
        "plan": "1. **Identify Region with Maximum Sales:**\n   - Create a subquery to calculate the total sales amount for each region by summing up the order amounts.\n   - Group the results by region to get the total sales per region.\n   - From this grouped result, select the region with the highest total sales amount.\n\n2. **Store Region with Maximum Sales:**\n   - Use a Common Table Expression (CTE) to store the region that has the maximum total sales.\n\n3. **Count Orders for the Identified Region:**\n   - In the main query, join the necessary tables to link orders to regions.\n   - Use a WHERE clause to filter the orders based on the region identified in the CTE.\n   - Count the total number of orders for this specific region.\n\n4. **Return the Result:**\n   - Output the count of orders as the final result.",
        "special_function": null
    },
    {
        "instance_id": "local128",
        "db": "BowlingLeague",
        "question": "List the bowlers, match number, game number, handicap score, tournament date, and location for only those bowlers who won their game with a handicap score of 190 or less at Thunderbird Lanes, Totem Lanes, and Bolero Lanes.",
        "SQL": "WITH QualifiedBowlers AS (\n    SELECT \n        BowlerID\n    FROM \n        Bowler_Scores bs\n    JOIN \n        Tourney_Matches tm ON bs.MatchID = tm.MatchID\n    JOIN \n        Tournaments t ON tm.TourneyID = t.TourneyID\n    WHERE \n        bs.HandiCapScore <= 190\n        AND bs.WonGame = 1\n        AND t.TourneyLocation IN ('Thunderbird Lanes', 'Totem Lanes', 'Bolero Lanes')\n    GROUP BY \n        BowlerID\n    HAVING \n        COUNT(DISTINCT t.TourneyLocation) = 3\n)\nSELECT \n    Bowlers.BowlerID, \n    Bowlers.BowlerFirstName, \n    Bowlers.BowlerLastName, \n    Bowler_Scores.MatchID,\n    Bowler_Scores.GameNumber,\n    Bowler_Scores.HandiCapScore,\n    Tournaments.TourneyDate,\n    Tournaments.TourneyLocation\nFROM \n    Bowlers\nJOIN \n    Bowler_Scores ON Bowlers.BowlerID = Bowler_Scores.BowlerID\nJOIN \n    Tourney_Matches ON Bowler_Scores.MatchID = Tourney_Matches.MatchID\nJOIN \n    Tournaments ON Tournaments.TourneyID = Tourney_Matches.TourneyID\nJOIN \n    QualifiedBowlers qb ON Bowlers.BowlerID = qb.BowlerID\nWHERE \n    Bowler_Scores.HandiCapScore <= 190\n    AND Bowler_Scores.WonGame = 1\n    AND Tournaments.TourneyLocation IN ('Thunderbird Lanes', 'Totem Lanes', 'Bolero Lanes');",
        "external_knowledge": null,
        "plan": "1. **Identify Qualified Bowlers**:\n   - Create a temporary result set (CTE) to identify bowlers who meet the initial conditions.\n   - Filter records where the bowler\u2019s handicap score is 190 or less, and they won the game.\n   - Further filter these records to include only those where the tournament location is one of the specified locations.\n   - Group by bowler ID to ensure each bowler is considered only once.\n   - Use a HAVING clause to ensure the bowler has participated in tournaments at all three specified locations.\n\n2. **Retrieve Detailed Information**:\n   - Select the required details including bowler's ID, first name, last name, match number, game number, handicap score, tournament date, and location.\n   - Join the main tables to connect bowlers to their scores, match details, and tournament information.\n   - Join with the previously identified qualified bowlers to filter the final results.\n   - Apply the same filters again on the final query to ensure only the relevant records are selected, specifically those with a handicap score of 190 or less, who won the game, and the tournament location is one of the specified locations.",
        "special_function": null
    },
    {
        "instance_id": "local130",
        "db": "school_scheduling",
        "question": "Could you provide a list of last names for all students who completed English courses, including their quintile ranks based on their grades, and sorted from the highest to the lowest grade quintile?",
        "SQL": "SELECT \n  S1.StudLastName AS LastName,\n  (CASE \n      WHEN RankInCategory <= 0.2 * NumStudents THEN 'First'\n      WHEN RankInCategory <= 0.4 * NumStudents THEN 'Second'\n      WHEN RankInCategory <= 0.6 * NumStudents THEN 'Third'\n      WHEN RankInCategory <= 0.8 * NumStudents THEN 'Fourth'\n      ELSE 'Fifth' \n   END) AS Quintile\nFROM\n  (SELECT \n     Students.StudLastName,\n     (SELECT COUNT(*)\n      FROM Classes\n      INNER JOIN Student_Schedules AS SS2 ON Classes.ClassID = SS2.ClassID\n      INNER JOIN Subjects AS S3 ON S3.SubjectID = Classes.SubjectID\n      WHERE S3.CategoryID = 'ENG'\n        AND SS2.Grade >= Student_Schedules.Grade) AS RankInCategory\n   FROM \n     Subjects\n   INNER JOIN Classes ON Subjects.SubjectID = Classes.SubjectID\n   INNER JOIN Student_Schedules ON Student_Schedules.ClassID = Classes.ClassID\n   INNER JOIN Students ON Students.StudentID = Student_Schedules.StudentID\n   WHERE \n     Student_Schedules.ClassStatus = 2 \n     AND Subjects.CategoryID = 'ENG'\n  ) AS S1,\n  (SELECT COUNT(*) AS NumStudents\n   FROM Classes AS C2\n   INNER JOIN Student_Schedules AS SS3 ON C2.ClassID = SS3.ClassID\n   INNER JOIN Subjects AS S2 ON S2.SubjectID = C2.SubjectID\n   WHERE SS3.ClassStatus = 2 \n     AND S2.CategoryID = 'ENG'\n  ) AS StudCount\nORDER BY \n  S1.RankInCategory DESC;",
        "external_knowledge": null,
        "plan": "1. **Identify Completed Courses**: Filter out records of students who have completed courses in a specific subject category.\n\n2. **Determine Ranks**: For each student who completed courses in the specified category, calculate their rank by comparing their grade with the grades of other students in the same category. This rank represents the number of students who have grades equal to or higher than the student.\n\n3. **Calculate Total Students**: Count the total number of students who have completed courses in the specified category.\n\n4. **Assign Quintiles**: Based on the rank calculated in step 2 and the total number of students from step 3, assign a quintile to each student. The quintiles are determined by dividing the student population into five equal parts. For example:\n   - The top 20% of students are in the 'First' quintile.\n   - The next 20% are in the 'Second' quintile, and so on.\n\n5. **Select and Order Results**: Select the last name of each student and their corresponding quintile. Order the final results based on the rank in descending order, so that students with the highest grades appear first.",
        "special_function": null
    },
    {
        "instance_id": "local131",
        "db": "EntertainmentAgency",
        "question": "Could you list each musical style with the number of times it appears as a 1st, 2nd, or 3rd preference in a single row per style?",
        "SQL": "SELECT \n  Musical_Styles.StyleName,\n  COUNT(RankedPreferences.FirstStyle)\n    AS FirstPreference,\n  COUNT(RankedPreferences.SecondStyle)\n    AS SecondPreference,\n  COUNT(RankedPreferences.ThirdStyle)\n    AS ThirdPreference\nFROM Musical_Styles,\n (SELECT (CASE WHEN\n    Musical_Preferences.PreferenceSeq = 1\n               THEN Musical_Preferences.StyleID\n               ELSE Null END) As FirstStyle,\n         (CASE WHEN\n    Musical_Preferences.PreferenceSeq = 2\n               THEN Musical_Preferences.StyleID\n               ELSE Null END) As SecondStyle,\n         (CASE WHEN\n    Musical_Preferences.PreferenceSeq = 3\n               THEN Musical_Preferences.StyleID\n               ELSE Null END) AS ThirdStyle\n   FROM Musical_Preferences)  AS RankedPreferences\nWHERE Musical_Styles.StyleID =\n         RankedPreferences.FirstStyle\n  OR Musical_Styles.StyleID =\n         RankedPreferences.SecondStyle\n  OR Musical_Styles.StyleID =\n         RankedPreferences.ThirdStyle\nGROUP BY StyleID, StyleName\nHAVING COUNT(FirstStyle) > 0\n     OR     COUNT(SecondStyle) > 0\n     OR     COUNT(ThirdStyle) > 0\nORDER BY FirstPreference DESC,\n        SecondPreference DESC,\n        ThirdPreference DESC, StyleID;",
        "external_knowledge": null,
        "plan": "1. **Identify Musical Preferences**: Extract and label the style preferences based on their ranking position (first, second, third) from the preferences table.\n    - If the preference sequence is 1, label it as `FirstStyle`.\n    - If the preference sequence is 2, label it as `SecondStyle`.\n    - If the preference sequence is 3, label it as `ThirdStyle`.\n\n2. **Join Preferences with Styles**: Link the extracted and labeled preferences with the styles table to get the style names.\n    - Join on the style identifier from both the styles table and the preferences (considering each rank separately).\n\n3. **Count Preferences by Rank**: Calculate the number of times each style appears as a first, second, or third preference.\n    - For each style, count how many times it is listed as `FirstStyle`.\n    - Similarly, count how many times it is listed as `SecondStyle`.\n    - Similarly, count how many times it is listed as `ThirdStyle`.\n\n4. **Filter Out Styles with No Preferences**: Ensure that only styles which have at least one count in any of the preference ranks (first, second, or third) are included.\n    - Apply a condition to filter out styles with zero counts in all preference categories.\n\n5. **Group and Aggregate Data**: Group the results by style identifier and style name to aggregate the counts of preferences.\n    - Use the grouping to ensure that each style appears only once in the result set.\n\n6. **Order the Results**: Sort the aggregated results in descending order based on the counts of first preferences, then second preferences, and finally third preferences.\n    - If the counts are equal, ensure a consistent order by including the style identifier.\n\nThis structured approach ensures that the query identifies and counts the preferences accurately, while filtering and organizing the results to meet the initial user instruction.",
        "special_function": null
    },
    {
        "instance_id": "local133",
        "db": "EntertainmentAgency",
        "question": "In a scoring system where the first preference in musical styles receives 3 points, the second 2 points, and the third 1 point, calculate the total weighted score for each style ranked by at least one user. Determine the absolute differences between each style's weighted score and the average score across all styles.",
        "SQL": "WITH ScoreCounts AS (\n  SELECT \n    Musical_Styles.StyleName,\n    (COUNT(RankedPreferences.FirstStyle) * 3 + \n     COUNT(RankedPreferences.SecondStyle) * 2 + \n     COUNT(RankedPreferences.ThirdStyle) * 1) AS WeightedScore\n  FROM \n    Musical_Styles\n  LEFT JOIN \n    (SELECT \n       (CASE WHEN Musical_Preferences.PreferenceSeq = 1 \n           THEN Musical_Preferences.StyleID END) AS FirstStyle,\n       (CASE WHEN Musical_Preferences.PreferenceSeq = 2 \n           THEN Musical_Preferences.StyleID END) AS SecondStyle,\n       (CASE WHEN Musical_Preferences.PreferenceSeq = 3 \n           THEN Musical_Preferences.StyleID END) AS ThirdStyle\n     FROM \n       Musical_Preferences) AS RankedPreferences\n  ON \n    Musical_Styles.StyleID = RankedPreferences.FirstStyle\n    OR Musical_Styles.StyleID = RankedPreferences.SecondStyle\n    OR Musical_Styles.StyleID = RankedPreferences.ThirdStyle\n  GROUP BY \n    Musical_Styles.StyleID, Musical_Styles.StyleName\n  HAVING \n    COUNT(RankedPreferences.FirstStyle) > 0 \n    OR COUNT(RankedPreferences.SecondStyle) > 0 \n    OR COUNT(RankedPreferences.ThirdStyle) > 0\n),\nAvgScoreCTE AS (\n  SELECT \n    AVG(WeightedScore) AS AvgScoreValue\n  FROM \n    ScoreCounts\n)\nSELECT \n  SC.StyleName,\n  ABS(SC.WeightedScore - AC.AvgScoreValue) \nFROM \n  ScoreCounts SC\nCROSS JOIN \n  AvgScoreCTE AC",
        "external_knowledge": null,
        "plan": "1. **Calculate Weighted Scores**:\n    - Define a temporary result set to calculate the weighted scores for each category.\n    - Use conditional aggregation to assign weights to preferences: 3 for first, 2 for second, and 1 for third.\n    - Join the preferences data with the categories data to align preferences with their respective categories.\n    - Group by category to ensure scores are calculated for each category separately.\n    - Include categories that have at least one preference recorded.\n\n2. **Calculate Average Score**:\n    - Create another temporary result set to calculate the average of the weighted scores derived in the previous step.\n\n3. **Find Closest Score**:\n    - Join the weighted scores with the average score.\n    - Order the results by the absolute difference between each category's weighted score and the average score.\n    - In case of ties (i.e., multiple categories having the same difference from the average), order them alphabetically by category name.\n\n4. **Limit the Result**:\n    - Limit the final result to return only the category with the closest weighted score to the average, ensuring it is the first alphabetically in case of ties.",
        "special_function": null
    },
    {
        "instance_id": "local132",
        "db": "EntertainmentAgency",
        "question": "Show entertainer and customer pairs where both the first and second style preferences of customers match the first and second strengths of entertainers (or vice versa), displaying only the entertainer's stage name and the customer's last name.",
        "SQL": "WITH EntertainerStrengths AS (\n    SELECT e.EntertainerID,\n           e.EntStageName,\n           MAX(CASE WHEN es.StyleStrength = 1 THEN es.StyleID END) AS FirstStyle,\n           MAX(CASE WHEN es.StyleStrength = 2 THEN es.StyleID END) AS SecondStyle,\n           MAX(CASE WHEN es.StyleStrength = 3 THEN es.StyleID END) AS ThirdStyle\n    FROM Entertainers e\n    LEFT JOIN Entertainer_Styles es ON e.EntertainerID = es.EntertainerID\n    GROUP BY e.EntertainerID, e.EntStageName\n),\nCustomerPreferences AS (\n    SELECT c.CustomerID,\n           c.CustLastName,\n           MAX(CASE WHEN mp.PreferenceSeq = 1 THEN mp.StyleID END) AS FirstStyle,\n           MAX(CASE WHEN mp.PreferenceSeq = 2 THEN mp.StyleID END) AS SecondStyle,\n           MAX(CASE WHEN mp.PreferenceSeq = 3 THEN mp.StyleID END) AS ThirdStyle\n    FROM Customers c\n    LEFT JOIN Musical_Preferences mp ON c.CustomerID = mp.CustomerID\n    GROUP BY c.CustomerID, c.CustLastName\n)\nSELECT e.EntStageName AS StageName,\n       c.CustLastName AS LastName\nFROM EntertainerStrengths e\nJOIN CustomerPreferences c ON e.FirstStyle = c.FirstStyle AND e.SecondStyle = c.SecondStyle\nUNION\nSELECT e.EntStageName,\n       c.CustLastName \nFROM EntertainerStrengths e\nJOIN CustomerPreferences c ON e.FirstStyle = c.SecondStyle AND e.SecondStyle = c.FirstStyle\nORDER BY e.EntStageName, c.CustLastName;",
        "external_knowledge": null,
        "plan": "Display customers and their first, second and third-ranked preferences along with entertainers and their first, second and third-ranked strengths, then match customers to entertainers when the first or second preference matches the first or second strength",
        "special_function": null
    },
    {
        "instance_id": "local141",
        "db": "AdventureWorks",
        "question": "How did each salesperson's annual total sales compare to their annual sales quota? Provide the difference between their total sales and the quota for each year, organized by salesperson and year.",
        "SQL": "WITH Sales_CTE AS (\n    SELECT \n        SalesPersonID, \n        SUM(TotalDue) AS TotalSales, \n        strftime('%Y', OrderDate) AS SalesYear\n    FROM \n        SalesOrderHeader\n    WHERE \n        SalesPersonID <> ''\n    GROUP BY \n        SalesPersonID, \n        strftime('%Y', OrderDate)\n),\nSales_Quota_CTE AS (\n    SELECT \n        BusinessEntityID, \n        SUM(SalesQuota) AS SalesQuota, \n        strftime('%Y', QuotaDate) AS SalesQuotaYear\n    FROM \n        SalesPersonQuotaHistory\n    GROUP BY \n        BusinessEntityID, \n        strftime('%Y', QuotaDate)\n)\nSELECT \n    Sales_CTE.SalesPersonID,\n    Sales_CTE.SalesYear,\n    CAST(Sales_CTE.TotalSales AS TEXT) AS TotalSales,\n    Sales_Quota_CTE.SalesQuotaYear,\n    CAST(Sales_Quota_CTE.SalesQuota AS TEXT) AS SalesQuota,\n    CAST(Sales_CTE.TotalSales - Sales_Quota_CTE.SalesQuota AS TEXT) AS Amt_Above_or_Below_Quota\nFROM \n    Sales_CTE\nJOIN \n    Sales_Quota_CTE \n    ON Sales_Quota_CTE.BusinessEntityID = Sales_CTE.SalesPersonID\n    AND Sales_CTE.SalesYear = Sales_Quota_CTE.SalesQuotaYear\nORDER BY \n    Sales_CTE.SalesPersonID, \n    Sales_CTE.SalesYear;",
        "external_knowledge": null,
        "plan": "1. **Aggregate Sales Data:**\n   - Create a temporary dataset that summarizes the annual total sales for each salesperson.\n   - Filter out any records where the salesperson identifier is missing.\n   - Group the data by salesperson and year, calculating the total sales for each combination.\n\n2. **Aggregate Quota Data:**\n   - Create another temporary dataset that summarizes the annual sales quota for each salesperson.\n   - Group the data by salesperson and year, calculating the total quota for each combination.\n\n3. **Combine Aggregated Data:**\n   - Join the two temporary datasets on the salesperson identifier and year, ensuring that each salesperson's annual total sales data aligns with their corresponding annual sales quota data.\n\n4. **Calculate Difference:**\n   - For each matched record, calculate the difference between the total sales and the sales quota.\n\n5. **Format and Output:**\n   - Select relevant columns, including salesperson identifier, year, total sales, sales quota, and the calculated difference.\n   - Ensure that the results are ordered by salesperson and year for clarity.\n   - Convert numerical results to text format for consistent output representation.",
        "special_function": null
    },
    {
        "instance_id": "local152",
        "db": "imdb_movies",
        "question": "Can you provide the top 9 directors by movie count, including their ID, name, number of movies, average inter-movie duration (rounded to the nearest integer), average rating (rounded to 2 decimals), total votes, minimum and maximum ratings, and total movie duration? Sort the output first by movie count in descending order and then by total movie duration in descending order.",
        "SQL": "WITH movie_date_info AS\n(\n    SELECT d.name_id, n.name, d.movie_id,\n           m.date_published, \n           (SELECT MIN(m2.date_published)\n            FROM director_mapping d2\n            JOIN movies m2 ON d2.movie_id = m2.id\n            WHERE d2.name_id = d.name_id \n            AND m2.date_published > m.date_published) AS next_movie_date\n    FROM director_mapping d\n    JOIN names AS n ON d.name_id = n.id\n    JOIN movies AS m ON d.movie_id = m.id\n),\n\ndate_difference AS\n(\n    SELECT *, \n           JULIANDAY(next_movie_date) - JULIANDAY(date_published) AS diff\n    FROM movie_date_info\n),\n \navg_inter_days AS\n(\n    SELECT name_id, AVG(diff) AS avg_inter_movie_days\n    FROM date_difference\n    WHERE diff IS NOT NULL\n    GROUP BY name_id\n),\n\nfinal_result_table AS\n(\n    SELECT d.name_id AS director_id,\n           n.name AS director_name,\n           COUNT(d.movie_id) AS number_of_movies,\n           ROUND(a.avg_inter_movie_days) AS inter_movie_days,\n           ROUND(AVG(r.avg_rating), 2) AS avg_rating,\n           SUM(r.total_votes) AS total_votes,\n           MIN(r.avg_rating) AS min_rating,\n           MAX(r.avg_rating) AS max_rating,\n           SUM(m.duration) AS total_duration,\n           ROW_NUMBER() OVER(ORDER BY COUNT(d.movie_id) DESC) AS director_row_rank\n    FROM names AS n \n    JOIN director_mapping AS d ON n.id = d.name_id\n    JOIN ratings AS r ON d.movie_id = r.movie_id\n    JOIN movies AS m ON m.id = r.movie_id\n    JOIN avg_inter_days AS a ON a.name_id = d.name_id\n    GROUP BY director_id\n),\ntop_9 AS \n(\n\tSELECT *\n\tFROM final_result_table\n\tORDER BY director_row_rank\n\tLIMIT 9\n)\n\nSELECT *\nFROM top_9\nORDER BY number_of_movies DESC, total_duration DESC\nLIMIT 9;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local230",
        "db": "imdb_movies",
        "question": "Determine the top three genres with the most movies rated above 8, and then identify the top four directors who have directed the most films rated above 8 within those genres. List these directors and their respective movie counts.",
        "SQL": "WITH top_genre AS (\n  SELECT \n    g.genre, \n    COUNT(g.movie_id) AS movie_count\n  FROM \n    genre AS g\n  INNER JOIN \n    ratings AS r ON g.movie_id = r.movie_id\n  WHERE \n    avg_rating > 8\n  GROUP BY \n    genre\n  ORDER BY \n    movie_count DESC\n  LIMIT 3\n),\ntop_director AS (\n  SELECT \n    n.name AS director_name,\n    COUNT(g.movie_id) AS movie_count\n  FROM \n    names AS n \n  INNER JOIN \n    director_mapping AS dm ON n.id = dm.name_id \n  INNER JOIN \n    genre AS g ON dm.movie_id = g.movie_id \n  INNER JOIN \n    ratings AS r ON r.movie_id = g.movie_id,\n    top_genre\n  WHERE \n    g.genre IN (top_genre.genre) \n    AND avg_rating > 8\n  GROUP BY \n    director_name\n  ORDER BY \n    movie_count DESC, director_name ASC\n)\nSELECT \n  *\nFROM \n  top_director\nLIMIT \n  4;",
        "external_knowledge": null,
        "plan": "1. Identify the top 3 genres with the highest number of movies that have an average rating above 8.\n2. Identify the top 3 directors who have directed the most movies within the top 3 genres identified in the previous subquery, with an average rating above 8.\n3. Select the top 3 directors from the top_director subquery.",
        "special_function": null
    },
    {
        "instance_id": "local156",
        "db": "bank_sales_trading",
        "question": "Can you analyze the yearly average cost of Bitcoin purchases by region, excluding the first year's data? Rank the regions based on these averages each year and calculate the annual percentage change in cost.",
        "SQL": "WITH cte_dollar_cost_average AS (\n  SELECT\n    strftime('%Y', substr(bitcoin_transactions.txn_date, 7, 4) || '-' || substr(bitcoin_transactions.txn_date, 4, 2) || '-' || substr(bitcoin_transactions.txn_date, 1, 2)) AS year_start,\n    bitcoin_members.region,\n    SUM(bitcoin_transactions.quantity * bitcoin_prices.price) / SUM(bitcoin_transactions.quantity) AS btc_dca\n  FROM bitcoin_transactions\n  INNER JOIN bitcoin_prices\n    ON bitcoin_transactions.ticker = bitcoin_prices.ticker\n    AND bitcoin_transactions.txn_date = bitcoin_prices.market_date\n  INNER JOIN bitcoin_members\n    ON bitcoin_transactions.member_id = bitcoin_members.member_id\n  WHERE bitcoin_transactions.ticker = 'BTC'\n    AND bitcoin_transactions.txn_type = 'BUY'\n  GROUP BY year_start, bitcoin_members.region\n),\n\ncte_window_functions AS (\n  SELECT\n    year_start,\n    region,\n    btc_dca,\n    (SELECT COUNT(*) \n     FROM cte_dollar_cost_average AS sub\n     WHERE sub.year_start = cte_dollar_cost_average.year_start \n       AND sub.btc_dca <= cte_dollar_cost_average.btc_dca) AS dca_ranking,\n    (SELECT btc_dca \n     FROM cte_dollar_cost_average AS sub\n     WHERE sub.region = cte_dollar_cost_average.region \n       AND sub.year_start < cte_dollar_cost_average.year_start\n     ORDER BY sub.year_start DESC\n     LIMIT 1) AS previous_btc_dca,\n     ROW_NUMBER() OVER (PARTITION BY region ORDER BY year_start) AS rn\n  FROM cte_dollar_cost_average\n)\n\nSELECT\n  year_start,\n  region,\n  ROUND(btc_dca, 2) AS btc_dca,\n  dca_ranking,\n  ROUND(\n    100.0 * (btc_dca - previous_btc_dca) / previous_btc_dca,\n    2\n  ) AS dca_percentage_change\nFROM cte_window_functions\nWHERE rn > 1\nORDER BY region, year_start;",
        "external_knowledge": null,
        "plan": "1. **Create Initial Data Set (CTE: cte_dollar_cost_average)**:\n   - Extract the year from the transaction date.\n   - Group the transactions by year and region.\n   - Calculate the average cost for purchases by taking the weighted average of the purchase quantities and prices.\n   - This results in a dataset with columns for the year, region, and the calculated average cost.\n\n2. **Rank and Calculate Changes (CTE: cte_window_functions)**:\n   - For each year and region, determine the ranking based on the average cost.\n   - Retrieve the average cost from the previous year for each region.\n   - Assign a row number to each region-year pair to facilitate filtering out the first year, which lacks a previous year for comparison.\n\n3. **Final Selection and Calculations**:\n   - Select records from the window functions CTE, excluding the first year for each region.\n   - Exclude the year 2017 from the results.\n   - For each record, include the year, region, average cost, ranking, and the annual percentage change in average cost.\n   - Round the average cost and percentage change to two decimal places for readability.\n   - Sort the results by region and year to provide a clear and ordered output.",
        "special_function": null
    },
    {
        "instance_id": "local157",
        "db": "bank_sales_trading",
        "question": "For our upcoming meeting, please provide the daily percentage change in trading volume for all tickers from August 1 to August 10, 2021. Use the Bitcoin transaction data to calculate.",
        "SQL": "WITH cte_adjusted_prices AS (\n  SELECT\n    ticker,\n    market_date,\n    CASE\n      WHEN substr(volume, -1) = 'K' THEN cast(substr(volume, 1, length(volume) - 1) AS REAL) * 1000\n      WHEN substr(volume, -1) = 'M' THEN cast(substr(volume, 1, length(volume) - 1) AS REAL) * 1000000\n      WHEN volume = '-' THEN 0\n      ELSE cast(volume AS REAL)\n    END AS volume\n  FROM bitcoin_prices\n),\ncte_previous_volume AS (\n  SELECT\n    ticker,\n    market_date,\n    volume,\n    LAG(volume) OVER (\n      PARTITION BY ticker\n      ORDER BY strftime('%Y-%m-%d', substr(market_date, 7, 4) || '-' || substr(market_date, 4, 2) || '-' || substr(market_date, 1, 2))\n    ) AS previous_volume\n  FROM cte_adjusted_prices\n  WHERE volume != 0\n)\nSELECT\n  ticker,\n  market_date,\n  volume,\n  previous_volume,\n  ROUND(\n    100.0 * (volume - previous_volume) / previous_volume,\n    2\n  ) AS daily_change\nFROM cte_previous_volume\nWHERE strftime('%Y-%m-%d', substr(market_date, 7, 4) || '-' || substr(market_date, 4, 2) || '-' || substr(market_date, 1, 2))\n  BETWEEN '2021-08-01' AND '2021-08-10'\nORDER BY ticker, market_date;",
        "external_knowledge": null,
        "plan": "1. **Data Transformation for Volume:**\n   - Create a temporary dataset where the trading volumes are standardized.\n   - If the volume ends with 'K', multiply the numerical part by 1000.\n   - If the volume ends with 'M', multiply the numerical part by 1,000,000.\n   - If the volume is '-', set it to 0.\n   - Otherwise, convert the volume to a numeric value as it is.\n\n2. **Calculate Previous Volume:**\n   - Create another temporary dataset to calculate the previous day's volume for each ticker.\n   - Partition the data by ticker and order by the date to align each volume with the previous day\u2019s volume using the LAG function.\n   - Filter out records where the volume is 0 to avoid division by zero in later calculations.\n\n3. **Calculate Daily Percentage Change:**\n   - Select the relevant data from the temporary dataset, including the ticker, date, current volume, and previous volume.\n   - Calculate the daily percentage change in volume using the formula: \\((\\text{current volume} - \\text{previous volume}) / \\text{previous volume} \\times 100\\).\n   - Round the percentage change to two decimal places.\n\n4. **Filter and Order Results:**\n   - Filter the results to include only the dates between August 1 and August 10, 2021.\n   - Order the final output by ticker and date to present a clear trend analysis.\n\nBy following these steps, the query provides the daily percentage change in trading volume for each ticker within the specified date range, supporting trend analysis for strategic planning.",
        "special_function": null
    },
    {
        "instance_id": "local163",
        "db": "education_business",
        "question": "Which university faculty members' salaries are closest to the average salary for their respective ranks? Please provide the ranks, first names, last names, and salaries.university",
        "SQL": "WITH AvgSalaries AS (\n    SELECT \n        facrank AS FacRank,\n        AVG(facsalary) AS AvSalary\n    FROM \n        university_faculty\n    GROUP BY \n        facrank\n),\nSalaryDifferences AS (\n    SELECT \n        university_faculty.facrank AS FacRank, \n        university_faculty.facfirstname AS FacFirstName, \n        university_faculty.faclastname AS FacLastName, \n        university_faculty.facsalary AS Salary, \n        ABS(university_faculty.facsalary - AvgSalaries.AvSalary) AS Diff\n    FROM \n        university_faculty\n    JOIN \n        AvgSalaries ON university_faculty.facrank = AvgSalaries.FacRank\n),\nMinDifferences AS (\n    SELECT \n        FacRank, \n        MIN(Diff) AS MinDiff\n    FROM \n        SalaryDifferences\n    GROUP BY \n        FacRank\n)\nSELECT \n    s.FacRank, \n    s.FacFirstName, \n    s.FacLastName, \n    s.Salary\nFROM \n    SalaryDifferences s\nJOIN \n    MinDifferences m ON s.FacRank = m.FacRank AND s.Diff = m.MinDiff;",
        "external_knowledge": null,
        "plan": "1. **Calculate Average Salaries by Rank**:\n   - Create a temporary table to compute the average salary for each rank.\n   - Group the data by rank and calculate the average salary for each group.\n\n2. **Determine Salary Differences**:\n   - Create another temporary table to list each faculty member along with their rank, first name, last name, salary, and the absolute difference between their salary and the average salary for their rank.\n   - Join the original data with the previously created temporary table to associate each faculty member with the average salary for their rank.\n\n3. **Find Minimum Differences**:\n   - Create a temporary table to determine the minimum salary difference for each rank.\n   - Group the data by rank and find the minimum difference in salary from the previously calculated differences.\n\n4. **Retrieve Closest Salaries**:\n   - Join the temporary table containing faculty members and their salary differences with the temporary table containing minimum differences.\n   - Select and display the rank, first name, last name, and salary for faculty members whose salary difference matches the minimum difference for their respective ranks.\n\nThis step-by-step plan ensures that the faculty members with salaries closest to the average salary for their ranks are identified and their relevant details are retrieved.",
        "special_function": null
    },
    {
        "instance_id": "local168",
        "db": "city_legislation",
        "question": "What is the average salary for remote Data Analyst jobs requiring the top three most in-demand skills?",
        "SQL": "WITH skills_demand AS (\n    SELECT\n        skills_dim.skill_id,\n        COUNT(skills_job_dim.job_id) AS demand_count\n    FROM job_postings_fact\n    INNER JOIN skills_job_dim ON job_postings_fact.job_id = skills_job_dim.job_id\n    INNER JOIN skills_dim ON skills_job_dim.skill_id = skills_dim.skill_id\n    WHERE\n        (job_title_short = 'Data Analyst' or job_title =  'Data Analyst')\n        AND salary_year_avg IS NOT NULL\n        AND job_work_from_home = True \n    GROUP BY\n        skills_dim.skill_id\n    ORDER BY demand_count DESC\n    LIMIT 3\n),\naverage_salary AS (\n    SELECT \n        skills_job_dim.skill_id,\n        AVG(job_postings_fact.salary_year_avg) AS avg_salary\n    FROM job_postings_fact\n    INNER JOIN skills_job_dim ON job_postings_fact.job_id = skills_job_dim.job_id\n    INNER JOIN skills_dim ON skills_job_dim.skill_id = skills_dim.skill_id\n    WHERE\n        (job_title_short = 'Data Analyst' or job_title =  'Data Analyst')\n        AND salary_year_avg IS NOT NULL\n        AND job_work_from_home = True \n    GROUP BY\n        skills_job_dim.skill_id\n),\ntop_skills_with_salary AS (\n    SELECT\n        average_salary.avg_salary\n    FROM\n        skills_demand\n    INNER JOIN average_salary ON skills_demand.skill_id = average_salary.skill_id\n)\nSELECT\n    AVG(avg_salary) AS avg_salary\nFROM\n    top_skills_with_salary;",
        "external_knowledge": null,
        "plan": "1. **Identify Top Skills by Demand:**\n   - Create a temporary dataset to count how many job postings require each skill.\n   - Filter the dataset to include only remote job postings for the specified job title that have a salary listed.\n   - Group the data by skill and count the number of job postings for each skill.\n   - Order the skills by the count in descending order and select the top three most in-demand skills.\n\n2. **Calculate Average Salary for Each Skill:**\n   - Create another temporary dataset to calculate the average salary for job postings that require each skill.\n   - Filter the dataset similarly to include only remote job postings for the specified job title with a listed salary.\n   - Group the data by skill and calculate the average salary for job postings that require each skill.\n\n3. **Combine Top Skills with Their Salaries:**\n   - Join the dataset of top three most in-demand skills with the dataset of average salaries by skill.\n   - Select the average salary values for these top skills.\n\n4. **Calculate Overall Average Salary:**\n   - Calculate the average of the average salaries obtained for the top three most in-demand skills.\n   - Return this value as the final result.",
        "special_function": null
    },
    {
        "instance_id": "local169",
        "db": "city_legislation",
        "question": "What is the annual retention rate for Colorado legislators who started their first term between 1917 and 1999, tracked up to 20 years later?",
        "SQL": "WITH RECURSIVE generate_series AS (\n    SELECT 1 AS period\n    UNION ALL\n    SELECT period + 1\n    FROM generate_series\n    WHERE period < 20\n),\nlegislators_first_term AS (\n    SELECT \n        id_bioguide,\n        MIN(term_start) AS first_term\n    FROM \n        legislators_terms\n    GROUP BY \n        id_bioguide\n),\ncohort AS (\n    SELECT \n        COUNT(DISTINCT a.id_bioguide) AS cohort_size\n    FROM \n        legislators_first_term a\n    WHERE \n        a.first_term BETWEEN '1917-01-01' AND '1999-12-31'\n),\nretained_cohort AS (\n    SELECT \n        (strftime('%Y', f.date) - strftime('%Y', d.first_term)) AS period,\n        COUNT(DISTINCT d.id_bioguide) AS cohort_retained\n    FROM \n        legislators_first_term d\n    JOIN \n        legislators_terms e ON d.id_bioguide = e.id_bioguide\n    LEFT JOIN \n        legislation_date_dim f ON f.date BETWEEN e.term_start AND e.term_end\n                         AND strftime('%m', f.date) = '12'\n                         AND strftime('%d', f.date) = '31'\n    WHERE \n        strftime('%Y', f.date) - strftime('%Y', d.first_term) BETWEEN 1 AND 20\n    GROUP BY \n        period\n)\nSELECT \n    gs.period, \n    rc.cohort_retained * 1.0 / c.cohort_size AS retention_rate\nFROM \n    generate_series gs\nLEFT JOIN \n    retained_cohort rc ON gs.period = rc.period\nJOIN \n    cohort c ON 1=1\nORDER BY \n    gs.period;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local171",
        "db": "city_legislation",
        "question": "What is the number of male legislators from Louisiana who have served more than 30 years since their first term, grouped by their years of service, for periods less than 50 years?",
        "SQL": "WITH RECURSIVE generate_series AS (\n    SELECT \n        0 AS period\n    UNION ALL\n    SELECT \n        period + 1\n    FROM \n        generate_series\n    WHERE \n        period < 49  \n),\nlegislators_first_term AS (\n    SELECT \n        id_bioguide,\n        MIN(term_start) AS first_term,\n        (SELECT state FROM legislators_terms lt2\n        WHERE lt2.id_bioguide = lt1.id_bioguide\n        ORDER BY term_start LIMIT 1) AS first_state\n    FROM \n        legislators_terms lt1\n    GROUP BY \n        id_bioguide\n),\ncohort AS (\n    SELECT \n        b.gender, \n        a.first_state, \n        COUNT(DISTINCT a.id_bioguide) AS cohort_size\n    FROM \n        legislators_first_term a\n    JOIN \n        legislators b ON a.id_bioguide = b.id_bioguide\n    GROUP BY \n        b.gender, a.first_state\n)\n\nSELECT \n    IFNULL((strftime('%Y', f.date) - strftime('%Y', d.first_term)), 0) AS period,\n    COUNT(DISTINCT d.id_bioguide) AS cohort_retained\nFROM \n    legislators_first_term d\nJOIN \n    legislators_terms e ON d.id_bioguide = e.id_bioguide\nLEFT JOIN \n    legislation_date_dim f ON f.date BETWEEN e.term_start AND e.term_end\n                 AND strftime('%m', f.date) = '12'\n                 AND strftime('%d', f.date) = '31'\nJOIN \n    legislators g ON d.id_bioguide = g.id_bioguide\nWHERE \n    g.gender = 'M'\n    AND IFNULL((strftime('%Y', f.date) - strftime('%Y', d.first_term)), 0) > 30\n    AND d.first_state = 'LA'\nGROUP BY \n    d.first_state, g.gender, period\nORDER BY \n    d.first_state, g.gender, period;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local167",
        "db": "city_legislation",
        "question": "Which state has the highest number of female legislators whose term end dates fall on December 31st, and what is that count? Please provide state name abbreviation.",
        "SQL": "WITH RECURSIVE generate_series AS (\n    SELECT \n        0 AS period\n    UNION ALL\n    SELECT \n        period + 1\n    FROM \n        generate_series\n    WHERE \n        period < 20\n),\nlegislators_first_term AS (\n    SELECT \n        id_bioguide,\n        MIN(term_start) AS first_term,\n        (SELECT state FROM legislators_terms lt2\n        WHERE lt2.id_bioguide = lt1.id_bioguide\n        ORDER BY term_start LIMIT 1) AS first_state\n    FROM \n        legislators_terms lt1\n    GROUP BY \n        id_bioguide\n),\ncohort AS (\n    SELECT \n        b.gender,\n        a.first_state, \n        COUNT(DISTINCT a.id_bioguide) AS cohort_size\n    FROM \n        legislators_first_term a\n    JOIN \n        legislators b ON a.id_bioguide = b.id_bioguide\n    GROUP BY \n        b.gender, \n        a.first_state\n)\nSELECT \n    d.first_state AS state,\n    COUNT(DISTINCT d.id_bioguide) AS count_\nFROM \n    legislators_first_term d\nJOIN \n    legislators_terms e ON d.id_bioguide = e.id_bioguide\nLEFT JOIN \n    legislation_date_dim f ON f.date BETWEEN e.term_start AND e.term_end\n                     AND strftime('%m', f.date) = '12'\n                     AND strftime('%d', f.date) = '31'\nJOIN \n    legislators g ON d.id_bioguide = g.id_bioguide\nWHERE \n    g.gender = 'F'  \nGROUP BY \n    d.first_state\nORDER BY \n    COUNT(DISTINCT d.id_bioguide) DESC  \nLIMIT 1;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local170",
        "db": "city_legislation",
        "question": "Which states have a consistently non-zero retention rate for legislators of each gender across every two-year interval (0, 2, 4, 6, 8, 10) during the first 10 years after they begin serving? Please provide state name abbreviation.",
        "SQL": "WITH RECURSIVE generate_series AS (\n    SELECT \n        0 AS period\n    UNION ALL\n    SELECT \n        period + 1\n    FROM \n        generate_series\n    WHERE \n        period < 10\n),\nlegislators_first_term AS (\n    SELECT \n        id_bioguide,\n        MIN(term_start) AS first_term,\n        (SELECT state FROM legislators_terms lt2\n        WHERE lt2.id_bioguide = lt1.id_bioguide\n        ORDER BY term_start LIMIT 1) AS first_state\n    FROM \n        legislators_terms lt1\n    GROUP BY \n        id_bioguide\n),\ncohort AS (\n    SELECT \n        b.gender, \n        a.first_state, \n        COUNT(DISTINCT a.id_bioguide) AS cohort_size\n    FROM \n        legislators_first_term a\n    JOIN \n        legislators b ON a.id_bioguide = b.id_bioguide\n    GROUP BY \n        b.gender, \n        a.first_state\n),\nretained_cohort AS (\n    SELECT \n        d.first_state,\n        g.gender,\n        IFNULL((strftime('%Y', f.date) - strftime('%Y', d.first_term)), 0) AS period,\n        COUNT(DISTINCT d.id_bioguide) AS cohort_retained\n    FROM \n        legislators_first_term d\n    JOIN \n        legislators_terms e ON d.id_bioguide = e.id_bioguide\n    LEFT JOIN \n        legislation_date_dim f ON f.date BETWEEN e.term_start AND e.term_end\n                         AND strftime('%m', f.date) = '12'\n                         AND strftime('%d', f.date) = '31'\n    JOIN \n        legislators g ON d.id_bioguide = g.id_bioguide\n    GROUP BY \n        d.first_state, \n        g.gender, period\n),\ncohort_retained_pct AS (\n    SELECT \n        c.gender, \n        c.first_state, \n        gs.period, \n        c.cohort_size,\n        COALESCE(rc.cohort_retained, 0) AS cohort_retained,\n        COALESCE(rc.cohort_retained, 0) * 1.0 / c.cohort_size AS pct_retained\n    FROM \n        cohort c\n    CROSS JOIN \n        generate_series gs\n    LEFT JOIN \n        retained_cohort rc ON c.first_state = rc.first_state AND c.gender = rc.gender AND gs.period = rc.period\n),\nnon_zero_states AS (\n    SELECT \n        first_state\n    FROM \n        cohort_retained_pct\n    WHERE \n        period IN (0, 2, 4, 6, 8, 10)\n    GROUP BY \n        first_state\n    HAVING \n        COUNT(DISTINCT period) = 6\n       AND SUM(CASE WHEN pct_retained = 0 THEN 1 ELSE 0 END) = 0\n)\nSELECT DISTINCT \n    first_state AS state\nFROM \n    non_zero_states;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local193",
        "db": "sqlite-sakila",
        "question": "Could you find out the average percentage of the total lifetime sales (LTV) that occur in the first 7 and 30 days after a customer's initial purchase? Also, include the average total lifetime sales (LTV). Please exclude customers with zero lifetime sales. The percentage should be shown with %, and the 7- and 30-day periods should be based on the exact number of hours-minutes-seconds, not calendar days.",
        "SQL": "WITH\n    first_order AS (\n        SELECT q1.*\n        FROM (\n            SELECT \n                p.customer_id,\n                p.payment_date,\n                ROW_NUMBER() OVER (PARTITION BY p.customer_id ORDER BY p.payment_date) AS row_num\n            FROM \n                payment p\n        ) q1\n        WHERE \n            q1.row_num = 1\n    ),\n    interval_sales AS (\n        SELECT \n            fo.customer_id,\n            (SELECT SUM(p2.amount)\n             FROM payment p2\n             WHERE p2.customer_id = fo.customer_id\n               AND JULIANDAY(p2.payment_date) - JULIANDAY(fo.payment_date) < 7 AND JULIANDAY(p2.payment_date) - JULIANDAY(fo.payment_date) >= 0) AS first7_sales,\n            (SELECT SUM(p2.amount)\n             FROM payment p2\n             WHERE p2.customer_id = fo.customer_id\n               AND JULIANDAY(p2.payment_date) - JULIANDAY(fo.payment_date) < 30 AND JULIANDAY(p2.payment_date) - JULIANDAY(fo.payment_date) >= 0) AS first30_sales,\n            (SELECT SUM(p2.amount)\n             FROM payment p2\n             WHERE p2.customer_id = fo.customer_id) AS LTV\n        FROM \n            first_order fo\n    )\nSELECT \n    (AVG(i_s.first7_sales / i_s.LTV * 100) || '%') AS avg_first7_percent,\n    (AVG(i_s.first30_sales / i_s.LTV * 100) || '%') AS avg_first30_percent,\n    AVG(i_s.LTV) AS avg_ltv\nFROM \n    interval_sales i_s\nWHERE \n    i_s.LTV > 0;",
        "external_knowledge": null,
        "plan": "1. This subquery identifies the first payment date for each customer.\n2. This subquery calculates sales metrics for each customer in 7, 14-day intervals relative to their first payment date.\n3. Finally, aggregates and calculates the desired metrics.",
        "special_function": null
    },
    {
        "instance_id": "local194",
        "db": "sqlite-sakila",
        "question": "Please provide a list of the top three revenue-generating films for each actor, along with the average revenue per actor in those films, calculated by dividing the total film revenue equally among the actors for each film.",
        "SQL": "WITH\n    actor_count AS (\n        SELECT \n            f.film_id,\n            f.title,\n            COUNT(fa.actor_id) AS num_actors\n        FROM \n            film f\n        JOIN \n            film_actor fa ON fa.film_id = f.film_id\n        GROUP BY \n            f.film_id, f.title\n        ORDER BY \n            f.film_id\n    ),\n    film_revenue AS (\n        SELECT \n            i.film_id,\n            SUM(p.amount) AS gross_revenue\n        FROM \n            payment p\n        JOIN \n            rental r ON r.rental_id = p.rental_id\n        JOIN \n            inventory i ON i.inventory_id = r.inventory_id\n        GROUP BY \n            i.film_id\n        ORDER BY \n            i.film_id\n    ),\n    film_rev_per_actor AS (\n        SELECT \n            ac.title,\n            fr.gross_revenue / ac.num_actors * 1.0 AS rev_per_actor\n        FROM \n            actor_count ac\n        JOIN \n            film_revenue fr ON fr.film_id = ac.film_id\n    )\nSELECT \n    *\nFROM \n    film_rev_per_actor\nORDER BY \n    rev_per_actor DESC\nLIMIT \n    3;",
        "external_knowledge": null,
        "plan": "1. **Calculate the Number of Actors per Film:**\n   - Create a temporary result set that includes each film along with the total number of actors involved in that film. This is done by joining the films with their actors and grouping the results by film.\n\n2. **Calculate Gross Revenue per Film:**\n   - Create another temporary result set that includes each film along with the total gross revenue it generated. This is achieved by joining the payments with rentals and inventory items, and then grouping by film.\n\n3. **Calculate Revenue per Actor for Each Film:**\n   - Combine the results from the first two steps to calculate the revenue generated per actor for each film. This involves joining the two temporary result sets on the film identifier and dividing the gross revenue by the number of actors for each film. Round the result to two decimal places for precision.\n\n4. **Retrieve Top-Performing Films:**\n   - Select the necessary details from the combined result set and order the films based on the calculated revenue per actor in descending order to identify the top-performing films.\n\n5. **Limit the Results:**\n   - Limit the final output to the top three films that generate the most revenue per actor.",
        "special_function": null
    },
    {
        "instance_id": "local195",
        "db": "sqlite-sakila",
        "question": "Please find out how widespread the appeal of our top five actors is. What percentage of our customers have rented films featuring these actors?",
        "SQL": "WITH\n    actors_sales AS (\n        SELECT \n            a.actor_id,\n            a.first_name,\n            a.last_name,\n            SUM(p.amount) OVER (PARTITION BY a.actor_id) AS gross_sales,\n            ROW_NUMBER() OVER (PARTITION BY a.actor_id) AS row_num\n        FROM \n            actor a\n        JOIN \n            film_actor fa ON fa.actor_id = a.actor_id\n        JOIN \n            film f ON f.film_id = fa.film_id\n        JOIN \n            inventory i ON i.film_id = f.film_id\n        JOIN \n            rental r ON r.inventory_id = i.inventory_id\n        JOIN \n            payment p ON p.rental_id = r.rental_id\n        ORDER BY \n            a.actor_id, row_num\n    ),\n    top5 AS (\n        SELECT \n            actor_id,\n            first_name || ' ' || last_name AS full_name,\n            gross_sales\n        FROM \n            actors_sales\n        WHERE \n            row_num = 1\n        ORDER BY \n            gross_sales DESC\n        LIMIT \n            5\n    ),\n    top_movies AS (\n        SELECT \n            f.film_id,\n            f.title\n        FROM \n            top5 t5\n        JOIN \n            film_actor fa ON fa.actor_id = t5.actor_id\n        JOIN \n            film f ON f.film_id = fa.film_id\n        GROUP BY \n            f.film_id\n    ),\n    customer_rentals AS (\n        SELECT \n            c.customer_id,\n            i.film_id\n        FROM \n            customer c\n        JOIN \n            payment p ON p.customer_id = c.customer_id\n        JOIN \n            rental r ON r.rental_id = p.rental_id\n        JOIN \n            inventory i ON i.inventory_id = r.inventory_id\n        ORDER BY \n            c.customer_id\n    ),\n    customer_top_movies AS (\n        SELECT DISTINCT \n            cr.customer_id\n        FROM \n            customer_rentals cr\n        WHERE \n            cr.film_id IN (\n                SELECT \n                    tm.film_id\n                FROM \n                    top_movies tm\n            )\n    )\nSELECT \n    ROUND(100.0 * (SELECT COUNT(customer_id) FROM customer_top_movies) / (SELECT COUNT(customer_id) FROM customer), 2) AS answer;",
        "external_knowledge": null,
        "plan": "1. Calculate the total gross sales for each actor and assign a row number to each actor's sales data.\n2. Identify the top 5 highest-grossing actors.\n3. Identify the movies associated with the top 5 highest-grossing actors.\n4. Get a list of all rentals by each customer.\n5. Identify customers who have rented at least one movie from the top 5 highest-grossing actors.\n6. Calculate the percentage of customers who have rented at least one movie from the top 5 highest-grossing actors.",
        "special_function": null
    },
    {
        "instance_id": "local196",
        "db": "sqlite-sakila",
        "question": "For the ratings of the first movie rented by customers, please provide the average total spend and the average number of subsequent rentals for each rating category.",
        "SQL": "WITH\n    pay_order AS (\n        SELECT \n            p.*,\n            ROW_NUMBER() OVER (PARTITION BY p.customer_id ORDER BY p.payment_date) AS p_order\n        FROM \n            payment p\n    ),\n    first_order AS (\n        SELECT \n            po.payment_id,\n            po.customer_id,\n            po.rental_id\n        FROM \n            pay_order po\n        WHERE \n            po.p_order = 1\n    ),\n    first_rating AS (\n        SELECT \n            fo.customer_id,\n            f.film_id,\n            f.title,\n            f.rating\n        FROM \n            first_order fo\n        JOIN \n            rental r ON r.rental_id = fo.rental_id\n        JOIN \n            inventory i ON i.inventory_id = r.inventory_id\n        JOIN \n            film f ON f.film_id = i.film_id\n    ),\n    total_spending AS (\n        SELECT \n            p.customer_id,\n            SUM(p.amount) AS total_spend\n        FROM \n            payment p\n        GROUP BY \n            p.customer_id\n    ),\n\tsubsequent_rentals AS (\n    \tSELECT\n        \tr.customer_id,\n        \tCOUNT(*) - 1 AS subsequent_rentals\n    \tFROM\n    \t    rental r\n    \tGROUP BY\n    \t    r.customer_id\n\t)\nSELECT \n    fr.rating,\n    AVG(ts.total_spend) AS avg_spend,\n    AVG(sr.subsequent_rentals) AS avg_subsequent_rentals_number\nFROM \n    first_rating fr\nJOIN \n    total_spending ts ON ts.customer_id = fr.customer_id\nJOIN\n\tsubsequent_rentals sr ON sr.customer_id = fr.customer_id\nGROUP BY \n    fr.rating\nORDER BY \n    avg_spend DESC;",
        "external_knowledge": null,
        "plan": "1. Assign an row_number as order number to each payment per customer, ordered by payment \n2. Identify the first payment (order) for each customer.\n3. Retrieve the details of the film associated with each customer's first rental.\n4. Calculate the total spending and the number of subsequent rentals for each customer.\n5. Combine the first rental's rating with the customer's total spending and subsequent rentals.\n6. Calculate the average total spending and average number of subsequent rentals for each film rating.\n7. Retrieve and display the results from rating_count, ordered by average spending in descending order.",
        "special_function": null
    },
    {
        "instance_id": "local197",
        "db": "sqlite-sakila",
        "question": "Can you determine which of our top 10 paying customers had the highest payment difference in any given month? I\u2019d like to know the highest payment difference for this customer, with the result rounded to two decimal places.",
        "SQL": "WITH result_table AS (\n  SELECT \n    strftime('%m', pm.payment_date) AS pay_mon, \n    customer_id,\n    COUNT(pm.amount) AS pay_countpermon, \n    SUM(pm.amount) AS pay_amount \n  FROM \n    payment AS pm \n  GROUP BY \n    pay_mon, \n    customer_id\n), \ntop10_customer AS (\n  SELECT \n    customer_id,\n    SUM(tb.pay_amount) AS total_payments \n  FROM \n    result_table AS tb \n  GROUP BY \n    customer_id\n  ORDER BY \n    SUM(tb.pay_amount) DESC \n  LIMIT \n    10\n), \ndifference_per_mon AS (\n  SELECT \n    pay_mon AS month_number, \n    pay_mon AS month, \n    tb.pay_countpermon, \n    tb.pay_amount, \n    ABS(tb.pay_amount - LAG(tb.pay_amount) OVER (PARTITION BY tb.customer_id)) AS diff \n  FROM \n    result_table tb \n    JOIN top10_customer top ON top.customer_id = tb.customer_id\n) \nSELECT \n  month, \n  ROUND(max_diff, 2) AS max_diff \nFROM (\n  SELECT \n    month, \n    diff, \n    month_number, \n    MAX(diff) OVER (PARTITION BY month) AS max_diff \n  FROM \n    difference_per_mon\n) AS max_per_mon \nWHERE \n  diff = max_diff \nORDER BY \n  max_diff DESC \nLIMIT \n  1;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local199",
        "db": "sqlite-sakila",
        "question": "Can you identify the year and month with the highest rental orders created by the store's staff for each store? Please list the store ID, the year, the month, and the total rentals for those dates.",
        "SQL": "WITH result_table AS (\n  SELECT \n    strftime('%Y', RE.RENTAL_DATE) AS YEAR, \n    strftime('%m', RE.RENTAL_DATE) AS RENTAL_MONTH, \n    ST.STORE_ID, \n    COUNT(RE.RENTAL_ID) AS count \n  FROM \n    RENTAL RE \n    JOIN STAFF ST ON RE.STAFF_ID = ST.STAFF_ID \n  GROUP BY \n    YEAR, \n    RENTAL_MONTH, \n    ST.STORE_ID \n), \nmonthly_sales AS (\n  SELECT \n    YEAR, \n    RENTAL_MONTH, \n    STORE_ID, \n    SUM(count) AS total_rentals \n  FROM \n    result_table \n  GROUP BY \n    YEAR, \n    RENTAL_MONTH, \n    STORE_ID\n),\nstore_max_sales AS (\n  SELECT \n    STORE_ID, \n    YEAR, \n    RENTAL_MONTH, \n    total_rentals, \n    MAX(total_rentals) OVER (PARTITION BY STORE_ID) AS max_rentals \n  FROM \n    monthly_sales\n)\nSELECT \n  STORE_ID, \n  YEAR, \n  RENTAL_MONTH, \n  total_rentals \nFROM \n  store_max_sales \nWHERE \n  total_rentals = max_rentals\nORDER BY \n  STORE_ID;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local201",
        "db": "modern_data",
        "question": "Identify the first 10 words (of length 4 to 5, starting with 'r') sorted alphabetically that have at least one anagram. Provide the count of anagrams for each word.",
        "SQL": "WITH RECURSIVE char_split(word, char, rest, sorted) AS (\n  SELECT\n    words AS word,\n    SUBSTR(words, 1, 1) AS char,\n    SUBSTR(words, 2) AS rest,\n    '' AS sorted\n  FROM word_list\n  UNION ALL\n  SELECT\n    word,\n    SUBSTR(rest, 1, 1),\n    SUBSTR(rest, 2),\n    sorted || char\n  FROM char_split\n  WHERE rest <> ''\n),\nsorted_words AS (\n  SELECT\n    word,\n    GROUP_CONCAT(char, '') AS sorted\n  FROM (\n    SELECT\n      word,\n      char\n    FROM char_split\n    ORDER BY word, char\n  )\n  GROUP BY word\n),\nget_anagram AS (\n  SELECT \n    s1.word AS word,\n    CASE\n      -- Only check words of the same length.\n      WHEN LENGTH(s1.sorted) = LENGTH(s2.sorted) THEN\n        CASE\n          -- If sorted words are the same, they contain the same letters and are anagrams\n          WHEN s1.sorted = s2.sorted THEN s2.word\n          ELSE NULL\n        END\n    END AS anagram\n  FROM sorted_words AS s1\n  JOIN sorted_words AS s2\n  ON s1.word <> s2.word\n)\nSELECT\n  word,\n  COUNT(anagram) AS anagram_count\nFROM\n  get_anagram\nWHERE \n  anagram IS NOT NULL\nAND \n  LENGTH(word) > 3\nAND \n  LENGTH(word) <= 5\nAND \n  word LIKE 'r%'\nGROUP BY \n  word\nORDER BY \n  word\nLIMIT 10;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local202",
        "db": "city_legislation",
        "question": "For alien data, how many of the top 10 states by alien population have a higher percentage of friendly aliens than hostile aliens, with an average alien age exceeding 200?",
        "SQL": "WITH alien_aggression AS (\n    SELECT\n        state,\n        SUM(CASE WHEN aggressive = 1 THEN 1 ELSE 0 END) AS n_hostile_aliens,\n        SUM(CASE WHEN aggressive = 0 THEN 1 ELSE 0 END) AS n_friendly_aliens\n    FROM alien_data\n    GROUP BY state\n),\nalien_stats AS (\n    SELECT\n        alien_data.state,\n        COUNT(*) AS alien_population_total,\n        ROUND(AVG(age)) AS avg_alien_age,\n        alien_aggression.n_friendly_aliens,\n        alien_aggression.n_hostile_aliens\n    FROM alien_data\n    JOIN alien_aggression ON alien_data.state = alien_aggression.state\n    GROUP BY alien_data.state, alien_aggression.n_friendly_aliens, alien_aggression.n_hostile_aliens\n),\ntop_states AS (\n    SELECT state\n    FROM alien_stats\n    ORDER BY alien_population_total DESC\n    LIMIT 10\n)\nSELECT\n    COUNT(*) AS number_of_states\nFROM\n    alien_stats\nWHERE\n    state IN (SELECT state FROM top_states)\n    AND ROUND((CAST(n_friendly_aliens AS REAL) / CAST(alien_population_total AS REAL)) * 100, 2) >\n    ROUND((CAST(n_hostile_aliens AS REAL) / CAST(alien_population_total AS REAL)) * 100, 2)\n    AND avg_alien_age > 200;",
        "external_knowledge": null,
        "plan": "1. **Calculate Aggression Counts**:\n   - Create a temporary table to calculate the number of friendly and hostile individuals for each region.\n   - For each record, increment the count of hostile or friendly individuals based on a specific condition.\n\n2. **Aggregate Alien Statistics**:\n   - Create another temporary table to gather statistics for each region.\n   - Compute the total number of individuals and the average age.\n   - Join this data with the aggression counts from the first temporary table.\n\n3. **Identify Top Regions**:\n   - From the aggregated statistics, select the top 10 regions based on total population size.\n\n4. **Filter and Count Regions**:\n   - Select records from the aggregated statistics where the region is one of the top 10 identified previously.\n   - Ensure the percentage of friendly individuals is greater than the percentage of hostile individuals.\n   - Ensure the average age of individuals in the region exceeds a specified threshold.\n   - Count the number of regions that meet these criteria and return the result.",
        "special_function": null
    },
    {
        "instance_id": "local209",
        "db": "delivery_center",
        "question": "What is the ratio of completed orders to total orders for the store with the highest number of orders?",
        "SQL": "WITH store_order_counts AS (\n    SELECT\n        s.store_name,\n        COUNT(o.order_id) AS total_orders\n    FROM\n        orders o \n    LEFT JOIN\n        stores s ON o.store_id = s.store_id \n    GROUP BY \n        s.store_name\n    ORDER BY \n        total_orders DESC\n    LIMIT 1  \n),\ndeliveries_completed AS (\n    SELECT\n        s.store_name,\n        COUNT(o.order_id) AS deliveries_completed\n    FROM\n        orders o \n    LEFT JOIN\n        stores s ON o.store_id = s.store_id \n    INNER JOIN (\n            SELECT \n                DISTINCT delivery_order_id\n            FROM deliveries\n            WHERE delivery_status = 'DELIVERED'\n        ) AS ud ON o.delivery_order_id = ud.delivery_order_id\n    GROUP BY \n        s.store_name\n)\nSELECT\n    CAST(dc.deliveries_completed AS REAL) / NULLIF(CAST(soc.total_orders AS REAL), 0) AS completion_ratio\nFROM\n    store_order_counts soc\nLEFT JOIN\n    deliveries_completed dc ON soc.store_name = dc.store_name;",
        "external_knowledge": null,
        "plan": "1. Calculate the total number of orders for each store and select the top 5 stores with the most orders\n2. Calculate the number of completed deliveries for each store\n3. Calculate the number of cancelled deliveries for each store\n4. Calculate the delivery completion ratio for each store",
        "special_function": null
    },
    {
        "instance_id": "local210",
        "db": "delivery_center",
        "question": "Can you identify the hubs that saw more than a 20% increase in finished orders from February to March?",
        "SQL": "WITH february_orders AS (\n    SELECT\n        h.hub_name AS hub_name,\n        COUNT(*) AS orders_february\n    FROM \n        orders o \n    LEFT JOIN\n        stores s ON o.store_id = s.store_id \n    LEFT JOIN \n        hubs h ON s.hub_id = h.hub_id \n    WHERE o.order_created_month = 2 AND o.order_status = 'FINISHED'\n    GROUP BY\n        h.hub_name\n),\nmarch_orders AS (\n    SELECT\n        h.hub_name AS hub_name,\n        COUNT(*) AS orders_march\n    FROM \n        orders o \n    LEFT JOIN\n        stores s ON o.store_id = s.store_id \n    LEFT JOIN \n        hubs h ON s.hub_id = h.hub_id \n    WHERE o.order_created_month = 3 AND o.order_status = 'FINISHED'\n    GROUP BY\n        h.hub_name\n)\nSELECT\n    fo.hub_name\nFROM\n    february_orders fo\nLEFT JOIN \n    march_orders mo ON fo.hub_name = mo.hub_name\nWHERE \n    fo.orders_february > 0 AND \n    mo.orders_march > 0 AND\n    (CAST((mo.orders_march - fo.orders_february) AS REAL) / CAST(fo.orders_february AS REAL)) > 0.2  -- Filter for hubs with more than a 20% increase",
        "external_knowledge": null,
        "plan": "1. Calculate the number of finished orders for each hub in Februrary\n2. Calculate the number of finished orders for each hub in March\n3. Calculate the percentage difference in orders between February and March",
        "special_function": null
    },
    {
        "instance_id": "local212",
        "db": "delivery_center",
        "question": "Can you find 5 delivery drivers with the highest average number of daily deliveries?",
        "SQL": "WITH media_entregador AS (\n\tSELECT \n\t\tentregador,\n\t\tCAST(strftime('%m', data) AS INTEGER) AS mes,\n\t\tROUND(AVG(entregas_por_dia), 2) AS media_entregas_por_dia,\n\t\tRANK() OVER(PARTITION BY CAST(strftime('%m', data) AS INTEGER) ORDER BY AVG(entregas_por_dia) DESC) AS rank_entregador\n\tFROM (\n\t\tSELECT\n\t\t\tud.driver_id AS entregador,\n\t\t\tDATE(o.order_moment_created) AS data,\n\t\t\tCOUNT(o.order_id) AS entregas_por_dia\n\t\tFROM \n\t\t\torders o \n\t\tINNER JOIN (\n\t\t\tSELECT \n\t\t\t\tDISTINCT delivery_order_id, \n\t\t\t\tdriver_id, \n\t\t\t\tdelivery_status\n\t\t\tFROM \n\t\t\t\tdeliveries    \t\n\t\t) AS ud\n\t\tON o.delivery_order_id = ud.delivery_order_id\n\t\tLEFT JOIN \n\t\t\tdrivers dr ON ud.driver_id = dr.driver_id\t\n\t\tWHERE \n\t\t\to.order_status = 'FINISHED' AND \n\t\t\tud.driver_id IS NOT NULL AND  \n\t\t\tud.delivery_status = 'DELIVERED'\n\t\tGROUP BY \n\t\t\tud.driver_id, \n\t\t\tCAST(strftime('%Y-%m-%d', o.order_moment_created) AS DATE)\n\t) AS subquery\n\tGROUP BY\n\t\tentregador,\n\t\tCAST(strftime('%m', data) AS INTEGER)\n)\nSELECT\n\tentregador AS driver_id\nFROM\n\tmedia_entregador\nWHERE\n\trank_entregador <= 5\nORDER BY \n\tmes,\n\trank_entregador;",
        "external_knowledge": null,
        "plan": "1. **Subquery to Calculate Daily Deliveries:**\n   - Select the driver ID, the date of the order, and the count of orders per day.\n   - Filter the orders to include only those that are finished and delivered.\n   - Group the results by driver ID and the specific day.\n\n2. **Main Query with Common Table Expression (CTE):**\n   - Create a CTE to calculate the average daily deliveries per driver, grouped by driver and month.\n   - Convert the date to extract the month.\n   - Calculate the average number of daily deliveries for each driver within the month and round it to two decimal places.\n   - Rank the drivers within each month based on their average daily deliveries in descending order.\n\n3. **Final Selection:**\n   - Select the driver IDs from the CTE where the rank is 5 or less, meaning we are interested in the top 5 drivers with the highest average daily deliveries.\n   - Order the final results by month and rank to ensure the output is organized.\n\nThis reference plan ensures the selection of the top 5 drivers with the highest average number of daily deliveries, grouped and ranked within each month.",
        "special_function": null
    },
    {
        "instance_id": "local218",
        "db": "EU_soccer",
        "question": "Can you calculate the median from the highest season goals of each team?",
        "SQL": "WITH goals_per_club AS (\n    SELECT \n        team,\n        season,\n        SUM(goals) AS total_goals\n    FROM (\n        SELECT \n            home_team_api_id AS team,\n            season,\n            home_team_goal AS goals\n        FROM \n            match\n        UNION ALL\n        SELECT \n            away_team_api_id AS team,\n            season,\n            away_team_goal AS goals\n        FROM \n            match\n    ) AS goals_data\n    GROUP BY \n        team, season\n),\nmax_goals_per_team AS (\n    SELECT \n        team,\n        MAX(total_goals) AS max_goals\n    FROM \n        goals_per_club\n    GROUP BY \n        team\n),\nranked_goals AS (\n    SELECT \n        max_goals,\n        ROW_NUMBER() OVER (ORDER BY max_goals) AS row_num,\n        COUNT(*) OVER () AS total_count\n    FROM \n        max_goals_per_team\n)\nSELECT \n    AVG(max_goals) AS median_max_goals\nFROM \n    ranked_goals\nWHERE \n    row_num IN ((total_count + 1) / 2, (total_count + 2) / 2);",
        "external_knowledge": null,
        "plan": "1. Calculate total goals per team per season\n2. Find the highest goal tally for each team across seasons\n3. Rank these highest goal tallies\n4. Compute the median of these highest tallies",
        "special_function": null
    },
    {
        "instance_id": "local219",
        "db": "EU_soccer",
        "question": "Which single team has the fewest wins in each league?",
        "SQL": "WITH match_view AS(\nSELECT\n    M.id,\n    L.name AS league,\n    M.season,\n    M.match_api_id,\n    T.team_long_name AS home_team,\n    TM.team_long_name AS away_team,\n    M.home_team_goal,\n    M.away_team_goal,\n    P1.player_name AS home_gk,\n    P2.player_name AS home_center_back_1,\n    P3.player_name AS home_center_back_2,\n    P4.player_name AS home_right_back,\n    P5.player_name AS home_left_back,\n    P6.player_name AS home_midfield_1,\n    P7.player_name AS home_midfield_2,\n    P8.player_name AS home_midfield_3,\n    P9.player_name AS home_midfield_4,\n    P10.player_name AS home_second_forward,\n    P11.player_name AS home_center_forward,\n    P12.player_name AS away_gk,\n    P13.player_name AS away_center_back_1,\n    P14.player_name AS away_center_back_2,\n    P15.player_name AS away_right_back,\n    P16.player_name AS away_left_back,\n    P17.player_name AS away_midfield_1,\n    P18.player_name AS away_midfield_2,\n    P19.player_name AS away_midfield_3,\n    P20.player_name AS away_midfield_4,\n    P21.player_name AS away_second_forward,\n    P22.player_name AS away_center_forward,\n    M.goal,\n    M.card\nFROM\n    match M\nLEFT JOIN\n    league L ON M.league_id = L.id\nLEFT JOIN\n    team T ON M.home_team_api_id = T.team_api_id\nLEFT JOIN\n    team TM ON M.away_team_api_id = TM.team_api_id\nLEFT JOIN\n    player P1 ON M.home_player_1 = P1.player_api_id\nLEFT JOIN\n    player P2 ON M.home_player_2 = P2.player_api_id\nLEFT JOIN\n    player P3 ON M.home_player_3 = P3.player_api_id\nLEFT JOIN\n    player P4 ON M.home_player_4 = P4.player_api_id\nLEFT JOIN\n    player P5 ON M.home_player_5 = P5.player_api_id\nLEFT JOIN\n    player P6 ON M.home_player_6 = P6.player_api_id\nLEFT JOIN\n    player P7 ON M.home_player_7 = P7.player_api_id\nLEFT JOIN\n    player P8 ON M.home_player_8 = P8.player_api_id\nLEFT JOIN\n    player P9 ON M.home_player_9 = P9.player_api_id\nLEFT JOIN\n    player P10 ON M.home_player_10 = P10.player_api_id\nLEFT JOIN\n    player P11 ON M.home_player_11 = P11.player_api_id\nLEFT JOIN\n    player P12 ON M.away_player_1 = P12.player_api_id\nLEFT JOIN\n    player P13 ON M.away_player_2 = P13.player_api_id\nLEFT JOIN\n    player P14 ON M.away_player_3 = P14.player_api_id\nLEFT JOIN\n    player P15 ON M.away_player_4 = P15.player_api_id\nLEFT JOIN\n    player P16 ON M.away_player_5 = P16.player_api_id\nLEFT JOIN\n    player P17 ON M.away_player_6 = P17.player_api_id\nLEFT JOIN\n    player P18 ON M.away_player_7 = P18.player_api_id\nLEFT JOIN\n    player P19 ON M.away_player_8 = P19.player_api_id\nLEFT JOIN\n    player P20 ON M.away_player_9 = P20.player_api_id\nLEFT JOIN\n    player P21 ON M.away_player_10 = P21.player_api_id\nLEFT JOIN\n    player P22 ON M.away_player_11 = P22.player_api_id\n),\nmatch_score AS\n(\n    SELECT  -- Displaying teams and their goals as home_team\n        id,\n        home_team AS team,\n        CASE\n            WHEN home_team_goal > away_team_goal THEN 1 ELSE 0 END AS Winning_match\n    FROM\n        match_view\n\n    UNION ALL\n\n    SELECT  -- Displaying teams and their goals as away_team\n        id,\n        away_team AS team,\n        CASE\n            WHEN away_team_goal > home_team_goal THEN 1 ELSE 0 END AS Winning_match\n    FROM\n        match_view\n),\nwinning_matches AS\n(\n    SELECT  -- Displaying total match wins for each team\n        MV.league,\n        M.team,\n        COUNT(CASE WHEN M.Winning_match = 1 THEN 1 END) AS wins,\n        ROW_NUMBER() OVER(PARTITION BY MV.league ORDER BY COUNT(CASE WHEN M.Winning_match = 1 THEN 1 END) ASC) AS rn\n    FROM\n        match_score M\n    JOIN\n        match_view MV\n    ON\n        M.id = MV.id\n    GROUP BY\n        MV.league,\n        team\n    ORDER BY\n        league,\n        wins ASC\n)\nSELECT\n    league,\n    team\nFROM\n    winning_matches\nWHERE\n    rn = 1  -- Getting the team with the least number of wins in each league\nORDER BY\n    league;",
        "external_knowledge": null,
        "plan": "1. Calculate whether each team won each match, either as a home team or an away team\n2. Calculate the total number of wins for each team and ranks teams within each league by their number of wins\n3. Select the team with the fewest wins in each league",
        "special_function": null
    },
    {
        "instance_id": "local221",
        "db": "EU_soccer",
        "question": "Tell me top10 teams with the most wins across the league",
        "SQL": "WITH match_view AS(\nSELECT\n    M.id,\n    L.name AS league,\n    M.season,\n    M.match_api_id,\n    T.team_long_name AS home_team,\n    TM.team_long_name AS away_team,\n    M.home_team_goal,\n    M.away_team_goal,\n    P1.player_name AS home_gk,\n    P2.player_name AS home_center_back_1,\n    P3.player_name AS home_center_back_2,\n    P4.player_name AS home_right_back,\n    P5.player_name AS home_left_back,\n    P6.player_name AS home_midfield_1,\n    P7.player_name AS home_midfield_2,\n    P8.player_name AS home_midfield_3,\n    P9.player_name AS home_midfield_4,\n    P10.player_name AS home_second_forward,\n    P11.player_name AS home_center_forward,\n    P12.player_name AS away_gk,\n    P13.player_name AS away_center_back_1,\n    P14.player_name AS away_center_back_2,\n    P15.player_name AS away_right_back,\n    P16.player_name AS away_left_back,\n    P17.player_name AS away_midfield_1,\n    P18.player_name AS away_midfield_2,\n    P19.player_name AS away_midfield_3,\n    P20.player_name AS away_midfield_4,\n    P21.player_name AS away_second_forward,\n    P22.player_name AS away_center_forward,\n    M.goal,\n    M.card\nFROM\n    match M\nLEFT JOIN\n    league L ON M.league_id = L.id\nLEFT JOIN\n    team T ON M.home_team_api_id = T.team_api_id\nLEFT JOIN\n    team TM ON M.away_team_api_id = TM.team_api_id\nLEFT JOIN\n    player P1 ON M.home_player_1 = P1.player_api_id\nLEFT JOIN\n    player P2 ON M.home_player_2 = P2.player_api_id\nLEFT JOIN\n    player P3 ON M.home_player_3 = P3.player_api_id\nLEFT JOIN\n    player P4 ON M.home_player_4 = P4.player_api_id\nLEFT JOIN\n    player P5 ON M.home_player_5 = P5.player_api_id\nLEFT JOIN\n    player P6 ON M.home_player_6 = P6.player_api_id\nLEFT JOIN\n    player P7 ON M.home_player_7 = P7.player_api_id\nLEFT JOIN\n    player P8 ON M.home_player_8 = P8.player_api_id\nLEFT JOIN\n    player P9 ON M.home_player_9 = P9.player_api_id\nLEFT JOIN\n    player P10 ON M.home_player_10 = P10.player_api_id\nLEFT JOIN\n    player P11 ON M.home_player_11 = P11.player_api_id\nLEFT JOIN\n    player P12 ON M.away_player_1 = P12.player_api_id\nLEFT JOIN\n    player P13 ON M.away_player_2 = P13.player_api_id\nLEFT JOIN\n    player P14 ON M.away_player_3 = P14.player_api_id\nLEFT JOIN\n    player P15 ON M.away_player_4 = P15.player_api_id\nLEFT JOIN\n    player P16 ON M.away_player_5 = P16.player_api_id\nLEFT JOIN\n    player P17 ON M.away_player_6 = P17.player_api_id\nLEFT JOIN\n    player P18 ON M.away_player_7 = P18.player_api_id\nLEFT JOIN\n    player P19 ON M.away_player_8 = P19.player_api_id\nLEFT JOIN\n    player P20 ON M.away_player_9 = P20.player_api_id\nLEFT JOIN\n    player P21 ON M.away_player_10 = P21.player_api_id\nLEFT JOIN\n    player P22 ON M.away_player_11 = P22.player_api_id\n),\nmatch_score AS (\n    SELECT\n        id,\n        home_team AS team,\n        CASE\n            WHEN home_team_goal > away_team_goal THEN 1 ELSE 0 END AS Winning_match\n    FROM\n        match_view\n\n    UNION ALL\n\n    SELECT\n        id,\n        away_team AS team,\n        CASE\n            WHEN away_team_goal > home_team_goal THEN 1 ELSE 0 END AS Winning_match\n    FROM\n        match_view\n),\nwinning_matches AS (\n    SELECT\n        team,\n        COUNT(CASE WHEN Winning_match = 1 THEN 1 END) AS wins\n    FROM\n        match_score\n    GROUP BY\n        team\n),\nmost_wins_team AS (\n    SELECT\n        team,\n        wins,\n        ROW_NUMBER() OVER(ORDER BY wins DESC) AS rn\n    FROM\n        winning_matches\n)\n\nSELECT\n    team\nFROM\n    most_wins_team\nWHERE\n    rn <= 10;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local220",
        "db": "EU_soccer",
        "question": "Who is the player with the most wins?",
        "SQL": "WITH match_view AS (\n    SELECT M.id,\n           L.name AS league,\n           M.season,\n           M.match_api_id,\n           T.team_long_name AS home_team,\n           TM.team_long_name AS away_team,\n           M.home_team_goal,\n           M.away_team_goal,\n           P1.player_name AS home_gk,\n           P2.player_name AS home_center_back_1,\n           P3.player_name AS home_center_back_2,\n           P4.player_name AS home_right_back,\n           P5.player_name AS home_left_back,\n           P6.player_name AS home_midfield_1,\n           P7.player_name AS home_midfield_2,\n           P8.player_name AS home_midfield_3,\n           P9.player_name AS home_midfield_4,\n           P10.player_name AS home_second_forward,\n           P11.player_name AS home_center_forward,\n           P12.player_name AS away_gk,\n           P13.player_name AS away_center_back_1,\n           P14.player_name AS away_center_back_2,\n           P15.player_name AS away_right_back,\n           P16.player_name AS away_left_back,\n           P17.player_name AS away_midfield_1,\n           P18.player_name AS away_midfield_2,\n           P19.player_name AS away_midfield_3,\n           P20.player_name AS away_midfield_4,\n           P21.player_name AS away_second_forward,\n           P22.player_name AS away_center_forward,\n           M.goal,\n           M.card\n      FROM match M\n           LEFT JOIN\n           league L ON M.league_id = L.id\n           LEFT JOIN\n           team T ON M.home_team_api_id = T.team_api_id\n           LEFT JOIN\n           team TM ON M.away_team_api_id = TM.team_api_id\n           LEFT JOIN\n           player P1 ON M.home_player_1 = P1.player_api_id\n           LEFT JOIN\n           player P2 ON M.home_player_2 = P2.player_api_id\n           LEFT JOIN\n           player P3 ON M.home_player_3 = P3.player_api_id\n           LEFT JOIN\n           player P4 ON M.home_player_4 = P4.player_api_id\n           LEFT JOIN\n           player P5 ON M.home_player_5 = P5.player_api_id\n           LEFT JOIN\n           player P6 ON M.home_player_6 = P6.player_api_id\n           LEFT JOIN\n           player P7 ON M.home_player_7 = P7.player_api_id\n           LEFT JOIN\n           player P8 ON M.home_player_8 = P8.player_api_id\n           LEFT JOIN\n           player P9 ON M.home_player_9 = P9.player_api_id\n           LEFT JOIN\n           player P10 ON M.home_player_10 = P10.player_api_id\n           LEFT JOIN\n           player P11 ON M.home_player_11 = P11.player_api_id\n           LEFT JOIN\n           player P12 ON M.away_player_1 = P12.player_api_id\n           LEFT JOIN\n           player P13 ON M.away_player_2 = P13.player_api_id\n           LEFT JOIN\n           player P14 ON M.away_player_3 = P14.player_api_id\n           LEFT JOIN\n           player P15 ON M.away_player_4 = P15.player_api_id\n           LEFT JOIN\n           player P16 ON M.away_player_5 = P16.player_api_id\n           LEFT JOIN\n           player P17 ON M.away_player_6 = P17.player_api_id\n           LEFT JOIN\n           player P18 ON M.away_player_7 = P18.player_api_id\n           LEFT JOIN\n           player P19 ON M.away_player_8 = P19.player_api_id\n           LEFT JOIN\n           player P20 ON M.away_player_9 = P20.player_api_id\n           LEFT JOIN\n           player P21 ON M.away_player_10 = P21.player_api_id\n           LEFT JOIN\n           player P22 ON M.away_player_11 = P22.player_api_id\n),\nwin_players AS (\n    SELECT home_gk AS player \n    FROM match_view \n    WHERE home_team_goal > away_team_goal\n    \n    UNION ALL\n    \n    SELECT home_center_back_1 AS player \n    FROM match_view \n    WHERE home_team_goal > away_team_goal\n    \n    UNION ALL\n    \n    SELECT home_center_back_2 AS player \n    FROM match_view \n    WHERE home_team_goal > away_team_goal\n    \n    UNION ALL\n    \n    SELECT home_right_back AS player \n    FROM match_view \n    WHERE home_team_goal > away_team_goal\n    \n    UNION ALL\n    \n    SELECT home_left_back AS player \n    FROM match_view \n    WHERE home_team_goal > away_team_goal\n    \n    UNION ALL\n    \n    SELECT home_midfield_1 AS player \n    FROM match_view \n    WHERE home_team_goal > away_team_goal\n    \n    UNION ALL\n    \n    SELECT home_midfield_2 AS player \n    FROM match_view \n    WHERE home_team_goal > away_team_goal\n    \n    UNION ALL\n    \n    SELECT home_midfield_3 AS player \n    FROM match_view \n    WHERE home_team_goal > away_team_goal\n    \n    UNION ALL\n    \n    SELECT home_midfield_4 AS player \n    FROM match_view \n    WHERE home_team_goal > away_team_goal\n    \n    UNION ALL\n    \n    SELECT home_second_forward AS player \n    FROM match_view \n    WHERE home_team_goal > away_team_goal\n    \n    UNION ALL\n    \n    SELECT home_center_forward AS player \n    FROM match_view \n    WHERE home_team_goal > away_team_goal\n    \n    UNION ALL\n    \n    SELECT away_gk AS player \n    FROM match_view \n    WHERE away_team_goal > home_team_goal\n    \n    UNION ALL\n    \n    SELECT away_center_back_1 AS player \n    FROM match_view \n    WHERE away_team_goal > home_team_goal\n    \n    UNION ALL\n    \n    SELECT away_center_back_2 AS player \n    FROM match_view \n    WHERE away_team_goal > home_team_goal\n    \n    UNION ALL\n    \n    SELECT away_right_back AS player \n    FROM match_view \n    WHERE away_team_goal > home_team_goal\n    \n    UNION ALL\n    \n    SELECT away_left_back AS player \n    FROM match_view \n    WHERE away_team_goal > home_team_goal\n    \n    UNION ALL\n    \n    SELECT away_midfield_1 AS player \n    FROM match_view \n    WHERE away_team_goal > home_team_goal\n    \n    UNION ALL\n    \n    SELECT away_midfield_2 AS player \n    FROM match_view \n    WHERE away_team_goal > home_team_goal\n    \n    UNION ALL\n    \n    SELECT away_midfield_3 AS player \n    FROM match_view \n    WHERE away_team_goal > home_team_goal\n    \n    UNION ALL\n    \n    SELECT away_midfield_4 AS player \n    FROM match_view \n    WHERE away_team_goal > home_team_goal\n    \n    UNION ALL\n    \n    SELECT away_second_forward AS player \n    FROM match_view \n    WHERE away_team_goal > home_team_goal\n    \n    UNION ALL\n    \n    SELECT away_center_forward AS player \n    FROM match_view \n    WHERE away_team_goal > home_team_goal\n),\n\nloss_players AS\n(\n    SELECT home_gk AS player \n    FROM match_view \n    WHERE home_team_goal < away_team_goal\n    \n    UNION ALL\n    \n    SELECT home_center_back_1 AS player \n    FROM match_view \n    WHERE home_team_goal < away_team_goal\n    \n    UNION ALL\n    \n    SELECT home_center_back_2 AS player \n    FROM match_view \n    WHERE home_team_goal < away_team_goal\n    \n    UNION ALL\n    \n    SELECT home_right_back AS player \n    FROM match_view \n    WHERE home_team_goal < away_team_goal\n    \n    UNION ALL\n    \n    SELECT home_left_back AS player \n    FROM match_view \n    WHERE home_team_goal < away_team_goal\n    \n    UNION ALL\n    \n    SELECT home_midfield_1 AS player \n    FROM match_view \n    WHERE home_team_goal < away_team_goal\n    \n    UNION ALL\n    \n    SELECT home_midfield_2 AS player \n    FROM match_view \n    WHERE home_team_goal < away_team_goal\n    \n    UNION ALL\n    \n    SELECT home_midfield_3 AS player \n    FROM match_view \n    WHERE home_team_goal < away_team_goal\n    \n    UNION ALL\n    \n    SELECT home_midfield_4 AS player \n    FROM match_view \n    WHERE home_team_goal < away_team_goal\n    \n    UNION ALL\n    \n    SELECT home_second_forward AS player \n    FROM match_view \n    WHERE home_team_goal < away_team_goal\n    \n    UNION ALL\n    \n    SELECT home_center_forward AS player \n    FROM match_view \n    WHERE home_team_goal < away_team_goal\n    \n    UNION ALL\n    \n    SELECT away_gk AS player \n    FROM match_view \n    WHERE away_team_goal < home_team_goal\n    \n    UNION ALL\n    \n    SELECT away_center_back_1 AS player \n    FROM match_view \n    WHERE away_team_goal < home_team_goal\n    \n    UNION ALL\n    \n    SELECT away_center_back_2 AS player \n    FROM match_view \n    WHERE away_team_goal < home_team_goal\n    \n    UNION ALL\n    \n    SELECT away_right_back AS player \n    FROM match_view \n    WHERE away_team_goal < home_team_goal\n    \n    UNION ALL\n    \n    SELECT away_left_back AS player \n    FROM match_view \n    WHERE away_team_goal < home_team_goal\n    \n    UNION ALL\n    \n    SELECT away_midfield_1 AS player \n    FROM match_view \n    WHERE away_team_goal < home_team_goal\n    \n    UNION ALL\n    \n    SELECT away_midfield_2 AS player \n    FROM match_view \n    WHERE away_team_goal < home_team_goal\n    \n    UNION ALL\n    \n    SELECT away_midfield_3 AS player \n    FROM match_view \n    WHERE away_team_goal < home_team_goal\n    \n    UNION ALL\n    \n    SELECT away_midfield_4 AS player \n    FROM match_view \n    WHERE away_team_goal < home_team_goal\n    \n    UNION ALL\n    \n    SELECT away_second_forward AS player \n    FROM match_view \n    WHERE away_team_goal < home_team_goal\n    \n    UNION ALL\n    \n    SELECT away_center_forward AS player \n    FROM match_view \n    WHERE away_team_goal < home_team_goal\n)\n\n\nSELECT * FROM (\n    SELECT player \n    FROM win_players \n    GROUP BY player\n    HAVING player IS NOT NULL\n    ORDER BY COUNT(*) DESC \n    LIMIT 1\n)\n\nUNION ALL\n\nSELECT * FROM (\n    SELECT player \n    FROM loss_players \n    GROUP BY player\n    HAVING player IS NOT NULL\n    ORDER BY COUNT(*) DESC \n    LIMIT 1\n);",
        "external_knowledge": null,
        "plan": "1. Find all players who were part of a winning team, whether they were playing at home or away\n2. Calculate the total wins for each player and select the player with the most wins",
        "special_function": null
    },
    {
        "instance_id": "local228",
        "db": "IPL",
        "question": "Identify the top three batsmen with the most runs and the top three bowlers with the most wickets in each season, displaying them in the same row for each season. In case of ties, prioritize players with lower player_ids. Exclude 'run out', 'hit wicket', and 'retired hurt' as out_types for bowlers.",
        "SQL": "WITH season_bat AS (\n    SELECT \n        m.season_id AS season_year, \n        p.player_id, \n        SUM(bs.runs_scored) AS runs, \n        ROW_NUMBER() OVER (PARTITION BY m.season_id ORDER BY SUM(bs.runs_scored) DESC, p.player_id ASC) AS runs_row\n    FROM \n        match m\n    JOIN \n        ball_by_ball b ON m.match_id = b.match_id\n    JOIN \n        batsman_scored bs ON b.match_id = bs.match_id \n        AND b.over_id = bs.over_id \n        AND b.ball_id = bs.ball_id \n        AND b.innings_no = bs.innings_no\n    JOIN \n        player p ON p.player_id = b.striker\n    GROUP BY \n        m.season_id, p.player_id\n),\nseason_bowl AS (\n    SELECT \n        m.season_id AS season_year, \n        p.player_id, \n        COUNT(wt.player_out) AS wickets,\n        ROW_NUMBER() OVER (PARTITION BY m.season_id ORDER BY COUNT(wt.player_out) DESC, p.player_id ASC) AS wickets_row\n    FROM \n        match m\n    JOIN \n        ball_by_ball b ON m.match_id = b.match_id\n    JOIN \n        wicket_taken wt ON b.match_id = wt.match_id \n        AND b.over_id = wt.over_id \n        AND b.ball_id = wt.ball_id \n        AND b.innings_no = wt.innings_no\n    JOIN \n        player p ON p.player_id = b.bowler\n    WHERE \n        wt.kind_out NOT IN ('run out', 'hit wicket', 'retired hurt')\n    GROUP BY \n        m.season_id, p.player_id\n)\nSELECT \n    sbat.season_year AS 'season id', \n    sbat.player_id AS batsman, \n    sbat.runs, \n    sbowl.player_id AS bowler, \n    sbowl.wickets\nFROM \n    season_bat sbat\nJOIN \n    season_bowl sbowl ON sbat.season_year = sbowl.season_year AND sbat.runs_row = sbowl.wickets_row\nWHERE \n    sbat.runs_row <= 3 AND sbowl.wickets_row <= 3\nORDER BY \n    sbat.season_year, sbat.runs_row;",
        "external_knowledge": null,
        "plan": "1. **Identify Top Batsmen for Each Season:**\n   - Create a temporary result set that calculates the total runs scored by each player in each season.\n   - For each player in each season, assign a rank based on the total runs scored, prioritizing players with lower IDs in case of ties.\n\n2. **Identify Top Bowlers for Each Season:**\n   - Create another temporary result set that calculates the total wickets taken by each player in each season, excluding specific types of outs.\n   - For each player in each season, assign a rank based on the total wickets taken, prioritizing players with lower IDs in case of ties.\n\n3. **Filter Top Batsmen and Bowlers:**\n   - From the temporary result sets, select only the top three batsmen and top three bowlers for each season based on their ranks.\n\n4. **Combine Results:**\n   - Join the filtered lists of top batsmen and top bowlers by matching the season and the rank.\n\n5. **Output the Results:**\n   - Select the season, player IDs of the top batsmen and bowlers, along with their respective total runs and wickets.\n   - Order the results by season and rank to ensure the output is organized and easy to read.",
        "special_function": null
    },
    {
        "instance_id": "local229",
        "db": "IPL",
        "question": "Find the IDs of players who scored the highest number of partnership runs for each match. The output should include the IDs of two players, each with their individual scores and the total partnership score. For each pair, the player with the higher individual score should be listed as player 1, and the player with the lower score as player 2. In cases where both players have the same score, the player with the higher ID should be player 1, and the player with the lower ID should be player 2. There can be multiple rows for a single match.",
        "SQL": "WITH ball_extend AS (\n    SELECT \n        b.*,\n        CASE \n            WHEN b.striker < b.non_striker THEN b.striker \n            ELSE b.non_striker \n        END AS u1,\n        CASE \n            WHEN b.striker > b.non_striker THEN b.striker \n            ELSE b.non_striker \n        END AS u2\n    FROM \n        ball_by_ball b\n),\nmatch_part AS (\n    SELECT \n        be.match_id, \n        be.u1, \n        be.u2, \n        SUM(bs.runs_scored) AS runs,\n        RANK() OVER(PARTITION BY be.match_id ORDER BY SUM(bs.runs_scored) DESC) AS prank\n    FROM \n        ball_extend be\n    JOIN \n        batsman_scored bs ON be.match_id = bs.match_id \n        AND be.over_id = bs.over_id \n        AND be.ball_id = bs.ball_id \n        AND be.innings_no = bs.innings_no\n    GROUP BY \n        be.match_id, be.u1, be.u2\n    ORDER BY \n        be.match_id, SUM(bs.runs_scored) DESC\n),\nmatch_part2 AS (\n    SELECT \n        b.match_id, \n        mp.u1, \n        mp.u2, \n        mp.runs, \n        mp.prank,\n        SUM(bs.runs_scored) AS u1_runs,\n        (mp.runs - SUM(bs.runs_scored)) AS u2_runs\n    FROM \n        match_part mp\n    JOIN \n        ball_by_ball b ON mp.match_id = b.match_id \n    JOIN \n        batsman_scored bs ON b.match_id = bs.match_id \n        AND b.over_id = bs.over_id \n        AND b.ball_id = bs.ball_id \n        AND b.innings_no = bs.innings_no\n    WHERE \n        mp.u1 = b.striker \n        AND mp.u2 = b.non_striker\n    GROUP BY \n        b.match_id, mp.u1, mp.u2, mp.runs, mp.prank\n),\nmatch_part3 AS (\n    SELECT \n        match_id,\n        CASE \n            WHEN u1_runs > u2_runs THEN u1 \n            WHEN u1_runs = u2_runs THEN (CASE WHEN u1 > u2 THEN u1 ELSE u2 END)\n            ELSE u2 \n        END AS player1_id,\n        CASE \n            WHEN u1_runs < u2_runs THEN u1 \n            WHEN u1_runs = u2_runs THEN (CASE WHEN u1 > u2 THEN u2 ELSE u1 END)\n            ELSE u2 \n        END AS player2_id,\n        CASE \n            WHEN u1_runs >= u2_runs THEN u1_runs \n            ELSE u2_runs \n        END AS runs1,\n        CASE \n            WHEN u1_runs <= u2_runs THEN u1_runs \n            ELSE u2_runs \n        END AS runs2,\n        runs, prank\n    FROM \n        match_part2\n)\nSELECT \n    match_id, \n    player1_id, \n    player2_id, \n    runs1, \n    runs2, \n    runs AS pship_runs \nFROM \n    match_part3\nWHERE \n    prank <= 1\nORDER BY \n    pship_runs DESC, match_id ASC;",
        "external_knowledge": null,
        "plan": "1. **Data Preparation**: Extend the original data by adding two new columns that ensure the smaller ID is always in the first new column and the larger ID in the second new column. This helps in standardizing the player pairings.\n\n2. **Calculate Partnership Runs**:\n    - Create a summarized dataset that groups the data by match and player pairs.\n    - For each group, calculate the total partnership runs and assign a rank based on the total runs within each match.\n\n3. **Detailed Scoring Information**:\n    - Join the summarized dataset with the original data to get individual run contributions for each player in the pair.\n    - Calculate the individual scores for the two players in each partnership and retain the total partnership runs and rank.\n\n4. **Normalize Player IDs**:\n    - For each partnership, determine the order of player IDs based on their individual scores. Ensure the player with the higher score is consistently listed first.\n    - If the scores are tied, use the player IDs to determine the order.\n\n5. **Final Output**:\n    - Filter the results to include only the top-ranked partnerships (i.e., the partnerships with the highest total runs) for each match.\n    - Select and display the match ID, player IDs in the correct order, their individual scores, and the total partnership runs.\n    - Sort the final output by total partnership runs in descending order and by match ID in ascending order.",
        "special_function": null
    },
    {
        "instance_id": "local244",
        "db": "music",
        "question": "Calculate the duration of each track, classify them as short, medium, or long, output the minimum and maximum time for each kind (in minutes) and the total revenue for each category, group by the category.",
        "SQL": "WITH temp_t1 AS (\n    SELECT \n        MIN(Milliseconds) AS Limit1,\n        AVG(Milliseconds),\n        (AVG(Milliseconds) + MIN(Milliseconds)) / 2 AS Limit2,\n        (MAX(Milliseconds) + AVG(Milliseconds)) / 2 AS Limit3,\n        MAX(Milliseconds) AS Limit4\n    FROM Track\n),\ncateg AS (\n    SELECT \n        TrackId,\n        CASE \n            WHEN t.Milliseconds < (SELECT Limit2 FROM temp_t1) THEN 'Short'\n            WHEN t.Milliseconds < (SELECT Limit3 FROM temp_t1) THEN 'Medium'\n            WHEN t.Milliseconds <= (SELECT Limit4 FROM temp_t1) THEN 'Long'\n        END AS LengthCateg\n    FROM Track t \n)\nSELECT \n    CASE \n        WHEN c.LengthCateg = 'Short' THEN (SELECT Limit1 / 60000.0 FROM temp_t1)\n        WHEN c.LengthCateg = 'Medium' THEN (SELECT Limit2 / 60000.0 FROM temp_t1)\n        WHEN c.LengthCateg = 'Long' THEN (SELECT Limit3 / 60000.0 FROM temp_t1)\n    END AS From_Minutes,\n    CASE \n        WHEN c.LengthCateg = 'Short' THEN (SELECT Limit2 / 60000.0 FROM temp_t1)\n        WHEN c.LengthCateg = 'Medium' THEN (SELECT Limit3 / 60000.0 FROM temp_t1)\n        WHEN c.LengthCateg = 'Long' THEN (SELECT Limit4 / 60000.0 FROM temp_t1)\n    END AS To_Minutes,\n    c.LengthCateg,\n    SUM(i.UnitPrice * i.Quantity) AS TotalPrice\nFROM categ c\nJOIN InvoiceLine i ON c.TrackId = i.TrackId\nGROUP BY c.LengthCateg\nHAVING c.LengthCateg IS NOT NULL\nORDER BY TotalPrice;",
        "external_knowledge": "music_length_type.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local253",
        "db": "education_business",
        "question": "For the salary dataset, create a detailed SQL report that compares the top 5 companies by average salary in Mumbai, Pune, New Delhi, and Hyderabad to the national average salary. The report should include the following columns: Location, Company Name, Average Salary in State, and Average Salary in Country.",
        "SQL": "WITH salary_cleaned AS (\n    SELECT \n        *,\n        REPLACE(REPLACE(REPLACE(Salary, '/yr', ''), '\u20b9', ''), ',', '') AS salary_clean\n    FROM SalaryDataset\n),\nstate_avg_salary AS (\n    SELECT \n        location,\n        companyname,\n        AVG(CAST(salary_clean AS REAL)) AS avg_sal_state\n    FROM salary_cleaned\n    WHERE location IN ('Mumbai', 'Pune', 'New Delhi', 'Hyderabad')\n    GROUP BY location, companyname\n),\nstate_ranked AS (\n    SELECT \n        location,\n        companyname,\n        avg_sal_state,\n        ROW_NUMBER() OVER (PARTITION BY location ORDER BY avg_sal_state DESC) AS row_number\n    FROM state_avg_salary\n),\nstate_top5 AS (\n    SELECT \n        location,\n        companyname,\n        avg_sal_state\n    FROM state_ranked\n    WHERE row_number <= 5\n),\ncountry_avg_salary AS (\n    SELECT \n        companyname,\n        ROUND(AVG(CAST(salary_clean AS REAL)), 2) AS avg_salary_final\n    FROM salary_cleaned\n    GROUP BY companyname\n)\nSELECT \n    state_top5.location,\n    state_top5.companyname,\n    state_top5.avg_sal_state,\n    country_avg_salary.avg_salary_final AS avg_salary_country\nFROM state_top5\nJOIN country_avg_salary ON state_top5.companyname = country_avg_salary.companyname",
        "external_knowledge": null,
        "plan": "1. **Clean Salary Data**:\n   - Create a temporary dataset where the salary values are cleaned by removing specific characters and symbols to make them numeric.\n\n2. **Calculate State Average Salary**:\n   - From the cleaned salary data, compute the average salary for each company located in specified cities.\n   - Group the results by location and company.\n\n3. **Rank Companies by State Average Salary**:\n   - Assign a rank to each company within each location based on the computed average salary, ordered from highest to lowest.\n\n4. **Filter Top 5 Companies by Location**:\n   - From the ranked companies, select only the top 5 companies for each location.\n\n5. **Calculate National Average Salary**:\n   - Compute the average salary for each company across the entire dataset to get the national average.\n\n6. **Compile Final Report**:\n   - Join the top 5 companies per location with their corresponding national average salary.\n   - The final report includes columns for location, company name, average salary in the location, and average salary across the country.",
        "special_function": null
    },
    {
        "instance_id": "local258",
        "db": "IPL",
        "question": "What are the total wickets taken by each bowler, their economy rate, their strike rate, and their best performance in a single match (most wickets taken, in the format \"wickets-runs\")? Ignore the extra runs data.",
        "SQL": "WITH wickets_table AS (\n    SELECT \n        bowler,\n        pl.player_name AS player_name,\n        COUNT(kind_out) AS wickets \n    FROM wicket_taken wkt\n    JOIN ball_by_ball byb ON byb.match_id = wkt.match_id \n        AND byb.over_id = wkt.over_id \n        AND byb.ball_id = wkt.ball_id \n        AND byb.innings_no = wkt.innings_no\n    JOIN player pl ON byb.bowler = pl.player_id\n    GROUP BY player_name\n),\nballs_table AS (\n    SELECT \n        bowler,\n        COUNT(ball_id) AS Balls_Bowled \n    FROM ball_by_ball\n    GROUP BY bowler\n),\neconomy_table AS (\n    SELECT\n        a.bowler,\n        COUNT(DISTINCT a.match_id) AS Innings_Bowled,\n        a.bowler_name,\n        a.role,\n        SUM(a.runs) AS runs_given\n    FROM (\n        SELECT\n            byb.match_id AS match_id,\n            byb.over_id AS over_id,\n            byb.ball_id AS ball_id,\n            byb.innings_no AS innings_no,\n            byb.team_batting AS team_batting,\n            striker,\n            non_striker,\n            bowler,\n            pm.role,\n            batting_hand AS batting_hand,\n            pl.player_name AS bowler_name,\n            runs_scored AS runs \n        FROM ball_by_ball byb\n        JOIN batsman_scored bsco ON byb.ball_id = bsco.ball_id \n            AND byb.match_id = bsco.match_id \n            AND byb.over_id = bsco.over_id \n            AND byb.innings_no = bsco.innings_no\n        JOIN player_match pm ON bsco.match_id = pm.match_id\n        JOIN player pl ON byb.bowler = pl.player_id\n        GROUP BY striker, byb.match_id, byb.over_id, byb.ball_id, bsco.innings_no\n        ORDER BY bsco.innings_no ASC\n    ) a\n    GROUP BY a.bowler_name\n    ORDER BY a.bowler\n),\nbest_bowling_table AS (\n    SELECT\n        a.match_id,\n        a.bowler,\n        MAX(a.wickets),\n        a.runs_given,\n        MAX(a.wickets) || '-' || a.runs_given AS Best_Bowling_figure\n    FROM (\n        SELECT\n            a.match_id,\n            a.bowler,\n            a.wickets,\n            a.runs_given\n        FROM (\n            SELECT \n                byb.match_id, \n                bowler, \n                COUNT(byb.ball_id) AS wickets,\n                SUM(bs.runs_scored) AS runs_given\n            FROM ball_by_ball byb\n            JOIN wicket_taken wkt ON byb.match_id = wkt.match_id \n                AND byb.over_id = wkt.over_id \n                AND byb.ball_id = wkt.ball_id \n                AND byb.innings_no = wkt.innings_no\n            JOIN batsman_scored bs ON bs.match_id = byb.match_id\n            \tAND byb.over_id = bs.over_id\n            \tAND byb.ball_id = bs.ball_id\n            \tAND byb.innings_no = bs.innings_no\n            GROUP BY byb.match_id, bowler\n        )a )a\n    GROUP BY a.bowler\n    ORDER BY a.wickets DESC\n)\nSELECT\n    c.bowler,\n    c.player_name,\n    c.wickets,\n    c.economy_rate,\n    c.bowler_strike_rate,\n    best_bowling_table.Best_Bowling_figure\nFROM (\n    SELECT \n        a.bowler,\n        a.player_name,\n        a.wickets,\n        balls_table.Balls_Bowled AS Balls_bowled,\n        economy_table.runs_given AS runs_given,\n        6 * (1.0 * economy_table.runs_given / balls_table.Balls_Bowled) AS economy_rate,\n        1.0 * balls_table.Balls_Bowled / a.wickets AS bowler_strike_rate\n    FROM wickets_table a\n    JOIN balls_table ON a.bowler = balls_table.bowler\n    JOIN economy_table ON a.bowler = economy_table.bowler\n    ORDER BY a.wickets DESC\n) c\nJOIN best_bowling_table ON c.bowler = best_bowling_table.bowler\nORDER BY wickets DESC",
        "external_knowledge": "baseball_game_special_words_definition.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local259",
        "db": "IPL",
        "question": "For each player, list their ID, name, most frequent role across all matches, batting hand, bowling skill, total runs scored, total matches played, total dismissals, batting average, highest score in a single match, number of matches where their score exceeded 30, 50, and 100, total balls faced in their career, strike rate, total wickets taken, economy rate, and their best performance in a single match (most wickets taken, in the format \"wickets taken-runs given\"). Ignore the extra runs data.",
        "SQL": "WITH player_role_count AS (\n    SELECT \n        pm.player_id AS striker,\n        pm.role,\n        COUNT(pm.role) AS role_count,\n        ROW_NUMBER() OVER (PARTITION BY pm.player_id ORDER BY COUNT(pm.role) DESC) AS rn\n    FROM player_match pm\n    GROUP BY pm.player_id, pm.role\n),\ntop_role AS (\n    SELECT \n        striker,\n        role\n    FROM player_role_count\n    WHERE rn = 1\n)\n-- The rest of your original query, using \"top_role\" to get the primary role\nSELECT \n    s.striker,\n    s.striker_name,\n    top_role.role,  -- Use the most frequent role from top_role table\n    s.batting_hand,\n    bowling_style_table.bowling_skill,\n    s.runs,\n    s.matches,\n    s.dismissals,\n    1.0 * s.runs / s.dismissals AS batt_avg,\n    s.Highest_score,\n    s.thirties, \n    s.fifties,\n    s.hundreds,\n    s.balls_faced_career,\n    s.strike_rate,\n    s.wickets_taken,\n    s.economy_rate,\n    s.best_bowling_figure\nFROM (\n    SELECT\n        f.striker,\n        f.striker_name,\n        f.batting_hand,\n        f.runs,\n        f.matches,\n        f.dismissals,\n        1.0 * f.runs / f.dismissals AS batt_avg,\n        f.Highest_score,\n        f.thirties, \n        f.fifties,\n        f.hundreds,\n        f.balls_faced_career,\n        f.strike_rate,\n        f.wickets_taken,\n        bowler_table.economy_rate AS economy_rate,\n        bowler_table.bowler_strike_rate AS strike_rate,\n        bowler_table.best_bowling_figure AS best_bowling_figure\n    FROM (\n        SELECT \n            g.striker,\n            g.striker_name,\n            g.batting_hand,\n            g.runs,\n            g.matches,\n            g.dismissals,\n            1.0 * g.runs / g.dismissals AS batt_avg,\n            g.Highest_score,\n            g.thirties, \n            g.fifties,\n            g.hundreds,\n            g.balls_faced_career,\n            g.strike_rate,\n            bowlers_table.wickets AS wickets_taken\n        FROM (\n            SELECT\n                f.striker,\n                f.striker_name,\n                f.batting_hand,\n                f.runs,\n                f.matches,\n                dissmissals_table.dismissals AS dismissals,\n                1.0 * f.runs / dissmissals_table.dismissals AS batt_avg,\n                f.Highest_score,\n                f.thirties, \n                f.fifties,\n                f.hundreds,\n                f.balls_faced_career,\n                f.strike_rate\n            FROM (\n                SELECT \n                    e.striker,\n                    e.striker_name,\n                    e.batting_hand,\n                    e.runs,\n                    e.matches,\n                    1.0 * e.runs / e.matches AS batt_avg,\n                    e.Highest_score,\n                    e.thirties, \n                    e.fifties,\n                    e.hundreds,\n                    SUM(ball_face_by_batsman_per_match) AS balls_faced_career,\n                    100 * (1.0 * e.runs / SUM(ball_face_by_batsman_per_match)) AS strike_rate\n                FROM (\n                    SELECT\n                        d.striker,\n                        d.striker_name,\n                        d.batting_hand,\n                        d.runs,\n                        d.matches,\n                        100.0 * d.runs / d.matches AS batt_avg,\n                        MAX(d.max_individual_score_per_match) AS Highest_score,\n                        d.thirties, \n                        d.fifties,\n                        d.hundreds\n                    FROM (\n                        SELECT \n                            c.striker,\n                            c.striker_name,\n                            c.batting_hand,\n                            c.runs,\n                            c.matches,\n                            c.max_individual_score_per_match,\n                            fifties_hundreds_table.thirties, \n                            fifties_hundreds_table.fifties,\n                            fifties_hundreds_table.hundreds\n                        FROM (\n                            SELECT \n                                runs_table.striker,\n                                runs_table.striker_name,\n                                runs_table.matches,\n                                runs_table.batting_hand,\n                                runs_table.runs,\n                                highest_score.runs AS max_individual_score_per_match\n                            FROM (\n                                SELECT \n                                    * \n                                FROM (\n                                    SELECT\n                                        a.striker,\n                                        COUNT(DISTINCT a.match_id) AS Matches,\n                                        a.striker_name,\n                                        a.batting_hand,\n                                        SUM(a.runs) AS runs\n                                    FROM (\n                                        SELECT\n                                            byb.match_id AS match_id,\n                                            byb.over_id AS over_id,\n                                            byb.ball_id AS ball_id,\n                                            byb.innings_no AS innings_no,\n                                            byb.team_batting AS team_batting,\n                                            striker,\n                                            non_striker,\n                                            bowler,\n                                            batting_hand AS batting_hand,\n                                            pl.player_name AS striker_name,\n                                            runs_scored AS runs\n                                        FROM ball_by_ball byb\n                                        JOIN batsman_scored bsco ON byb.ball_id = bsco.ball_id \n                                            AND byb.match_id = bsco.match_id \n                                            AND byb.over_id = bsco.over_id \n                                            AND byb.innings_no = bsco.innings_no\n                                        JOIN player_match pm ON bsco.match_id = pm.match_id\n                                        JOIN player pl ON byb.striker = pl.player_id\n                                        GROUP BY\n                                            striker,\n                                            byb.match_id,\n                                            byb.over_id,\n                                            byb.ball_id\n                                        ORDER BY bsco.innings_no ASC\n                                    ) a\n                                    GROUP BY a.striker_name\n                                    ORDER BY a.striker\n                                ) b\n                                GROUP BY b.striker_name, b.striker, b.matches\n                                ORDER BY b.runs DESC\n                            ) runs_table\n                            JOIN (\n                                SELECT \n                                    * \n                                FROM (\n                                    SELECT \n                                        striker,\n                                        bs.match_id,\n                                        player_name,\n                                        byb.innings_no, \n                                        SUM(runs_scored) AS runs \n                                    FROM ball_by_ball byb\n                                    JOIN batsman_scored bs ON byb.match_id = bs.match_id \n                                        AND byb.over_id = bs.over_id \n                                        AND byb.ball_id = bs.ball_id \n                                        AND byb.innings_no = bs.innings_no\n                                    JOIN player pl ON byb.striker = pl.player_id\n                                    GROUP BY \n                                        bs.match_id,\n                                        player_name\n                                    ORDER BY byb.innings_no\n                                ) a\n                                ORDER BY a.runs DESC\n                            ) highest_score ON runs_table.striker = highest_score.striker \n                                AND runs_table.striker = highest_score.striker\n                        ) c\n                        JOIN (\n                            SELECT \n                                b.striker,\n                                b.player_name,\n                                SUM(b.thirties) AS thirties,\n                                SUM(b.fifties) AS fifties,\n                                SUM(b.hundreds) AS hundreds \n                            FROM (\n                                SELECT *,\n                                    (CASE WHEN a.runs >= 30 THEN 1 ELSE 0 END) AS thirties,\n                                    (CASE WHEN a.runs >= 50 THEN 1 ELSE 0 END) AS fifties,\n                                    (CASE WHEN a.runs >= 100 THEN 1 ELSE 0 END) AS hundreds \n                                FROM (\n                                    SELECT \n                                        striker,\n                                        bs.match_id,\n                                        player_name,\n                                        byb.innings_no, \n                                        SUM(runs_scored) AS runs \n                                    FROM ball_by_ball byb\n                                    JOIN batsman_scored bs ON byb.match_id = bs.match_id \n                                        AND byb.over_id = bs.over_id \n                                        AND byb.ball_id = bs.ball_id \n                                        AND byb.innings_no = bs.innings_no\n                                    JOIN player pl ON byb.striker = pl.player_id\n                                    GROUP BY \n                                        bs.match_id,\n                                        player_name\n                                    ORDER BY byb.innings_no\n                                ) a\n                                ORDER BY a.runs DESC\n                            ) b\n                            GROUP BY b.player_name\n                            ORDER BY b.striker\n                        ) fifties_hundreds_table ON c.striker = fifties_hundreds_table.striker\n                    ) d\n                    GROUP BY d.striker_name\n                    ORDER BY d.matches DESC\n                ) e\n                JOIN (\n                    SELECT \n                        match_id,\n                        striker,\n                        COUNT(ball_id) AS ball_face_by_batsman_per_match \n                    FROM ball_by_ball\n                    GROUP BY striker, match_id\n                ) ball_faced_table ON e.striker = ball_faced_table.striker\n                GROUP BY e.striker\n            ) f\n            JOIN (\n                SELECT \n                    * \n                FROM (\n                    SELECT \n                        player_out,\n                        COUNT(kind_out) AS dismissals \n                    FROM wicket_taken wkt\n                    GROUP BY player_out\n                ) a\n            ) dissmissals_table ON f.striker = dissmissals_table.player_out\n            GROUP BY f.striker\n        ) g\n        JOIN (\n            SELECT \n                * \n            FROM (\n                SELECT \n                    bowler,\n                    pl.player_name AS player_name,\n                    COUNT(kind_out) AS wickets \n                FROM wicket_taken wkt\n                JOIN ball_by_ball byb ON byb.match_id = wkt.match_id \n                    AND byb.over_id = wkt.over_id \n                    AND byb.ball_id = wkt.ball_id \n                    AND byb.innings_no = wkt.innings_no\n                JOIN player pl ON byb.bowler = pl.player_id\n                GROUP BY player_name\n            ) a\n            ORDER BY a.wickets DESC\n        ) bowlers_table ON g.striker = bowlers_table.bowler\n        ORDER BY g.striker\n    ) f\n    JOIN (\n        SELECT\n            c.bowler,\n            c.player_name,\n            c.wickets,\n            c.economy_rate,\n            c.bowler_strike_rate,\n            best_bowling_table.best_bowling_figure\n        FROM (\n            SELECT \n                a.bowler,\n                a.player_name,\n                a.wickets,\n                balls_table.balls_bowled AS Balls_bowled,\n                economy_table.runs_given AS runs_given,\n                6 * (1.0 * economy_table.runs_given / balls_table.balls_bowled) AS economy_rate,\n                1.0 * balls_table.balls_bowled / a.wickets AS bowler_strike_rate\n            FROM (\n                SELECT \n                    bowler,\n                    pl.player_name AS player_name,\n                    COUNT(kind_out) AS wickets \n                FROM wicket_taken wkt\n                JOIN ball_by_ball byb ON byb.match_id = wkt.match_id \n                    AND byb.over_id = wkt.over_id \n                    AND byb.ball_id = wkt.ball_id \n                    AND byb.innings_no = wkt.innings_no\n                JOIN player pl ON byb.bowler = pl.player_id\n                GROUP BY player_name\n            ) a\n            JOIN (\n                SELECT \n                    bowler,\n                    COUNT(ball_id) AS Balls_Bowled \n                FROM ball_by_ball\n                GROUP BY bowler\n            ) balls_table ON a.bowler = balls_table.bowler\n            JOIN (\n                SELECT \n                    * \n                FROM (\n                    SELECT\n                        a.bowler,\n                        COUNT(DISTINCT a.match_id) AS Innings_Bowled,\n                        a.bowler_name,\n                        SUM(a.runs) AS runs_given\n                    FROM (\n                        SELECT\n                            byb.match_id AS match_id,\n                            byb.over_id AS over_id,\n                            byb.ball_id AS ball_id,\n                            byb.innings_no AS innings_no,\n                            byb.team_batting AS team_batting,\n                            striker,\n                            non_striker,\n                            bowler,\n                           \tbatting_hand AS batting_hand,\n                            pl.player_name AS bowler_name,\n                            runs_scored AS runs \n                        FROM ball_by_ball byb\n                        JOIN batsman_scored bsco ON byb.ball_id = bsco.ball_id \n                            AND byb.match_id = bsco.match_id \n                            AND byb.over_id = bsco.over_id \n                            AND byb.innings_no = bsco.innings_no\n                        JOIN player_match pm ON bsco.match_id = pm.match_id\n                        JOIN player pl ON byb.bowler = pl.player_id\n                        GROUP BY\n                            striker,\n                            byb.match_id,\n                            byb.over_id,\n                            byb.ball_id,\n                            bsco.innings_no\n                        ORDER BY bsco.innings_no ASC\n                    ) a\n                    GROUP BY a.bowler_name\n                    ORDER BY a.bowler\n                ) b\n                ORDER BY b.runs_given DESC\n            ) economy_table ON a.bowler = economy_table.bowler\n            ORDER BY a.wickets DESC\n        ) c\n        JOIN (\n            SELECT\n                a.match_id,\n                a.bowler,\n                MAX(a.wickets) AS max_wickets,\n                a.runs_given,\n                MAX(a.wickets) || '-' || a.runs_given AS Best_Bowling_figure\n            FROM (\n                SELECT\n                    wt.match_id,\n                    wt.bowler,\n                    wt.wickets,\n                    rt.runs_given\n                FROM (\n                    SELECT \n                        byb.match_id,\n                        bowler,\n                        COUNT(byb.ball_id) AS wickets \n                    FROM ball_by_ball byb\n                    JOIN wicket_taken wkt ON byb.match_id = wkt.match_id \n                        AND byb.over_id = wkt.over_id \n                        AND byb.ball_id = wkt.ball_id \n                        AND byb.innings_no = wkt.innings_no\n                    GROUP BY byb.match_id, bowler\n                ) wt\n                JOIN (\n                    SELECT \n                        byb.match_id,\n                        bowler,\n                        SUM(runs_scored) AS runs_given \n                    FROM ball_by_ball byb\n            \t\tJOIN wicket_taken wkt ON byb.match_id = wkt.match_id \n                \t\tAND byb.over_id = wkt.over_id \n                \t\tAND byb.ball_id = wkt.ball_id \n                \t\tAND byb.innings_no = wkt.innings_no\n                    JOIN batsman_scored bs ON byb.match_id = bs.match_id \n                        AND byb.over_id = bs.over_id \n                        AND byb.ball_id = bs.ball_id \n                        AND byb.innings_no = bs.innings_no\n                    GROUP BY bs.match_id, bowler\n                ) rt ON rt.match_id = wt.match_id\n            ) a\n            GROUP BY a.bowler\n            ORDER BY a.wickets DESC\n        ) best_bowling_table ON c.bowler = best_bowling_table.bowler\n    ) bowler_table ON f.striker_name = bowler_table.player_name\n) s\nJOIN top_role ON s.striker = top_role.striker  -- Join to get the most frequent role\nJOIN (\n    SELECT \n        player_name,\n        bowling_skill \n    FROM player p\n) bowling_style_table ON s.striker_name = bowling_style_table.player_name;",
        "external_knowledge": "baseball_game_special_words_definition.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local262",
        "db": "stacking",
        "question": "Which models have more instances where traditional models perform worse than the Stack model than the total times these models were evaluated across all steps and versions?",
        "SQL": "WITH total AS (\n    SELECT \n        name, COUNT(*) AS count\n    FROM\n        solution\n    GROUP BY name\n),\nstep_counts AS (\n    SELECT \n        A.name,\n        SUM(CASE WHEN A.step = 1 AND A.max_test_score < B.stack_score THEN 1 ELSE 0 END) AS count_step_1,\n        SUM(CASE WHEN A.step = 2 AND A.max_test_score < B.stack_score THEN 1 ELSE 0 END) AS count_step_2,\n        SUM(CASE WHEN A.step = 3 AND A.max_test_score < B.stack_score THEN 1 ELSE 0 END) AS count_step_3\n    FROM (\n        SELECT \n            name, version, step, MAX(test_score) AS max_test_score\n        FROM\n            model_score\n        WHERE\n            model NOT IN ('Stack')\n        GROUP BY name, version, step\n    ) AS A\n    INNER JOIN (\n        SELECT \n            name, version, step, test_score AS stack_score\n        FROM\n            model_score\n        WHERE\n            model = 'Stack'\n    ) AS B ON A.name = B.name AND A.version = B.version AND A.step = B.step\n    GROUP BY A.name\n)\nSELECT \n    total.name AS problem\nFROM \n    total\nINNER JOIN \n    step_counts ON total.name = step_counts.name\nWHERE \n    (count_step_1 + count_step_2 + count_step_3) > total.count;",
        "external_knowledge": null,
        "plan": "1. **Calculate Total Evaluations**:\n   - Create a subquery to determine the total number of evaluations for each model by counting all the evaluations, grouping by the model's name.\n\n2. **Identify Performance Per Step**:\n   - Create another subquery to identify, for each model (excluding a specific one), the number of times it performs worse than the specific model in each step.\n   - This involves:\n     - Filtering and grouping the model data by model name, version, and step, and calculating the maximum score for each step.\n     - Joining this data with the scores of the specific model for the corresponding name, version, and step.\n     - Summing up the cases where the traditional model's maximum score is less than the specific model's score for each of the steps.\n\n3. **Aggregate Poor Performances**:\n   - Sum the counts of poor performances across all steps for each model.\n\n4. **Compare and Filter**:\n   - Join the total evaluations data with the aggregated poor performance data based on the model name.\n   - Filter out models where the sum of poor performances across all steps is greater than the total number of evaluations of that model.\n\n5. **Select Models**:\n   - Select the names of the models that meet the condition identified in the previous step.",
        "special_function": null
    },
    {
        "instance_id": "local263",
        "db": "stacking",
        "question": "Which L1_model has the highest occurrence for each status ('strong,' where the maximum test score for non-'Stack' models is less than the 'Stack' score, and 'soft,' where it equals the 'Stack' score), and how many times does it occur?",
        "SQL": "WITH model_scores AS (\n    SELECT \n        name, \n        version, \n        step, \n        MAX(CASE WHEN model <> 'Stack' THEN test_score END) AS max_test_score,\n        MAX(CASE WHEN model = 'Stack' THEN test_score END) AS stack_score\n    FROM model_score\n    GROUP BY name, version, step\n),\ncombined AS (\n    SELECT \n        A.name, \n        A.version, \n        A.step, \n        C.L1_model, \n        CASE \n            WHEN A.max_test_score < A.stack_score THEN 'strong'\n            WHEN A.max_test_score = A.stack_score THEN 'soft'\n        END AS status\n    FROM model_scores A\n    INNER JOIN model C ON A.name = C.name AND A.version = C.version\n    WHERE A.max_test_score IS NOT NULL AND A.stack_score IS NOT NULL\n),\nfrequency AS (\n    SELECT \n        L1_model, \n        status, \n        COUNT(*) AS cnt\n    FROM combined\n    GROUP BY L1_model, status\n),\nmax_frequency AS (\n    SELECT \n        status, \n        MAX(cnt) AS max_cnt\n    FROM frequency\n    GROUP BY status\n)\nSELECT \n    f.status,\n    f.L1_model,\n    m.max_cnt\nFROM frequency f\nINNER JOIN max_frequency m ON f.status = m.status AND f.cnt = m.max_cnt\nORDER BY f.status;",
        "external_knowledge": null,
        "plan": " ",
        "special_function": null
    },
    {
        "instance_id": "local264",
        "db": "stacking",
        "question": "Which model category (L1_model) appears the most frequently across all steps and versions when comparing traditional models to the Stack model, and what is the total count of its occurrences?",
        "SQL": "WITH model_scores AS (\n    SELECT \n        DISTINCT name, \n        version, \n        step\n    FROM \n        model_score\n),\ncombined AS (\n    SELECT \n        C.L1_model\n    FROM \n        model_scores A\n    INNER JOIN model C ON A.name = C.name AND A.version = C.version\n),\nfrequency AS (\n    SELECT \n        L1_model, \n        COUNT(*) AS cnt\n    FROM \n        combined \n    GROUP BY \n        L1_model\n),\nmax_frequency AS (\n    SELECT \n        MAX(cnt) AS max_cnt\n    FROM \n        frequency\n)\nSELECT\n    f.L1_model,\n    max_cnt\nFROM \n    frequency f\nINNER JOIN max_frequency m ON f.cnt = m.max_cnt;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local269",
        "db": "oracle_sql",
        "question": "What is the average total quantity across all final packaging combinations, considering all items contained within each combination?",
        "SQL": "WITH RECURSIVE recursive_pr (root_id, packaging_id, contains_id, qty, lvl) AS (\n    SELECT\n        pr.packaging_id AS root_id,\n        pr.packaging_id,\n        pr.contains_id,\n        pr.qty,\n        1 AS lvl\n    FROM packaging_relations pr\n    WHERE pr.packaging_id NOT IN (\n        SELECT c.contains_id FROM packaging_relations c\n    )\n    UNION ALL\n    SELECT\n        rpr.root_id,\n        pr.packaging_id,\n        pr.contains_id,\n        rpr.qty * pr.qty AS qty,\n        rpr.lvl + 1 AS lvl\n    FROM recursive_pr rpr\n    JOIN packaging_relations pr ON pr.packaging_id = rpr.contains_id\n),\nranked_recursive_pr AS (\n    SELECT\n        rpr.*,\n        ROW_NUMBER() OVER (PARTITION BY rpr.root_id ORDER BY rpr.lvl) AS rpr_order\n    FROM recursive_pr rpr\n),\nleaf AS (\n    SELECT\n        rrp.*,\n        CASE\n            WHEN COALESCE(\n                (SELECT MIN(lvl) FROM ranked_recursive_pr WHERE root_id = rrp.root_id AND lvl > rrp.lvl),\n                0\n            ) > rrp.lvl THEN 0\n            ELSE 1\n        END AS is_leaf\n    FROM ranked_recursive_pr rrp\n),\npackaging_combination_quantities AS (\n    SELECT\n        p.id AS packaging_id,\n        c.id AS contained_item_id,\n        SUM(leaf.qty) AS total_qty\n    FROM leaf\n    JOIN packaging p ON p.id = leaf.root_id\n    JOIN packaging c ON c.id = leaf.contains_id\n    WHERE leaf.is_leaf = 1\n    GROUP BY p.id, c.id\n)\nSELECT\n    ROUND(AVG(total_qty), 2) AS avg_qty\nFROM packaging_combination_quantities;",
        "external_knowledge": null,
        "plan": "1. **Initialize Recursive Query**:\n   - Start by selecting the initial set of records from the relations table where the item is not contained by any other item.\n   - Assign a level of 1 to these records.\n\n2. **Recursive Step**:\n   - In each recursive step, join the current set of records with the relations table to find items that are contained by the current items.\n   - Multiply the quantity from the previous level by the quantity in the current level.\n   - Increment the level by 1.\n\n3. **Rank Records**:\n   - After the recursive query completes, rank the results for each root item based on their levels using row numbering.\n\n4. **Identify Leaf Nodes**:\n   - For each record, determine if it is a leaf node. A leaf node is identified by checking if there is no lower level for the same root item.\n\n5. **Calculate Total Quantities**:\n   - For each combination of packaging and contained items, sum up the quantities of the leaf nodes.\n\n6. **Compute Average Quantity**:\n   - Calculate the average of the total quantities across all packaging combinations and round the result to two decimal places.\n\nBy following these steps, the query achieves the objective of finding the average quantity across all packaging combinations.",
        "special_function": null
    },
    {
        "instance_id": "local270",
        "db": "oracle_sql",
        "question": "Which packaging containers include items in quantities greater than 500, considering all items contained within each container?",
        "SQL": "WITH RECURSIVE recursive_pr AS (\n   SELECT\n      pr.packaging_id AS root_id,\n      pr.packaging_id,\n      pr.contains_id,\n      pr.qty,\n      1 AS lvl\n   FROM packaging_relations pr\n   WHERE pr.packaging_id NOT IN (\n      SELECT contains_id FROM packaging_relations\n   )\n   UNION ALL\n   SELECT\n      rpr.root_id,\n      pr.packaging_id,\n      pr.contains_id,\n      rpr.qty * pr.qty AS qty,\n      rpr.lvl + 1 AS lvl\n   FROM recursive_pr rpr\n   JOIN packaging_relations pr ON pr.packaging_id = rpr.contains_id\n),\nleaf AS (\n   SELECT\n      root_id,\n      contains_id,\n      SUM(qty) AS qty\n   FROM (\n      SELECT\n         rpr.*,\n         CASE\n            WHEN COALESCE(LEAD(rpr.lvl) OVER (ORDER BY rpr.contains_id), 0) > rpr.lvl\n            THEN 0\n            ELSE 1\n         END AS is_leaf\n      FROM recursive_pr rpr\n   )\n   WHERE is_leaf = 1\n   GROUP BY root_id, contains_id\n)\nSELECT\n   p.name AS p_name,\n   c.name AS c_name\nFROM leaf\nJOIN packaging p ON p.id = leaf.root_id\nJOIN packaging c ON c.id = leaf.contains_id\nWHERE leaf.qty > 500\nORDER BY p.id, c.id;",
        "external_knowledge": null,
        "plan": "1. **Initial Recursive Query Setup**:\n   - Begin by defining a recursive common table expression (CTE) to traverse a hierarchical relationship of packaging containers.\n   - Select the root containers that are not contained within any other container, initializing the recursion with these root containers and their immediate contents.\n\n2. **Recursive Part of the Query**:\n   - Continue the recursive CTE by joining each level of containers with their contents.\n   - Multiply the quantities at each level to propagate the correct quantity through the hierarchy.\n   - Track the recursion depth to help identify the leaf nodes later.\n\n3. **Identify Leaf Nodes**:\n   - Use a subquery to determine which nodes are leaf nodes (i.e., they do not contain other items).\n   - Utilize window functions to detect changes in recursion level, marking the last level as a leaf node.\n\n4. **Summarize Quantities at Leaf Nodes**:\n   - Aggregate the quantities at the leaf node level to get the total quantity for each item contained within the root containers.\n\n5. **Final Selection**:\n   - Join the summarized quantities with the main table to get the names of the root containers and their contained items.\n   - Filter the results to include only those with quantities greater than the specified threshold (500).\n   - Order the final output by the identifiers of the root containers and their contained items to ensure a structured result.",
        "special_function": null
    },
    {
        "instance_id": "local272",
        "db": "oracle_sql",
        "question": "Which product ID, aisle, and position should be selected to pick the highest quantity for order 423, ensuring the picked quantity does not exceed the available inventory in warehouse 1, and calculate the quantity to be picked while prioritizing locations with earlier dates and smaller quantities?",
        "SQL": "with orderlines_cte as (\n    select\n        ol.order_id,\n        ol.product_id,\n        ol.qty,\n        ifnull(\n            sum(ol.qty) over (\n                partition by ol.product_id\n                order by ol.order_id\n                rows between unbounded preceding and 1 preceding\n            ), 0\n        ) + 1 as from_q,\n        ifnull(\n            sum(ol.qty) over (\n                partition by ol.product_id\n                order by ol.order_id\n                rows between unbounded preceding and 1 preceding\n            ), 0\n        ) + ol.qty as to_q\n    from orderlines ol\n    where ol.order_id = 423\n),\ninventory_cte as (\n    select\n        i.product_id,\n        i.qty,\n        ifnull(\n            sum(i.qty) over (\n                partition by i.product_id\n                order by i.purchased, i.qty\n                rows between unbounded preceding and 1 preceding\n            ), 0\n        ) as acc_prv_q,\n        i.purchased,\n        i.warehouse,\n        i.aisle,\n        i.position\n    from inventory_with_dims i\n)\nselect\n    i.product_id as product_id,\n    i.aisle as aisle,\n    i.position as position,\n    case\n        when i.qty < ob.qty - i.acc_prv_q then i.qty\n        else ob.qty - i.acc_prv_q\n    end as quantity_to_be_picked\nfrom orderlines_cte ol\njoin (\n    select\n        product_id,\n        sum(qty) as qty\n    from orderlines_cte\n    group by product_id\n) ob on ol.product_id = ob.product_id\njoin inventory_cte i on i.product_id = ob.product_id\nwhere i.acc_prv_q < ob.qty\n  and ol.to_q >= i.acc_prv_q + 1\n  and ol.from_q <= case\n        when i.acc_prv_q + i.qty < ob.qty then i.acc_prv_q + i.qty\n        else ob.qty\n    end\n  and i.warehouse = 1\norder by quantity_to_be_picked desc;",
        "external_knowledge": null,
        "plan": "1. **Identify Order Lines**:\n   - Create a temporary dataset of order lines for the specified order.\n   - For each line, calculate a range (`from_q` to `to_q`) representing cumulative quantities up to and including the current line.\n\n2. **Prepare Inventory Data**:\n   - Create a temporary dataset for the inventory.\n   - Calculate cumulative quantities for each product in each inventory location, ordered by date and quantity.\n\n3. **Aggregate Order Quantities**:\n   - Sum the quantities for each product in the specified order.\n\n4. **Join Order Lines and Inventory**:\n   - Combine the order lines with the aggregated order quantities.\n   - Join this combined dataset with the prepared inventory data based on matching product IDs.\n\n5. **Filter Inventory Locations**:\n   - Ensure the cumulative quantities in the inventory do not exceed the order quantity.\n   - Ensure the inventory range intersects with the order line range.\n   - Filter for inventory only in the specified warehouse.\n\n6. **Calculate Quantity to be Picked**:\n   - For each matched inventory location, determine the quantity to be picked without exceeding the available inventory or the required order quantity.\n\n7. **Prioritize and Select**:\n   - Order the results by the quantity to be picked in descending order to prioritize locations with earlier dates and smaller quantities.\n\nBy following these steps, the query selects the appropriate product ID, aisle, and position to pick the highest quantity for the specified order while ensuring it does not exceed the available inventory in the specified warehouse, and calculates the quantity to be picked.",
        "special_function": null
    },
    {
        "instance_id": "local273",
        "db": "oracle_sql",
        "question": "What is the average pick percentage for each product (by name), considering the quantity picked from inventory locations that are ordered by the earliest purchase date and smallest quantity, while ensuring that the picked quantity matches the overlapping range between the order quantity and the available inventory?",
        "SQL": "WITH olines AS (\n    SELECT\n        ol.order_id AS o_id,\n        ol.product_id AS p_id,\n        ol.qty,\n        COALESCE(SUM(ol.qty) OVER (\n            PARTITION BY ol.product_id\n            ORDER BY ol.order_id\n            ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING\n        ), 0) + 1 AS from_q,\n        COALESCE(SUM(ol.qty) OVER (\n            PARTITION BY ol.product_id\n            ORDER BY ol.order_id\n            ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING\n        ), 0) + ol.qty AS to_q\n    FROM orderlines ol\n),\norderbatch AS (\n    SELECT\n        ol.p_id,\n        SUM(ol.qty) AS qty\n    FROM olines ol\n    GROUP BY ol.p_id\n),\nfifo AS (\n    SELECT\n        wh, ai, pos, p_id, loc_q,\n        MIN(loc_q, ord_q - acc_prv_q) AS pick_q,\n        acc_prv_q + 1 AS from_q,\n        MIN(acc_prv_q + loc_q, ord_q) AS to_q\n    FROM (\n        SELECT\n            i.product_id AS p_id,\n            ob.qty AS ord_q,\n            i.qty AS loc_q,\n            COALESCE(SUM(i.qty) OVER (\n                PARTITION BY i.product_id\n                ORDER BY i.purchased, i.qty\n                ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING\n            ), 0) AS acc_prv_q,\n            i.purchased,\n            i.warehouse AS wh,\n            i.aisle AS ai,\n            i.position AS pos\n        FROM orderbatch ob\n        JOIN inventory_with_dims i ON i.product_id = ob.p_id\n    )\n    WHERE acc_prv_q < ord_q\n),\npicked_data AS (\n    SELECT\n        f.wh, f.ai, f.pos, f.p_id,\n        f.pick_q, o.o_id,\n        MIN(\n            f.loc_q,\n            MIN(o.to_q, f.to_q) - MAX(o.from_q, f.from_q) + 1\n        ) AS q_f_o,\n        -- Calculate percentage of quantity picked\n        ROUND(\n            (MIN(\n                f.loc_q,\n                MIN(o.to_q, f.to_q) - MAX(o.from_q, f.from_q) + 1\n            ) * 100.0 / f.pick_q), 2\n        ) AS pick_percentage\n    FROM fifo f\n    JOIN olines o\n        ON o.p_id = f.p_id\n        AND o.to_q >= f.from_q\n        AND o.from_q <= f.to_q\n)\nSELECT\n    p.name AS product_name,\n    AVG(pd.pick_percentage) AS avg_pick_percentage\nFROM picked_data pd\nJOIN products p ON p.id = pd.p_id\nGROUP BY p.name\nORDER BY p.name;",
        "external_knowledge": null,
        "plan": "1. **Identify Order Lines:**\n   - Create a temporary dataset to track each order line's quantity and compute the cumulative quantity for each product up to the current order line.\n\n2. **Aggregate Order Quantities:**\n   - Sum the quantities for each product across all order lines to determine the total quantity ordered for each product.\n\n3. **Match Inventory to Orders:**\n   - Generate a temporary dataset to match each product's ordered quantity with available inventory, ordered by the earliest purchase date and smallest quantity.\n   - Calculate the cumulative quantity of inventory picked so far for each product.\n\n4. **Calculate Picked Quantities:**\n   - Determine the quantity to be picked from each inventory location, ensuring it does not exceed the remaining order quantity.\n   - Establish the range of cumulative quantities covered by the current pick operation.\n\n5. **Identify Picked Data:**\n   - Match the picked quantities from inventory to the corresponding order lines by overlapping their cumulative quantity ranges.\n   - Calculate the actual picked quantity for each order line and compute the pick percentage.\n\n6. **Compute Average Pick Percentage:**\n   - Join the picked data with product information to associate each pick percentage with the corresponding product.\n   - Calculate the average pick percentage for each product.\n\n7. **Output Results:**\n   - Select the product names and their corresponding average pick percentages.\n   - Order the results by product name for readability.",
        "special_function": null
    },
    {
        "instance_id": "local274",
        "db": "oracle_sql",
        "question": "Which products were picked for order 421, and what is the average number of units picked for each product, using FIFO (First-In, First-Out) method?",
        "SQL": "WITH olines AS (\n    SELECT\n        ol.order_id AS o_id,\n        ol.product_id AS p_id,\n        ol.qty,\n        IFNULL((\n            SELECT SUM(ol2.qty)\n            FROM orderlines ol2\n            WHERE ol2.product_id = ol.product_id\n              AND ol2.order_id < ol.order_id\n        ), 0) + 1 AS from_q,\n        IFNULL((\n            SELECT SUM(ol2.qty)\n            FROM orderlines ol2\n            WHERE ol2.product_id = ol.product_id\n              AND ol2.order_id < ol.order_id\n        ), 0) + ol.qty AS to_q\n    FROM orderlines ol\n    WHERE ol.order_id = 421\n), orderbatch AS (\n    SELECT\n        ol.p_id,\n        SUM(ol.qty) AS qty\n    FROM olines ol\n    GROUP BY ol.p_id\n), fifo AS (\n    SELECT\n        wh, ai, pos, p_id, loc_q,\n        MIN(loc_q, ord_q - acc_prv_q) AS pick_q,\n        acc_prv_q + 1 AS from_q,\n        MIN(acc_prv_q + loc_q, ord_q) AS to_q\n    FROM (\n        SELECT\n            i.product_id AS p_id,\n            ob.qty AS ord_q,\n            i.qty AS loc_q,\n            IFNULL((\n                SELECT SUM(i2.qty)\n                FROM inventory_with_dims i2\n                WHERE i2.product_id = i.product_id\n                  AND (i2.purchased < i.purchased OR (i2.purchased = i.purchased AND i2.qty < i.qty))\n            ), 0) AS acc_prv_q,\n            i.purchased,\n            i.warehouse AS wh,\n            i.aisle AS ai,\n            i.position AS pos\n        FROM orderbatch ob\n        JOIN inventory_with_dims i\n          ON i.product_id = ob.p_id\n    ) sub\n    WHERE acc_prv_q < ord_q\n), pick AS (\n    SELECT\n        f.wh, f.ai,\n        DENSE_RANK() OVER (\n            ORDER BY wh, ai\n        ) AS ai_rank,\n        f.pos, f.p_id,\n        f.pick_q, o.o_id,\n        MIN(\n            f.loc_q,\n            MIN(o.to_q, f.to_q) - MAX(o.from_q, f.from_q) + 1\n        ) AS q_f_o\n    FROM fifo f\n    JOIN olines o\n      ON o.p_id = f.p_id\n     AND o.to_q >= f.from_q\n     AND o.from_q <= f.to_q\n)\nSELECT\n    pr.name AS product_name,\n    AVG(p.pick_q) AS avg_units_picked\nFROM pick p\nJOIN products pr ON pr.id = p.p_id\nGROUP BY p.p_id, pr.name\nORDER BY p.p_id;",
        "external_knowledge": null,
        "plan": "1. **Create a subquery to gather order lines for the specific order:**\n   - Filter order lines to include only those belonging to the given order.\n   - Calculate the range of quantities (from `from_q` to `to_q`) for each product in the order by considering previous orders of the same product.\n\n2. **Summarize the total quantities required for each product in the order:**\n   - Aggregate the quantities from the previous step, grouping by product.\n\n3. **Prepare a FIFO inventory picking plan:**\n   - Calculate the cumulative quantity available for each product in the inventory, considering the FIFO principle.\n   - Determine the quantity to pick from each inventory location, ensuring it does not exceed the ordered quantity and follows the FIFO method.\n\n4. **Identify the picking details:**\n   - Join the FIFO inventory picking plan with the order lines to match products and their required quantity ranges.\n   - Calculate the actual quantity picked from each inventory location for the order.\n\n5. **Calculate the average units picked for each product:**\n   - Join the picking details with product information to get the product names.\n   - Compute the average quantity picked for each product.\n   - Group the results by product and order them by product identifier.\n\nThis plan ensures that the products picked for the order are determined using the FIFO method, and the average number of units picked for each product is accurately calculated.",
        "special_function": null
    },
    {
        "instance_id": "local275",
        "db": "oracle_sql",
        "question": "Which products (by name) had a seasonality-adjusted sales ratio consistently above 2 for the entire year of 2017, based on monthly sales data from January 2016?",
        "SQL": "WITH RECURSIVE date_series AS (\n    SELECT date('2016-01-01') AS mth, 1 AS ts\n    UNION ALL\n    SELECT date(mth, '+1 month'), ts + 1\n    FROM date_series\n    WHERE ts < 48\n), s1 AS (\n    SELECT\n        ms.product_id,\n        mths.mth,\n        mths.ts,\n        strftime('%Y', mths.mth) AS yr,\n        strftime('%m', mths.mth) AS mthno,\n        ms.qty\n    FROM date_series mths\n    LEFT JOIN (\n        SELECT product_id, mth, qty\n        FROM monthly_sales\n    ) ms\n    ON ms.mth = mths.mth\n), s2 AS (\n    SELECT\n        product_id,\n        mth,\n        ts,\n        yr,\n        mthno,\n        qty,\n        CASE\n            WHEN ts BETWEEN 7 AND 30 THEN\n                (ifnull(AVG(qty) OVER (\n                    PARTITION BY product_id\n                    ORDER BY ts\n                    ROWS BETWEEN 5 PRECEDING AND 6 FOLLOWING\n                ), 0) + ifnull(AVG(qty) OVER (\n                    PARTITION BY product_id\n                    ORDER BY ts\n                    ROWS BETWEEN 6 PRECEDING AND 5 FOLLOWING\n                ), 0)) / 2\n            ELSE\n                NULL\n        END AS cma\n    FROM s1\n), s3 AS (\n    SELECT\n        s2.product_id,\n        s2.mth,\n        s2.ts,\n        s2.yr,\n        s2.mthno,\n        s2.qty,\n        s2.cma,\n        ifnull(AVG(\n            CASE WHEN s2.qty = 0 THEN 0.0001 ELSE s2.qty END / NULLIF(s2.cma, 0)\n        ) OVER (\n            PARTITION BY s2.product_id, s2.mthno\n        ), 0) AS s\n    FROM s2\n)\nSELECT DISTINCT\n    p.name AS product_name\nFROM s3\nJOIN products p ON p.id = s3.product_id\nWHERE s3.s > 2\n    AND strftime('%Y', s3.mth) = '2017'\nORDER BY p.name, s3.mth;",
        "external_knowledge": "calculation_method.md",
        "plan": "1. **Create a Recursive Date Series:**\n   - Generate a series of dates starting from January 1, 2016, incrementing by one month until you have 48 months in total. This will be used to ensure all months are included, even if there are no sales data for some months.\n\n2. **Join Monthly Sales Data:**\n   - For each date in the series, join with the monthly sales data to associate the sales quantities with the corresponding months. This ensures that every month has a corresponding sales record, even if it\u2019s zero.\n\n3. **Calculate Centered Moving Average (CMA):**\n   - For months between the 7th and 30th month, calculate the centered moving average (CMA) of sales quantities. The CMA is computed by averaging the sales quantities over an 11-month window (5 months before and 5 months after the current month).\n\n4. **Compute Seasonality Index:**\n   - For each month, calculate the seasonality index by dividing the actual sales quantity by the CMA. If the CMA is zero, adjust to avoid division by zero.\n\n5. **Filter and Aggregate Data:**\n   - Identify products that have a seasonality-adjusted sales ratio greater than 2 for the year 2017. This involves checking if the seasonality index exceeds 2 and ensuring the year is 2017.\n\n6. **Join with Product Information:**\n   - Join the filtered results with the product information to retrieve the product names.\n\n7. **Select and Order Results:**\n   - Select distinct product names that meet the criteria and order the results by product name and month.\n\nBy following these steps, you ensure that the query identifies products with significant seasonality-adjusted sales increases in 2017, based on a detailed analysis of monthly sales data starting from January 2016.",
        "special_function": null
    },
    {
        "instance_id": "local277",
        "db": "oracle_sql",
        "question": "What is the average forecasted annual sales for products 4160 and 7790 for 2018? Use a weighted regression model based on sales data from January 2016, focusing on the first 36 months, with sales adjusted for seasonality during time steps 7 to 30.",
        "SQL": "WITH RECURSIVE mths AS (\n    SELECT\n        date('2016-01-01') AS mth,\n        1 AS ts\n    UNION ALL\n    SELECT\n        date(mth, '+1 month'),\n        ts + 1\n    FROM mths\n    WHERE ts < 48\n), s1 AS (\n    SELECT\n        ms.product_id,\n        mths.mth,\n        mths.ts,\n        strftime('%Y', mths.mth) AS yr,\n        strftime('%m', mths.mth) AS mthno,\n        COALESCE(ms.qty, 0) AS qty\n    FROM mths\n    LEFT JOIN monthly_sales ms ON ms.mth = mths.mth AND ms.product_id IN (4160, 7790)\n), s2 AS (\n    SELECT\n        product_id,\n        mth,\n        ts,\n        yr,\n        mthno,\n        qty,\n        CASE\n            WHEN ts BETWEEN 7 AND 30 THEN (\n                COALESCE(AVG(qty) OVER (\n                    PARTITION BY product_id\n                    ORDER BY ts\n                    ROWS BETWEEN 5 PRECEDING AND 6 FOLLOWING\n                ), 0) + COALESCE(AVG(qty) OVER (\n                    PARTITION BY product_id\n                    ORDER BY ts\n                    ROWS BETWEEN 6 PRECEDING AND 5 FOLLOWING\n                ), 0)) / 2\n            ELSE NULL\n        END AS cma\n    FROM s1\n), s3 AS (\n    SELECT\n        product_id,\n        mth,\n        ts,\n        yr,\n        mthno,\n        qty,\n        cma,\n        COALESCE(AVG(CASE\n            WHEN qty = 0 THEN 0.0001\n            ELSE qty\n        END / NULLIF(cma, 0)) OVER (\n            PARTITION BY product_id, mthno\n        ), 0) AS s\n    FROM s2\n), s4 AS (\n    SELECT\n        product_id,\n        mth,\n        ts,\n        yr,\n        mthno,\n        qty,\n        cma,\n        s,\n        CASE\n            WHEN ts <= 36 THEN COALESCE(CASE\n                WHEN qty = 0 THEN 0.0001\n                ELSE qty\n            END / NULLIF(s, 0), 0)\n        END AS des\n    FROM s3\n), s5 AS (\n    SELECT\n        product_id,\n        mth,\n        ts,\n        yr,\n        mthno,\n        qty,\n        cma,\n        s,\n        des,\n        (AVG(des * ts) OVER (PARTITION BY product_id) - \n        AVG(des) OVER (PARTITION BY product_id) * AVG(ts) OVER (PARTITION BY product_id)) /\n        (AVG(ts * ts) OVER (PARTITION BY product_id) - \n        AVG(ts) OVER (PARTITION BY product_id) * AVG(ts) OVER (PARTITION BY product_id)) AS slope,\n        AVG(des) OVER (PARTITION BY product_id) - \n        (AVG(des * ts) OVER (PARTITION BY product_id) - \n        AVG(des) OVER (PARTITION BY product_id) * AVG(ts) OVER (PARTITION BY product_id)) /\n        (AVG(ts * ts) OVER (PARTITION BY product_id) - \n        AVG(ts) OVER (PARTITION BY product_id) * AVG(ts) OVER (PARTITION BY product_id)) * AVG(ts) OVER (PARTITION BY product_id) AS intercept\n    FROM s4\n)\nSELECT\n    ROUND(AVG((intercept + ts * slope) * s), 2) AS avg_forecasted_annual_sales\nFROM s5\nWHERE yr = '2018';",
        "external_knowledge": "calculation_method.md",
        "plan": "1. **Generate Time Series**:\n   - Create a recursive common table expression (CTE) to generate a series of months starting from January 2016 for 48 months, assigning a time step to each month.\n\n2. **Join Sales Data**:\n   - Left join the generated time series with the sales data for the specified products, ensuring that months with no sales data are included with a quantity of zero.\n\n3. **Calculate Centered Moving Average (CMA)**:\n   - For time steps between 7 and 30, calculate the centered moving average (CMA) using a weighted average over a window of 12 months, centered on the current month.\n\n4. **Calculate Seasonal Index**:\n   - Compute the seasonal index for each month by taking the average ratio of actual sales to CMA, replacing zero sales with a small value to avoid division by zero.\n\n5. **Deseasonalize Sales**:\n   - Deseasonalize the sales data for the first 36 months by dividing actual sales by the seasonal index.\n\n6. **Compute Regression Parameters**:\n   - Calculate the slope and intercept for a linear regression model using the deseasonalized sales data and the time steps.\n\n7. **Forecast Sales**:\n   - Use the regression model to forecast sales for 2018 by applying the slope and intercept to the time steps and adjusting by the seasonal index.\n\n8. **Calculate Average Forecasted Sales**:\n   - Average the forecasted sales values for 2018 and round to two decimal places to obtain the average forecasted annual sales for the specified products.",
        "special_function": null
    },
    {
        "instance_id": "local279",
        "db": "oracle_sql",
        "question": "For each product, provide the product_id, month in 2019, and the smallest difference between its ending inventory and minimum required level, based on a monthly inventory adjustment model that includes restocking when levels fall below the minimum.",
        "SQL": "WITH RECURSIVE mb_recur AS (\n   -- Initial data selection\n   SELECT\n      it.product_id,\n      date('2018-12-01') AS mth,\n      0 AS qty,\n      0 AS inv_begin,\n      NULL AS date_purch,\n      0 AS p_qty,\n      it.qty AS inv_end,\n      pm.qty_minimum,\n      pm.qty_purchase\n   FROM inventory_totals it\n   JOIN product_minimums pm ON pm.product_id = it.product_id\n\n   UNION ALL\n\n   -- Recursive step\n   SELECT\n      mb.product_id,\n      date(mb.mth, '+1 month') AS mth,\n      CASE\n         WHEN mbr.inv_end - COALESCE(mb.qty, 0) > 0 THEN mbr.inv_end - COALESCE(mb.qty, 0)\n         ELSE 0\n      END AS qty,\n      mbr.inv_end AS inv_begin,\n      CASE\n         WHEN mbr.inv_end - COALESCE(mb.qty, 0) < mbr.qty_minimum THEN\n            date(mbr.mth, '+' || CAST(((strftime('%s', date(mb.mth, '+1 month')) - strftime('%s', mbr.mth)) * (mbr.inv_end - mbr.qty_minimum)) / mb.qty AS INTEGER) || ' month')\n      END AS date_purch,\n      CASE\n         WHEN mbr.inv_end - COALESCE(mb.qty, 0) < mbr.qty_minimum THEN mbr.qty_purchase\n      END AS p_qty,\n      mbr.inv_end - COALESCE(mb.qty, 0) + CASE\n         WHEN mbr.inv_end - COALESCE(mb.qty, 0) < mbr.qty_minimum THEN mbr.qty_purchase\n         ELSE 0\n      END AS inv_end,\n      mbr.qty_minimum,\n      mbr.qty_purchase\n   FROM mb_recur mbr\n   JOIN monthly_budget mb ON mb.product_id = mbr.product_id AND mb.mth = date(mbr.mth, '+1 month')\n   LEFT JOIN monthly_orders mo ON mo.product_id = mb.product_id AND mo.mth = mb.mth\n),\ninventory_diff AS (\n   SELECT\n      m.product_id,\n      m.mth,\n      ABS(m.inv_end - m.qty_minimum) AS diff\n   FROM mb_recur m\n   WHERE m.mth >= date('2019-01-01')\n     AND m.mth < date('2020-01-01')\n),\nmin_diff AS (\n   SELECT\n      product_id,\n      MIN(diff) AS min_diff\n   FROM inventory_diff\n   GROUP BY product_id\n)\nSELECT\n   d.product_id,\n   d.mth AS date,\n   d.diff\nFROM inventory_diff d\nJOIN min_diff md ON d.product_id = md.product_id AND d.diff = md.min_diff\nORDER BY d.product_id, d.mth;",
        "external_knowledge": null,
        "plan": "1. **Define a Recursive Common Table Expression (CTE)**:\n   - Initialize the recursive CTE by selecting initial values for each product, including the initial date, quantities, inventory levels, and other necessary parameters.\n   - Ensure to set up the base case of the recursion with the initial month and inventory levels.\n\n2. **Recursive Step**:\n   - Set up the recursive part of the CTE to iterate through each month.\n   - Calculate the inventory at the beginning and end of each month, considering purchases and required minimum levels.\n   - Determine the purchase date and quantity if the inventory falls below the minimum required level.\n   - Update the inventory for the next month based on the purchases made.\n\n3. **Compute Inventory Differences**:\n   - Create another CTE to calculate the absolute difference between the ending inventory and the minimum required level for each product and month.\n   - Filter the results to include only the year 2019.\n\n4. **Find Minimum Differences**:\n   - Aggregate the differences by product to find the smallest difference for each product within the specified timeframe.\n   \n5. **Final Selection**:\n   - Join the aggregated minimum differences back with the detailed differences to retrieve the specific month and difference value for each product.\n   - Order the final result by product and month for clear presentation.",
        "special_function": null
    },
    {
        "instance_id": "local283",
        "db": "EU_soccer",
        "question": "Analyze our match data to identify the name, leagues, and countries of the champion team for each season. Include the total points accumulated by each team.",
        "SQL": "WITH TABLE_1 AS (\n    SELECT \n        Match.id,\n        Country.name AS country_name, \n        League.name AS league_name, \n        season, \n        stage, \n        date,\n        HT.team_long_name AS home_team,\n        AT.team_long_name AS away_team,\n        home_team_goal, \n        away_team_goal,\n        CASE\n            WHEN home_team_goal > away_team_goal THEN \"Win\"\n            WHEN home_team_goal < away_team_goal THEN \"Loss\"\n            ELSE \"Tie\"\n        END AS home_team_result, \n        CASE\n            WHEN away_team_goal > home_team_goal THEN \"Win\"\n            WHEN away_team_goal < home_team_goal THEN \"Loss\"\n            ELSE \"Tie\"\n        END AS away_team_result\n    FROM Match\n    JOIN Country ON Country.id = Match.country_id\n    JOIN League ON League.id = Match.league_id\n    LEFT JOIN Team AS HT ON HT.team_api_id = Match.home_team_api_id\n    LEFT JOIN Team AS AT ON AT.team_api_id = Match.away_team_api_id\n),\nHOME_TEAM AS (\n    SELECT \n        id,\n        country_name, \n        league_name, \n        season, \n        stage, \n        date,\n        home_team AS team, \n        'Home' AS team_type,\n        home_team_goal AS goals,\n        home_team_result AS result\n    FROM TABLE_1\n),\nAWAY_TEAM AS (\n    SELECT \n        id,\n        country_name, \n        league_name, \n        season, \n        stage, \n        date,\n        away_team AS team, \n        'Away' AS team_type,\n        away_team_goal AS goals,\n        away_team_result AS result\n    FROM TABLE_1\n), \nTABLE_2 AS (\n    SELECT * \n    FROM HOME_TEAM\n    UNION ALL\n    SELECT * \n    FROM AWAY_TEAM\n),\nTABLE_3 AS (\n    SELECT *, \n        CASE \n            WHEN result = 'Win' THEN 3\n            WHEN result = 'Tie' THEN 1\n            ELSE 0\n        END AS points\n    FROM TABLE_2\n), \nTABLE_4 AS (\n    SELECT\n        season,\n        team,\n        league_name,\n        country_name,\n        SUM(points) AS total_points,\n        RANK() OVER(PARTITION BY season ORDER BY SUM(points) DESC) AS season_rank\n    FROM TABLE_3\n    GROUP BY \n        season, \n        team,\n        league_name,\n        country_name\n    ORDER BY total_points DESC\n)\nSELECT * \nFROM TABLE_4 \nWHERE season_rank = 1\nORDER BY total_points DESC;",
        "external_knowledge": null,
        "plan": "1. **Data Preparation:**\n   - Create an initial dataset by joining match data with country, league, and team information.\n   - Determine the results for each match, identifying whether the home team or away team won, lost, or tied.\n\n2. **Separate Team Data:**\n   - Generate two subsets of data:\n     - One for matches from the perspective of the home team.\n     - Another for matches from the perspective of the away team.\n\n3. **Combine Team Data:**\n   - Combine the two subsets into a single dataset, ensuring all matches are included with appropriate team identifiers and results.\n\n4. **Assign Points:**\n   - For each match, assign points based on the result:\n     - 3 points for a win.\n     - 1 point for a tie.\n     - 0 points for a loss.\n\n5. **Aggregate Points:**\n   - Group the combined dataset by season and team.\n   - Calculate the total points accumulated by each team for each season.\n\n6. **Rank Teams:**\n   - Rank the teams within each season based on their total points, with the highest points getting the top rank.\n\n7. **Identify Champions:**\n   - Filter the ranked data to retain only the top-ranked team (champion) for each season.\n\n8. **Final Output:**\n   - Select and display the relevant information (team name, league, country, total points) for the champion team of each season.\n   - Order the results by total points in descending order.",
        "special_function": null
    },
    {
        "instance_id": "local284",
        "db": "bank_sales_trading",
        "question": "For veg whsle data, can you generate a summary of our items' loss rates? Include the average loss rate, and also break down the count of items that are below, above, and within one standard deviation from this average.",
        "SQL": "WITH avg_loss_tb AS (\n    SELECT \n        AVG([Loss_Rate_%]) AS avg_loss_rate,\n        COUNT([index]) AS total_num \n    FROM \n        veg_loss_rate_df\n    WHERE \n        [loss_rate_%] <> ''\n), \nstd AS (\n    SELECT \n        ROUND(SQRT(SUM(POWER(([Loss_Rate_%] - (SELECT avg_loss_rate FROM avg_loss_tb)), 2)) / alt.total_num), 2) AS std\n    FROM \n        veg_loss_rate_df lrd, avg_loss_tb alt\n)\nSELECT\n    AVG([Loss_Rate_%]) AS 'avg_loss_rate_%',\n    SUM(\n        CASE\n            WHEN [Loss_Rate_%] BETWEEN (SELECT avg_loss_rate FROM avg_loss_tb) - (SELECT std FROM std) AND (SELECT avg_loss_rate FROM avg_loss_tb) + (SELECT std FROM std) THEN 1\n            ELSE 0\n        END\n    ) AS items_within_stdev,\n    SUM(\n        CASE\n            WHEN [Loss_Rate_%] > ((SELECT avg_loss_rate FROM avg_loss_tb) + (SELECT std FROM std)) THEN 1\n            ELSE 0\n        END\n    ) AS above_stdev,\n    SUM(\n        CASE\n            WHEN [Loss_Rate_%] < ((SELECT avg_loss_rate FROM avg_loss_tb) - (SELECT std FROM std)) THEN 1\n            ELSE 0\n        END\n    ) AS items_below_stdev\nFROM \n    veg_loss_rate_df;",
        "external_knowledge": null,
        "plan": "1. **Calculate Average Loss Rate and Total Count:**\n   - Create a temporary table to store the average loss rate and the total number of items from the dataset.\n   - Filter out any records where the loss rate is empty.\n\n2. **Calculate Standard Deviation:**\n   - Create another temporary table to calculate the standard deviation of the loss rates.\n   - Use the previously calculated average loss rate.\n   - Apply the standard deviation formula which involves summing the squared differences from the mean, dividing by the number of items, and then taking the square root.\n   - Round the result to two decimal places.\n\n3. **Main Query - Summarize Loss Rates:**\n   - Select the average loss rate directly from the dataset.\n   - Count the number of items that fall within one standard deviation from the average loss rate using a conditional sum.\n   - Count the number of items with a loss rate above one standard deviation from the average using another conditional sum.\n   - Count the number of items with a loss rate below one standard deviation from the average using yet another conditional sum.\n\n4. **Combine Results:**\n   - Use the previously calculated average loss rate and standard deviation within the main query to perform the conditional checks and aggregations.\n   - Output the average loss rate, the count of items within one standard deviation, the count of items above one standard deviation, and the count of items below one standard deviation.",
        "special_function": null
    },
    {
        "instance_id": "local285",
        "db": "bank_sales_trading",
        "question": "For veg whsle data, can you analyze our financial performance over the years 2020 to 2023? I need insights into the average wholesale price, maximum wholesale price, minimum wholesale price, wholesale price difference, total wholesale price, total selling price, average loss rate, total loss, and profit for each category within each year. Round all calculated values to two decimal places.",
        "SQL": "WITH item_2020 AS (\n    SELECT\n        strftime('%Y', v.[txn_date]) AS 'yr',\n        c.category_code,\n        c.category_name,\n        ROUND(AVG(`whsle_px_rmb-kg`), 2) AS avg_whole_sale,\n        ROUND(MAX(`whsle_px_rmb-kg`), 2) AS max_whole_sale,\n        ROUND(MIN(`whsle_px_rmb-kg`), 2) AS min_whole_sale,\n        ROUND(ROUND(MAX(`whsle_px_rmb-kg`), 2) - ROUND(MIN(`whsle_px_rmb-kg`), 2), 2) AS whole_sale_diff,\n        ROUND(SUM((v.`qty_sold(kg)`) * (w.`whsle_px_rmb-kg`)), 2) AS whole_sale_price,\n        ROUND(SUM((v.`unit_selling_px_rmb/kg`) * (v.`qty_sold(kg)`)), 2) AS selling_price,\n        ROUND(AVG(alr.`loss_rate_%`), 2) AS 'avg_loss_rate_%'\n    FROM veg_txn_df v\n    LEFT JOIN veg_whsle_df w ON v.txn_date = w.whsle_date AND v.item_code = w.item_code\n    LEFT JOIN veg_cat c ON v.item_code = c.item_code\n    LEFT JOIN veg_loss_rate_df alr ON alr.item_code = v.item_code\n    WHERE v.`qty_sold(kg)` > 0 AND yr = '2020'\n    GROUP BY strftime('%Y', v.[txn_date]), c.category_code, c.category_name\n),\nitem_2021 AS (\n    SELECT\n        strftime('%Y', v.[txn_date]) AS 'yr',\n        c.category_code,\n        c.category_name,\n        ROUND(AVG(`whsle_px_rmb-kg`), 2) AS avg_whole_sale,\n        ROUND(MAX(`whsle_px_rmb-kg`), 2) AS max_whole_sale,\n        ROUND(MIN(`whsle_px_rmb-kg`), 2) AS min_whole_sale,\n        ROUND(ROUND(MAX(`whsle_px_rmb-kg`), 2) - ROUND(MIN(`whsle_px_rmb-kg`), 2), 2) AS whole_sale_diff,\n        ROUND(SUM((v.`qty_sold(kg)`) * (w.`whsle_px_rmb-kg`)), 2) AS whole_sale_price,\n        ROUND(SUM((v.`unit_selling_px_rmb/kg`) * (v.`qty_sold(kg)`)), 2) AS selling_price,\n        ROUND(AVG(alr.`loss_rate_%`), 2) AS 'avg_loss_rate_%'\n    FROM veg_txn_df v\n    LEFT JOIN veg_whsle_df w ON v.txn_date = w.whsle_date AND v.item_code = w.item_code\n    LEFT JOIN veg_cat c ON v.item_code = c.item_code\n    LEFT JOIN veg_loss_rate_df alr ON alr.item_code = v.item_code\n    WHERE v.`qty_sold(kg)` > 0 AND yr = '2021'\n    GROUP BY strftime('%Y', v.[txn_date]), c.category_code, c.category_name\n),\nitem_2022 AS (\n    SELECT\n        strftime('%Y', v.[txn_date]) AS 'yr',\n        c.category_code,\n        c.category_name,\n        ROUND(AVG(`whsle_px_rmb-kg`), 2) AS avg_whole_sale,\n        ROUND(MAX(`whsle_px_rmb-kg`), 2) AS max_whole_sale,\n        ROUND(MIN(`whsle_px_rmb-kg`), 2) AS min_whole_sale,\n        ROUND(ROUND(MAX(`whsle_px_rmb-kg`), 2) - ROUND(MIN(`whsle_px_rmb-kg`), 2), 2) AS whole_sale_diff,\n        ROUND(SUM((v.`qty_sold(kg)`) * (w.`whsle_px_rmb-kg`)), 2) AS whole_sale_price,\n        ROUND(SUM((v.`unit_selling_px_rmb/kg`) * (v.`qty_sold(kg)`)), 2) AS selling_price,\n        ROUND(AVG(alr.`loss_rate_%`), 2) AS 'avg_loss_rate_%'\n    FROM veg_txn_df v\n    LEFT JOIN veg_whsle_df w ON v.txn_date = w.whsle_date AND v.item_code = w.item_code\n    LEFT JOIN veg_cat c ON v.item_code = c.item_code\n    LEFT JOIN veg_loss_rate_df alr ON alr.item_code = v.item_code\n    WHERE v.`qty_sold(kg)` > 0 AND yr = '2022'\n    GROUP BY strftime('%Y', v.[txn_date]), c.category_code, c.category_name\n),\nitem_2023 AS (\n    SELECT\n        strftime('%Y', v.[txn_date]) AS 'yr',\n        c.category_code,\n        c.category_name,\n        ROUND(AVG(`whsle_px_rmb-kg`), 2) AS avg_whole_sale,\n        ROUND(MAX(`whsle_px_rmb-kg`), 2) AS max_whole_sale,\n        ROUND(MIN(`whsle_px_rmb-kg`), 2) AS min_whole_sale,\n        ROUND(MAX(`whsle_px_rmb-kg`), 2) - ROUND(MIN(`whsle_px_rmb-kg`), 2) AS whole_sale_diff,\n        ROUND(SUM((v.`qty_sold(kg)`) * (w.`whsle_px_rmb-kg`)), 2) AS whole_sale_price,\n        ROUND(SUM((v.`unit_selling_px_rmb/kg`) * (v.`qty_sold(kg)`)), 2) AS selling_price,\n        ROUND(AVG(alr.`loss_rate_%`), 2) AS 'avg_loss_rate_%'\n    FROM veg_txn_df v\n    LEFT JOIN veg_whsle_df w ON v.txn_date = w.whsle_date AND v.item_code = w.item_code\n    LEFT JOIN veg_cat c ON v.item_code = c.item_code\n    LEFT JOIN veg_loss_rate_df alr ON alr.item_code = v.item_code\n    WHERE v.`qty_sold(kg)` > 0 AND yr = '2023'\n    GROUP BY strftime('%Y', v.[txn_date]), c.category_code, c.category_name\n),\nfinal_item AS (\n    SELECT * FROM item_2020\n    UNION\n    SELECT * FROM item_2021\n    UNION\n    SELECT * FROM item_2022\n    UNION\n    SELECT * FROM item_2023\n)\n\nSELECT *,\n    ROUND(((`avg_loss_rate_%` * whole_sale_price) / 100.00), 2) AS total_loss,\n    ROUND(((selling_price - whole_sale_price) - (`avg_loss_rate_%` * whole_sale_price) / 100.00), 2) AS profit\nFROM final_item;",
        "external_knowledge": null,
        "plan": "1. **Define Subqueries for Each Year:**\n   - Create subqueries for the years 2020, 2021, 2022, and 2023.\n   - For each year, select transactions and relevant join data.\n\n2. **Extract Yearly Data:**\n   - Use a function to extract the year from the transaction date.\n   - Ensure only transactions with a positive quantity sold are considered.\n\n3. **Aggregate Data by Category:**\n   - Group data by year and category.\n   - Calculate average, maximum, and minimum wholesale prices.\n   - Determine the difference between the maximum and minimum wholesale prices.\n   - Compute the total wholesale price and total selling price.\n   - Calculate the average loss rate.\n\n4. **Combine Yearly Data:**\n   - Use a UNION operation to combine the results of the subqueries for 2020, 2021, 2022, and 2023 into a single dataset.\n\n5. **Calculate Additional Metrics:**\n   - Calculate the total loss by applying the average loss rate to the total wholesale price.\n   - Calculate profit by subtracting the total wholesale price and total loss from the total selling price.\n\n6. **Final Selection:**\n   - Select all columns from the combined dataset, along with the calculated total loss and profit for each category within each year.",
        "special_function": null
    },
    {
        "instance_id": "local286",
        "db": "electronic_sales",
        "question": "Prepare a comprehensive performance report on our sellers, focusing on total sales, average item price, average review scores, and packing times. Ensure that the report includes only those sellers who have sold a quantity of more than 100 products and highlight the product category names in English with the highest sales volume.",
        "SQL": "WITH products2 AS (\n    SELECT \n        product_id, \n        cn.product_category_name_english AS product_name\n    FROM \n        products AS pd\n    LEFT JOIN \n        product_category_name_translation AS cn\n    ON \n        pd.product_category_name = cn.product_category_name\n),\nproduct_frequency AS (\n    SELECT \n        oi.seller_id, \n        pd2.product_name, \n        COUNT(*) AS product_count\n    FROM \n        order_items AS oi\n    JOIN \n        products2 AS pd2 \n    ON \n        oi.product_id = pd2.product_id\n    GROUP BY \n        oi.seller_id, pd2.product_name\n),\nmax_product_frequency AS (\n    SELECT \n        pf.seller_id, \n        pf.product_name AS highlight_product,\n        pf.product_count\n    FROM \n        product_frequency pf\n    WHERE \n        (pf.seller_id, pf.product_count) IN (\n            SELECT \n                seller_id, \n                MAX(product_count)\n            FROM \n                product_frequency\n            GROUP BY \n                seller_id\n        )\n),\nreviewscore AS (\n    SELECT\n        oi.seller_id,\n        AVG(orv.review_score) AS avg_review_score\n    FROM\n        order_items oi\n        JOIN orders o ON oi.order_id = o.order_id\n        JOIN order_reviews orv ON o.order_id = orv.order_id\n    GROUP BY\n        oi.seller_id\n),\nseller2 AS (\n    SELECT \n        oi.seller_id, \n        SUM(oi.price) AS total_sales,\n        AVG(oi.price) AS avg_price,\n        AVG(julianday(o.order_delivered_carrier_date) - julianday(o.order_purchase_timestamp)) AS avg_packing_time,\n        COUNT(*) AS product_cnt,\n        mpf.highlight_product\n    FROM \n        order_items AS oi\n    LEFT JOIN \n        orders AS o \n    ON \n        o.order_id = oi.order_id\n    LEFT JOIN \n        max_product_frequency AS mpf \n    ON \n        oi.seller_id = mpf.seller_id\n    GROUP BY \n        oi.seller_id\n)\nSELECT \n    s.seller_id, \n    s.product_cnt, \n    s.avg_price, \n    s.total_sales, \n    s.avg_packing_time, \n    rs.avg_review_score, \n    s.highlight_product\nFROM \n    seller2 s\nJOIN \n    reviewscore rs\nON rs.seller_id = s.seller_id\nWHERE \n    s.product_cnt > 100",
        "external_knowledge": null,
        "plan": "1. **Translate Product Categories:**\n   - Create a temporary table to map product IDs to their English category names by joining the products table with the category translation table.\n\n2. **Calculate Product Frequency:**\n   - Generate a temporary table to count how many times each product appears for each seller by joining the order items table with the translated products table. Group the results by seller and product.\n\n3. **Identify Top-Selling Product for Each Seller:**\n   - Create a temporary table to determine the top-selling product for each seller. This involves selecting the product with the highest count for each seller from the product frequency table.\n\n4. **Aggregate Seller Performance Metrics:**\n   - Construct a temporary table to aggregate various performance metrics for each seller, including total sales, average item price, average review scores, and average packing times. This is achieved by joining the order items, orders, and reviews tables. Also, include the top-selling product from the previous step and group the results by seller.\n\n5. **Filter Sellers by Product Count:**\n   - Select sellers who handle more than 100 products from the aggregated performance metrics table created in the previous step.\n\n6. **Present the Final Report:**\n   - Retrieve and display the final report containing seller ID, product count, average price, total sales, average packing time, average review score, and their top-selling product for sellers who meet the product count criteria.",
        "special_function": null
    },
    {
        "instance_id": "local301",
        "db": "bank_sales_trading",
        "question": "For weekly-sales data, I need an analysis of our sales performance around mid-June for the years 2018, 2019, and 2020. Specifically, calculate the percentage change in sales between the four weeks leading up to June 15 and the four weeks following June 15 for each year.",
        "SQL": "SELECT \n    before_effect,\n    after_effect,\n    after_effect - before_effect AS change_amount,\n    ROUND(((after_effect * 1.0 / before_effect) - 1) * 100, 2) AS percent_change,\n    '2018' AS year\nFROM (\n    SELECT \n        SUM(CASE WHEN delta_weeks BETWEEN 1 AND 4 THEN sales END) AS after_effect,\n        SUM(CASE WHEN delta_weeks BETWEEN -3 AND 0 THEN sales END) AS before_effect\n    FROM (\n        SELECT \n            week_date,\n            ROUND((JULIANDAY(week_date) - JULIANDAY('2018-06-15')) / 7.0) + 1 AS delta_weeks,\n            sales \n        FROM cleaned_weekly_sales\n    ) add_delta_weeks\n) AS add_before_after\nUNION ALL\nSELECT \n    before_effect,\n    after_effect,\n    after_effect - before_effect AS change_amount,\n    ROUND(((after_effect * 1.0 / before_effect) - 1) * 100, 2) AS percent_change,\n    '2019' AS year\nFROM (\n    SELECT \n        SUM(CASE WHEN delta_weeks BETWEEN 1 AND 4 THEN sales END) AS after_effect,\n        SUM(CASE WHEN delta_weeks BETWEEN -3 AND 0 THEN sales END) AS before_effect\n    FROM (\n        SELECT \n            week_date,\n            ROUND((JULIANDAY(week_date) - JULIANDAY('2019-06-15')) / 7.0) + 1 AS delta_weeks,\n            sales \n        FROM cleaned_weekly_sales\n    ) add_delta_weeks\n) AS add_before_after\nUNION ALL\nSELECT \n    before_effect,\n    after_effect,\n    after_effect - before_effect AS change_amount,\n    ROUND(((after_effect * 1.0 / before_effect) - 1) * 100, 2) AS percent_change,\n    '2020' AS year\nFROM (\n    SELECT \n        SUM(CASE WHEN delta_weeks BETWEEN 1 AND 4 THEN sales END) AS after_effect,\n        SUM(CASE WHEN delta_weeks BETWEEN -3 AND 0 THEN sales END) AS before_effect\n    FROM (\n        SELECT \n            week_date,\n            ROUND((JULIANDAY(week_date) - JULIANDAY('2020-06-15')) / 7.0) + 1 AS delta_weeks,\n            sales \n        FROM cleaned_weekly_sales\n    ) add_delta_weeks\n) AS add_before_after\nORDER BY year;",
        "external_knowledge": null,
        "plan": "1. **Subquery for Date Difference Calculation**:\n    - Calculate the difference in days between each record's date and June 15 of the respective year.\n    - Convert this difference into weeks by dividing by 7 and rounding the result.\n    - Label these weekly differences as `delta_weeks`.\n\n2. **Subquery for Aggregating Sales Before and After June 15**:\n    - Sum the sales for the four weeks following June 15 (`delta_weeks` between 1 and 4) and label this sum as `after_effect`.\n    - Sum the sales for the four weeks leading up to June 15 (`delta_weeks` between -3 and 0) and label this sum as `before_effect`.\n\n3. **Main Query for Year 2018**:\n    - Retrieve `before_effect` and `after_effect` from the subquery.\n    - Calculate the absolute change in sales as `after_effect - before_effect`.\n    - Calculate the percentage change as `((after_effect / before_effect) - 1) * 100` and round it to two decimal places.\n    - Label the result with the year '2018'.\n\n4. **Repeat Steps for Year 2019**:\n    - Perform the same calculations and aggregations as in step 3, but for the year 2019.\n    - Label the result with the year '2019'.\n\n5. **Repeat Steps for Year 2020**:\n    - Perform the same calculations and aggregations as in step 3, but for the year 2020.\n    - Label the result with the year '2020'.\n\n6. **Combine Results Using UNION ALL**:\n    - Combine the results from the three years (2018, 2019, 2020) using the `UNION ALL` operator.\n\n7. **Order the Final Results**:\n    - Order the combined results by year for clarity.",
        "special_function": null
    },
    {
        "instance_id": "local302",
        "db": "bank_sales_trading",
        "question": "Analyze the average sales performance impact 12 weeks before and after June 15, 2020, across various attributes like regions, platforms, age bands, demographics, and customer types. Identify and provide the attribute with the highest negative impact on sales, detailing the average percentage change in sales for that attribute.",
        "SQL": "SELECT metric, AVG(percent_change) AS avg_percent_change\nFROM (\n    SELECT 'region' AS metric,\n           LOWER(region) AS value,\n           before_effect,\n           after_effect,\n           after_effect - before_effect AS change,\n           (((after_effect * 1.0 / before_effect) - 1) * 100) AS percent_change\n    FROM (\n        SELECT region,\n               SUM(CASE WHEN delta_weeks BETWEEN 1 AND 12 THEN sales END) AS after_effect,\n               SUM(CASE WHEN delta_weeks BETWEEN -11 AND 0 THEN sales END) AS before_effect\n        FROM (\n            SELECT region,\n                   week_date,\n                   ((JULIANDAY(week_date) - JULIANDAY('2020-06-15')) / 7.0) + 1 AS delta_weeks,\n                   sales \n            FROM cleaned_weekly_sales\n        ) add_delta_weeks\n        GROUP BY region\n    ) AS add_before_after\n\n    UNION ALL \n\n    SELECT 'platform' AS metric,\n           LOWER(platform) AS value,\n           before_effect,\n           after_effect,\n           after_effect - before_effect AS change,\n           (((after_effect * 1.0 / before_effect) - 1) * 100) AS percent_change\n    FROM (\n        SELECT platform,\n               SUM(CASE WHEN delta_weeks BETWEEN 1 AND 12 THEN sales END) AS after_effect,\n               SUM(CASE WHEN delta_weeks BETWEEN -11 AND 0 THEN sales END) AS before_effect\n        FROM (\n            SELECT platform,\n                   week_date,\n                   ((JULIANDAY(week_date) - JULIANDAY('2020-06-15')) / 7.0) + 1 AS delta_weeks,\n                   sales \n            FROM cleaned_weekly_sales\n        ) add_delta_weeks\n        GROUP BY platform\n    ) AS add_before_after\n\n    UNION ALL \n\n    SELECT 'age_band' AS metric,\n           LOWER(age_band) AS value,\n           before_effect,\n           after_effect,\n           after_effect - before_effect AS change,\n           (((after_effect * 1.0 / before_effect) - 1) * 100) AS percent_change\n    FROM (\n        SELECT age_band,\n               SUM(CASE WHEN delta_weeks BETWEEN 1 AND 12 THEN sales END) AS after_effect,\n               SUM(CASE WHEN delta_weeks BETWEEN -11 AND 0 THEN sales END) AS before_effect\n        FROM (\n            SELECT age_band,\n                   week_date,\n                   ((JULIANDAY(week_date) - JULIANDAY('2020-06-15')) / 7.0) + 1 AS delta_weeks,\n                   sales \n            FROM cleaned_weekly_sales\n        ) add_delta_weeks\n        GROUP BY age_band\n    ) AS add_before_after\n\n    UNION ALL \n\n    SELECT 'demographic' AS metric,\n           LOWER(demographic) AS value,\n           before_effect,\n           after_effect,\n           after_effect - before_effect AS change,\n           (((after_effect * 1.0 / before_effect) - 1) * 100) AS percent_change\n    FROM (\n        SELECT demographic,\n               SUM(CASE WHEN delta_weeks BETWEEN 1 AND 12 THEN sales END) AS after_effect,\n               SUM(CASE WHEN delta_weeks BETWEEN -11 AND 0 THEN sales END) AS before_effect\n        FROM (\n            SELECT demographic,\n                   week_date,\n                   ((JULIANDAY(week_date) - JULIANDAY('2020-06-15')) / 7.0) + 1 AS delta_weeks,\n                   sales \n            FROM cleaned_weekly_sales\n        ) add_delta_weeks\n        GROUP BY demographic\n    ) AS add_before_after\n\n    UNION ALL \n\n    SELECT 'customer_type' AS metric,\n           LOWER(customer_type) AS value,\n           before_effect,\n           after_effect,\n           after_effect - before_effect AS change,\n           (((after_effect * 1.0 / before_effect) - 1) * 100) AS percent_change\n    FROM (\n        SELECT customer_type,\n               SUM(CASE WHEN delta_weeks BETWEEN 1 AND 12 THEN sales END) AS after_effect,\n               SUM(CASE WHEN delta_weeks BETWEEN -11 AND 0 THEN sales END) AS before_effect\n        FROM (\n            SELECT customer_type,\n                   week_date,\n                   ((JULIANDAY(week_date) - JULIANDAY('2020-06-15')) / 7.0) + 1 AS delta_weeks,\n                   sales \n            FROM cleaned_weekly_sales\n        ) add_delta_weeks\n        GROUP BY customer_type\n    ) AS add_before_after\n) AS tmp\nGROUP BY metric\nORDER BY avg_percent_change\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "1. **Define Metrics and Time Frames:**\n   - Identify the metrics to be analyzed: region, platform, age band, demographic, and customer type.\n   - Calculate the sales performance for two time frames: 12 weeks before and 12 weeks after a specific date (June 15, 2020).\n\n2. **Calculate Week Differences:**\n   - For each metric, determine the number of weeks before and after the specific date by calculating the difference in days and converting it to weeks.\n\n3. **Aggregate Sales Data:**\n   - For each metric, sum the sales data for the periods before and after the specific date.\n   - Use conditional aggregation to separate sales data into \"before\" and \"after\" periods based on the calculated week differences.\n\n4. **Compute Sales Change and Percentage Change:**\n   - For each metric, calculate the change in sales by subtracting the total sales in the \"before\" period from the total sales in the \"after\" period.\n   - Compute the percentage change in sales by comparing the \"after\" sales to the \"before\" sales, and convert it to a percentage.\n\n5. **Combine Results for All Metrics:**\n   - Combine the results for all the metrics into a single dataset, ensuring that each metric's results are labeled appropriately.\n\n6. **Calculate Average Percentage Change:**\n   - For each metric, compute the average percentage change in sales across all instances of that metric.\n\n7. **Identify the Highest Negative Impact:**\n   - Sort the metrics based on their average percentage change in ascending order to identify the metric with the highest negative impact on sales.\n   - Select the metric with the lowest average percentage change and limit the result to the top one.\n\n8. **Output the Result:**\n   - Return the metric name and its corresponding average percentage change to identify the area with the highest negative impact on sales performance.",
        "special_function": null
    },
    {
        "instance_id": "local329",
        "db": "log",
        "question": "How many unique sessions visited the /regist/input page and then the /regist/confirm page, in that order?",
        "SQL": "WITH mst_fallout_step AS (\n  -- Define the stages and paths\n  SELECT \n      1 AS step, '/regist/input' AS path\n  UNION ALL\n  SELECT \n      2 AS step, '/regist/confirm' AS path\n),\nform_log_with_fallout_step AS (\n  SELECT\n      l.session,\n      m.step,\n      m.path,\n      MAX(l.stamp) AS max_stamp,\n      MIN(l.stamp) AS min_stamp\n  FROM\n      mst_fallout_step AS m\n      JOIN form_log AS l\n      ON m.path = l.path\n  WHERE \n      status = ''\n  GROUP BY \n      l.session, m.step, m.path\n),\nform_log_with_mod_fallout_step AS (\n  SELECT\n      session,\n      step,\n      path,\n      max_stamp,\n      (SELECT MIN(min_stamp)\n  FROM \n      form_log_with_fallout_step AS prev\n  WHERE \n      prev.session = curr.session AND prev.step = curr.step - 1\n      ) AS lag_min_stamp,\n      (SELECT \n          MIN(step) \n       FROM \n          form_log_with_fallout_step AS min_step\n       WHERE \n          min_step.session = curr.session\n      ) AS min_step,\n      (SELECT \n          COUNT(*)\n       FROM \n           form_log_with_fallout_step AS count_step\n       WHERE \n           count_step.session = curr.session AND count_step.step <= curr.step\n       ) AS cum_count\n  FROM form_log_with_fallout_step AS curr\n),\nfallout_log AS (\n  SELECT\n    session,\n    step,\n    path,\n    max_stamp\n  FROM \n    form_log_with_mod_fallout_step\n  WHERE \n    min_step = 1\n    AND step = cum_count\n    AND (lag_min_stamp IS NULL OR max_stamp >= lag_min_stamp)\n),\ninput_to_confirm_counts AS (\n  SELECT\n    COUNT(DISTINCT input.session) AS count\n  FROM \n    fallout_log AS input\n  JOIN \n    fallout_log AS confirm\n  ON \n    input.session = confirm.session\n  WHERE \n    input.path = '/regist/input'\n    AND confirm.path = '/regist/confirm'\n    AND input.max_stamp < confirm.max_stamp\n)\nSELECT\n  count\nFROM \n  input_to_confirm_counts;",
        "external_knowledge": null,
        "plan": "1. **Define Stages and Paths**:\n   - Create a temporary table to define the stages and their corresponding paths. In this case, stage 1 is associated with the first page, and stage 2 is associated with the second page.\n\n2. **Map Logs to Stages**:\n   - Join the log data with the stages based on the paths.\n   - For each session and stage, calculate the earliest and latest timestamps when the stage was accessed.\n   - Filter out any logs with non-empty statuses.\n\n3. **Calculate Previous Stage Timestamps**:\n   - For each session and stage, retrieve the minimum timestamp from the previous stage.\n   - Determine the initial stage for each session.\n   - Count the cumulative stages completed by each session up to the current stage.\n\n4. **Identify Valid Session Paths**:\n   - Filter sessions where the initial stage is correctly identified.\n   - Ensure the cumulative count of stages matches the current stage.\n   - Validate that the timestamp conditions between stages are satisfied (i.e., the current stage's timestamp should be greater than or equal to the previous stage's timestamp).\n\n5. **Count Sessions Matching Path Sequence**:\n   - Join the valid session paths for the first stage with those of the second stage within the same session.\n   - Ensure that the timestamp of the first stage is earlier than the timestamp of the second stage.\n   - Count the number of unique sessions that follow the specified path sequence from the first stage to the second stage.\n\n6. **Return the Result**:\n   - Output the count of unique sessions that visited the first page and then the second page in the specified order.",
        "special_function": null
    },
    {
        "instance_id": "local330",
        "db": "log",
        "question": "For each web page, how many unique user sessions either start or end there",
        "SQL": "WITH activity_log_with_landing_exit AS (\n  SELECT\n    session,\n    path,\n    stamp,\n    FIRST_VALUE(path) OVER (\n      PARTITION BY session\n      ORDER BY stamp ASC\n      ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n    ) AS landing,\n    LAST_VALUE(path) OVER (\n      PARTITION BY session\n      ORDER BY stamp ASC\n      ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n    ) AS exit\n  FROM activity_log\n),\nlanding_count AS (\n  SELECT\n    landing AS path,\n    COUNT(DISTINCT session) AS count\n  FROM\n    activity_log_with_landing_exit\n  GROUP BY landing\n),\nexit_count AS (\n  SELECT\n    exit AS path,\n    COUNT(DISTINCT session) AS count\n  FROM\n    activity_log_with_landing_exit\n  GROUP BY exit\n),\ncombined_counts AS (\n  SELECT path, SUM(count) AS total_count\n  FROM (\n    SELECT path, count FROM landing_count\n    UNION ALL\n    SELECT path, count FROM exit_count\n  ) AS combined\n  GROUP BY path\n)\nSELECT path, total_count\nFROM combined_counts\nORDER BY total_count DESC",
        "external_knowledge": null,
        "plan": "1. **Identify Session Boundaries**:\n   - Create a temporary dataset that includes each session's first and last path entries by utilizing window functions. This helps in determining the landing and exit paths for each session.\n\n2. **Calculate Landing Path Counts**:\n   - From the temporary dataset, count the number of distinct sessions for each landing path. This results in a dataset that represents how many times each path was a landing path.\n\n3. **Calculate Exit Path Counts**:\n   - Similarly, from the same temporary dataset, count the number of distinct sessions for each exit path. This results in a dataset that represents how many times each path was an exit path.\n\n4. **Combine Landing and Exit Counts**:\n   - Merge the two datasets (landing counts and exit counts) into one, ensuring that paths that appear in both datasets are combined. Sum the counts for each path to get the total count of visits across both landing and exit types.\n\n5. **Determine the Path with the Highest Total Count**:\n   - Sort the combined dataset by the total count of visits in descending order and select the top entry. This final step gives the path with the highest combined total of landing and exit counts.",
        "special_function": null
    },
    {
        "instance_id": "local331",
        "db": "log",
        "question": "List the three most common third actions users take after visiting the `/detail` page twice in a row, including each action's occurrence count.",
        "SQL": "WITH activity_log_with_lead_path AS (\n  SELECT\n    session,\n    stamp,\n    path AS path0,\n    COALESCE(LEAD(path, 1) OVER(PARTITION BY session ORDER BY stamp ASC), 'END') AS path1,\n    COALESCE(LEAD(path, 2) OVER(PARTITION BY session ORDER BY stamp ASC), 'END') AS path2\n  FROM\n    activity_log\n),\nraw_user_flow AS (\n  SELECT\n    path0,\n    SUM(COUNT(1)) OVER() AS count0,\n    path1,\n    SUM(COUNT(1)) OVER(PARTITION BY path0, path1) AS count1,\n    path2,\n    COUNT(1) AS count2\n  FROM\n    activity_log_with_lead_path\n  WHERE\n    path0 = '/detail'\n  GROUP BY\n    path0, path1, path2\n),\nranked_user_flow AS (\n  SELECT\n    path0,\n    count0,\n    path1,\n    count1,\n    path2,\n    count2,\n    LAG(path0) OVER(ORDER BY count1 DESC, count2 DESC) AS prev_path0,\n    LAG(path0 || path1) OVER(ORDER BY count1 DESC, count2 DESC) AS prev_path0_path1,\n    LAG(path0 || path1 || path2) OVER(ORDER BY count1 DESC, count2 DESC) AS prev_path0_path1_path2\n  FROM\n    raw_user_flow\n)\nSELECT\n  path2,\n  count2\nFROM\n  ranked_user_flow\nWHERE\n  path0 = '/detail' AND path1 = '/detail'\nORDER BY\n  count2 DESC\nLIMIT 3;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local358",
        "db": "log",
        "question": "How many users are there in each age category (20s, 30s, 40s, 50s, and others)?",
        "SQL": "WITH mst_users_with_int_birth_date AS (\n  SELECT\n    *,\n    REPLACE(strftime('%Y%m%d', 'now'), '-', '') AS int_cur_date,\n    REPLACE(birth_date, '-', '') AS int_birth_date\n  FROM \n      mst_users\n),\n\nmst_users_with_age AS (\n  SELECT\n    *,\n    FLOOR((int_cur_date - int_birth_date) / 10000) AS age\n  FROM \n      mst_users_with_int_birth_date\n),\n\nmst_users_with_category AS (\n  SELECT\n    user_id,\n    sex,\n    age,\n    CASE\n      WHEN age BETWEEN 50 AND 59 THEN '50s'\n      WHEN age BETWEEN 40 AND 49 THEN '40s'\n      WHEN age BETWEEN 30 AND 39 THEN '30s'\n      WHEN age BETWEEN 20 AND 29 THEN '20s'\n    ELSE\n        'others'\n    END AS category\n  FROM \n      mst_users_with_age\n)\nSELECT\n  category,\n  COUNT(user_id) AS user_count\nFROM \n    mst_users_with_category\nGROUP BY \n    category\nORDER BY \n    category;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local360",
        "db": "log",
        "question": "Identify the sessions with the fewest events lacking both '/detail' clicks and '/complete' conversions, considering only non-empty search types. If multiple sessions share the lowest count, include all of them. For each session, display the associated paths and search types.",
        "SQL": "WITH activity_log_with_session_click_conversion_flag AS (\n  SELECT\n    session,\n    stamp,\n    path,\n    search_type,\n    CASE\n      WHEN LAG(path) OVER (PARTITION BY session ORDER BY stamp DESC) = '/detail'\n        THEN 1\n      ELSE 0\n    END AS has_session_click,\n    SIGN(\n      SUM(CASE WHEN path = '/complete' THEN 1 ELSE 0 END)\n      OVER (PARTITION BY session ORDER BY stamp DESC\n        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)\n    ) AS has_session_conversion\n  FROM\n    activity_log\n),\n\ncounts AS (\n  SELECT\n    session,\n    path,\n    search_type,\n    COUNT(*) AS count_zeros\n  FROM\n    activity_log_with_session_click_conversion_flag\n  WHERE\n    has_session_click = 0\n    AND has_session_conversion = 0\n    AND search_type IS NOT NULL\n    AND TRIM(search_type) <> ''\n  GROUP BY\n    session,\n    path,\n    search_type\n),\n\nmin_count AS (\n  SELECT\n    MIN(count_zeros) AS min_zeros\n  FROM\n    counts\n)\n\nSELECT\n  c.session,\n  c.path,\n  c.search_type\nFROM\n  counts c\nJOIN\n  min_count mc ON c.count_zeros = mc.min_zeros\nORDER BY\n  c.count_zeros;",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local344",
        "db": "f1",
        "question": "How many times has each type of overtake occurred in Formula 1?",
        "SQL": "SELECT\n  overtake_type,\n  COUNT(*) AS overtake_count\nFROM (\n  SELECT DISTINCT\n    lap_positions.race_id,\n    lap_positions.driver_id AS overtaking_driver_id,\n    lap_positions.lap,\n    lap_positions.position AS current_position,\n    previous_lap.position AS previous_position,\n    cars_behind_this_lap.driver_id AS overtaken_driver_id,\n    CASE\n      WHEN retirements.driver_id IS NOT NULL THEN 'R'\n      WHEN pit_stops.lap = lap_positions.lap THEN 'P'\n      WHEN pit_stops.milliseconds > overtaking_lap_times.running_milliseconds - overtaken_lap_times.running_milliseconds THEN 'P'\n      WHEN lap_positions.lap = 1 AND (previous_lap.position - cars_behind_this_lap_results.grid) <= 2 THEN 'S'\n      ELSE 'T'\n    END AS overtake_type,\n    CASE\n      WHEN retirements.driver_id IS NOT NULL THEN retirements.retirement_type\n      WHEN pit_stops.lap = lap_positions.lap THEN 'Pit Stop (Pit Entry)'\n      WHEN pit_stops.milliseconds > overtaking_lap_times.running_milliseconds - overtaken_lap_times.running_milliseconds THEN 'Pit Stop (Pit Exit)'\n      WHEN lap_positions.lap = 1 AND (previous_lap.position - cars_behind_this_lap_results.grid) <= 2 THEN 'Start'\n      ELSE 'Track'\n    END AS overtake_desc\n  FROM lap_positions\n    INNER JOIN races_ext AS races\n      ON races.race_id = lap_positions.race_id\n      AND races.is_pit_data_available = 1\n    INNER JOIN lap_positions AS previous_lap\n      ON previous_lap.race_id = lap_positions.race_id\n      AND previous_lap.driver_id = lap_positions.driver_id\n      AND previous_lap.lap = lap_positions.lap - 1\n    INNER JOIN lap_positions AS cars_behind_this_lap /*Join to ALL cars behind on this lap*/\n      ON cars_behind_this_lap.race_id = lap_positions.race_id\n      AND cars_behind_this_lap.lap = lap_positions.lap\n      AND cars_behind_this_lap.position > lap_positions.position\n    LEFT JOIN results AS cars_behind_this_lap_results\n      ON cars_behind_this_lap_results.race_id = lap_positions.race_id\n      AND cars_behind_this_lap_results.driver_id = cars_behind_this_lap.driver_id\n    LEFT JOIN lap_positions AS cars_behind_last_lap\n      ON cars_behind_last_lap.race_id = lap_positions.race_id\n      AND cars_behind_last_lap.lap = lap_positions.lap - 1\n      AND cars_behind_last_lap.driver_id = cars_behind_this_lap.driver_id /*NOT lap_positions.driver_id!!!!!!*/\n      AND cars_behind_last_lap.position > previous_lap.position\n    LEFT JOIN retirements\n      ON retirements.race_id = lap_positions.race_id\n      AND retirements.lap = lap_positions.lap\n      AND retirements.driver_id = cars_behind_this_lap.driver_id\n    LEFT JOIN pit_stops AS pit_stops\n      ON pit_stops.race_id = lap_positions.race_id\n      AND pit_stops.lap BETWEEN lap_positions.lap - 1 AND lap_positions.lap /*This lets us bring in pit stops from the \"previous\" lap*/\n      AND pit_stops.driver_id = cars_behind_this_lap.driver_id\n    LEFT JOIN lap_times_ext AS overtaking_lap_times\n      ON overtaking_lap_times.race_id = lap_positions.race_id\n      AND overtaking_lap_times.driver_id = lap_positions.driver_id\n      AND overtaking_lap_times.lap = pit_stops.lap - 1\n    LEFT JOIN lap_times_ext AS overtaken_lap_times\n      ON overtaken_lap_times.race_id = lap_positions.race_id\n      AND overtaken_lap_times.driver_id = pit_stops.driver_id\n      AND overtaken_lap_times.lap = pit_stops.lap - 1\n  WHERE\n    cars_behind_last_lap.driver_id IS NULL /*The car was NOT behind last lap (but is behind this lap due to the INNER JOIN)*/\n) AS overtakes\nGROUP BY overtake_type;",
        "external_knowledge": "f1_overtake.md",
        "plan": "1. **Select Overtake Type and Count**: The query aims to count the number of occurrences for each type of overtake in Formula 1.\n\n2. **Subquery Setup**: Start with a subquery to prepare the data required for counting overtakes. This subquery includes various joins and calculations to determine the overtake types.\n\n3. **Distinct Overtakes**: Ensure that only distinct overtakes are considered by selecting unique combinations of race, driver, lap, and positions.\n\n4. **Join Previous Lap Data**: Join the current lap positions with the previous lap positions of the same driver to compare and identify changes in positions.\n\n5. **Identify Overtaken Cars**: Join the current lap positions with the positions of cars that are behind the current driver on the same lap.\n\n6. **Exclude Cars that Were Behind in Previous Lap**: Use a left join to exclude cars that were already behind in the previous lap to focus on new overtakes.\n\n7. **Identify Special Conditions**: Use left joins with additional tables to identify if the overtake was influenced by special conditions such as retirements, pit stops, or if it occurred at the start of the race.\n\n8. **Determine Overtake Type**: Use conditional logic to classify the overtake into types like retirement-induced, pit stop-related, start of the race, or regular track overtake.\n\n9. **Finalize Subquery Data**: Select the relevant fields from the subquery that include the overtake type and additional details for each identified overtake.\n\n10. **Count Overtakes by Type**: Group the results by the overtake type and count the occurrences of each type to get the final result.\n\n11. **Return Results**: The final query returns the count of each type of overtake, satisfying the user's request to know how many times each type of overtake occurred in Formula 1.",
        "special_function": null
    },
    {
        "instance_id": "local336",
        "db": "f1",
        "question": "How many overtakes of each type occurred during the first five laps of the race?",
        "SQL": "SELECT\n  overtake_type,\n  COUNT(*) AS overtake_count\nFROM (\n  SELECT DISTINCT\n    lap_positions.race_id,\n    lap_positions.driver_id AS overtaking_driver_id,\n    lap_positions.lap,\n    cars_behind_this_lap.driver_id AS overtaken_driver_id,\n    CASE\n      WHEN retirements.driver_id IS NOT NULL THEN 'R'\n      WHEN pit_stops.lap = lap_positions.lap THEN 'P'\n      WHEN pit_stops.milliseconds > overtaking_lap_times.running_milliseconds - overtaken_lap_times.running_milliseconds THEN 'P'\n      WHEN lap_positions.lap = 1 AND (previous_lap.position - cars_behind_this_lap_results.grid) <= 2 THEN 'S'\n      ELSE 'T'\n    END AS overtake_type\n  FROM lap_positions\n    INNER JOIN races_ext AS races\n      ON races.race_id = lap_positions.race_id\n      AND races.is_pit_data_available = 1\n    INNER JOIN lap_positions AS previous_lap\n      ON previous_lap.race_id = lap_positions.race_id\n      AND previous_lap.driver_id = lap_positions.driver_id\n      AND previous_lap.lap = lap_positions.lap - 1\n    INNER JOIN lap_positions AS cars_behind_this_lap\n      ON cars_behind_this_lap.race_id = lap_positions.race_id\n      AND cars_behind_this_lap.lap = lap_positions.lap\n      AND cars_behind_this_lap.position > lap_positions.position\n    LEFT JOIN results AS cars_behind_this_lap_results\n      ON cars_behind_this_lap_results.race_id = lap_positions.race_id\n      AND cars_behind_this_lap_results.driver_id = cars_behind_this_lap.driver_id\n    LEFT JOIN lap_positions AS cars_behind_last_lap\n      ON cars_behind_last_lap.race_id = lap_positions.race_id\n      AND cars_behind_last_lap.lap = lap_positions.lap - 1\n      AND cars_behind_last_lap.driver_id = cars_behind_this_lap.driver_id\n      AND cars_behind_last_lap.position > previous_lap.position\n    LEFT JOIN retirements\n      ON retirements.race_id = lap_positions.race_id\n      AND retirements.lap = lap_positions.lap\n      AND retirements.driver_id = cars_behind_this_lap.driver_id\n    LEFT JOIN pit_stops AS pit_stops\n      ON pit_stops.race_id = lap_positions.race_id\n      AND pit_stops.lap BETWEEN lap_positions.lap - 1 AND lap_positions.lap\n      AND pit_stops.driver_id = cars_behind_this_lap.driver_id\n    LEFT JOIN lap_times_ext AS overtaking_lap_times\n      ON overtaking_lap_times.race_id = lap_positions.race_id\n      AND overtaking_lap_times.driver_id = lap_positions.driver_id\n      AND overtaking_lap_times.lap = pit_stops.lap - 1\n    LEFT JOIN lap_times_ext AS overtaken_lap_times\n      ON overtaken_lap_times.race_id = lap_positions.race_id\n      AND overtaken_lap_times.driver_id = pit_stops.driver_id\n      AND overtaken_lap_times.lap = pit_stops.lap - 1\n  WHERE\n    cars_behind_last_lap.driver_id IS NULL\n    AND lap_positions.lap <= 5 /* Filter for the first five laps */\n) AS overtakes\nGROUP BY overtake_type;",
        "external_knowledge": "f1_overtake.md",
        "plan": "1. **Identify Overtakes**: Start by determining which cars were overtaken in each lap by comparing the current lap's positions with the previous lap's positions.\n\n2. **Filter for First Five Laps**: Limit the data to only include overtakes that occurred within the first five laps of the race.\n\n3. **Classify Overtake Types**: For each identified overtake, classify the type of overtake based on specific conditions:\n   - If the overtaken driver retired during the lap, classify as 'R'.\n   - If the overtake occurred during a pit stop, classify as 'P'.\n   - If the time spent during the pit stop was greater than the time difference between the overtaking and overtaken drivers, classify as 'P'.\n   - If the overtake occurred in the first lap and the position change was small, classify as 'S'.\n   - Otherwise, classify as 'T'.\n\n4. **Ensure Unique Overtakes**: Use distinct records to avoid counting the same overtake multiple times.\n\n5. **Group and Count Overtakes**: Group the overtakes by their type and count the occurrences of each type.\n\n6. **Return Results**: Present the count of each type of overtake that occurred during the first five laps of the race.",
        "special_function": null
    },
    {
        "instance_id": "local335",
        "db": "f1",
        "question": "Which five constructors have had the most seasons in the 21st century where their drivers scored the fewest points in a Formula 1 season?",
        "SQL": "WITH points AS (\n    SELECT \n        year AS season,\n        driver_id,\n        constructor_id,\n        SUM(points) AS points\n      FROM \n        results JOIN races ON results.race_id = races.race_id\n     GROUP BY \n         year,\n         driver_id,\n         constructor_id\n    HAVING \n         SUM(points) > 0\n),\ntops AS (\n    SELECT \n         season,\n         MIN(points) FILTER (WHERE driver_id IS NOT NULL) AS min_driver_points,\n         MIN(points) FILTER (WHERE constructor_id IS NOT NULL) AS min_constructor_points\n      FROM \n         points\n     GROUP BY \n         season\n),\nlosers AS (\n    SELECT \n         tops.season,\n         lose_driver.driver_id,\n         lose_driver.points AS driver_points,\n         lose_constructor.constructor_id,\n         lose_constructor.points AS constructor_points\n      FROM \n         tops JOIN points AS lose_driver ON lose_driver.season = tops.season AND \n                                    lose_driver.constructor_id IS NOT NULL AND \n                                    lose_driver.points = tops.min_constructor_points\n              JOIN points AS lose_constructor ON lose_constructor.season = tops.season AND \n                                         lose_constructor.driver_id IS NOT NULL AND \n                                         lose_constructor.points = tops.min_driver_points\n)\nSELECT \n     constructors.name AS Constructor\n  FROM \n     losers JOIN constructors ON losers.constructor_id = constructors.constructor_id\n WHERE \n     losers.season >= 2001\n GROUP BY \n     losers.constructor_id,\n     constructors.name\n ORDER BY \n     COUNT( * ) DESC\n LIMIT 5;",
        "external_knowledge": null,
        "plan": "1. **Identify Seasons with Points**:\n    - Combine data from two related sets to determine the total points scored by each participant in each season.\n    - Filter to include only entries where participants scored points.\n\n2. **Find Minimum Points per Season**:\n    - For each season, find the lowest points scored by any participant, considering both individual participants and their associated groups separately.\n\n3. **Determine Participants with Fewest Points**:\n    - For each season, identify the participants and their groups who scored the fewest points based on the previously calculated minimum points.\n\n4. **Filter and Count Seasons in the 21st Century**:\n    - Focus on seasons starting from the year 2001.\n    - Count how many times each group appears as having participants with the fewest points in a season.\n\n5. **Select Top Groups**:\n    - Sort the groups by the number of seasons they had participants scoring the fewest points.\n    - Limit the result to the top five groups.",
        "special_function": null
    },
    {
        "instance_id": "local309",
        "db": "f1",
        "question": "For each year, which driver and which constructor scored the most points? I want the full name of each driver.",
        "SQL": "with year_points as (\n    select races.year,\n           drivers.forename || ' ' || drivers.surname as driver,\n           constructors.name as constructor,\n           sum(results.points) as points\n    from results\n    left join races on results.race_id = races.race_id  -- Ensure these columns exist in your schema\n    left join drivers on results.driver_id = drivers.driver_id  -- Ensure these columns exist in your schema\n    left join constructors on results.constructor_id = constructors.constructor_id  -- Ensure these columns exist in your schema\n    group by races.year, driver\n    union\n    select races.year,\n           null as driver,\n           constructors.name as constructor,\n           sum(results.points) as points\n    from results\n    left join races on results.race_id = races.race_id  -- Ensure these columns exist in your schema\n    left join drivers on results.driver_id = drivers.driver_id  -- Ensure these columns exist in your schema\n    left join constructors on results.constructor_id = constructors.constructor_id  -- Ensure these columns exist in your schema\n    group by races.year, constructor\n),\nmax_points as (\n    select year,\n           max(case when driver is not null then points else null end) as max_driver_points,\n           max(case when constructor is not null then points else null end) as max_constructor_points\n    from year_points\n    group by year\n)\nselect max_points.year,\n       drivers_year_points.driver,\n       constructors_year_points.constructor\nfrom max_points\nleft join year_points as drivers_year_points on\n    max_points.year = drivers_year_points.year and\n    max_points.max_driver_points = drivers_year_points.points and\n    drivers_year_points.driver is not null\nleft join year_points as constructors_year_points on\n    max_points.year = constructors_year_points.year and\n    max_points.max_constructor_points = constructors_year_points.points and\n    constructors_year_points.constructor is not null\norder by max_points.year;",
        "external_knowledge": null,
        "plan": "1. **Calculate Yearly Points for Drivers and Constructors:**\n   - Create a temporary dataset to store the total points scored by each driver and constructor for each year.\n   - For drivers, concatenate their first and last names to form their full name and calculate their total points per year.\n   - For constructors, calculate their total points per year.\n\n2. **Identify Maximum Points per Year:**\n   - Create another temporary dataset to store the maximum points scored by any driver and any constructor for each year.\n   - Use conditional logic to separately find the maximum points for drivers and constructors.\n\n3. **Match Maximum Points to Corresponding Driver and Constructor:**\n   - Join the maximum points dataset with the yearly points dataset to identify the driver and constructor who scored the maximum points for each year.\n   - Ensure that the driver and constructor names are not null when performing the join.\n\n4. **Select and Order Results:**\n   - Select the year, the driver with the maximum points, and the constructor with the maximum points.\n   - Order the final results by year to present them in chronological order.",
        "special_function": null
    },
    {
        "instance_id": "local310",
        "db": "f1",
        "question": "List the three years where the sum of the highest points achieved by any driver and any constructor was the lowest",
        "SQL": "with year_points as (\n    select \n        races.year,\n        drivers.forename || ' ' || drivers.surname as driver,\n        constructors.name as constructor,\n        sum(results.points) as points\n    from \n        results\n    left join races on results.race_id = races.race_id\n    left join drivers on results.driver_id = drivers.driver_id\n    left join constructors on results.constructor_id = constructors.constructor_id\n    group by \n        races.year, \n        driver\n    union\n    select \n        races.year,\n        null as driver,\n        constructors.name as constructor,\n        sum(results.points) as points\n    from \n        results\n    left join races on results.race_id = races.race_id\n    left join drivers on results.driver_id = drivers.driver_id\n    left join constructors on results.constructor_id = constructors.constructor_id\n    group by \n        races.year, \n        constructor\n),\nmax_points as (\n    select \n        year,\n        max(case when driver is not null then points else null end) as max_driver_points,\n        max(case when constructor is not null then points else null end) as max_constructor_points\n    from \n        year_points\n    group by \n        year\n)\nselect \n    max_points.year\nfrom \n    max_points\nleft join year_points as drivers_year_points on\n    max_points.year = drivers_year_points.year and\n    max_points.max_driver_points = drivers_year_points.points and\n    drivers_year_points.driver is not null\nleft join year_points as constructors_year_points on\n    max_points.year = constructors_year_points.year and\n    max_points.max_constructor_points = constructors_year_points.points and\n    constructors_year_points.constructor is not null\norder by \n    max_points.max_driver_points + max_points.max_constructor_points ASC\nLIMIT 3;",
        "external_knowledge": null,
        "plan": "1. **Create Yearly Points Data**:\n   - Define a common table expression (CTE) to calculate yearly points for both drivers and constructors.\n   - Aggregate points by year and driver, and by year and constructor separately.\n   - Combine the results using a UNION to form a unified dataset containing year, driver, constructor, and their respective points.\n\n2. **Identify Maximum Points Per Year**:\n   - Define another CTE to determine the maximum points per year for both drivers and constructors.\n   - Use conditional logic to select the maximum points for a driver and a constructor for each year from the previously created dataset.\n\n3. **Retrieve Years with Corresponding Points**:\n   - Create a SELECT query to retrieve the years.\n   - Join the yearly maximum points data with the original yearly points data to match the maximum points to their respective drivers and constructors.\n   - Ensure that the join conditions correctly match the maximum points of drivers and constructors for the respective years, filtering out null values appropriately.\n\n4. **Sort and Limit Results**:\n   - Order the resulting data by the sum of maximum points for drivers and constructors in ascending order.\n   - Limit the results to the top three years with the lowest combined points between the top driver and top constructor.\n\n5. **Output**:\n   - Select and output the three years with the lowest combined points as per the specified criteria.",
        "special_function": null
    },
    {
        "instance_id": "local311",
        "db": "f1",
        "question": "Which constructors had the top 3 combined points from their best driver and team, and in which years did they achieve them?",
        "SQL": "with year_points as (\n    select \n        races.year,\n        drivers.forename || ' ' || drivers.surname as driver,\n        constructors.name as constructor,\n        sum(results.points) as points\n    from \n        results\n    left join races on results.race_id = races.race_id\n    left join drivers on results.driver_id = drivers.driver_id\n    left join constructors on results.constructor_id = constructors.constructor_id\n    group by \n        races.year, \n        driver\n    union\n    select \n        races.year,\n        null as driver,\n        constructors.name as constructor,\n        sum(results.points) as points\n    from \n        results\n    left join races on results.race_id = races.race_id\n    left join drivers on results.driver_id = drivers.driver_id\n    left join constructors on results.constructor_id = constructors.constructor_id\n    group by \n        races.year, \n        constructor\n),\nmax_points as (\n    select \n        year,\n        constructor,\n        max(case when driver is not null then points else null end) as max_driver_points,\n        max(case when constructor is not null then points else null end) as max_constructor_points\n    from \n        year_points\n    group by \n        year\n)\nselect \n    constructors_year_points.year,\n    max_points.constructor,\n    max_points.max_driver_points + max_points.max_constructor_points AS combined_points\nfrom \n    max_points\nleft join year_points as drivers_year_points on\n    max_points.year = drivers_year_points.year and\n    max_points.max_driver_points = drivers_year_points.points and\n    drivers_year_points.driver is not null\nleft join year_points as constructors_year_points on\n    max_points.year = constructors_year_points.year and\n    max_points.max_constructor_points = constructors_year_points.points and\n    constructors_year_points.constructor is not null\norder by \n    combined_points DESC\nLIMIT 3;",
        "external_knowledge": null,
        "plan": "1. **Create a Temporary Data Set for Yearly Points:**\n   - Combine data from multiple sources to get yearly points for each driver and constructor.\n   - Use a `LEFT JOIN` to merge related data.\n   - Group the data by year and driver/constructor to calculate total points.\n\n2. **Union Driver and Constructor Points:**\n   - Perform a union operation to integrate points for drivers and constructors.\n   - Ensure to separately handle drivers and constructors while calculating points.\n\n3. **Create Another Temporary Data Set for Maximum Points:**\n   - Aggregate the maximum points per year for both drivers and constructors.\n   - Use conditional logic to separate driver points and constructor points.\n\n4. **Calculate Combined Points:**\n   - Select the year and constructor.\n   - Add the maximum points of the best driver and the constructor within the same year to get combined points.\n\n5. **Join with Original Data Set to Get Details:**\n   - Join back with the original temporary data set to match maximum driver points and constructor points with their corresponding details.\n   - Ensure joins are done on the year and the specific points calculated.\n\n6. **Order and Limit the Results:**\n   - Order the results by the combined points in descending order to get the highest scores at the top.\n   - Limit the output to the top 3 entries to get the top constructors with their highest combined points.\n\n7. **Output:**\n   - Return the year, constructor name, and combined points for the top 3 results.",
        "special_function": null
    },
    {
        "instance_id": "local354",
        "db": "f1",
        "question": "Which Formula 1 drivers, during the 1950s, had seasons in which they did not change their constructors at the beginning and end of the year and participated in at least two different race rounds within those seasons?",
        "SQL": "WITH drives_prelim AS (\n  SELECT DISTINCT\n    races.year,\n    results.driver_id,\n    races.round,\n    results.constructor_id,\n    COALESCE(\n      CASE \n        WHEN results.constructor_id = LAG(results.constructor_id) OVER (\n          PARTITION BY races.year, results.driver_id\n          ORDER BY races.round ASC\n        ) THEN 0 \n        ELSE 1 \n      END, 1\n    ) AS is_first_race,\n    COALESCE(\n      CASE \n        WHEN results.constructor_id = LEAD(results.constructor_id) OVER (\n          PARTITION BY races.year, results.driver_id\n          ORDER BY races.round ASC\n        ) THEN 0 \n        ELSE 1 \n      END, 1\n    ) AS is_last_race\n  FROM \n      results\n  INNER JOIN races ON races.race_id = results.race_id\n),\nfirst_last_races AS (\n  SELECT\n    year,\n    driver_id,\n    MIN(round) AS first_round,\n    MAX(round) AS last_round\n  FROM \n      drives_prelim\n  GROUP BY \n      year, \n      driver_id\n)\nSELECT DISTINCT\n  dp.driver_id\nFROM \n    drives_prelim dp\nINNER JOIN first_last_races flr\n  ON dp.year = flr.year\n  AND dp.driver_id = flr.driver_id\n  AND (dp.round = flr.first_round OR dp.round = flr.last_round)\nWHERE \n    dp.is_first_race = 0\n  AND dp.is_last_race = 0\n  AND dp.year BETWEEN 1950 AND 1959\nGROUP BY \n    dp.driver_id\nHAVING \n    COUNT(DISTINCT dp.round) > 1;",
        "external_knowledge": null,
        "plan": "1. **Identify Preliminary Data**: Create a preliminary dataset that includes distinct combinations of years, driver IDs, race rounds, and constructor IDs. For each driver in each year, determine if a race is their first or last race of that season by checking if the constructor ID changes from the previous or next race using window functions.\n\n2. **Determine First and Last Races**: From the preliminary dataset, calculate the first and last race rounds for each driver in each year.\n\n3. **Filter Out First and Last Races**: Join the preliminary dataset with the dataset of first and last races to identify races that are either the first or last of the season for each driver.\n\n4. **Exclude First and Last Races**: Filter the results to exclude races that are the first or last race for the driver in that season.\n\n5. **Filter by Decade**: Narrow down the dataset to include only the years within the 1950s.\n\n6. **Ensure Multiple Races**: Group the remaining data by driver IDs and ensure that each driver participated in more than one race that was neither their first nor their last race of the season.\n\n7. **Select Unique Driver IDs**: Finally, select the distinct driver IDs who meet all the criteria and return this list as the result.",
        "special_function": null
    },
    {
        "instance_id": "local355",
        "db": "f1",
        "question": "Calculate the average first and last rounds of races missed by drivers each year. Only include drivers who missed fewer than three races annually and switched teams between their first and last missed races",
        "SQL": "WITH hiatus_prelim AS (\n  SELECT DISTINCT\n    races.year,\n    driver_standings.driver_id,\n    races.round,\n    previous_results.constructor_id AS previous_constructor_id,\n    next_results.constructor_id AS next_constructor_id,\n    CASE\n      WHEN previous_results.constructor_id IS NOT NULL THEN 1\n      ELSE 0\n    END AS is_first_race,\n    CASE\n      WHEN next_results.constructor_id IS NOT NULL THEN 1\n      ELSE 0\n    END AS is_last_race\n  FROM driver_standings_ext AS driver_standings\n  INNER JOIN races_ext AS races ON races.race_id = driver_standings.race_id\n  LEFT JOIN results\n    ON results.race_id = driver_standings.race_id\n    AND results.driver_id = driver_standings.driver_id\n  LEFT JOIN races_ext AS previous_race\n    ON previous_race.year = races.year\n    AND previous_race.round = races.round - 1\n  LEFT JOIN results AS previous_results\n    ON previous_results.race_id = previous_race.race_id\n    AND previous_results.driver_id = driver_standings.driver_id\n  LEFT JOIN races_ext AS next_race\n    ON next_race.year = races.year\n    AND next_race.round = races.round + 1\n  LEFT JOIN results AS next_results\n    ON next_results.race_id = next_race.race_id\n    AND next_results.driver_id = driver_standings.driver_id\n  WHERE results.driver_id IS NULL\n),\nfirst_race AS (\n  SELECT\n    year,\n    driver_id,\n    round AS first_round,\n    ROW_NUMBER() OVER (PARTITION BY year, driver_id ORDER BY round ASC) AS drive_id,\n    previous_constructor_id\n  FROM hiatus_prelim\n  WHERE is_first_race = 1\n),\nlast_race AS (\n  SELECT\n    year,\n    driver_id,\n    round AS last_round,\n    ROW_NUMBER() OVER (PARTITION BY year, driver_id ORDER BY round ASC) AS drive_id,\n    next_constructor_id\n  FROM hiatus_prelim\n  WHERE is_last_race = 1\n),\nmissed_races AS (\n  SELECT\n    driver_id,\n    year,\n    COUNT(*) AS missed_count  -- Count all missed rounds\n  FROM hiatus_prelim\n  GROUP BY driver_id, year\n  HAVING COUNT(*) < 3  -- Less than 3 missed rounds\n)\nSELECT\n  AVG(first_race.first_round) AS avg_first_round,\n  AVG(last_race.last_round) AS avg_last_round\nFROM driver_standings_ext AS driver_standings\nINNER JOIN races_ext AS races ON races.race_id = driver_standings.race_id\nLEFT JOIN results\n  ON results.race_id = driver_standings.race_id\n  AND results.driver_id = driver_standings.driver_id\nINNER JOIN first_race\n  ON first_race.year = races.year\n  AND first_race.driver_id = driver_standings.driver_id\nINNER JOIN last_race\n  ON last_race.year = races.year\n  AND last_race.driver_id = driver_standings.driver_id\n  AND last_race.drive_id = first_race.drive_id\nINNER JOIN missed_races\n  ON missed_races.year = races.year\n  AND missed_races.driver_id = driver_standings.driver_id\nWHERE results.driver_id IS NULL\n  AND first_race.previous_constructor_id != last_race.next_constructor_id;",
        "external_knowledge": null,
        "plan": "1. **Identify Missed Races**: \n   - Create a temporary list of races where drivers did not participate. For each missed race, also track the constructor information for the races immediately before and after the missed race.\n\n2. **Determine First and Last Missed Races**:\n   - From the list of missed races, identify the first missed race by considering the race immediately before it.\n   - Similarly, identify the last missed race by considering the race immediately after it.\n\n3. **Filter Drivers with Less than 3 Missed Races**:\n   - Count the number of missed races for each driver in each year. Keep only those drivers who missed fewer than 3 races in a year.\n\n4. **Calculate Average Round Numbers**:\n   - For drivers who meet the above criteria, calculate the average round number of their first missed race.\n   - Similarly, calculate the average round number of their last missed race.\n\n5. **Ensure Different Constructors Before and After Missed Races**:\n   - Only consider those missed races where the constructor for the race immediately before the first missed race is different from the constructor for the race immediately after the last missed race.\n\n6. **Final Output**:\n   - Return the average round numbers of the first and last missed races for the filtered drivers.",
        "special_function": null
    },
    {
        "instance_id": "local356",
        "db": "f1",
        "question": "Can you tell me the full names of drivers who have been overtaken more times than they have performed overtakes?",
        "SQL": "With overtakes AS (\nSELECT DISTINCT\n  lap_positions.race_id,\n  lap_positions.driver_id AS overtaking_driver_id,\n  lap_positions.lap,\n  lap_positions.position AS current_position,\n  previous_lap.position AS previous_position,\n  cars_behind_this_lap.driver_id AS overtaken_driver_id,\n  CASE\n    WHEN retirements.driver_id IS NOT NULL THEN 'R'\n    WHEN pit_stops.lap = lap_positions.lap THEN 'P'\n    WHEN pit_stops.milliseconds > overtaking_lap_times.running_milliseconds - overtaken_lap_times.running_milliseconds THEN 'P'\n    WHEN lap_positions.lap = 1 AND (previous_lap.position - cars_behind_this_lap_results.grid) <= 2 THEN 'S'\n    ELSE 'T'\n  END AS overtake_type,\n  CASE\n    WHEN retirements.driver_id IS NOT NULL\n    THEN retirements.retirement_type\n    WHEN pit_stops.lap = lap_positions.lap\n    THEN 'Pit Stop (Pit Entry)'\n    WHEN pit_stops.milliseconds > overtaking_lap_times.running_milliseconds - overtaken_lap_times.running_milliseconds\n    THEN 'Pit Stop (Pit Exit)'\n    WHEN lap_positions.lap = 1 AND (previous_lap.position - cars_behind_this_lap_results.grid) <= 2\n    THEN 'Start'\n    ELSE 'Track'\n  END AS overtake_desc\n\nFROM lap_positions\n  INNER JOIN races_ext AS races\n    ON races.race_id = lap_positions.race_id\n    AND races.is_pit_data_available = 1\n  INNER JOIN lap_positions AS previous_lap\n    ON previous_lap.race_id = lap_positions.race_id\n    AND previous_lap.driver_id = lap_positions.driver_id\n    AND previous_lap.lap = lap_positions.lap - 1\n\n  INNER JOIN lap_positions AS cars_behind_this_lap\n    ON cars_behind_this_lap.race_id = lap_positions.race_id\n    AND cars_behind_this_lap.lap = lap_positions.lap\n    AND cars_behind_this_lap.position > lap_positions.position\n  LEFT JOIN results AS cars_behind_this_lap_results\n    ON cars_behind_this_lap_results.race_id = lap_positions.race_id\n    AND cars_behind_this_lap_results.driver_id = cars_behind_this_lap.driver_id\n  LEFT JOIN lap_positions AS cars_behind_last_lap\n    ON cars_behind_last_lap.race_id = lap_positions.race_id\n    AND cars_behind_last_lap.lap = lap_positions.lap - 1\n    AND cars_behind_last_lap.driver_id = cars_behind_this_lap.driver_id\n    AND cars_behind_last_lap.position > previous_lap.position\n\n  LEFT JOIN retirements\n    ON retirements.race_id = lap_positions.race_id\n    AND retirements.lap = lap_positions.lap\n    AND retirements.driver_id = cars_behind_this_lap.driver_id\n\n  LEFT JOIN pit_stops AS pit_stops\n    ON pit_stops.race_id = lap_positions.race_id\n    AND pit_stops.lap BETWEEN lap_positions.lap - 1 AND lap_positions.lap\n    AND pit_stops.driver_id = cars_behind_this_lap.driver_id\n  LEFT JOIN lap_times_ext AS overtaking_lap_times\n    ON overtaking_lap_times.race_id = lap_positions.race_id\n    AND overtaking_lap_times.driver_id = lap_positions.driver_id\n    AND overtaking_lap_times.lap = pit_stops.lap - 1\n  LEFT JOIN lap_times_ext AS overtaken_lap_times\n    ON overtaken_lap_times.race_id = lap_positions.race_id\n    AND overtaken_lap_times.driver_id = pit_stops.driver_id\n    AND overtaken_lap_times.lap = pit_stops.lap - 1\n\nWHERE\n  cars_behind_last_lap.driver_id IS NULL /*The car was NOT behind last lap (but is behind this lap due to the INNER JOIN)*/\n\nORDER BY\n  lap_positions.race_id,\n  lap_positions.lap,\n  lap_positions.position\n),\nall_drivers AS (\n    SELECT overtaking_driver_id AS driverId FROM overtakes\n    UNION\n    SELECT overtaken_driver_id AS driverId FROM overtakes\n),\novertakes_performed AS (\n    SELECT overtaking_driver_id AS driverId, COUNT(*) AS overtakes_performed\n    FROM overtakes\n    GROUP BY overtaking_driver_id\n),\novertakes_received AS (\n    SELECT overtaken_driver_id AS driverId, COUNT(*) AS overtakes_received\n    FROM overtakes\n    GROUP BY overtaken_driver_id\n)\nSELECT\n    d.forename || ' ' || d.surname AS full_name\nFROM\n    all_drivers ad\nLEFT JOIN overtakes_performed op ON ad.driverId = op.driverId\nLEFT JOIN overtakes_received orv ON ad.driverId = orv.driverId\nJOIN drivers d ON ad.driverId = d.driver_id\nWHERE \n    COALESCE(orv.overtakes_received, 0) > COALESCE(op.overtakes_performed, 0);",
        "external_knowledge": null,
        "plan": "1. **Aggregate Overtakes by Drivers**: Create a summary that counts the number of times each driver has performed an overtaking maneuver.\n\n2. **Aggregate Being Overtaken by Drivers**: Create another summary that counts the number of times each driver has been overtaken.\n\n3. **Combine Both Summaries**: Merge the two summaries to create a comprehensive view of each driver\u2019s overtaking and being overtaken counts. Use a full outer join to ensure all drivers are included, even if they only appear in one of the summaries. Use default values to handle any missing data.\n\n4. **Identify Drivers with More Overtakes Against Them**: Filter this combined summary to retain only those drivers who have been overtaken more times than they have overtaken others.\n\n5. **Retrieve Full Names**: Join the filtered list of drivers with another table that contains driver details to retrieve the full names of the drivers.\n\n6. **Output the Result**: Select and return the full names of the drivers who meet the specified condition.",
        "special_function": null
    },
    {
        "instance_id": "sf001",
        "db": "GLOBAL_WEATHER__CLIMATE_DATA_FOR_BI.STANDARD_TILE",
        "question": "Assuming today is April 1, 2024, I would like to know the daily snowfall amounts greater than 6 inches for each U.S. postal code during the week ending after the first two full weeks of the previous year. Show the postal code, date, and snowfall amount.",
        "SQL": "WITH timestamps AS\n(   \n    SELECT\n        DATE_TRUNC(year,DATEADD(year,-1,DATE '2024-08-29')) AS ref_timestamp,\n        LAST_DAY(DATEADD(week,2 + CAST(WEEKISO(ref_timestamp) != 1 AS INTEGER),ref_timestamp),week) AS end_week,\n        DATEADD(day, day_num - 7, end_week) AS date_valid_std\n    FROM\n    (   \n        SELECT\n            ROW_NUMBER() OVER (ORDER BY SEQ1()) AS day_num\n        FROM\n            TABLE(GENERATOR(rowcount => 7))\n    ) \n)\nSELECT\n    country,\n    postal_code,\n    date_valid_std,\n    tot_snowfall_in \nFROM \n    GLOBAL_WEATHER__CLIMATE_DATA_FOR_BI.standard_tile.history_day\nNATURAL INNER JOIN\n    timestamps\nWHERE\n    country='US' AND\n    tot_snowfall_in > 6.0 \nORDER BY \n    postal_code,date_valid_std\n;",
        "external_knowledge": null,
        "plan": "1. Define the initial reference date one year prior to today and identify the relevant week\u2019s end based on this date.\n2. Select snowfall data from standard_tile.history_day for locations in the U.S. where the total snowfall exceeds 6 inches, using the generated dates.\n3. Sort the results by postal code and date for structured output.",
        "special_function": [
            "DATE_TRUNC",
            "LAST_DAY",
            "DATEADD",
            "WEEKISO"
        ]
    },
    {
        "instance_id": "sf003",
        "db": "GLOBAL_GOVERNMENT.CYBERSYN",
        "question": "From 2015 to 2020, which zip code in Census Zip Code Tabulation Areas had the second-highest annual population growth rate, given a minimum estimate of 25,000 people over a 5-year period? Include the zip code, state abbreviation, and growth rate.",
        "SQL": "WITH zip_stats AS (\n    SELECT\n        YEAR(ts.date) AS year,\n        ts.geo_id AS zip,\n        rship.related_geo_name AS state,\n        ts.value AS population,\n        LAG(ts.value, 1) OVER (PARTITION BY zip ORDER BY year ASC) AS prev_year_population,\n        population / prev_year_population - 1 AS pct_growth,\n        population - prev_year_population AS absolute_change\n    FROM GLOBAL_GOVERNMENT.CYBERSYN.american_community_survey_timeseries AS ts\n    JOIN GLOBAL_GOVERNMENT.CYBERSYN.american_community_survey_attributes AS att\n        ON ts.variable = att.variable\n    JOIN GLOBAL_GOVERNMENT.CYBERSYN.geography_index AS geo\n        ON ts.geo_id = geo.geo_id\n    JOIN GLOBAL_GOVERNMENT.CYBERSYN.geography_relationships AS rship\n        ON ts.geo_id = rship.geo_id AND rship.related_level = 'State'\n    WHERE\n        att.series_type = 'Total Population'\n        AND att.measurement_type = 'Estimate'\n        AND att.measurement_period = '5YR'\n        AND geo.level = 'CensusZipCodeTabulationArea'\n        AND ts.value > 25000\n)\n\nSELECT\n    year,\n    zip,\n    state,\n    pct_growth AS growth_percentage\nFROM (\n    SELECT\n        *,\n        ROW_NUMBER() OVER (PARTITION BY year ORDER BY pct_growth DESC NULLS LAST) AS annual_rank\n    FROM zip_stats\n    WHERE year BETWEEN 2015 AND 2020\n)\nWHERE annual_rank = 2\nORDER BY year;",
        "external_knowledge": null,
        "plan": "1.  Extract key population metrics for each ZIP code, including current and previous year populations, percentage growth, and absolute change\n2. Restrict analysis to ZIP codes recorded in the American Community Survey with a population greater than 5000, using a 5-year measurement period, and only include data from 2012 onward.\n3. Specifically extract the ranked data for the year 2022 to focus on the most recent trends in ZIP code population changes",
        "special_function": null
    },
    {
        "instance_id": "sf002",
        "db": "FINANCE__ECONOMICS.CYBERSYN",
        "question": "As of December 31, 2022, list the top 10 active large banks, each with assets over $10 billion, that have the highest percentage of uninsured assets based on quarterly estimates. Provide the names of these banks and their respective percentages of uninsured assets.",
        "SQL": "WITH big_banks AS (\n    SELECT id_rssd\n    FROM FINANCE__ECONOMICS.CYBERSYN.financial_institution_timeseries\n    WHERE variable = 'ASSET'\n      AND date = '2022-12-31'\n      AND value > 1E10\n)\nSELECT name\nFROM FINANCE__ECONOMICS.CYBERSYN.financial_institution_timeseries AS ts\nINNER JOIN FINANCE__ECONOMICS.CYBERSYN.financial_institution_attributes AS att ON (ts.variable = att.variable)\nINNER JOIN FINANCE__ECONOMICS.CYBERSYN.financial_institution_entities AS ent ON (ts.id_rssd = ent.id_rssd)\nINNER JOIN big_banks ON (big_banks.id_rssd = ts.id_rssd)\nWHERE ts.date = '2022-12-31'\n  AND att.variable_name = '% Insured (Estimated)'\n  AND att.frequency = 'Quarterly'\n  AND ent.is_active = True\nORDER BY (1 - value) DESC\nLIMIT 10;",
        "external_knowledge": null,
        "plan": "1. **Filter Large Banks:**\n   - Identify and select banks with assets greater than $10 billion as of December 31, 2022.\n   \n2. **Join Relevant Tables:**\n   - Join the selected large banks with the financial institution time series data to get additional financial details.\n   - Further join with the financial institution attributes to obtain the variable names and their frequencies.\n   - Also join with financial institution entities to ensure the banks are currently active.\n   \n3. **Filter by Date and Variable:**\n   - Filter the combined data to only include records from December 31, 2022.\n   - Specifically, filter to include records that contain the estimated percentage of insured assets, which are reported quarterly.\n   \n4. **Calculate Uninsured Percentage:**\n   - Calculate the percentage of uninsured assets by subtracting the insured percentage from 1.\n   \n5. **Order and Limit Results:**\n   - Sort the results by the highest percentage of uninsured assets in descending order.\n   - Limit the output to the top 10 banks with the highest uninsured asset percentages.\n   \n6. **Select Bank Names:**\n   - Finally, select and display the names of these top 10 banks.",
        "special_function": null
    },
    {
        "instance_id": "sf044",
        "db": "FINANCE__ECONOMICS.CYBERSYN",
        "question": "What was the percentage change in post-market close prices for the Magnificent 7 tech companies from January 1 to June 30, 2024?",
        "SQL": "WITH ytd_performance AS (\n  SELECT\n    ticker,\n    MIN(date) OVER (PARTITION BY ticker) AS start_of_year_date,\n    FIRST_VALUE(value) OVER (PARTITION BY ticker ORDER BY date ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS start_of_year_price,\n    MAX(date) OVER (PARTITION BY ticker) AS latest_date,\n    LAST_VALUE(value) OVER (PARTITION BY ticker ORDER BY date ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS latest_price\n  FROM FINANCE__ECONOMICS.CYBERSYN.stock_price_timeseries\n  WHERE\n    ticker IN ('AAPL', 'MSFT', 'AMZN', 'GOOGL', 'META', 'TSLA', 'NVDA')\n    AND date BETWEEN DATE '2024-01-01' AND DATE '2024-06-30'  -- Adjusted to cover only from the start of 2024 to the end of June 2024\n    AND variable_name = 'Post-Market Close'\n)\nSELECT\n  ticker,\n  (latest_price - start_of_year_price) / start_of_year_price * 100 AS percentage_change_ytd\nFROM\n  ytd_performance\nGROUP BY\n  ticker, start_of_year_date, start_of_year_price, latest_date, latest_price\nORDER BY percentage_change_ytd DESC;",
        "external_knowledge": null,
        "plan": "1. **Define a Common Table Expression (CTE)**: Create a temporary result set to hold intermediate calculations for the specified date range.\n   \n2. **Select Relevant Data**: Filter the dataset to include only records for the specified tech companies and within the date range from January 1 to June 30, 2024.\n\n3. **Calculate Start-of-Year Metrics**:\n   - Identify the earliest date for each company within the specified range.\n   - Retrieve the price corresponding to this earliest date.\n\n4. **Calculate Latest Metrics**:\n   - Identify the latest date for each company within the specified range.\n   - Retrieve the price corresponding to this latest date.\n\n5. **Compute Percentage Change**:\n   - For each company, calculate the percentage change in prices from the start-of-year price to the latest price within the specified range.\n   - Use the formula: \\((\\text{latest price} - \\text{start-of-year price}) / \\text{start-of-year price} \\times 100\\)\n\n6. **Select and Format Output**:\n   - Select the company identifier and the computed percentage change.\n   - Ensure the results are grouped by relevant attributes to avoid duplications.\n\n7. **Order Results**:\n   - Sort the output by the percentage change in descending order to highlight the highest changes first.",
        "special_function": null
    },
    {
        "instance_id": "sf006",
        "db": "FINANCE__ECONOMICS.CYBERSYN",
        "question": "Calculate the percentage change in the number of active financial branch entities for each state, comparing the counts on March 1, 2020, and December 31, 2021. Active entities are those operational on the specified dates.",
        "SQL": "WITH pre_covid AS (\n    SELECT state_abbreviation,\n           COUNT(*) AS pre_covid_count\n    FROM FINANCE__ECONOMICS.CYBERSYN.financial_branch_entities\n    WHERE start_date <= '2020-03-01'\n      AND (end_date >= '2020-03-01' OR end_date IS NULL)\n    GROUP BY state_abbreviation\n)\nSELECT cur.state_abbreviation,\n       pre_covid_count,\n       COUNT(*) AS current_count,\n       (current_count / CAST(pre_covid_count AS FLOAT)) - 1 AS pct_change\nFROM FINANCE__ECONOMICS.CYBERSYN.financial_branch_entities AS cur\nINNER JOIN pre_covid ON (cur.state_abbreviation = pre_covid.state_abbreviation)\nWHERE (end_date IS NULL OR end_date >= '2021-12-31')\nGROUP BY cur.state_abbreviation, pre_covid_count\nORDER BY pct_change DESC;",
        "external_knowledge": null,
        "plan": "1. **Define Initial Active Entities Before Specific Date**:\n    - Create a temporary subset of data to count the number of active entities in each state as of a specified date (before March 1, 2020).\n    - Include only those entities whose start date is on or before this date and are still active on this date or have no end date.\n\n2. **Aggregate Pre-COVID Counts**:\n    - Group the filtered data by state and count the number of active entities for each state before the specified date.\n    - Store these counts in a temporary table for later use.\n\n3. **Filter Active Entities at End of Specified Period**:\n    - From the main dataset, filter the entities to include those that are still active by the end of a specified period (end of 2021).\n    - Ensure that the end date is either null (indicating ongoing activity) or extends beyond the end of the specified period.\n\n4. **Join Pre-COVID and Current Data**:\n    - Join the current active entities data with the pre-COVID counts based on the state identifier.\n    - This allows comparison of the number of active entities before the specified date with those active at the end of the specified period.\n\n5. **Calculate Current Counts and Change Ratio**:\n    - Group the joined data by state and count the number of currently active entities for each state.\n    - Calculate the change ratio by dividing the current count by the pre-COVID count and subtracting one to express the change as a percentage.\n\n6. **Order Results by Change Ratio**:\n    - Order the results based on the calculated percentage change in descending order to show states with the highest increase in active entities first.\n\n7. **Select and Display Relevant Columns**:\n    - Select the state identifier, pre-COVID counts, current counts, and the calculated percentage change to be displayed in the final output.",
        "special_function": null
    },
    {
        "instance_id": "sf008",
        "db": "US_REAL_ESTATE.CYBERSYN",
        "question": "Determine the percentage change in gross income inflow and the seasonally-adjusted purchase-only home price index for the Phoenix-Mesa-Scottsdale, AZ Metro Area from January 1, 2023, to December 31, 2023. Gross income inflow refers to the total adjusted gross income from all financial entities within the specified metro area",
        "SQL": "WITH county_map AS (\n    SELECT\n        geo_id,\n        geo_name,\n        related_geo_id,\n        related_geo_name\n    FROM US_REAL_ESTATE.CYBERSYN.geography_relationships\n    WHERE geo_name = 'Phoenix-Mesa-Scottsdale, AZ Metro Area'\n    AND related_level = 'County'\n), \ngross_income_data AS (\n    SELECT\n        geo_id,\n        date,\n        SUM(value) AS gross_income_inflow\n    FROM US_REAL_ESTATE.CYBERSYN.irs_origin_destination_migration_timeseries AS ts\n    JOIN county_map ON (county_map.related_geo_id = ts.to_geo_id)\n    WHERE ts.variable_name = 'Adjusted Gross Income'\n    GROUP BY geo_id, date\n), \nhome_price_data AS (\n    SELECT LAST_DAY(date, 'year') AS end_date, AVG(value) AS home_price_index\n    FROM US_REAL_ESTATE.CYBERSYN.fhfa_house_price_timeseries AS ts\n    JOIN US_REAL_ESTATE.CYBERSYN.fhfa_house_price_attributes AS att\n        ON (ts.variable = att.variable)\n    WHERE geo_id IN (SELECT geo_id FROM county_map)\n      AND att.index_type = 'purchase-only'\n      AND att.seasonally_adjusted = TRUE\n    GROUP BY end_date\n),\ncombined_data AS (\n    SELECT\n        gid.date,\n        gid.gross_income_inflow,\n        hpi.home_price_index\n    FROM gross_income_data AS gid\n    JOIN home_price_data AS hpi ON (gid.date = hpi.end_date)\n),\naggregated_data AS (\n    SELECT\n        MIN(date) AS first_year,\n        MAX(date) AS last_year\n    FROM combined_data\n),\nsummary_data AS (\n    SELECT\n        first_year,\n        last_year,\n        first_income.gross_income_inflow AS first_year_income,\n        last_income.gross_income_inflow AS last_year_income,\n        first_index.home_price_index AS first_year_index,\n        last_index.home_price_index AS last_year_index\n    FROM aggregated_data AS ad\n    JOIN combined_data AS first_income ON (first_income.date = ad.first_year)\n    JOIN combined_data AS last_income ON (last_income.date = ad.last_year)\n    JOIN combined_data AS first_index ON (first_index.date = ad.first_year)\n    JOIN combined_data AS last_index ON (last_index.date = ad.last_year)\n)\nSELECT\n    ((last_year_income - first_year_income) / first_year_income) * 100 AS income_growth_percent,\n    ((last_year_index - first_year_index) / first_year_index) * 100 AS index_growth_percent\nFROM summary_data;",
        "external_knowledge": null,
        "plan": "1. Identify counties within the Phoenix-Mesa-Scottsdale, AZ Metro Area by mapping geographic relationships.\n2. Aggregate the adjusted gross income data for these counties.\n3. Calculate the average home price index, focusing on purchase-only data and seasonally adjusting for accuracy.\n4. Join the gross income data with home price data based on their respective dates.\n5. Identify the earliest and latest dates in the combined dataset to define the analysis timeframe.\n6. Extract income and home price indices for both the start and end of the period.\n7. Calculate the percentage growth for gross income and home prices from the start to the end of the period.",
        "special_function": null
    },
    {
        "instance_id": "sf010",
        "db": "US_REAL_ESTATE.CYBERSYN",
        "question": "What are the cumulative percentages of mortgages near default in California for each recorded date in 2023, including those 90 to 180 days past due, in forbearance, or in the process of foreclosure, bankruptcy, or deed in lieu?",
        "SQL": "SELECT\n    ts.date,\n    SUM(ts.value) AS pct_near_default\nFROM\n    US_REAL_ESTATE.CYBERSYN.fhfa_mortgage_performance_timeseries AS ts\n    JOIN US_REAL_ESTATE.CYBERSYN.fhfa_mortgage_performance_attributes AS att ON (ts.variable = att.variable)\n    JOIN US_REAL_ESTATE.CYBERSYN.geography_index AS geo ON (geo.geo_id = ts.geo_id)\nWHERE\n    att.variable_group IN (\n        'Percent 90 to 180 Days Past Due Date',\n        'Percent in Forbearance',\n        'Percent in the Process of Foreclosure, Bankruptcy, or Deed in Lieu'\n    )\n    AND att.market = 'All Mortgages'\n    AND geo.geo_name = 'California'\n    AND YEAR(ts.date) = 2023\nGROUP BY\n    ts.date\nORDER BY\n    ts.date;",
        "external_knowledge": null,
        "plan": "1. **Identify relevant records**: Select records that are related to specific categories of mortgage performance, such as those that are 90 to 180 days past due, in forbearance, or in the process of foreclosure, bankruptcy, or deed in lieu.\n\n2. **Filter by geographical location**: Narrow down the dataset to include only records pertaining to California.\n\n3. **Filter by time period**: Further filter the dataset to include only records from the year 2023.\n\n4. **Join datasets**: Combine the main dataset with additional datasets to incorporate necessary attributes and geographical information.\n\n5. **Group by date**: Group the filtered records by each recorded date to aggregate data for each day.\n\n6. **Calculate cumulative percentages**: For each date, sum the values that represent the percentages of mortgages in the specified categories to obtain the cumulative percentage.\n\n7. **Sort results**: Order the resulting cumulative percentages by date in ascending order to maintain a chronological sequence.\n\nThis plan ensures that you get the cumulative percentages of mortgages near default in California for each recorded date in 2023, focusing on the specified conditions.",
        "special_function": null
    },
    {
        "instance_id": "sf037",
        "db": "US_REAL_ESTATE.CYBERSYN",
        "question": "Calculate the shortest driving distance in miles between each 'The Home Depot' store, identified by its POI ID, and its nearest 'Lowe's Home Improvement' store",
        "SQL": "WITH joined_data AS (\n    SELECT poi.poi_id, poi.poi_name, addr.longitude, addr.latitude,\n           addr.number, addr.street_directional_prefix, addr.street,\n           addr.street_type, addr.street_directional_suffix,\n           addr.unit, addr.city, addr.state, addr.zip\n    FROM US_REAL_ESTATE.CYBERSYN.point_of_interest_index AS poi\n    JOIN US_REAL_ESTATE.CYBERSYN.point_of_interest_addresses_relationships AS map\n        ON (poi.poi_id = map.poi_id)\n    JOIN US_REAL_ESTATE.CYBERSYN.us_addresses AS addr\n        ON (map.address_id = addr.address_id)\n)\nSELECT home_depot.poi_id,\n       ST_DISTANCE(\n           ST_MAKEPOINT(home_depot.longitude, home_depot.latitude),\n           ST_MAKEPOINT(lowes.longitude, lowes.latitude)\n       ) / 1609 AS distance_miles\nFROM joined_data AS home_depot\nJOIN joined_data AS lowes\nWHERE home_depot.poi_name = 'The Home Depot'\n  AND lowes.poi_name = 'Lowe''s Home Improvement'\nQUALIFY ROW_NUMBER() OVER (PARTITION BY home_depot.poi_id ORDER BY distance_miles NULLS LAST) = 1;",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Create a common dataset that includes both Home Depot and Lowe's locations, gather point of interest information along with geographical coordinates.\n2. For each Home Depot location, calculate the distance to all Lowe's locations and convert these distances from meters to miles.\n3. Determine the nearest Lowe's store for each Home Depot by ranking the distances and selecting the smallest one",
        "special_function": null
    },
    {
        "instance_id": "sf012",
        "db": "WEATHER__ENVIRONMENT.CYBERSYN",
        "question": "What were the total amounts of building and contents damage reported under the National Flood Insurance Program in the City of New York for each year from 2010 to 2019?",
        "SQL": "SELECT \n    YEAR(claims.date_of_loss)               AS year_of_loss,\n    claims.nfip_community_name,\n    SUM(claims.building_damage_amount) AS total_building_damage_amount,\n    SUM(claims.contents_damage_amount) AS total_contents_damage_amount\nFROM WEATHER__ENVIRONMENT.CYBERSYN.fema_national_flood_insurance_program_claim_index claims\nWHERE \n    claims.nfip_community_name = 'City Of New York' \n    AND year_of_loss >=2010 AND year_of_loss <=2019\nGROUP BY year_of_loss, claims.nfip_community_name\nORDER BY year_of_loss, claims.nfip_community_name;",
        "external_knowledge": null,
        "plan": "1. **Select the Year of Loss**: Extract the year from the date of loss for each claim to be used for grouping the results by year.\n\n2. **Filter by Location**: Ensure that only claims from the specified city are considered.\n\n3. **Filter by Date Range**: Include only those claims where the year of loss falls within the specified range (from 2010 to 2019).\n\n4. **Calculate Total Building Damage**: Sum the building damage amounts for the filtered claims to get the total building damage for each year.\n\n5. **Calculate Total Contents Damage**: Sum the contents damage amounts for the filtered claims to get the total contents damage for each year.\n\n6. **Group Results by Year and Location**: Group the results by year and the specified city to consolidate the totals for each year.\n\n7. **Order the Results**: Sort the final results by year and city to present the data in a structured and chronological order.",
        "special_function": null
    },
    {
        "instance_id": "sf018",
        "db": "BRAZE_USER_EVENT_DEMO_DATASET.PUBLIC",
        "question": "Examine user engagement with push notifications within a specified one-hour window on June 1, 2023.",
        "SQL": "WITH push_send AS (\n    SELECT\n        id,\n        app_group_id,\n        user_id,\n        campaign_id,\n        message_variation_id,\n        platform,\n        ad_tracking_enabled,\n        TO_TIMESTAMP(TIME) AS \"TIME\",\n        'Send' AS \"EVENT_TYPE\"\n    FROM\n        BRAZE_USER_EVENT_DEMO_DATASET.PUBLIC.USERS_MESSAGES_PUSHNOTIFICATION_SEND_VIEW\n    WHERE\n        TO_TIMESTAMP(TIME) BETWEEN '2023-06-01 08:00:00' AND '2023-06-01 09:00:00'\n),\npush_bounce AS (\n    SELECT\n        id,\n        app_group_id,\n        user_id,\n        campaign_id,\n        message_variation_id,\n        platform,\n        ad_tracking_enabled,\n        TO_TIMESTAMP(TIME) AS \"TIME\",\n        'Bounce' AS \"EVENT_TYPE\"\n    FROM\n        BRAZE_USER_EVENT_DEMO_DATASET.PUBLIC.USERS_MESSAGES_PUSHNOTIFICATION_BOUNCE_VIEW\n    WHERE\n        TO_TIMESTAMP(TIME) BETWEEN '2023-06-01 08:00:00' AND '2023-06-01 09:00:00'\n),\npush_open AS (\n    SELECT\n        id,\n        app_group_id,\n        user_id,\n        campaign_id,\n        message_variation_id,\n        platform,\n        ad_tracking_enabled,\n        TO_TIMESTAMP(TIME) AS \"TIME\",\n        'Open' AS \"EVENT_TYPE\",\n        carrier,\n        browser,\n        device_model\n    FROM\n        BRAZE_USER_EVENT_DEMO_DATASET.PUBLIC.USERS_MESSAGES_PUSHNOTIFICATION_OPEN_VIEW\n    WHERE\n        TO_TIMESTAMP(TIME) BETWEEN '2023-06-01 08:00:00' AND '2023-06-01 09:00:00'\n),\npush_open_influence AS (\n    SELECT\n        id,\n        app_group_id,\n        user_id,\n        campaign_id,\n        message_variation_id,\n        platform,\n        TO_TIMESTAMP(TIME) AS \"TIME\",\n        'Influenced Open' AS \"EVENT_TYPE\",\n        carrier,\n        browser,\n        device_model\n    FROM\n        BRAZE_USER_EVENT_DEMO_DATASET.PUBLIC.USERS_MESSAGES_PUSHNOTIFICATION_INFLUENCEDOPEN_VIEW\n    WHERE\n        TO_TIMESTAMP(TIME) BETWEEN '2023-06-01 08:00:00' AND '2023-06-01 09:00:00'\n)\nSELECT\n    ps.app_group_id,\n    ps.campaign_id,\n    ps.user_id,\n    ps.time,\n    po.time push_open_time,\n    ps.message_variation_id,\n    ps.platform,\n    ps.ad_tracking_enabled,\n    po.carrier,\n    po.browser,\n    po.device_model,\n    COUNT(\n        DISTINCT ps.id\n    ) push_notification_sends,\n    COUNT(\n        DISTINCT ps.user_id\n    ) unique_push_notification_sends,\n    COUNT(\n        DISTINCT pb.id\n    ) push_notification_bounced,\n    COUNT(\n        DISTINCT pb.user_id\n    ) unique_push_notification_bounced,\n    COUNT(\n        DISTINCT po.id\n    ) push_notification_open,\n    COUNT(\n        DISTINCT po.user_id\n    ) unique_push_notification_opened,\n    COUNT(\n        DISTINCT poi.id\n    ) push_notification_influenced_open,\n    COUNT(\n        DISTINCT poi.user_id\n    ) unique_push_notification_influenced_open\nFROM\n    push_send ps\n    LEFT JOIN push_bounce pb\n    ON ps.message_variation_id = pb.message_variation_id\n    AND ps.user_id = pb.user_id\n    AND ps.app_group_id = pb.app_group_id\n    LEFT JOIN push_open po\n    ON ps.message_variation_id = po.message_variation_id\n    AND ps.user_id = po.user_id\n    AND ps.app_group_id = po.app_group_id\n    LEFT JOIN push_open_influence poi\n    ON ps.message_variation_id = poi.message_variation_id\n    AND ps.user_id = poi.user_id\n    AND ps.app_group_id = poi.app_group_id\nGROUP BY\n    1,2,3,4,5,6,7,8,9,10,11;",
        "temporal": "Yes",
        "external_knowledge": "PushNotificationAnalysis.md",
        "plan": "1. **Define Subqueries for Each Event Type**:\n   - Create subqueries for four types of events: send, bounce, open, and influenced open.\n   - For each event type, select relevant columns and convert the event timestamp to a standard format.\n   - Assign a specific event type label to each subquery for identification.\n\n2. **Filter Events by Time Window**:\n   - Apply a time filter to each subquery to include only events that occurred within the specified one-hour window on June 1, 2023.\n\n3. **Aggregate Events into Main Query**:\n   - Use the subqueries to form the main query.\n   - Join the subqueries based on common identifiers such as user, campaign, and message variation.\n\n4. **Perform Left Joins for Comprehensive Data**:\n   - Use left joins to ensure all send events are included, even if there are no corresponding bounce, open, or influenced open events.\n   - Match events based on user, campaign, and message variation identifiers.\n\n5. **Select and Group Data for Analysis**:\n   - Select relevant columns for the final output, including timestamps and device information from open events.\n   - Group the results to aggregate data by unique combinations of user, campaign, and message variation.\n\n6. **Count Distinct Events and Users**:\n   - Calculate the count of distinct events and distinct users for each event type (send, bounce, open, influenced open).\n   - Include these counts as separate columns in the final output.\n\n7. **Output Aggregated Results**:\n   - Provide the final aggregated results, which include counts of distinct events and users for each event type within the specified time window.",
        "special_function": null
    },
    {
        "instance_id": "sf035",
        "db": "BRAZE_USER_EVENT_DEMO_DATASET.PUBLIC",
        "question": "How many unique users started sessions each day within each app group between June 1, 2023, and June 7, 2023? Also show the app group ID and the start day of the session.",
        "SQL": "WITH session_joins AS (\n    SELECT\n        ss.app_group_id,\n        ss.time AS ss_time,\n        ss.user_id\n    FROM\n        BRAZE_USER_EVENT_DEMO_DATASET.PUBLIC.users_behaviors_app_sessionstart_view ss\n        JOIN BRAZE_USER_EVENT_DEMO_DATASET.PUBLIC.users_behaviors_app_firstsession_view fs\n        ON ss.user_id = fs.user_id\n        AND ss.app_group_id = fs.app_group_id\n    WHERE\n        ss.time >= DATEDIFF(s, '1970-01-01', '2023-06-01') AND\n        ss.time < DATEDIFF(s, '1970-01-01', '2023-06-08')\n)\nSELECT\n    app_group_id,\n    DATE_TRUNC('day', DATEADD(s, ss_time, '1970-01-01')) AS session_start_day,\n    COUNT(\n        DISTINCT user_id\n    ) AS count_distinct_users\nFROM\n    session_joins\nGROUP BY\n    1, 2\nORDER BY\n    session_start_day;",
        "external_knowledge": null,
        "plan": "1. **Define a Common Table Expression (CTE):** Create a temporary result set named `session_joins` to simplify the main query. This CTE will hold relevant session start data filtered by specific conditions.\n\n2. **Select Required Data in CTE:**\n   - Retrieve the app group identifier, session start time, and user identifier from the session start data.\n   - Join this session start data with another dataset that ensures the user has an initial session, filtering by matching user identifiers and app group identifiers.\n\n3. **Apply Date Filter:**\n   - Ensure the session start time falls within the specified date range (from June 1, 2023, to June 7, 2023). Convert these dates to a comparable format (seconds since epoch).\n\n4. **Main Query - Aggregate Results:**\n   - From the `session_joins` CTE, select the app group identifier and truncate the session start time to the day level, converting it back from seconds to a date format.\n\n5. **Count Unique Users:**\n   - Count the number of distinct users who started sessions for each combination of app group identifier and truncated session start day.\n\n6. **Group and Order Results:**\n   - Group the results by app group identifier and session start day to get the count of unique users for each day within each app group.\n   - Order the results by the session start day to present the data in chronological order.\n\nThis plan ensures that the query retrieves and counts unique users starting sessions each day within the specified date range and groups them by app group and day.",
        "special_function": null
    },
    {
        "instance_id": "sf029",
        "db": "AMAZON_VENDOR_ANALYTICS__SAMPLE_DATASET.PUBLIC",
        "question": "Generate a daily detailed sales report for each product under the 'Manufacturing' distributor view, covering the 30 days leading up to February 6, 2022. The report should include the total and average values for sales units, revenue, average selling price (ASP), glance views, conversion rate, shipped units, shipped revenue, net profit margin (PPM), and inventory details.",
        "SQL": "select date\n, asin\n, max(sa.product_title)\n, sum(sa.ordered_units) as ordered_units, sum(sa.ordered_revenue) as ordered_revenue\n, sa.ordered_revenue/NULLIF(sa.ordered_units,0) as ASP\n, sum(tr.glance_views) as glance_views\n, CAST(sa.ordered_units as decimal(5,2))/NULLIF(tr.glance_views,0) as conversion_rate\n, sum(sa.shipped_units) as shipped_units\n, sum(sa.shipped_revenue) as shipped_revenue\n, avg(net_ppm) as net_ppm\n, avg(procurable_product_oos) as procurable_product_oos\n, sum(sellable_on_hand_units) as sellable_on_hand_units\n, sum(sellable_on_hand_inventory) as sellable_on_hand_value\n, sum(net_received) as net_received_value\n, sum(net_received_units) as net_received_units\n, sum(open_purchase_order_quantity) as open_purchase_order_quantity\n, sum(unfilled_customer_ordered_units) as unfilled_customer_ordered_units\n, avg(vendor_confirmation_rate) as vendor_confirmation_rate\n, avg(receive_fill_rate) as receive_fill_rate\n, avg(sell_through_rate) as sell_through_rate\n, avg(overall_vendor_lead_time_days) as vendor_lead_time_days\n\nfrom AMAZON_VENDOR_ANALYTICS__SAMPLE_DATASET.PUBLIC.retail_analytics_sales sa\njoin AMAZON_VENDOR_ANALYTICS__SAMPLE_DATASET.PUBLIC.retail_analytics_traffic tr\nusing (date, asin, program, period, distributor_view)\njoin AMAZON_VENDOR_ANALYTICS__SAMPLE_DATASET.PUBLIC.retail_analytics_inventory inv\nusing (date, asin, program, period, distributor_view)\njoin AMAZON_VENDOR_ANALYTICS__SAMPLE_DATASET.PUBLIC.retail_analytics_net_ppm ppm\nusing (date, asin, program, period, distributor_view)\n\nwhere date >= to_date('2022-02-06','YYYY-MM-DD') -30\nand distributor_view = 'Manufacturing'\nand period = 'DAILY' \n\ngroup by 1, 2, 6, 8\norder by 1 desc",
        "external_knowledge": null,
        "plan": "1. **Select Relevant Fields**: Choose the necessary fields to include in the report such as date, product identifier, product title, sales units, revenue, average selling price, glance views, conversion rate, shipped units, shipped revenue, net profit margin, and various inventory details.\n\n2. **Aggregate Data**: Compute aggregations such as the sum of ordered units and revenue, total glance views, shipped units, and revenue, as well as averages for net profit margin and inventory metrics.\n\n3. **Calculate Metrics**:\n   - **Average Selling Price (ASP)**: Calculate by dividing the total ordered revenue by the total ordered units, ensuring to handle division by zero.\n   - **Conversion Rate**: Determine by dividing the ordered units by the total glance views, handling division by zero as well.\n\n4. **Join Tables**: Combine data from multiple tables containing sales, traffic, inventory, and net profit margin information using common keys (date, product identifier, program, period, distributor view).\n\n5. **Filter Data**: Apply filters to include records starting from 30 days prior to the specified end date (February 6, 2022) and to ensure the distributor view is 'Manufacturing' and the period is 'DAILY'.\n\n6. **Group Data**: Group the data by relevant fields to ensure that aggregations are calculated for each product and day correctly.\n\n7. **Order Data**: Sort the resulting report by date in descending order to display the most recent data first.\n\n8. **Output the Report**: Generate the final report with the aggregated and calculated metrics for each product on a daily basis within the specified date range.",
        "special_function": null
    },
    {
        "instance_id": "sf040",
        "db": "US_ADDRESSES__POI.CYBERSYN",
        "question": "Find the top 10 northernmost addresses in Florida's largest zip code area. What are their address numbers, street names, and types?",
        "SQL": "WITH zip_areas AS (\n    SELECT\n        geo.geo_id,\n        geo.geo_name AS zip,\n        states.related_geo_name AS state,\n        countries.related_geo_name AS country,\n        ST_AREA(TRY_TO_GEOGRAPHY(value)) AS area\n    FROM US_ADDRESSES__POI.CYBERSYN.geography_index AS geo\n    JOIN US_ADDRESSES__POI.CYBERSYN.geography_relationships AS states\n        ON (geo.geo_id = states.geo_id AND states.related_level = 'State')\n    JOIN US_ADDRESSES__POI.CYBERSYN.geography_relationships AS countries\n        ON (geo.geo_id = countries.geo_id AND countries.related_level = 'Country')\n    JOIN US_ADDRESSES__POI.CYBERSYN.geography_characteristics AS chars\n        ON (geo.geo_id = chars.geo_id AND chars.relationship_type = 'coordinates_geojson')\n    WHERE geo.level = 'CensusZipCodeTabulationArea'\n),\n\nzip_area_ranks AS (\n    SELECT\n        *,\n        ROW_NUMBER() OVER (PARTITION BY country, state ORDER BY area DESC, geo_id) AS zip_area_rank\n    FROM zip_areas\n)\n\nSELECT addr.number, addr.street, addr.street_type\nFROM US_ADDRESSES__POI.CYBERSYN.us_addresses AS addr\nJOIN zip_area_ranks AS areas\n    ON (addr.id_zip = areas.geo_id)\nWHERE addr.state = 'FL' AND areas.country = 'United States' AND areas.zip_area_rank = 1\nORDER BY LATITUDE DESC\nLIMIT 10;",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. **Identify Geographic Areas**:\n   - Retrieve geographic areas and their corresponding identifiers.\n   - Calculate the area size for each geographic region.\n   - Associate each geographic region with its state and country.\n\n2. **Filter for Zip Code Areas**:\n   - Narrow down the geographic regions to those that are specifically categorized as zip code areas.\n\n3. **Rank Zip Code Areas by Size**:\n   - Rank the zip code areas within each state and country based on their calculated area size, from largest to smallest.\n   - Assign a ranking number to each zip code area.\n\n4. **Identify Florida's Largest Zip Code Area**:\n   - Focus on addresses within Florida.\n   - Identify the largest zip code area within Florida by using the previously calculated rankings.\n\n5. **Select Address Details**:\n   - For the addresses within the largest zip code area in Florida, retrieve the address numbers, street names, and street types.\n\n6. **Sort by Latitude**:\n   - Order these addresses by their latitude in descending order to find the northernmost addresses.\n\n7. **Limit Results**:\n   - Limit the final results to the top 10 addresses to fulfill the requirement of finding the top 10 northernmost addresses.",
        "special_function": null
    },
    {
        "instance_id": "sf009",
        "db": "NETHERLANDS_OPEN_MAP_DATA.NETHERLANDS",
        "question": "A real estate company is looking for a comparison of the building types in Amsterdam and Rotterdam. They need to know the total surface area and the number of buildings for each type of building in both cities. Can you provide the building class and subclass, along with the total surface area and the number of buildings for both Amsterdam and Rotterdam?",
        "SQL": "WITH BUILDING AS(\nSELECT A.NAMES['local']::VARCHAR AS NAME\n     ,B.Class\n     ,B.subclass\n     ,count(*) num_buidling\n     ,SUM(B.surface_area_sq_m) sum_surface_area_sq_m\n FROM NETHERLANDS_OPEN_MAP_DATA.NETHERLANDS.V_ADMINISTRATIVE A\nINNER\n JOIN(SELECT Class\n            ,subclass\n            ,surface_area_sq_m\n            ,GEO_CORDINATES\n        FROM NETHERLANDS_OPEN_MAP_DATA.NETHERLANDS.V_BUILDING\n       WHERE SUBSTR(QUADKEY\n             ,0\n             ,8) IN ('12020210', '12020211')) B\n   ON ST_COVERS(A.GEO_CORDINATES\n            ,B.GEO_CORDINATES)\nWHERE A.id IN('r324431@56'\n            ,'r47811@69')\nGROUP BY A.NAMES['local']\n     ,B.Class\n     ,B.subclass\nORDER BY B.class\n     ,B.subclass)\nSELECT CLASS,\nSUBCLASS,\nMAX(CASE WHEN NAME='Amsterdam' THEN sum_surface_area_sq_m END) AS SUM_SURFACE_AREA_SQ_M_AMSTERDAM,\nMAX(CASE WHEN NAME='Rotterdam' THEN sum_surface_area_sq_m END) AS SUM_SURFACE_AREA_SQ_M_ROTTERDAM,\nMAX(CASE WHEN NAME='Amsterdam' THEN num_buidling END) AS NUM_OF_BUILDING_AMSTERDAM,\nMAX(CASE WHEN NAME='Rotterdam' THEN num_buidling END) AS NUM_OF_BUILDING_ROTTERDAM\nFROM BUILDING\nGROUP BY CLASS,SUBCLASS\nORDER BY CLASS,SUBCLASS;",
        "external_knowledge": null,
        "plan": "Tell me the total surface area and number of buildings for each class and subclass in Amsterdam and Rotterdam, based on specific QUADKEY segments '12020210' and '12020211'. Show class, subclass, surface areas, and building counts for both cities.",
        "special_function": null
    },
    {
        "instance_id": "sf013",
        "db": "NETHERLANDS_OPEN_MAP_DATA.NETHERLANDS",
        "question": "Determine the total length of roads for each class and subclass in Amsterdam and Rotterdam, based on specific QUADKEY segments '12020210' and '12020211'? Show the class, subclass, and total road lengths for both cities",
        "SQL": "WITH ROAD AS(\nSELECT A.NAMES['local']::VARCHAR AS NAME\n     ,R.Class\n     ,R.subclass\n     ,SUM(R.length_m) AS SUM_OF_LENGTH\n FROM NETHERLANDS_OPEN_MAP_DATA.NETHERLANDS.V_ADMINISTRATIVE A\nINNER\n JOIN(SELECT Class\n            ,subclass\n            ,length_m\n            ,GEO_CORDINATES\n        FROM NETHERLANDS_OPEN_MAP_DATA.NETHERLANDS.V_ROAD \n       WHERE SUBSTR(QUADKEY\n             ,0\n             ,8) IN ('12020210', '12020211')) R\n   ON ST_COVERS(A.GEO_CORDINATES\n            ,R.GEO_CORDINATES)\nWHERE A.id IN('r324431@56'\n            ,'r47811@69')\nGROUP BY A.NAMES['local']\n     ,R.Class\n     ,R.subclass\nORDER BY R.class\n     ,R.subclass)\nSELECT CLASS,\nSUBCLASS,\nMAX(CASE WHEN NAME='Amsterdam' THEN SUM_OF_LENGTH END) AS AMSTERDAM,\nMAX(CASE WHEN NAME='Rotterdam' THEN SUM_OF_LENGTH END) AS ROTTERDAM\nFROM ROAD\nGROUP BY CLASS,SUBCLASS\nORDER BY CLASS,SUBCLASS;",
        "external_knowledge": null,
        "plan": "1. **Create a Temporary Data Set:**\n   - Define a common table expression (CTE) to temporarily store the required data.\n\n2. **Filter and Join Data:**\n   - Select necessary attributes from an administrative data source.\n   - Join this data with another data set containing road information.\n   - Ensure the join condition checks if the geographical coordinates of the roads fall within the administrative boundaries.\n\n3. **Filter by Quadkey Segments:**\n   - Apply a filter to select only those road records whose quadkey segments match the specified values.\n\n4. **Limit Administrative Areas:**\n   - Further filter the administrative data to include only the specified areas.\n\n5. **Calculate Total Length:**\n   - Group the data by administrative area, road class, and subclass.\n   - Sum the lengths of roads within each group.\n\n6. **Sort Data:**\n   - Sort the grouped data by road class and subclass for consistency.\n\n7. **Pivot Data:**\n   - In the final selection, pivot the summarized data to display the total road lengths for each class and subclass separately for each specified area.\n   - Use conditional aggregation to assign the summed lengths to the correct city.\n\n8. **Group and Order Final Output:**\n   - Group the final output by road class and subclass.\n   - Order the results by class and subclass to provide a structured output.",
        "special_function": null
    },
    {
        "instance_id": "sf041",
        "db": "YES_ENERGY__SAMPLE_DATA.YES_ENERGY_SAMPLE",
        "question": "Produce a report for ERCOT on October 1, 2022, that combines hourly data on day-ahead and real-time prices from node ID 10000697078, load forecasts (datatypeid 19060) and actual loads, plus wind (forecast datatypeid 9285, actual datatypeid 16) and solar (forecast datatypeid 662, actual datatypeid 650) generation forecasts and actuals from object ID 10000712973. This report should include time zone alignments, peak classifications, and net load calculations, providing insights into daily operational dynamics and efficiency.",
        "SQL": "with datetimes_ as ( --Grabbing standardized datetime data to which all data will be joined.\n        select *\n        from YES_ENERGY__SAMPLE_DATA.YES_ENERGY_SAMPLE.iso_market_times_sample\n        where iso = 'ERCOT'\n            and datetime = '2022-10-01' -- Setting date specifically for October 1st, 2022.\n),\n    prices_ as (\n        select *\n        from YES_ENERGY__SAMPLE_DATA.YES_ENERGY_SAMPLE.dart_prices_sample\n        where iso = 'E' --ISO code (E = ERCOT).\n            and objectid = 10000697078 --Price node unique numeric id.\n),\n    load_forc_ as ( --Load forecast data. To find datatypes, use the v_data_catalog_basic view to get the datatypeid and storage table in which the data is located.\n        select *\n        from YES_ENERGY__SAMPLE_DATA.YES_ENERGY_SAMPLE.ts_forecast_sample\n        where datatypeid = 19060\n            and objectid = 10000712973\n),\n    load_ as ( --Real time load data.\n        select *\n        from YES_ENERGY__SAMPLE_DATA.YES_ENERGY_SAMPLE.dart_loads_sample\n        where objectid = 10000712973\n),\n    wind_gen_forc_ as ( --Wind generation forecast data.\n        select *\n        from YES_ENERGY__SAMPLE_DATA.YES_ENERGY_SAMPLE.ts_forecast_sample\n        where datatypeid = 9285\n           and objectid = 10000712973\n),\n    wind_gen_ as ( --Real time wind data.\n        select trunc(dateadd('second', 3599, datetime), 'hour') datetime --Converts the hourly rollup of 5 min data to hour ending (the 5 min are already interval ending themselves; just needs to be done for Snowflake).\n              ,avg(value) wind_gen --Raw real time wind generation data is reported at the 5 min granularity.\n        from YES_ENERGY__SAMPLE_DATA.YES_ENERGY_SAMPLE.ts_gen_sample\n        where datatypeid = 16\n            and objectid = 10000712973\n        group by all\n),\n    solar_gen_forc_ as ( --Solar generation forecast data.\n        select *\n        from YES_ENERGY__SAMPLE_DATA.YES_ENERGY_SAMPLE.ts_forecast_sample\n        where datatypeid = 662\n            and objectid = 10000712973\n),\n    solar_gen_ as ( --Real time solar generation data.\n        select trunc(dateadd('second', 3599, datetime), 'hour') datetime --Converts the hourly rollup of 5 min data to hour ending (the 5 min are already interval ending themselves; just needs to be done for Snowflake).\n              ,avg(value) solar_gen --Raw real time solar generation data is reported at the 5 min granularity.\n        from YES_ENERGY__SAMPLE_DATA.YES_ENERGY_SAMPLE.ts_gen_sample\n        where datatypeid = 650\n            and objectid = 10000712973\n        group by all\n),\n    combined_wide_data_ as ( --Combining the datatypes from above and grabbing only a subset of the columns. You can grab / \"preserve\" more columns such as the datatypeids from above by updating the code.\n        select mt.iso --Datetime attributes.\n              ,mt.datetime\n              ,mt.timezone              \n              ,mt.datetime_utc\n              ,mt.onpeak\n              ,mt.offpeak\n              ,mt.wepeak\n              ,mt.wdpeak\n              ,mt.marketday\n              ,ob1.objectname price_node_name --Object for the price data.\n              ,pr.objectid price_node_id\n              ,pr.dalmp\n              ,pr.rtlmp\n              ,ob2.objectname load_zone_name --Object for the ISO-wide load and generation data. You can also associate this object explicitly with each of the subsequent datatypes by updating the code.\n              ,lf.objectid load_zone_id\n              ,lf.value load_forecast\n              ,lf.publishdate load_forecast_publish_date            \n              ,ld.rtload\n              ,wf.value wind_gen_forecast\n              ,wf.publishdate wind_gen_forecast_publish_date\n              ,wg.wind_gen\n              ,sf.value solar_gen_forecast\n              ,sf.publishdate solar_gen_forecast_publish_date\n              ,sg.solar_gen\n              ,load_forecast - wind_gen_forecast - solar_gen_forecast net_load_forecast --Easily calculate additional columns / features such as net load (load minus renewable generation).\n              ,ld.rtload - wg.wind_gen - sg.solar_gen net_load_real_time --Second calculated feature for real time net load.\n        from datetimes_ mt\n        inner join prices_ pr\n            on (mt.datetime = pr.datetime)\n        inner join load_forc_ lf\n            on (mt.datetime = lf.datetime)\n        inner join load_ ld\n            on (mt.datetime = ld.datetime)    \n        inner join wind_gen_forc_ wf\n            on (mt.datetime = wf.datetime)\n        inner join wind_gen_ wg\n            on (mt.datetime = wg.datetime)    \n        inner join solar_gen_forc_ sf\n            on (mt.datetime = sf.datetime)\n        inner join solar_gen_ sg\n            on (mt.datetime = sg.datetime)\n        inner join YES_ENERGY__SAMPLE_DATA.YES_ENERGY_SAMPLE.ds_object_list_sample ob1\n            on (pr.objectid = ob1.objectid)\n        inner join YES_ENERGY__SAMPLE_DATA.YES_ENERGY_SAMPLE.ds_object_list_sample ob2\n            on (lf.objectid = ob2.objectid)             \n)\nselect *\nfrom combined_wide_data_\norder by datetime asc;",
        "temporal": "Yes",
        "external_knowledge": "ERCOT_Daily_Market_Dynamics_Report.md",
        "plan": "1. **Standardize Date and Time Data:**\n   - Extract the standardized datetime data for a specific region and date to serve as a reference point for all subsequent data integrations.\n\n2. **Extract Pricing Data:**\n   - Retrieve pricing information for the specified region and date, filtering by the unique identifier for the price node.\n\n3. **Load Forecast Data:**\n   - Obtain load forecast data for the region, filtering by the relevant data type and unique identifier.\n\n4. **Real-Time Load Data:**\n   - Fetch real-time load data for the region, using the unique identifier for the load zone.\n\n5. **Wind Generation Forecast Data:**\n   - Gather wind generation forecast data for the region, filtering by the appropriate data type and unique identifier.\n\n6. **Real-Time Wind Generation Data:**\n   - Retrieve real-time wind generation data, converting the data from a 5-minute interval to an hourly interval.\n\n7. **Solar Generation Forecast Data:**\n   - Obtain solar generation forecast data for the region, filtering by the relevant data type and unique identifier.\n\n8. **Real-Time Solar Generation Data:**\n   - Fetch real-time solar generation data, converting the data from a 5-minute interval to an hourly interval.\n\n9. **Combine Data:**\n   - Integrate all the previously extracted datasets into a single, comprehensive dataset.\n   - Include attributes such as datetime, pricing, load forecasts, real-time load, and renewable generation data (both forecasted and real-time).\n   - Perform calculations to derive additional insights like net load forecast and real-time net load by subtracting the renewable generation from the load data.\n\n10. **Finalize and Sort Data:**\n    - Select the combined data and order it by datetime in ascending order to prepare the final report.",
        "special_function": null
    },
    {
        "instance_id": "sf011",
        "db": "CENSUS_GALAXY__ZIP_CODE_TO_BLOCK_GROUP_SAMPLE.PUBLIC",
        "question": "Determine the population distribution within each block group relative to its census tract in New York State using 2021 ACS data. Include block group ID, census value, state county tract ID, total tract population, and the population ratio of each block group.",
        "SQL": "WITH TractPop AS (\n    SELECT\n        CG.\"BlockGroupID\",\n        FCV.\"CensusValue\",\n        CG.\"StateCountyTractID\",\n        CG.\"BlockGroupPolygon\"\n    FROM\n        CENSUS_GALAXY__ZIP_CODE_TO_BLOCK_GROUP_SAMPLE.PUBLIC.\"Dim_CensusGeography\" CG\n    JOIN\n        CENSUS_GALAXY__ZIP_CODE_TO_BLOCK_GROUP_SAMPLE.PUBLIC.\"Fact_CensusValues_ACS2021\" FCV\n        ON CG.\"BlockGroupID\" = FCV.\"BlockGroupID\"\n    WHERE\n        CG.\"StateAbbrev\" = 'NY'\n        AND FCV.\"MetricID\" = 'B01003_001E'\n),\n\nTractGroup AS (\n    SELECT\n        CG.\"StateCountyTractID\",\n        SUM(FCV.\"CensusValue\") AS \"TotalTractPop\"\n    FROM\n        CENSUS_GALAXY__ZIP_CODE_TO_BLOCK_GROUP_SAMPLE.PUBLIC.\"Dim_CensusGeography\" CG\n    JOIN\n        CENSUS_GALAXY__ZIP_CODE_TO_BLOCK_GROUP_SAMPLE.PUBLIC.\"Fact_CensusValues_ACS2021\" FCV\n        ON CG.\"BlockGroupID\" = FCV.\"BlockGroupID\"\n    WHERE\n        CG.\"StateAbbrev\" = 'NY'\n        AND FCV.\"MetricID\" = 'B01003_001E'\n    GROUP BY\n        CG.\"StateCountyTractID\"\n)\n\nSELECT\n    TP.\"BlockGroupID\",\n    TP.\"CensusValue\",\n    TP.\"StateCountyTractID\",\n    TG.\"TotalTractPop\",\n    CASE WHEN TG.\"TotalTractPop\" <> 0 THEN TP.\"CensusValue\" / TG.\"TotalTractPop\" ELSE 0 END AS \"BlockGroupRatio\"\nFROM\n    TractPop TP\nJOIN\n    TractGroup TG\n    ON TP.\"StateCountyTractID\" = TG.\"StateCountyTractID\";",
        "external_knowledge": null,
        "plan": "Determine the population distribution within each block group relative to its census tract in New York State using 2021 ACS data. Include block group ID, census value, state county tract ID, total tract population, and the population ratio of each block group.",
        "special_function": null
    },
    {
        "instance_id": "sf014",
        "db": "CENSUS_GALAXY__AIML_MODEL_DATA_ENRICHMENT_SAMPLE.PUBLIC",
        "question": "What is the New York State ZIP code with the highest number of commuters traveling over one hour, according to 2021 ACS data? Include the zip code, the total commuters, state benchmark for this duration, and state population.",
        "SQL": "WITH Commuters AS (\n    SELECT\n        GE.\"ZipCode\",\n        SUM(CASE WHEN M.\"MetricID\" = 'B08303_013E' THEN F.\"CensusValueByZip\" ELSE 0 END +\n            CASE WHEN M.\"MetricID\" = 'B08303_012E' THEN F.\"CensusValueByZip\" ELSE 0 END) AS \"Num_Commuters_1Hr_Travel_Time\"\n    FROM\n        CENSUS_GALAXY__AIML_MODEL_DATA_ENRICHMENT_SAMPLE.PUBLIC.\"LU_GeographyExpanded\" GE\n    JOIN\n        CENSUS_GALAXY__AIML_MODEL_DATA_ENRICHMENT_SAMPLE.PUBLIC.\"Fact_CensusValues_ACS2021_ByZip\" F\n        ON GE.\"ZipCode\" = F.\"ZipCode\"\n    JOIN\n        CENSUS_GALAXY__AIML_MODEL_DATA_ENRICHMENT_SAMPLE.PUBLIC.\"Dim_CensusMetrics\" M\n        ON F.\"MetricID\" = M.\"MetricID\"\n    WHERE\n        GE.\"PreferredStateAbbrev\" = 'NY'\n        AND (M.\"MetricID\" = 'B08303_013E' OR M.\"MetricID\" = 'B08303_012E') -- Metric IDs for commuters with 1+ hour travel time\n    GROUP BY\n        GE.\"ZipCode\"\n),\n\nStateBenchmark AS (\n    SELECT\n        SB.\"StateAbbrev\",\n        SUM(SB.\"StateBenchmarkValue\") AS \"StateBenchmark_Over1HrTravelTime\",\n        SB.\"TotalStatePopulation\"\n    FROM\n        CENSUS_GALAXY__AIML_MODEL_DATA_ENRICHMENT_SAMPLE.PUBLIC.\"Fact_StateBenchmark_ACS2021\" SB\n    WHERE\n        SB.\"MetricID\" IN ('B08303_013E', 'B08303_012E')\n        AND SB.\"StateAbbrev\" = 'NY'\n    GROUP BY\n        SB.\"StateAbbrev\", SB.\"TotalStatePopulation\"\n)\n\nSELECT\n    C.\"ZipCode\",\n    SUM(C.\"Num_Commuters_1Hr_Travel_Time\") AS \"Total_Commuters_1Hr_Travel_Time\",\n    SB.\"StateBenchmark_Over1HrTravelTime\",\n    SB.\"TotalStatePopulation\",\nFROM\n    Commuters C\nCROSS JOIN\n    StateBenchmark SB\nGROUP BY\n    C.\"ZipCode\", SB.\"StateBenchmark_Over1HrTravelTime\", SB.\"TotalStatePopulation\"\nORDER BY\n    \"Total_Commuters_1Hr_Travel_Time\" DESC\nLIMIT 1;",
        "external_knowledge": null,
        "plan": "What is the New York State ZIP code with the highest number of commuters traveling over one hour, according to 2021 ACS data? Include the total commuters, state benchmark for this duration, and state population.",
        "special_function": null
    }
]